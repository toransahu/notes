{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"Unclassified/","text":"Public Key # Private Key # x509 # ASN.1 # DER # Some data. PEM # Header + Base64 of Some Data + Footer","title":"Public Key"},{"location":"Unclassified/#public-key","text":"","title":"Public Key"},{"location":"Unclassified/#private-key","text":"","title":"Private Key"},{"location":"Unclassified/#x509","text":"","title":"x509"},{"location":"Unclassified/#asn1","text":"","title":"ASN.1"},{"location":"Unclassified/#der","text":"Some data.","title":"DER"},{"location":"Unclassified/#pem","text":"Header + Base64 of Some Data + Footer","title":"PEM"},{"location":"Computer-Science/algo/","text":"Algorithms # Table of Contents Algorithms Introduction What is Algorithm? Why Analysis of Algorithms? Goal of Analysis of Algorithms? What is Running Time Analysis? What is Running Time? How to compare Algorithms? What is Rate of Growth? Commonly used Rate of Growths What is Asymptotic-ness? What is Asymptotic Notation? Big-O 'O' Notation Omega '\u03a9' Notation Theta '\u0398' Notation Type of Analysis Worst Case Best Case Average Case Amortized Analysis How is it different than average-case analyse? Techniques Recurrence Relation Ways to solve recurrence relation Master Theorem Sorting Algorithms Types 1. Bubble Sort: 2. Selection Sort: 3. Insertion Sort: 4. Merge Sort 5. Quick Sort Recursive(arr, left, right): 5.1 Quick Sort Iterative: 6. Heap Sort 7. Bucket Sort 8. Counting Sort 9. Radix Sort. 10. Tim sort Quick Sort Vs Merge Sort Searching Algorithms Types 1. Linear 2. Binary 3. Jump 4. Interpolation 5. Exponential 6. Ternary Linkedlist Algorithms Remove duplicates Reverse the Linkedlist Cycle / Loop Detection Algorithms Floyd's Cycle Detection Algorithm Axiom A Lemma A Proof Corollary / Inference A.1 Lemma B Proof Applications Pseudo Code Analysis Tree Algorithms Depth First Search (DFS) Breadth First Search (BFS) / Level order traversal Heap Algorithms Heap implemented using (Complete) Binary Tree heapify() Pseudo Code Analysis Asymptotically Amortized percolateDown() Pseudo Code Analysis Recurrence relation Graph Algorithms Depth First Search (DFS) Idea Applications Implementation - Standard (Recursive) Properties Pseudo Code Implementation Using Stack (Iterative, Which is same as Recursive one) Pseudo Code DFS Tree Breadth First Search (BFS) / Level order traversal Idea Applications Properties Implementation Using Queue (Iterative) Pseudo Code Topological Sort Idea Applications Properties Implementation Using Stack / DFS Pseudo Code Implementation Using Queue / In-degree Pseudo Code Detect Cycle in Graph Single-Source Shortest Path in Unweighted Graph Idea Applications Properties Implementation Using Queue / BFS Pseudo Code Single-Source Shortest Path in Weighted Directed Acyclic Graph (DAG) Idea Applications Properties Implementation Using Stack / DFS (Topological Sort) Pseudo Code Optimization Dijkstra's Algorithm [Single-Source Shortest Path in Weighted (non-negative) Graph] Idea Applications Properties Pseudo Code Implementation Using Adjacency List, Priority Queue, BFS Optimization Extra Bellman-Ford Algorithm [Single-Source Shortest Path in Weighted (negative) Graph] Idea Applications Properties Pseudo Code Implementation Using Dynamic Programing Optimization Extra Floyd-Warshall Algorithm [All-Pair Shortest-Path in Weighted (negative) Graph] Idea Applications Properties Pseudo Code Pseudo Code - detect negative weight cycle Pseudo Code - reconstruct shortest-path Implementation using Adjacency Matrix Optimization Extra Kruskal Minimum Cost Spanning Tree Algorithm Idea Applications Properties Implementation Using Stack / DFS Pseudo Code Prim's Minumum Cost Spanning Tree Knuth-Morris-Pratt Algorithm Bipartite Matching Iterative Deepening Depth First Search (Depth Limited Search) A* Search Ternary Search Meet in the middle Strongly Connected Components (SCC) Edmonds-Karp Algorithm Hungarian Algorithm Sweep Line Algorithm Graham scan Tarjan's Algorithm Z algorithm Hill Climbing Number Theory Modular Arithmetic Fermat\u2019s Theorem Chinese Remainder Theorem(CRT) Euclidian Method for GCD Logarithmic Exponentiation Sieve of Eratosthenes Euler\u2019s Totient Function Geometric Algorithms 2D Rotation and Scale Matrices 2D Rotation and Translation Matrices 2D Changing Coordinate Systems 3D Rotation and Scale Matrices 3D Changing Coordinate Systems Greedy Algorithms Elementary cases : Fractional Knapsack Problem, Task Scheduling Data Compression using Huffman Trees Activity Selection Dynamic Programing Intro Memoization Tabulation Examples Edit Distance Knapsack problem 0/1 Knapsack Problem 0/1 Knapsack Problem (with repetition of items) Knapsack Problem (with fractional items) Matrix Chain Multiplication Longest Common Substring Longest Common Subsequence Longest Increasing Monotonical Subsequence Rod Cutting Misc Recursion Huffman Coding Regex Algorithm (Pattern Matching and Parsing) Hashing- Hash Functions Monotone Chains Algorithm Coordinate Compression Ford-Fulkerson Method Preflow-Push Algorithm Dinic's Algorithm Monte Carlo method or Metropolis Algorithm Krylov Subspace Iteration Method Householder Matrix Decomposition QR Algorithm Fast Fourier Transform Integer Relation Detection Algorithm Fast Multipole algorithm MinMax Algorithm Divide and Conquer Algorithm Nomenclature References Introduction # What is Algorithm? # Step by step instructions to solve a problem. Why Analysis of Algorithms? # To determine efficient amongst multiple solution in terms of Time & Space consumed. Goal of Analysis of Algorithms? # To compare solutions/algorithms mainly in terms of running time , but also in other factors like memory , developers effort, readability, simplicity etc. What is Running Time Analysis? # Determining how processing time increases with the size of the input (or problem). What is Running Time? # Execution time taken by a program in a particular machine? NO . This metric should be independent of other factors like: programing language, execution enviroment i.e. computer, CPU, RAM etc. So, we express the Running Time as a mathematical function of the input size i.e. f(n) f(n) . How to compare Algorithms? # Using Running Time. What is Rate of Growth? # The rate at which the Running Time increases with the size of the input. aka. the rate at which the value of f(n) f(n) increases with the n n . Say, g(n) g(n) such that f(n) \\propto g(n) f(n) \\propto g(n) . Such, g(n) g(n) are known as asymptotic in nature to f(n) f(n) . Commonly used Rate of Growths # 1 < log{log n} < \\sqrt{log n} < log^2n < 2^{log n} < n < log(n!) < n log n < n^2 < 2^n < 4^n < n! < 2^{2^n} 1 < log{log n} < \\sqrt{log n} < log^2n < 2^{log n} < n < log(n!) < n log n < n^2 < 2^n < 4^n < n! < 2^{2^n} What is Asymptotic-ness? # For a given function f(n) f(n) if, another function g(n) g(n) tries to approximate f(n) f(n) ; then g(n) g(n) is called asymptotic curve for f(n) f(n) . What is Asymptotic Notation? # Syntax/Symbol/Expression to represent the different Asymptotic nature of a function. Lets understand this by taking an example of a function f(n) f(n) (which may represent Running Time of an algorithm/solution to a problem) in terms of size of input ( n n ). Big-O 'O' Notation # This notation gives smallest rate of growth g(n) g(n) which is greater than or equal to the Running Time f(n) f(n) of the given algorithm/solution. i.e. there exists some positive constants n_0 n_0 & c c : such that 0 \\le f(n) \\le cg(n) 0 \\le f(n) \\le cg(n) ; where n \\ge n_0 n \\ge n_0 This is also called asymptotically tight upper bound of the given function. f(n) = O(g(n)) f(n) = O(g(n)) In this representation/expression f(n) f(n) or O(g(n)) O(g(n)) represents Running Time of an algorithm, n n represents the size of the input, g(n) g(n) represents the Rate of Growth of the Running Time of the algorithm, and O O represents the nature of the asymptoticness of curve g(n) g(n) with curve f(n) f(n) . Omega '\u03a9' Notation # This notation gives largest rate of growth g(n) g(n) which is less than or equal to the Running Time f(n) f(n) of the given algorithm/solution. i.e. there exists some positive constants n_0 n_0 & c c : such that 0 \\le cg(n) \\le f(n) 0 \\le cg(n) \\le f(n) ; where n \\ge n_0 n \\ge n_0 This is also called asymptotically tight lower bound of the given function. f(n) = \\Omega(g(n)) f(n) = \\Omega(g(n)) Theta '\u0398' Notation # This notation gives a rate of growth g(n) g(n) such that it falls in between the tight lower & upper bound of theRunning Time of the given algorithm. i.e. there exists some positive constants c_1 c_1 , c_2 c_2 , and n_0 n_0 : such that 0 \\le c_1g(n) \\le f(n) \\le c_2g(n) 0 \\le c_1g(n) \\le f(n) \\le c_2g(n) ; where n \\ge n_0 n \\ge n_0 This is also called asymptotically tight bound of the given function. f(n) = \\Theta(g(n)) f(n) = \\Theta(g(n)) Type of Analysis # Types of analyzing an algorithm. Worst Case # This considers the size of input for which the algorithm may take longest time (or algorithm runs slowest). Thus it defines the Rate of Growth (the influencing factor in the Running Time) for such input. We can use any of the asymptotic notation to represent the worst-case Rate of Growth i.e. either O O or \\Theta \\Theta or \\Omega \\Omega . There is no hard rule to use O O to represent the worst-case. Asymptotic notations just tells about the asymptotic nature of the function/curve. It has nothing in specific with the worst/avg/best case analysis. However, there could be a suitable notation of choice for a type of analysis. Let's take an example of an algorithm whose Running Time seems to be f(n) = n^2 + 2n + 1 f(n) = n^2 + 2n + 1 . We can represent the worst-case running time by \\Theta(n^2) \\Theta(n^2) as for some positive constant c_1, c_2, n_0 c_1, c_2, n_0 , where n \\ge n_0 n \\ge n_0 for which the following holds true: 0 \\le c_1g(n) \\le f(n) \\le c_2g(n) 0 \\le c_1g(n) \\le f(n) \\le c_2g(n) i.e. 0 \\le c_1*n^2 \\le n^2 + 2n + 1 \\le c_2*n^2 0 \\le c_1*n^2 \\le n^2 + 2n + 1 \\le c_2*n^2 Suppose, n = 1 n = 1 & c_1=4 c_1=4 , c_2=4 c_2=4 ; then 0 \\le 4 \\le 4 \\le 4 0 \\le 4 \\le 4 \\le 4 . Suppose, n = 1 n = 1 & c_1=3 c_1=3 , c_2=5 c_2=5 ; then 0 \\le 3 \\le 4 \\le 5 0 \\le 3 \\le 4 \\le 5 . Also, to be on a safer side we can use O(n^2) O(n^2) to express worst-case running time, like: For some positive constant c, n_0 c, n_0 , where n \\ge n_0 n \\ge n_0 for which the following holds true: 0 \\le f(n) \\le cg(n) 0 \\le f(n) \\le cg(n) i.e. 0 \\le n^2 + 2n + 1 \\le c*n^2 0 \\le n^2 + 2n + 1 \\le c*n^2 Suppose, n = 1 n = 1 & c=4 c=4 ; then 0 \\le 4 \\le 4 0 \\le 4 \\le 4 . Suppose, n = 1 n = 1 & c=5 c=5 ; then 0 \\le 4 \\le 5 0 \\le 4 \\le 5 . So, we can say \\Theta \\Theta is a strong notion than O O . And also can say: if f(n) = \\Theta(g(n)) f(n) = \\Theta(g(n)) , then f(n) = O(g(n)) f(n) = O(g(n)) \\Theta(g(n)) \\subseteq O(g(n)) \\Theta(g(n)) \\subseteq O(g(n)) However, using \\Omega \\Omega to denote the worst-case is acceptable but doesn't makes sense. We can say something like: the longest time this algorithm can take will always be more than or equal to 5 minutes. Non sense. Not much of use. So, the suitable asymptotic notations for this case could be O O & \\Theta \\Theta . Best Case # This considers the size of input for which the algorithm may take lowest time (or algorithm runs fastest). Thus it defines the Rate of Growth (the influencing factor in the Running Time) for such input. Suitable asymptotic notations for this case could be \\Omega \\Omega & \\Theta \\Theta . Average Case # This considers a random size of input tries to define the Rate of Growth (the influencing factor in the Running Time) for such input. Suitable asymptotic notations for this case could be \\Theta \\Theta & O O . Amortized Analysis # Instead of analysis a data-structure operation and defining the time complexity/cost soley based on that single one, analyse a sequence of data-structure operations and average the cost of all those operations. So that it could be shown that the average cost of an operation is smaller, even though the cost of a single operation within the sequence is expensive. How is it different than average-case analyse? # Average case analysis is based on the input to the algorithm, as it assumes that the input provided in this case is a random sized. Thus it relies on a probablistic assumption about the input. While the amortized analysis does not rely on the input. It applies for all the inputs / any size of input. Amortized analysis guarantees on the worst-case cost of N N operations. Techniques # Aggregate Method Accounting Method Potential Method Recurrence Relation # Ways to solve recurrence relation # Substitution Method (aka Back substitution / Induction) Recursion Tree Master Theorem Let's understand deriving a recurrence relation and solving it using some examples. Example 1: Calculate the time complexity of finding an item in array using Binary Search ? Example 2: Calculate the time complexity of finding n^{th} n^{th} item of the Fibonacci Series ? A recurrsive solution says that: F(n) = \\left \\{ \\begin{array}{lcl} 0 & if & n=0 \\\\ 1 & if & n=1 \\\\ F(n-1) + F(n-2) & if & n>1 \\\\ \\end{array} \\right. F(n) = \\left \\{ \\begin{array}{lcl} 0 & if & n=0 \\\\ 1 & if & n=1 \\\\ F(n-1) + F(n-2) & if & n>1 \\\\ \\end{array} \\right. So, the recurrence relation should be: T(n) = \\left \\{ \\begin{array}{lcl} 1 & if & n \\le 1 \\\\ T(n-1) + T(n-2) & if & n>1 \\\\ \\end{array} \\right. T(n) = \\left \\{ \\begin{array}{lcl} 1 & if & n \\le 1 \\\\ T(n-1) + T(n-2) & if & n>1 \\\\ \\end{array} \\right. Solution using substitution method: \\begin{array}{r, c, l,l} T(n) & = & T(n-1) + T(n - 2) & \\text{(this will become harder to solve)} \\\\ T(n) & \\le & T(n-1) + T(n-1) & \\text{(so, approximate} \\space T(n-2) \\approx T(n-1) \\space \\text{for upper bound)} \\\\ \\therefore T(n) & \\le & 2 \\times T(n-1) & \\text{ eq. 1}\\\\ \\end{array} \\begin{array}{r, c, l,l} T(n) & = & T(n-1) + T(n - 2) & \\text{(this will become harder to solve)} \\\\ T(n) & \\le & T(n-1) + T(n-1) & \\text{(so, approximate} \\space T(n-2) \\approx T(n-1) \\space \\text{for upper bound)} \\\\ \\therefore T(n) & \\le & 2 \\times T(n-1) & \\text{ eq. 1}\\\\ \\end{array} Let's derive a few other terms using the eq. 1 \\begin{array}{r, c, l,l} T(n-1) & \\le & 2 \\times T(n-2) \\\\ T(n-2) & \\le & 2 \\times T(n-3) \\\\ ... \\\\ T(1) & = & 1 \\\\ T(0) & = & 1 \\\\ \\end{array} \\begin{array}{r, c, l,l} T(n-1) & \\le & 2 \\times T(n-2) \\\\ T(n-2) & \\le & 2 \\times T(n-3) \\\\ ... \\\\ T(1) & = & 1 \\\\ T(0) & = & 1 \\\\ \\end{array} Now do the substitution \\begin{array}{r, c, l,l} T(n) & \\le & 2 \\times T(n-1) \\\\ T(n) & \\le & 2 \\times [2 \\times T(n-2)] \\\\ T(n) & \\le & 2^3 \\times T(n-3) \\\\ T(n) & \\le & 2^4 \\times T(n-4) \\\\ ... \\\\ T(n) & \\le & 2^k \\times T(n-k) \\\\ \\end{array} \\begin{array}{r, c, l,l} T(n) & \\le & 2 \\times T(n-1) \\\\ T(n) & \\le & 2 \\times [2 \\times T(n-2)] \\\\ T(n) & \\le & 2^3 \\times T(n-3) \\\\ T(n) & \\le & 2^4 \\times T(n-4) \\\\ ... \\\\ T(n) & \\le & 2^k \\times T(n-k) \\\\ \\end{array} As we know \\begin{array}{r,c,l} T(0) & = & 1 \\\\ \\therefore T(n-k) & = & 1 \\\\ \\text{and } n-k & = & 0 \\\\ \\therefore k & = & n & \\text{(for } T(0) = 1 \\text{)} \\\\ \\end{array} \\begin{array}{r,c,l} T(0) & = & 1 \\\\ \\therefore T(n-k) & = & 1 \\\\ \\text{and } n-k & = & 0 \\\\ \\therefore k & = & n & \\text{(for } T(0) = 1 \\text{)} \\\\ \\end{array} Thus \\begin{array}{r,c,l} T(n) & \\le & 2^n \\times T(0) & \\text{(for } k=n \\text{)} \\\\ \\therefore T(n) & \\le & 2^n \\\\ \\therefore T(n) & = & O(2^n) & \\text{(this is upper bound of the worst case analysis)} \\\\ \\end{array} \\begin{array}{r,c,l} T(n) & \\le & 2^n \\times T(0) & \\text{(for } k=n \\text{)} \\\\ \\therefore T(n) & \\le & 2^n \\\\ \\therefore T(n) & = & O(2^n) & \\text{(this is upper bound of the worst case analysis)} \\\\ \\end{array} However, the tighter bound is \\Theta(\\phi^n) \\Theta(\\phi^n) where \\phi = \\frac{1+\\sqrt{5}}{2} \\approx 1.62 \\approx 2 \\phi = \\frac{1+\\sqrt{5}}{2} \\approx 1.62 \\approx 2 , called as Golden Ratio . Example 3: Calculate the time complexity of recursive solution of finding the number of ways to reach the top of a stair if allowed steps are 1, 2, and 3 ? Master Theorem # If the recurrence relation is of the form: T(n) = aT(n/b) + \\Theta(n^klog^pn) T(n) = aT(n/b) + \\Theta(n^klog^pn) TODO Sorting Algorithms # Types # 1. Bubble Sort: # Description: Swapping adjacent elements if there are in wrong orders. Time Complexity: Always O(n^2) O(n^2) (even for sorted arrays) Can be optimized for sorted array 2. Selection Sort: # Description: Pulls a minimum element from unsorted-subarray and appends infront of the array (as a sorted sub-array) Time Complexity: O(n^2) O(n^2) (all cases) Auxilary Space: O(1) O(1) 3. Insertion Sort: # Description: Keep first element on left side, start one by one from 2^{nd} 2^{nd} element (lets say ith element). Compare the picked element ( i^{th} i^{th} ) with all the elements in left sub-array (i.e. with i-1^{th} i-1^{th} to 0^{th} 0^{th} ). If LHS sub-array element is greater than i element, then shift that element right by one index. Repeat for all LHS elements. Fill the empty cell with picked element. Time Complexity: Best Case: O(n) O(n) ; if array is as small as n=1 or array is already sorted Avg Case: \\Theta(n^2) \\Theta(n^2) Worst Case: O(n^2) O(n^2) Auxilary Space: O(1) O(1) 4. Merge Sort # Description: Split the array in Binary fashion, till you get minimum/single entity Start merging 2 single entities --> you'll get 2 sorted sub-array --> merge both Continue merging till end Merge Function: Merges two sorted sub-arrays by using an extra space of O(n). Begin from 0th index of both sub-array (using pointers like i,j), do comparision in temporary (un-touched) array Make changes in original array till both i i & j j reaches the end of the sub-array Time Complexity: Merge: O(n) O(n) Merge Sort: Best Case: O(nlogn) O(nlogn) Avg Case: O(nlogn) O(nlogn) Worst Case: O(nlogn) O(nlogn) Auxiliary Space: Merge: O(n) O(n) Merge Sort: O(n) + O(1) O(n) + O(1) Algorithmic Paradigm: Divide and Conquer Implementations: Recursive only Sorting In Place: Yes (No in a typical implementation) Applications: Sorting linkedlist in O(nlogn) O(nlogn) Inversion Count Problem (i.e. in an array E_i>E_j>E_k E_i>E_j>E_k where i<j<k i<j<k ) In External Sorting 5. Quick Sort Recursive(arr, left, right): # Description: Partition the array about a pivot: re-arrange smaller elements in LHS & greater elements in RHS of pivot Return partitioned index (i.e. index of pivot after re-arrangement) Re-call quicksort for sub-array smaller than pivot Re-call quicksort for sub-array greater than pivot Partition Function(arr, left, right): Desc: Pick pivot is any preferable fashion: First element Last element Mean Randon Find index of pivot element in array by counting number of elements smaller than it + re-arranges elements smaller than pivot to left (and greater to right) in original Array. Swap pivot element with element at that index return the pivot/partioning index Returns: Partitioning Index (Re-calculated index of pivot) Time Complexity: O(n) O(n) Auxilary Space: O(1) O(1) Time Complexity: Best Case: O(nlogn) O(nlogn) (Occurs when pivot element is middle element value-wise) Avg Case: O(nlogn) O(nlogn) Worst Case: O(n^2) O(n^2) (Occurs when pivot element is either smaller or larger element) Auxilary Space: O(1) O(1) Algorithm Paradigm: Divide and Conquer Implementation: Recursive (Generally) and Iterative In-Place: Yes (because auxilary space O(n) O(n) ) Recurrence Relation: Best Case: T(n) = O(n) + 2*T(n/2) == O(nlogn) T(n) = O(n) + 2*T(n/2) == O(nlogn) Avg Case: T(n) = O(n) + 2*T(n/2) == O(nlogn) T(n) = O(n) + 2*T(n/2) == O(nlogn) Worst Case: T(n) = O(n) + T(n-1) == O(n^2) T(n) = O(n) + T(n-1) == O(n^2) Applications: In case of memory limitation this algo is used Advantages: One of the fastest algo for avg case Does not need additional memory, i.e. In-place processing/sorting Disadvantages: Worst case complexity O(n^2) O(n^2) speed is not guaranteed 5.1 Quick Sort Iterative: # Description: Use tha same Partition technique Instead of re-calling the quick_sort function, use stack to keep track of left and right indexes of the sub-arrays (LHS & RHS of partition_idx) found after partition. Keep doing this while stack is not empty. At the end partition() will arrange all the elements at their perfect index. 6. Heap Sort # Desc: Create a max-heap: Using HEAPIFY (aka BUILD_<MIN/MAX>_HEAP . To do so, start from last leaf and make max heap till root node. EXTRACT (Get and Delete) the Top/Root nodes from the heap one by one to get sorted items DELETE the top/root node Fill the empty root position by Swapping the last leaf with root node in max heap PERCOLATE_DOWN the (newly added) root node (by ignoring nodes added in step #3) till the loop covers all the nodes (no need to do anything with the last node in the heap) While doing step #2, why not add those items/nodes back again to the same array (start at the end, in right to left direction) so that we can avoid an auxilary space and utilize the same array (to store items in sorted order) Note: while doing step #2, make sure we're not considering the nodes added during step #3 Pre-requisites: Useful: When there is time (Quick's problem) and space (Merge's problem) bound Advantage: Worst case upper bound is O(nlogn) with only O(1) auxilary space Applications: Sort a nearly sorted (or K sorted) array k largest(or smallest) elements in an array Time Complexity: O(heapify) * O(n) = O(nlogn) O(heapify) * O(n) = O(nlogn) Auxilary Space: O(1) O(1) In-Place: Yes Implementation: Recursive (Heapify) Algorithm Paradigm: Comparision Based Data Structure: Array + Complete Binary Tree Stable: Not in general Note: Quick & Merge are better in practice. 7. Bucket Sort # Pre-requisites: Standard: A uniform distributed input array in a range of [0,1) [0,1) . CLRS. Generalized: A uniform distributed input array in a range of non negative integers + floats. [0, k) [0, k) . Efficient Hash Function (specially in case of \"Generalized\" implementation. Desc: Hashing: hash_table_size or number of buckets: = size of input array; Standard OR = int(sqrt(size)) int(sqrt(size)) ; Generalized hash_func() = (element/MAX)* (hash_table_size) Condition: if i < k i < k then hash(i) < hash(k) hash(i) < hash(k) Partion inp array on the basis of hash function, store then in right bucket/array Sort each array using Insertionsort Merge all sorted arrays into one. Useful: When input is uniformly distributed over a positive range Advantage: Sorting in O(n) O(n) Applications: When input is uniformly distributed over a positive range Recurrence Equation: \\Theta(n) + nO(2 - 1/n) \\Theta(n) + nO(2 - 1/n) Time Complexity: Best: \\Omega(n) \\Omega(n) Avg: \\Theta(n) \\Theta(n) Worst: \\Theta(n^2) \\Theta(n^2) ; When all the elements fall under single bucket Auxilary Space: O(bucket size) == O(n) O(bucket size) == O(n) In-Place: No Implementation: Iterative Algorithm Paradigm: Hashing, Partion Data Structure: Hashtable, Array Stable: Yes Note: If input is not uniformally distributed, but also bucketsort may still run in linear time. CLRS. Why Insertion sort is used here? How overall time complexity of bucketsort is still O(n) then? Following Standard way: As, input is uniformly distributed, On avg each bucket/array will have 1 elements (k/k=1) (k/k=1) , some may have zero and some may have multiple with same value. And as insertionsort's best case is O(n) O(n) (if array size is 1). Hence overall its O(n) O(n) 8. Counting Sort # Pre-requisites (Standard): Elements should be Non Negative Integers Over a range of 0 to k where k < size of array to maintain O(n) O(n) Desc: For each element X X in the input array find the number of elements smaller than X X . Steps: Store counts of each element in a counting array Add previous count to current count, to find index of last occurence of that element Reverse Iterate over input array & pick index of the element from counting array Put the element in output array and decrement the count by 1 Useful: same as pre-requisites Advantage: Sorting in O(n + k) O(n + k) Applications: Same as pre-requisites As a subroutine in Radix Sort Time Complexity: Best: \\Omega(n + k) \\Omega(n + k) Avg: \\Theta(n + k) \\Theta(n + k) Worst: O(n + k) O(n + k) Auxilary Space: O(counting + output array) == O(n + k) O(counting + output array) == O(n + k) In-Place: No Implementation: Iterative Algorithm Paradigm: Partial Hashing Data Structure: Hashtable, Array Stable: Yes (order of elements with same value in input array maintains same order in output) Comparion Sort: No Note: Can be extended to sort negative integers also 9. Radix Sort. # Pre-requisites (Standard): input array have non negative integers range should be 0 0 to n^c n^c where c c is some constant & numbers are represented in base n n or each number takes only log_2n log_2n bits Desc: for 1 1 to d d : where d is most significant digit position of MAX element in inp array do counting sort on array (considering current digit of each iteration) Useful: same as prerequisites Advantage: Better worst case performance than bucket sort's Worst case Applications: Card sorting machine Recurrence Equation: n*O(n + k) == O(n + k) n*O(n + k) == O(n + k) Time Complexity: Best Case: \\Omega(n + k) \\Omega(n + k) Avg Case: \\Theta(n + k) \\Theta(n + k) Worst Case: O(n + k) O(n + k) Auxilary Space: d*O(counting array + output array) = d*O(n + k) d*O(counting array + output array) = d*O(n + k) In-Place: No Implementation: Iterative Algorithm Paradigm: Partial Hashing Data Structure: Hashtable, array Stable: Yes Comparion Sort: No Note: 10. Tim sort # Quick Sort Vs Merge Sort # Merge sort is preferred over Quick sort when: Sorting a linkedlist: Quick sort is preferred over Merge sort when: There is memory limitation Searching Algorithms # Types # 1. Linear # 2. Binary # Pre-requisites: Sorted array of numbers (integers/floats) Method : Divide and search (conquer) Equation: T(n) = T(n/2) + O(1) T(n) = T(n/2) + O(1) Time Complexity: O(log n) O(log n) Implementation: Recursive Auxilary Space: O(log n) O(log n) recursion call stack space Iterative Auxilary Space: O(1) O(1) 3. Jump # Pre-requisites: Sorted array of numbers Desc: Enhancement over linear search. Jumps a step (of m m ) --by index-- and search, if it was searching value 52 and found 55 then will jump a step back and start linear search from that index. Similar to linear search. Recurrence Equation: Total number of comparision in worst case: (n/m) + (m-1) (n/m) + (m-1) i.e. (total number of jumps + linear search within one step) Best value of m = \\sqrt{n} m = \\sqrt{n} Time Complexity: O(\\sqrt{n}) O(\\sqrt{n}) i.e. between O(linear) O(linear) & O(binary) O(binary) Auxiliary Space : O(1) O(1) 4. Interpolation # Pre-requisites: Sorted array Desc: Enhancement over Binary search. Enhancement: Pivot is calculated based on interpolation, i.e. Pivot = left + \\frac{(right - left)} { (arr[right] - arr[left]) (e - arr[left])} Pivot = left + \\frac{(right - left)} { (arr[right] - arr[left]) (e - arr[left])} Time Complexity: O(log log n) O(log log n) (if elements are uniformaly distributed) Worst case: O(n) O(n) Auxilary space: O(1) O(1) 5. Exponential # Pre-requisites: Sorted array Desc: Enhancement over binary search. Enhancement: Find a range where element is present, and then do binary search found range. Range: Start with subarray of size 1 and chech if last element of subarray is greater than 'e'. If not, then increment size by two times Equation: Time Complexity: O(log n) O(log n) Auxilary Space: O(1) O(1) (if used iterative binary search) Advantage: Better to use if array is infinite (Unbounded Searches/ Unbounded Binary Search) 6. Ternary # Pre-requisites: Sorted array Desc: Divide and Conquer + Linear search Same as binary search. When searching space gets shorter, use linear search according to precision = right-left Time Complexity: O(log_3 n) O(log_3 n) Auxilary Space: Iterative = O(1) O(1) Recursive = O(log_3 n) O(log_3 n) recursive stack space Application: Unimodel Functions Linkedlist Algorithms # Remove duplicates # Remove duplicate nodes from an unsorted linkedlist (w/ auxilary space) Remove duplicate nodes from an unsorted linkedlist (w/o auxilary space) Reverse the Linkedlist # Reverse a singly linkedlist Reverse a portion of a singly linkedlist Reverse a doubly linkedlist Reverse a portion of a doubly linkedlist Cycle / Loop Detection Algorithms # Using Hashing/Mapping address of each visited nodes and lookup the map if current node is already mapped there By pointing all the visited nodes to A Dummy Node and check if current node already points to the Dummy node By adding a flag to each visited nodes and lookup the flag off current node if it is already flagged as visited - applicable only if the nodes are modifiable By assigning a special value to each visited nodes and lookup the value of the current node if it is already assigned that special value - applicable only if the range of the node values are given Using Floyd's Cycle Detection Algorithm Floyd's Cycle Detection Algorithm # 1 2 3 4 5 <------------------- S -------------------> <---- X ------- [a]-->[i]-->[j]-->[k]-->[l]-->[m]-------\u2192 [b] ------\u2192 [d] | \u2191 | | | \u2193 | [e] \u2190------ [c] \u2193 A. If a loop is suspected in a linkedlist, then run two pointers - one slow & another fast - starting from the head ( a a ). Run the faster pointer double the speed of the slower one. If they meet at some node ( c c ), then the loop exists & else not. B. If a loop has been detected using #A, then idetifying the start of the loop and removing the loop is also possible. Axiom A # If in a loop we're at some point i i , and if we finish K K complete (whole number) cycles/rounds of the loop, we'll be back to the same point i i . P_i \\cong P_i + K.L P_i \\cong P_i + K.L where: P_i P_i is a pointer at some i^{th} i^{th} node in the loop L L is the length (or number of nodes) of the loop K K is some non-negative integer Lemma A # If pointer P_s P_s and P_f P_f starts from the head ( a a ) of the linkedlist. P_s P_s moves 1 node at a time & P_f P_f moves 2 nodes at a time. Then the hypothesis is they will meet at some node ( c c ). Proof # Let's suppose they met X X unit far from the start node ( b b ) of the loop. Before they met each other, P_s P_s might have completed K_1 K_1 whole rounds of the loop and P_f P_f might have completed K_2 K_2 whole rounds. So, D_s = S + K_1.L + X D_s = S + K_1.L + X - where K_1 K_1 is a whole number D_f = S + K_2.L + X D_f = S + K_2.L + X - where K_2 K_2 is a whole number Also to note that, the speed of the pointer P_f P_f is twice the speed of P_s P_s , and they are moving for the same time interval. So, suppose D_s = D D_s = D , then D_f = 2D D_f = 2D . Then we can derive the following relation: \\frac{ \\begin{array}{l,c,l} +(2D & = & S + K_2.L + X) \\\\ -(D & = & S + K_1.L + X) \\end{array} }{ \\begin{array}{r,c,l} D & = & (K_2 - K_1).L \\\\ \\therefore D & = & K.L \\end{array} } \\frac{ \\begin{array}{l,c,l} +(2D & = & S + K_2.L + X) \\\\ -(D & = & S + K_1.L + X) \\end{array} }{ \\begin{array}{r,c,l} D & = & (K_2 - K_1).L \\\\ \\therefore D & = & K.L \\end{array} } This equation must holds true for some appropriate values of D D & K K . Thus we can also conclude that our hypothesis that, both slow & fast pointer will meet at some point, is true as well. Corollary / Inference A.1 # By Lemma-A, as we know that the speed of the pointer P_f P_f is twice the speed of P_s P_s , and they were moving for the same time interval. So, 2.S_s = S_f 2.S_s = S_f \\therefore 2.\\frac{D_s}{T} = \\frac{D_f}{T} \\therefore 2.\\frac{D_s}{T} = \\frac{D_f}{T} \\therefore 2.D_s = D_f \\therefore 2.D_s = D_f \\therefore 2(S + K_1.L + X) = S + K_2.L + X \\therefore 2(S + K_1.L + X) = S + K_2.L + X \\therefore 2S + 2K_1.L + 2X = S + K_2.L + X \\therefore 2S + 2K_1.L + 2X = S + K_2.L + X \\therefore S + X = K_2.L - 2K_1.L \\therefore S + X = K_2.L - 2K_1.L \\therefore S + X = (K_2 - 2K_1).L \\therefore S + X = (K_2 - 2K_1).L \\therefore S = K.L - X \\therefore S = K.L - X By this we can say that the moving K.L - X K.L - X units from the meeting point c c , within the loop, is exactly equal to S S , for some appropriate value of S, K, X S, K, X . (just remember this relationship) Lemma B # If move a pointer from the head a a of the linkedlist, and move another pointer from the meeting point c c , simultaniously, at the same speed, then the hypothesis is both the pointer will meet at the start node b b of the loop. (Note that both the pointers are moving in opposite direction towards each other, thus they are going to meet each other at some node.) In another word, the hypothesis is, the start node b b of the loop could be reached from the meeting point c c if we move a pointer S S units from the meeting point c c . Proof # We know that both the pointers (slow an fast) met at node c c (meeting point) previously as per Lemma-A. Now, assume that on moving these two new pointers - one starts from head a a & another starts from meeting point c c - meets at the start node ( b b ) of the loop. That means P_s P_s reaches the start node b b by travelling D_s D_s distance from the head. That is S S units. And, P_f P_f reaches the start node b b by travelling D_f D_f distance from the meeting point c c . That is L - X L - X units. But, it is also possible that P_f P_f might have travelled some cycles of the loop before meeting P_s P_s . Lets assume that P_f P_f might have travelled J_1 J_1 cycles of the loop. So, P_f P_f reaches the start node b b by travelling J_1.L + (L - X) J_1.L + (L - X) units in total. i.e. (J_1 + 1).L - X (J_1 + 1).L - X As the speed of both the pointers were same & distance were travelled within the same time interval, we can relate it like: S_s = S_f S_s = S_f \\therefore \\frac{D_s}{T} = \\frac{D_f}{T} \\therefore \\frac{D_s}{T} = \\frac{D_f}{T} \\therefore D_s = D_f \\therefore D_s = D_f \\therefore S = (J_1 + 1).L - X \\therefore S = (J_1 + 1).L - X \\therefore S = J.L - X \\therefore S = J.L - X With the help of Lemma-A & Inference-A.1 we can conclude that this relation S = J.L - X S = J.L - X holds true, thus our hypothesis is true as well. Applications # helpful in discovering infinite loop in a computer program more Pseudo Code # Input := L L : a singly linkedlist Output := bool : Loop detected, L^{'} L^{'} : the same linkedlist with loop removed (if any) if L == \\varnothing L == \\varnothing ; then return ( False, L False, L ) if L.Head.Next == \\varnothing L.Head.Next == \\varnothing ; then return ( False, L False, L ) if L.Head.Next == L.Head L.Head.Next == L.Head ; then L.Head.Next = \\varnothing L.Head.Next = \\varnothing return ( True, L True, L ) P_s = L.Head P_s = L.Head ; P_f = L.Head P_f = L.Head meetingPoint = \\varnothing meetingPoint = \\varnothing while True True if P_s.Next == P_f.Next.Next P_s.Next == P_f.Next.Next ; then meetingPoint = P_f.Next meetingPoint = P_f.Next break if P_f.Next == \\varnothing P_f.Next == \\varnothing || P_f.Next.Next == \\varnothing P_f.Next.Next == \\varnothing ; then return ( False, L False, L ) P_s = P_s.Next P_s = P_s.Next P_f = P_f.Next.Next P_f = P_f.Next.Next if meetingPoint == L.Head meetingPoint == L.Head ; then meetingPoint.Next = \\varnothing meetingPoint.Next = \\varnothing P_f.Next = \\varnothing P_f.Next = \\varnothing return ( True, L True, L ) P_s = L.Head; P_f = meetingPoint P_s = L.Head; P_f = meetingPoint while True True if P_s.Next == P_f.Next P_s.Next == P_f.Next ; then break P_s = P_s.Next P_s = P_s.Next P_f = P_f.Next P_f = P_f.Next P_f.Next = \\varnothing P_f.Next = \\varnothing return ( True, L True, L ) Analysis # Time Complexity: Worst case: O(S + K.L + X) + O(S) = O(N) O(S + K.L + X) + O(S) = O(N) ; when S >> L S >> L Avg case: O(S + K.L + X) + O(S) = O(N) O(S + K.L + X) + O(S) = O(N) ; Best case: O(S + K.L + X) + O(S) = O(1) O(S + K.L + X) + O(S) = O(1) ; when S=0; X=0 L = 1 S=0; X=0 L = 1 Space Complexity: O(1) O(1) Tree Algorithms # Depth First Search (DFS) # Breadth First Search (BFS) / Level order traversal # Heap Algorithms # Heap implemented using (Complete) Binary Tree # Basic Insert Top Extract Top Replace Internal percolate-down percolate-up heapify Creation Build heapify() # Pseudo Code # input = an array arr arr of numbers output = array ordered to follow min-heap properties return = none 1 2 for i in range ( arr . length - 1 to 0 ): percolate_down ( arr , i ) Analysis # Asymptotically # T(n) = n * O(log_2{n}) T(n) = n * O(log_2{n}) T(n) = O(nlog_2{n}) T(n) = O(nlog_2{n}) Amortized # Worst case running time of heapify() heapify() = percolate down all the nodes at last/leaf ( h h ) level to 0 steps/levels (note: level start from top, i.e. 0 and, h h is total height of the heap) + percolate down all the nodes at second last ( h-1 h-1 ) level to 1 steps/levels + percolate down all the nodes at third last ( h-2 h-2 ) level to 2 steps/levels + ... + percolate down all the nodes at h - i h - i level to i i steps/levels + ... + percolate down all the nodes at zeroth ( 0 0 ) level to h h steps/levels i.e. T(n) = \\sum_{i=1}^{h} T(n) = \\sum_{i=1}^{h} percolate down all the nodes at level= h-i h-i to i i steps/levels \\therefore T(n) = \\sum_{i=0}^{h} \\therefore T(n) = \\sum_{i=0}^{h} (number of nodes at level h - i h - i to percolate down) \\times \\times (cost of percolate down a node to i i levels) \\therefore T(n) = \\sum_{i=0}^{h} 2^{h-i} \\times O(i) \\therefore T(n) = \\sum_{i=0}^{h} 2^{h-i} \\times O(i) \\because \\because number of nodes a level x = 2^{x} x = 2^{x} \\therefore T(n) = \\sum_{i=0}^{\\log_2{n}} 2^{h-i} \\times i \\therefore T(n) = \\sum_{i=0}^{\\log_2{n}} 2^{h-i} \\times i \\because \\because height h h of a heap = \\log_2{n} \\log_2{n} \\therefore T(n) = \\sum_{i=0}^{\\log_2{n}} 2^{\\log_2{n}-i} \\times i \\therefore T(n) = \\sum_{i=0}^{\\log_2{n}} 2^{\\log_2{n}-i} \\times i \\therefore T(n) = \\sum_{i=0}^{\\log_2{n}} \\frac{2^{\\log_2{n}}}{2^i} \\times i \\therefore T(n) = \\sum_{i=0}^{\\log_2{n}} \\frac{2^{\\log_2{n}}}{2^i} \\times i \\therefore T(n) = \\sum_{i=0}^{\\log_2{n}} \\frac{n}{2^i} \\times i \\therefore T(n) = \\sum_{i=0}^{\\log_2{n}} \\frac{n}{2^i} \\times i \\therefore T(n) = n \\times \\sum_{i=0}^{\\log_2{n}} \\frac{i}{2^i} \\therefore T(n) = n \\times \\sum_{i=0}^{\\log_2{n}} \\frac{i}{2^i} \\therefore T(n) \\le n \\times 2 \\because \\sum_{i=0}^{\\infty} \\frac{i}{2^i} = 2 \\therefore T(n) \\le n \\times 2 \\because \\sum_{i=0}^{\\infty} \\frac{i}{2^i} = 2 (upper bound) \\therefore T(n) \\le O(n) \\therefore T(n) \\le O(n) percolateDown() # Pseudo Code # input = an array arr arr of numbers, some index i i (index starting at 0 0 ) output = item at index i i in the array gets repositioned to follow min-heap properties return = none if the node at given index i i is a leaf node or non-existing node return leftChildPos := 2*i + 1 leftChildPos := 2*i + 1 rightChildPos := 2*i + 2 rightChildPos := 2*i + 2 minimumNodePos := i minimumNodePos := i if leftChildPos leftChildPos exists and arr[leftChildPos] \\lt arr[i] arr[leftChildPos] \\lt arr[i] : minimumNodePos := leftChildPos minimumNodePos := leftChildPos if rightChildPos rightChildPos exists and arr[rightChildPos] \\lt arr[i] arr[rightChildPos] \\lt arr[i] : minimumNodePos := rightChildPos minimumNodePos := rightChildPos if i i and minimumNodePos minimumNodePos are not same: swap item at index minimumNodePos minimumNodePos and i i percolateDown(arr, minimumNodePos) percolateDown(arr, minimumNodePos) Analysis # At worst, a node might have to reposition/move from root to leaf position. Saying that, it might need tree/heap height / level steps i.e. \\log_2{n} \\log_2{n} . Where n n is number of nodes in the heap. Recurrence relation # T(n) = T(n/2) + C T(n) = T(n/2) + C i.e. size of the problem is reducing by half (no. of nodes at a left or right sub-tree) each time. By substitution method: \\frac{ \\begin{array}{l,c,l} T(n) = T(n/2) + C \\\\ T(n/2) = T(n/2^2) + C \\\\ T(n/4) = T(n/2^3) + C \\\\ ... \\\\ T(1) = 1 \\\\ T(0) = 1 \\\\ \\end{array} }{ \\begin{array}{r,c,l} T(n) = T(n/2^k) + k*C \\end{array} } \\frac{ \\begin{array}{l,c,l} T(n) = T(n/2) + C \\\\ T(n/2) = T(n/2^2) + C \\\\ T(n/4) = T(n/2^3) + C \\\\ ... \\\\ T(1) = 1 \\\\ T(0) = 1 \\\\ \\end{array} }{ \\begin{array}{r,c,l} T(n) = T(n/2^k) + k*C \\end{array} } Suppose for some k k , n/2^k n/2^k becomes 1 so T(n/2^k) = 1 T(n/2^k) = 1 i.e. T(1) = 1 T(1) = 1 . Thus, n/2^k =1 n/2^k =1 n = 2^k n = 2^k log_2{n} = log_2{2^k} log_2{n} = log_2{2^k} log_2{n} = k log_2{n} = k i.e. k = log_2{n} k = log_2{n} by putting the value of k k in form of n n T(n) = T(1) + log_2{n}*C T(n) = T(1) + log_2{n}*C T(n) = 1 + log_2{n}*C T(n) = 1 + log_2{n}*C T(n) = O(1) + O(log_2{n}) T(n) = O(1) + O(log_2{n}) T(n) = O(log_2{n}) T(n) = O(log_2{n}) Graph Algorithms # Depth First Search (DFS) # DFS works in similar manner as pre-order travesal of a tree. Idea # The idea is to randomly select a starting vertex and from there start traversing other vertices such that we go through a path at a time till its end depth and start printing the vertices from there while back tracking. Do the same for the rest of the untraversed/unvisited/unprinted paths/vertices. Imagine a person trying to figure out escape a maze. Trying to explore a path at once till its end (depth). Applications # Finding the path To check if the graph is bipartite To detect cycles in the graph Topological sort Solving puzzles as maze Finding connected components Finding strongly connected components Finding \"cut vertices\" Implementation - Standard (Recursive) # Desc: TODO Approach: Recursive DS Used: (Internally Stack - for recursive function calls) Time Complexity: Best: O(V + E) if used adjacency list; O(V x V) if used adjacency matrix Avg: same Worst: same Auxilary Space: Best: O(V) (to track the visited/unvisited vertices) Avg: same Worst: same Disadvantage: TODO Properties # TODO Pseudo Code # pick/choose a vertex as starting point - randomly find it adjacent vertices mark the the choosen vertex as visited & print it pick one of the adjacent vertices - randomly repeat [2-4] for this node as well; return repeat [4-5] for rest of the unvisited adjacent vertices Implementation Using Stack (Iterative, Which is same as Recursive one) # Desc: TODO Approach: Iterative DS Used: Stack Time Complexity: Best: O(V + E) if used adjacency list; O(V x V) if used adjacency matrix Avg: same Worst: same Auxilary Space: Best: O(V) (to track the visited/unvisited vertices + size of Stack) Avg: same Worst: same Disadvantage: TODO Pseudo Code # mark all the vertices as unvisited pick/choose a vertex as starting point - randomly put that into a stack; mark that as visited; print the vertex find any one of its (the vertex we printed recently) unvisited adjacent vertices put that into the stack; mark that as visited; print the vertex repeat [4-5] until we reach a vertex from where we can't move forward (i.e. it has no adjacent vertices or has no unvisited adjacent vertices) now start back tracking (using the stack) from current vertex pop the top element from the stack go back to that vertex repeat [4-6] for that vertex we may have some unvisited / disconncted graph (vertices) at this moment so, iterate through all the unvisited vertices (if any) repeat [2-6] for them DFS Tree # TODO Breadth First Search (BFS) / Level order traversal # BFS works in similar manner as level-order travesal of a tree. Idea # The idea is to randomly select a starting vertex and from there start traversing other vertices such that we visit the adjacent vertices first. Once we visited one level (adjacent vertices at a level), then find adjacent vertices of the previously visited ones, and repeat the step until we visited all the vertices. Applications # Finding the path To check if the graph is bipartite To find the shortest path between two vetices Finding connected components Properties # TODO Implementation Using Queue (Iterative) # Desc: TODO Approach: Iterative DS Used: Queue Time Complexity: Best: O(V + E) if used adjacency list; O(V x V) if used adjacency matrix Avg: same Worst: same Auxilary Space: Best: O(V) (to track the visited/unvisited vertices + size of Queue) Avg: same Worst: same Disadvantage: TODO Pseudo Code # mark all the vertices as unvisited pick/choose a vertex as starting point - randomly put it into a queue mark this as visited dequeue and get the item from the queue print the vertex find/explore its unvisited adjacent vertices mark all of them as visited put them into the queue repeat [3-5] until the queue is empty we may have some unvisited / disconncted graph (vertices) at this moment so, iterate through all the unvisited vertices (if any) repeat [2-6] for them Topological Sort # Topological sort, is an ordering of vertices of a DAG in which each vertices[1] comes before, all vertices [2], to which[2] it[1] has outgoing edges. Saying that, [1] will come before [2], if there is an edge from [1] to [2] Note: There is no solution if the graph is a) Undirected, or b) Directed with cycle(s) - it will cause a deadlock. Idea # Topological sort, arranges the vertices of a DAG in an order of their dependecies in other vertices. Meaning that, A vertex which is not dependent on (i.e. no edge incidents to it) will be first in the list, and A vertex which is dependent on most of the vertices (or a series of dependecies has to be fullfilled before it) has to be at the end of the sorted list. Applications # To execute some inter-dependent tasks/jobs Run pipeline of computing jobs To implement/evaluate formulae in Speadsheet cells serialization & deserialization To detect dead locks To check symbolic link loop (deadlock) Properties # A DAG may have one or more Topological Order If all the consecutive vertices in a topological order, are connected by edges, then these edges forms Hamiltonian path If the Hamiltonian path exists, then the topological order is unique; else the DAG can have two or more topological order Implementation Using Stack / DFS # Desc: TODO Approach: Iterative+Recursive DS Used: Stack Time Complexity: Best: O(V + E) if used adjacency list; O(V x V) if used adjacency matrix Avg: same Worst: same Auxilary Space: Best: O(V) (to track the visited/unvisited vertices + size of Stack) Avg: same Worst: same Disadvantage: TODO Pseudo Code # start [perform a tweaked version of DFS of the graph] mark all the vertices unvisited create two stacks (backtrack-stack & topo-stack) to hold |V| number of vertices in each (one to help backtrack, another for topological order) arbitrarily choose a vertex as root to start DFS push that vertex into the backtrack-stack mark that vertex as visited find one of the unvisited adjacent vertices put that unvisted adjacent vertex into the backtrack-stack mark that vertex as visited repeat [7-9] for the current vertex if there is no way further (i.e. no unvisited adjacent vertices); then pop the top element from the backtrack-stack push that popped vertex to the topo-stack go to that popped vertex while backtrack-stack is not empty; repeat [7-14] we may have some unvisited / disconncted graph (vertices) at this moment so, iterate through all the unvisited vertices (if any) repeat [4-15] for them while topo-stack is not empty; pop the top element from the stack and print them end Implementation Using Queue / In-degree # Desc: TODO Approach: Iterative DS Used: Queue Time Complexity: Best: O(V + E) if used adjacency list; O(V x V) if used adjacency matrix Avg: same Worst: same Auxilary Space: Best: O(V) (to track the visited/unvisited vertices + size of Queue) Avg: same Worst: same Disadvantage: TODO Pseudo Code # start traverse the graph (adjacency matrix/list) & calculate the in-degree of every vertices maintain an array to store in-degree of each vertices traverse the adjacency matrix/list increment the in-degree value corresponding to vertex if a new edge is known to incident on that if there are vertices with in-degree zero then goto step [4]; else goto step [10] create a queue & enqueue all those vertices with in-degree = 0 dequeue an element from the queue print that vertex find all the adjacent vertices of that vertex and decrement their in-degree if the in-degree of any of those adjacent vertices becomes zero, enqueue them while the queue is not empty; repeat [5-8] end Detect Cycle in Graph # Using DFS by maintaining the immediate call stack (i.e. before any backtrack) a.k.a. detecting back-edge Using BFS by maintaining 3 flags/colors/sets Using BFS by decreasing In-Degree of the nodes Single-Source Shortest Path in Unweighted Graph # Given a unweighted graph (directed/undirected) G = (V, E) , and a source/distinguished vertex S find the shortest path from S to every other vertices in G . Idea # If the given graph is undirected, treat is as a directed graph by replacing an undirected edge with two directed edges. Cycles may exist in the graph. As the graph is unweighted, we can consider the cost of a path between 2 adjacent vertices are either zero or equal (suppose 1) - apply the same for all the V in the G . Start traversing the graph starting from S in level-order (BFS fashion), meaning explore all the adjacent vertices of the given vertex, then explore the adjacent vertices of them - while doing so, increment the distance of the adjacent vertices (from its parents) by 1. Interestingly - we'll be calculating the distance of each vertices only once (keep track of visited vertices), thus we'll not encounter such condition where we'll be updating/overwriting the existing distance, and we also don't need to bother about checking if the newly calculated distance is lesser than the existing one. Why so? Why the calculated distance/path will be the shortest? Think we started traversing from S parallely (round-robin?) . And a Queue & BFS helped us to achieve so. This is possible because of the fact that - the distance got calculated by traversing a path - which was shortest among others (think level order traversing) thats why this path brought us to that vertext first (among others) and we calculated the distance. There could be multiple shortest path as well. At the end, all the vertices will have shortest distance calculated from the vertex S . Applications # finding fastest way to go from one place to another Properties # Implementation Using Queue / BFS # Desc: Approach: Iterative / Recursive DS Used: 1 Queue, 2 Arrays Time Complexity: O(V+E) if adjacency list is used; O(VxV) if adjacency matrix is used Best: -- Avg: -- Worst: -- Auxilary Space: O(V) Best: -- Avg: -- Worst: -- Disadvantage: can't be applied on a weighted graph Pseudo Code # start create a queue create an array to store distances of the vertices from source S vertex initialize the distance array d with -1 (or INF infinite) mark all the vertices unvisited set the distance from S to S zero enqueue the source vertex S mark that vertex as visited dequeue an element explore the adjacent vertices of that vertex ( u - dequeued vertex) set the distance of the adjacent vertices to: d( u ) + 1 enqueue them mark them as visited while queue is not empty; repeat [9-13] end Single-Source Shortest Path in Weighted Directed Acyclic Graph (DAG) # Idea # Applications # Properties # Implementation Using Stack / DFS (Topological Sort) # Desc: Approach: Topological Sort DS Used: Time Complexity: Best: Avg: \\Theta(V + E) \\Theta(V + E) Worst: Auxilary Space: Best: Avg: Worst: Disadvantage: Pseudo Code # Optimization # Dijkstra's Algorithm [Single-Source Shortest Path in Weighted (non-negative) Graph] # Given a weighted graph (directed/undirected) G = (V, E) G = (V, E) , and a source/distinguished vertex s s find the shortest path from s s to every other vertices in G G . Idea # If the given graph is undirected, treat it as a directed graph by replacing an undirected edge with two directed edges. And assigning the same given weight to both the edges. Cycles may exist in the graph. As the given graph is weighted in nature, simply following BFS and incrementing the distance (or just keep adding the prev. distance + weight) of a vertex may NOT lead us to find an optimal solution. Why so? Why wouldn't the same method help us here, as it did in the case of unweighted graph? As we'll start traversing the paths parallely (round-robin), the calculated distance (prev. distance + weight) would become the factor of comparision among candidate paths (to the same vertex from s s ). So we cannot rely on the closest/nearest (just based on number of edges inbetween) path because this may lead to an incorrect answer. The path with more edges could be the cheapest/shortest path. This has become an minimization/optimization problem. Which could either be solved by Greedy or Dynamic Programing approach. Dijkstra chose the Greedy one. So, this time need to explore all the paths from s s -> v v , visit the same vertex v v multiple times using all the possible (via adjacent vertices, say u u ) paths to that vertex from s s . And relax(update/overwrite) the existing distance of the vertex v v from s s when the newly calculated distance is lesser. Dijkstra says: Do this initial exercise: Calculate the distance of the immediate reachable (adjacent) vertices first, and set the distance of other vertices as \\infty \\infty Mark all the vertices as not done Then perform this sub task: Now pick the one (say u u ) from all vertices whose distance is minimal & is not done Find its adjacent vertices (say v v ) and Relax (recalculate the distance & update) them. No need to Relax the done marked adjacent vertices (not fruitful as per Dijkstra) . Relaxation: 1 2 if d[u] + w(u, v) < d[v]; then d[v] = d[u] + w(u, v) Now, mark that vertex u u as done . Then repeat the sub task until all the vertices are marked done . Applications # finding cheapest way to go from one place to another Properties # approach is Greedy in nature why/how? it's iteratively makes one greedy choice after another mark the distances of nearest vertices first choose the vertex with minimum distance the approach is also dynamic in nature because distances are updated using previously calculated values does not relax the edge going (incidenting) to already selected vertex thats why it may give in the cases of a graph with negative edges Pseudo Code # start create a set/array S S to maintain the vertices whose shortest-path has been finalized/determined create a min-priority queue Q Q (i.e. keyed by distance u.d u.d of the vertex u u ) initialize the distances u.d u.d of all the vertices u \\in G.V u \\in G.V with \\infty \\infty , but set the distance of source vertex to zero i.e. s.d = 0 s.d = 0 insert (enqueue) all the vertices u \\in G.V u \\in G.V into the priority queue Q Q create an array P P to keep track of the path ( u u --> v v ) while Q \\ne \\emptyset Q \\ne \\emptyset (min-priority queue is not empty) u = u = extract-min (dequeue a vertex u u ) from the priority queue (the one with minimum distance from the source vertex) S = S \\cup \\{u\\} S = S \\cup \\{u\\} (add the current vertex u u into the determined set S S ) (as u u has already gone through the relaxation & has the minimum distance amongst others) (think about s s during the inital step, s.d == 0 s.d == 0 ) explore [BFS] those adjacent vertices G.adj[u] - S G.adj[u] - S (of the current vertex u u ) whose shortest-path are not yet determined for all adjacent vertices v \\in G.adj[u] - S v \\in G.adj[u] - S relax (calculate, compare, & decrease-key the distance) d.v d.v for the vertex v v ; where calculate: the new distance \\delta(s, u) + w(u, v) \\delta(s, u) + w(u, v) compare: the old distance \\delta(s, u) + w(u, v) < d.v \\delta(s, u) + w(u, v) < d.v decrease-key: v.d = \\delta(s, u) + w(u, v) v.d = \\delta(s, u) + w(u, v) if the distance v.d v.d got relaxed due to current vertex u u ; then store the current vertex u u against this vertex v v in the path-array P P (this will help track the shortest path) end Implementation Using Adjacency List, Priority Queue, BFS # Approach: Greedy + BFS DS Used: 1 Priority Queue, 1 Array Time Complexity: Best: O(V) O(V) if no edges between the vertices (step-7 will run |V| times) Avg: O((V + E) * logV) O((V + E) * logV) if min-heap based priority queue is used DECREASE-KEY: O(log n) O(log n) EXTRACT-MIN: O(1) O(1) INSERT: O(log n) O(log n) step 5 = insert/enqueue |V| |V| vertices to a priority queue = O(V * log n) O(V * log n) step 7 * 7.d = loop over |E| |E| times (traverse through each edge exactly once) = O(E) O(E) step 7.d.i.iii = O(log n) O(log n) step 7 * 7.d * 7.d.i.iii = O(E * log n) O(E * log n) DOUBT: why the constraint \"If the graph is sufficiently sparse \u2014 in particular, E = O(V^2/log V) E = O(V^2/log V) \u2014 we can improve the algorithm by implementing the min-priority queue with a binary min-heap\" in CLRS? Why not always use min-heap based PQ? Worst: O(V + V^2)* logV) O(V + V^2)* logV) == O(E * logV) O(E * logV) if the graph is a complete graph, where |E| = |V|*(|V|-1)/2 |E| = |V|*(|V|-1)/2 Auxilary Space: O(V) O(V) Best: -- Avg: -- Worst: -- Disadvantage: does a blind search and wastes resources can't guarantee a solution for a negative weighted graph Optimization # Extra # I personally don't like the way CLRS has mentioned the time complexity O(V^2 + E) O(V^2 + E) = O(V^2) O(V^2) when array/list based priority queue is used. It says EXTRACT-MIN would have time complexity O(n) O(n) , which is a bad implementation IMO. Rather, I'd implement EXTRACT-MIN in O(1) O(1) & INSERT in O(n) O(n) . Why the algorithm may give incorrect answer in the case of a graph with negative edges? because the algorithm does not relax edge pointing (going/incidenting) to the already selected vertex Why/how the algorithm is greedy? [see properties section] Can Dijkstra's algorithm handle negative edge & result correct answer if we add a positive constant to all the edges? Bellman-Ford Algorithm [Single-Source Shortest Path in Weighted (negative) Graph] # Given a weighted graph (directed/undirected) G = (V, E) G = (V, E) , and a source/distinguished vertex s s find the shortest path from s s to every other vertices in G G . This algorithm can handle the negative weights in the graph which overcomes the drawbacks of Dijkstra's algorithm. Idea # If the given graph is undirected, treat is as a directed graph by replacing an undirected edge with two directed edges. And assigning the same given weight to both the edges. Cycles may exist in the graph. ( But not negative weight cycles ) As we've seen in previous simple/greedy algorithms that presence of negative weights may trip the approach and lead to incorrect answers. We need to be more careful and try all the possible paths to a vertex v v from source s s . And pick the shortest path (minimum distance) amongst them. This is again the same minimization/optimization problem, but approach is not Greedy this time. Bellman-Ford prefers to explore all the possibilies using Dynamic Programing approach - where the target would be to optimize the global solution rather just focusing on the local optimal solution. Bellman-Ford says: List down all the edges And start relaxing an edge u u -> v v for each edge pairs (u, v) (u, v) in G.E G.E Repeat the step-2 |V| - 1 |V| - 1 times, thus it will perform the relaxation of the each edge (u, v) (u, v) for |V| - 1 |V| - 1 times - because the vertex v v might be reachable from all the rest of the vertices at max Why such idea? Because in a worst-case, a simple shortest path from s s to v v might have maximum |V|-1 |V|-1 edges to pass by. Thus, to get a final correct/relaxed distance/answer, all the intermediate edges have to be relaxed as well. Thing to observe in step-2-3 is, each iteration guarantees relaxation/correctness of atleast one edge. (So, in first iteration it calculates the shortest path with atmost one edge in the path, in second iteration it calculates the shortest path with atmost two edges in the path, thus in i^{th} i^{th} iteration it calculates the shortest path with atmost i i edges in the path. So, in each of these repetitions, the number of vertices with correctly calculated distances grows, from which it follows that eventually all vertices will have their correct distances). So, a graph without cycle (assume) expects |V|-1 |V|-1 relaxation. Also, this idea has nothing to do with negative weights (till |V|-1 |V|-1 iterations) Illustration Repeat the step-2 one more time, and check if further relaxation is possible in |V| |V| ^{th}$ round; if so, then there exists a Negative Weight Cycle Relaxation: 1 2 if d[u] + w(u, v) < d[v]; then d[v] = d[u] + w(u, v) Applications # Negative weights are found in various applications of graphs. For example, instead of paying cost for a path, we may get some advantage if we follow the path. in networks, in routing information protocol (RIP) finding cheapest way to go (or send something) from one place to another Properties # follows Dynamic Programing approach works in bottom-up manner can detect a Negative Weight Cycle Pseudo Code # input : graph G(V, E) G(V, E) , weight function w w , and source s s output : a) True, b) distance of all the v \\in V v \\in V , c) the shortest path; iff there in no negative weight cycle reachable from source s s ; else a) False start create a distance array d d & initialize d[u] \\forall u \\in V d[u] \\forall u \\in V with \\infty \\infty create an array P P to keep track of the path ( u u --> v v ) set d[s] = 0 d[s] = 0 for |V| -1 |V| -1 times: for each edge (u, v) \\in E (u, v) \\in E : relax (u, v, w) (u, v, w) i.e. if d[v] \\gt d[u] + w(u, v) d[v] \\gt d[u] + w(u, v) : d[v] = d[u] + w(u, v) d[v] = d[u] + w(u, v) P[v] = u P[v] = u for each edge (u, v) \\in E (u, v) \\in E : if d[v] \\gt d[u] + w(u, v) d[v] \\gt d[u] + w(u, v) : return False return True end Implementation Using Dynamic Programing # Desc: Approach: Dynamic Programing DS Used: Time Complexity: Best: Avg: O(V * E) O(V * E) (at most |V|-1 |V|-1 number of edges might be between vertex s s & v v x # of edges to be relaxed) Worst: O(V^3) O(V^3) if the graph is a complete graph Auxilary Space: O(V) O(V) Best: Avg: Worst: Disadvantage: does not work if there is a negative weight cycle does not scale well Optimization # If no more relaxation happens in the Graph; then immediately stop/return. just maintain a flag & check the flag of the Extra # Can Bellman-Ford's algorithm handle negative edge cycle & result correct answer if we add a positive constant to all the edges? How negative weight cycle check works? A final scan of all the edges is performed and if any distance is updated, then a path of length |V| |V| edges has been found which can only occur if at least one negative cycle exists in the graph. Floyd-Warshall Algorithm [All-Pair Shortest-Path in Weighted (negative) Graph] # Given a weighted graph (directed/undirected) G(V, E) G(V, E) with vertices V V numbered 1 1 to N N , and a weight-function w_{ij} w_{ij} to denote weight of the edge between vertext i i & j j , then find the shortest path from each vertex i \\in V i \\in V to every other vertex j \\in V j \\in V in G G . This can be solved using shortest-path single-source algorithm, by running them for |V| |V| times considering each vertex as source each time. If Floyd-Warshall have chosen Dijkstra's algorithm then it would have cost: O(V) * O((V + E)* log V) O(V) * O((V + E)* log V) (considering it cannot handle negative weights). And O(V^3 log V) O(V^3 log V) in case of a dense graph. If Floyd-Warshall have chosen Bellman-Ford's algorithm then it would have cost: O(V) * O(V * E) O(V) * O(V * E) , and O(V^4) O(V^4) in case of a dense graph. However, those two algorithms were based on using Adjacency List, while this algorithm uses Adjacency Matrix. And even with that it solves the problem in \\Theta(V^3) \\Theta(V^3) . Note: If the given graph is undirected, treat it as a directed graph by replacing an undirected edge with two directed edges. And assigning the same given weight to both the edges Negative weights may exist in the graph Cycles may exist in the graph The algorithm assumes that there are no negative cycles if there are negative cycles, the Floyd\u2013Warshall algorithm can be used to detect them How? Its true that the distance between same vertex i i should be zero If that distance changes to a negative value, then there is a negative weight cycle Idea # It's reasonable to think/assume/say that a shortest-path p p from vertex i i to j j could have (may not have) some intermediate vertices from a set of vertices say \\{1, 2, 3, ..., k \\} \\{1, 2, 3, ..., k \\} . Also, the shortest-path p p may go through vertex k k and may not. If we denote: a shortest-distance (length of a shortest-path ) from vertex i i to j j via k k as d_{ij}^k d_{ij}^k and, a shortest-distance from vertex i i to j j - if there is only 1 edge between them (i.e. no extra vertext between them) as d_{ij}^0 d_{ij}^0 (suppose we denote this case by k=0 k=0 ) So, for each pair i, j \\in V i, j \\in V , the observation could be: if the shortest-path p p does NOT even have more than 1 edge. Then we can denote the shortest-distance by d_{ij}^0 = w_{ij} d_{ij}^0 = w_{ij} (i.e. whatever weight is initially given in the G G ). if the shortest-path p p does NOT go through k k ; i.e. the intermediate vertices falls in set \\{1, 2, 3, ..., k-1\\} \\{1, 2, 3, ..., k-1\\} only. Then, we can denote the shortest-distance by d_{ij}^{k-1} d_{ij}^{k-1} if the shortest-path p p GOES through k k ; then the path could be broken into two parts, say: i i --> k k (say path p_1 p_1 ) k k --> j j (say path p_2 p_2 ) where both the path using intermediate vertices from set \\{1, 2, 3, ..., k-1\\} \\{1, 2, 3, ..., k-1\\} . And, if the shortest-path p p from i i to j j goes via k k , then it should definitely be the concatenation of a shortest-path from i i to k k ( p_1 p_1 , using intermediate vertices from set \\{1, 2, 3, ..., k-1\\} \\{1, 2, 3, ..., k-1\\} ), and a shortest-path from k k to j j ( p_2 p_2 , only using intermediate vertices from set \\{1, 2, 3, ..., k-1\\} \\{1, 2, 3, ..., k-1\\} ). Then, we can denote the shortest-distance (of shortest-path p p ) by d_{ik}^{k-1} + d_{kj}^{k-1} d_{ik}^{k-1} + d_{kj}^{k-1} Then, from the above observations, we can deduce a relationship between the: shortest-path p p (or its distance) from vertex i i to j j some intermediate vertex k k weight-function w_{ij} w_{ij} given in the graph G G Which can be recursively defined as: d_{ij}^k = \\left \\{ \\begin{array}{lcl} w_{ij} & if & k=0 \\\\ min( d_{ij}^{k-1}, \\space d_{ik}^{k-1} + d_{kj}^{k-1}) & if & k \\gt 0 \\\\ \\end{array} \\right. d_{ij}^k = \\left \\{ \\begin{array}{lcl} w_{ij} & if & k=0 \\\\ min( d_{ij}^{k-1}, \\space d_{ik}^{k-1} + d_{kj}^{k-1}) & if & k \\gt 0 \\\\ \\end{array} \\right. Applications # find shortest path (from all the vertices) in directed graphs detect negative weight cycle in directed graphs many more real life applications (see this ) Properties # follows dynamic programing approach bottom up way (i.e. sub problems are already in indivisible state, so keep combining the results from sub problems till we get the final result) can detect negative weight cycle utilizes Adjacency Matrix Pseudo Code # input : graph G(V, E) G(V, E) with weight function w w (or just weight matrix W W ) output : shortest distance between each vertices Typically Floyd\u2013Warshall algorithm calculates distances only, and does not reconstruct the shortest path (i.e. does not return predecessor map), also does not tell if a negative weight cycle exists. Though those could be achieved by some additional simple logic/code/steps. N = |V| N = |V| // number of vertices D = (d_{ij}) D = (d_{ij}) // initialize 2-d array of size N*N N*N with \\infty \\infty , to store shortest-distance between vertices for (i, j) \\in G.E (i, j) \\in G.E // for all edges from vertex i i to j j d_{ij} = w_{ij} d_{ij} = w_{ij} // set distance from vertex i i to j j using w_{ij} w_{ij} for i \\in V i \\in V d_{ii} = 0 d_{ii} = 0 // set distance between same vertex ( i i to i i , i.e. self loop) to zero for k k = 1 to N N for i \\in V i \\in V for j \\in V j \\in V d_{ij} = min(d_{ij}, \\space d_{ik} + d_{kj}) d_{ij} = min(d_{ij}, \\space d_{ik} + d_{kj}) // relaxation return D D Pseudo Code - detect negative weight cycle # input : final distance matrix D D from Floyd\u2013Warshall algorithm output : True if negative weight cycle exists; else False N = |D| N = |D| for i i = 1 to N N for j j = 1 to N N if i == j i == j // if diagonal coordinate of the matrix if d_{ij} < 0 d_{ij} < 0 return True return False Pseudo Code - reconstruct shortest-path # TODO Implementation using Adjacency Matrix # Approach: Dynamic Programing DS Used: Matrix (2-d array) Time Complexity: \\Theta(V^3) \\Theta(V^3) Best: -- Avg: -- Worst: -- Auxilary Space: \\Theta(V^2) \\Theta(V^2) (if weight matrix and/or predecessor matrix has to be created) else \\Theta(1) \\Theta(1) Best: -- Avg: -- Worst: -- Disadvantage: -- Optimization # Extra # Kruskal Minimum Cost Spanning Tree Algorithm # Idea # Applications # Properties # Implementation Using Stack / DFS # Desc: Approach: DS Used: Time Complexity: Best: Avg: Worst: Auxilary Space: Best: Avg: Worst: Disadvantage: Pseudo Code # Prim's Minumum Cost Spanning Tree # Knuth-Morris-Pratt Algorithm # Bipartite Matching # Iterative Deepening Depth First Search (Depth Limited Search) # A* Search # Ternary Search # Meet in the middle # Strongly Connected Components (SCC) # Edmonds-Karp Algorithm # Hungarian Algorithm # Sweep Line Algorithm # Graham scan # Tarjan's Algorithm # Z algorithm # Hill Climbing # Number Theory # Modular Arithmetic # Fermat\u2019s Theorem # Chinese Remainder Theorem(CRT) # Euclidian Method for GCD # Logarithmic Exponentiation # Sieve of Eratosthenes # Euler\u2019s Totient Function # Geometric Algorithms # 2D Rotation and Scale Matrices # 2D Rotation and Translation Matrices # 2D Changing Coordinate Systems # 3D Rotation and Scale Matrices # 3D Changing Coordinate Systems # Greedy Algorithms # The problem should be solved in stages, by consider one step a time and one input at a time. The each step should find a local optimal solution, and those local optimal should lead to global optimal solution. These algorithms provides a predefined procedure to be followed in each step. An optimization problem can be solved using greedy approach. Elementary cases : Fractional Knapsack Problem, Task Scheduling # Data Compression using Huffman Trees # Activity Selection # Dynamic Programing # Dynamic programing is an approach to solve a problem which requires an optimal solutions amongst various other possible solutions. So, the goal is to get a global optimal solution. To do so, we analyze the problem and identify a repeated subtask (step) in that. Then we make decision at each step considering current problem and solution to previously solved sub problem to calculate optimal solution. Intro # Memoization # Top - Down approach in a subproblem tree Intro divides a big problem into subproblems starts solving smallest problem first; approaching root of problem memoize/cache the solved subproblems for repeated/same subproblems, utilizes the cache/memoization table uses recursion Pros easy implementation Cons recusrsion stach could be deeper and that might create stack-overflow/memory issue Extra memo table fills up in bottom to top order Tabulation # Bottom - Up approach in a subproblem tree Intro Understands a root problem and finds subproblems to that starts solving subproblems (in a particular order which approaches the root problem, like smallest to bigger, 1 to 100 etc.) in iterative way store the answer to those subproblems in a table Pros avoids recursion stack issue suitable for extremly complicated problems suitable where optimization is concern because it gives flexibility of command over coding style Cons need more rigid thinking ahead of time to find the ordering of subproblems so that we do not need to backtrack and solve a smaller problem in order to solve a larger problem Extra table fills up in top to bottom order Examples # Edit Distance # Knapsack problem # 0/1 Knapsack Problem # 0/1 Knapsack Problem (with repetition of items) # Knapsack Problem (with fractional items) # Matrix Chain Multiplication # Longest Common Substring # Longest Common Subsequence # Longest Increasing Monotonical Subsequence # Rod Cutting # Misc # Recursion # Huffman Coding # Regex Algorithm (Pattern Matching and Parsing) # Hashing- Hash Functions # Monotone Chains Algorithm # Coordinate Compression # Ford-Fulkerson Method # Preflow-Push Algorithm # Dinic's Algorithm # Monte Carlo method or Metropolis Algorithm # Krylov Subspace Iteration Method # Householder Matrix Decomposition # QR Algorithm # Fast Fourier Transform # Integer Relation Detection Algorithm # Fast Multipole algorithm # MinMax Algorithm # Divide and Conquer Algorithm # Nomenclature # use uppercase letters to denote matrices and corresponding subscripted lowercase letters to denote their elements References # CLRS 3rd Edition Algorithms Unlocked - 2013, by Dr. Thomas Cormen https://gist.github.com/toransahu/bb1c9f1cd6490ff29c42fa229e827a2a https://www.freetechbooks.com/algorithms-and-data-structures-f11.html LaTex Math Syntax https://www.caam.rice.edu/~heinken/latex/symbols.pdf https://en.wikibooks.org/wiki/LaTeX/Mathematics https://math.meta.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference https://latex.wikia.org/wiki/Array_(LaTeX_environment) https://tex.stackexchange.com/questions/77589/what-do-the-pieces-of-latex-left-and-right-respectively-mean","title":"Algorithms"},{"location":"Computer-Science/algo/#algorithms","text":"","title":"Algorithms"},{"location":"Computer-Science/algo/#introduction","text":"","title":"Introduction"},{"location":"Computer-Science/algo/#what-is-algorithm","text":"Step by step instructions to solve a problem.","title":"What is Algorithm?"},{"location":"Computer-Science/algo/#why-analysis-of-algorithms","text":"To determine efficient amongst multiple solution in terms of Time & Space consumed.","title":"Why Analysis of Algorithms?"},{"location":"Computer-Science/algo/#goal-of-analysis-of-algorithms","text":"To compare solutions/algorithms mainly in terms of running time , but also in other factors like memory , developers effort, readability, simplicity etc.","title":"Goal of Analysis of Algorithms?"},{"location":"Computer-Science/algo/#what-is-running-time-analysis","text":"Determining how processing time increases with the size of the input (or problem).","title":"What is Running Time Analysis?"},{"location":"Computer-Science/algo/#what-is-running-time","text":"Execution time taken by a program in a particular machine? NO . This metric should be independent of other factors like: programing language, execution enviroment i.e. computer, CPU, RAM etc. So, we express the Running Time as a mathematical function of the input size i.e. f(n) f(n) .","title":"What is Running Time?"},{"location":"Computer-Science/algo/#how-to-compare-algorithms","text":"Using Running Time.","title":"How to compare Algorithms?"},{"location":"Computer-Science/algo/#what-is-rate-of-growth","text":"The rate at which the Running Time increases with the size of the input. aka. the rate at which the value of f(n) f(n) increases with the n n . Say, g(n) g(n) such that f(n) \\propto g(n) f(n) \\propto g(n) . Such, g(n) g(n) are known as asymptotic in nature to f(n) f(n) .","title":"What is Rate of Growth?"},{"location":"Computer-Science/algo/#commonly-used-rate-of-growths","text":"1 < log{log n} < \\sqrt{log n} < log^2n < 2^{log n} < n < log(n!) < n log n < n^2 < 2^n < 4^n < n! < 2^{2^n} 1 < log{log n} < \\sqrt{log n} < log^2n < 2^{log n} < n < log(n!) < n log n < n^2 < 2^n < 4^n < n! < 2^{2^n}","title":"Commonly used Rate of Growths"},{"location":"Computer-Science/algo/#what-is-asymptotic-ness","text":"For a given function f(n) f(n) if, another function g(n) g(n) tries to approximate f(n) f(n) ; then g(n) g(n) is called asymptotic curve for f(n) f(n) .","title":"What is Asymptotic-ness?"},{"location":"Computer-Science/algo/#what-is-asymptotic-notation","text":"Syntax/Symbol/Expression to represent the different Asymptotic nature of a function. Lets understand this by taking an example of a function f(n) f(n) (which may represent Running Time of an algorithm/solution to a problem) in terms of size of input ( n n ).","title":"What is Asymptotic Notation?"},{"location":"Computer-Science/algo/#big-o-o-notation","text":"This notation gives smallest rate of growth g(n) g(n) which is greater than or equal to the Running Time f(n) f(n) of the given algorithm/solution. i.e. there exists some positive constants n_0 n_0 & c c : such that 0 \\le f(n) \\le cg(n) 0 \\le f(n) \\le cg(n) ; where n \\ge n_0 n \\ge n_0 This is also called asymptotically tight upper bound of the given function. f(n) = O(g(n)) f(n) = O(g(n)) In this representation/expression f(n) f(n) or O(g(n)) O(g(n)) represents Running Time of an algorithm, n n represents the size of the input, g(n) g(n) represents the Rate of Growth of the Running Time of the algorithm, and O O represents the nature of the asymptoticness of curve g(n) g(n) with curve f(n) f(n) .","title":"Big-O 'O' Notation"},{"location":"Computer-Science/algo/#omega-notation","text":"This notation gives largest rate of growth g(n) g(n) which is less than or equal to the Running Time f(n) f(n) of the given algorithm/solution. i.e. there exists some positive constants n_0 n_0 & c c : such that 0 \\le cg(n) \\le f(n) 0 \\le cg(n) \\le f(n) ; where n \\ge n_0 n \\ge n_0 This is also called asymptotically tight lower bound of the given function. f(n) = \\Omega(g(n)) f(n) = \\Omega(g(n))","title":"Omega '\u03a9' Notation"},{"location":"Computer-Science/algo/#theta-notation","text":"This notation gives a rate of growth g(n) g(n) such that it falls in between the tight lower & upper bound of theRunning Time of the given algorithm. i.e. there exists some positive constants c_1 c_1 , c_2 c_2 , and n_0 n_0 : such that 0 \\le c_1g(n) \\le f(n) \\le c_2g(n) 0 \\le c_1g(n) \\le f(n) \\le c_2g(n) ; where n \\ge n_0 n \\ge n_0 This is also called asymptotically tight bound of the given function. f(n) = \\Theta(g(n)) f(n) = \\Theta(g(n))","title":"Theta '\u0398' Notation"},{"location":"Computer-Science/algo/#type-of-analysis","text":"Types of analyzing an algorithm.","title":"Type of Analysis"},{"location":"Computer-Science/algo/#worst-case","text":"This considers the size of input for which the algorithm may take longest time (or algorithm runs slowest). Thus it defines the Rate of Growth (the influencing factor in the Running Time) for such input. We can use any of the asymptotic notation to represent the worst-case Rate of Growth i.e. either O O or \\Theta \\Theta or \\Omega \\Omega . There is no hard rule to use O O to represent the worst-case. Asymptotic notations just tells about the asymptotic nature of the function/curve. It has nothing in specific with the worst/avg/best case analysis. However, there could be a suitable notation of choice for a type of analysis. Let's take an example of an algorithm whose Running Time seems to be f(n) = n^2 + 2n + 1 f(n) = n^2 + 2n + 1 . We can represent the worst-case running time by \\Theta(n^2) \\Theta(n^2) as for some positive constant c_1, c_2, n_0 c_1, c_2, n_0 , where n \\ge n_0 n \\ge n_0 for which the following holds true: 0 \\le c_1g(n) \\le f(n) \\le c_2g(n) 0 \\le c_1g(n) \\le f(n) \\le c_2g(n) i.e. 0 \\le c_1*n^2 \\le n^2 + 2n + 1 \\le c_2*n^2 0 \\le c_1*n^2 \\le n^2 + 2n + 1 \\le c_2*n^2 Suppose, n = 1 n = 1 & c_1=4 c_1=4 , c_2=4 c_2=4 ; then 0 \\le 4 \\le 4 \\le 4 0 \\le 4 \\le 4 \\le 4 . Suppose, n = 1 n = 1 & c_1=3 c_1=3 , c_2=5 c_2=5 ; then 0 \\le 3 \\le 4 \\le 5 0 \\le 3 \\le 4 \\le 5 . Also, to be on a safer side we can use O(n^2) O(n^2) to express worst-case running time, like: For some positive constant c, n_0 c, n_0 , where n \\ge n_0 n \\ge n_0 for which the following holds true: 0 \\le f(n) \\le cg(n) 0 \\le f(n) \\le cg(n) i.e. 0 \\le n^2 + 2n + 1 \\le c*n^2 0 \\le n^2 + 2n + 1 \\le c*n^2 Suppose, n = 1 n = 1 & c=4 c=4 ; then 0 \\le 4 \\le 4 0 \\le 4 \\le 4 . Suppose, n = 1 n = 1 & c=5 c=5 ; then 0 \\le 4 \\le 5 0 \\le 4 \\le 5 . So, we can say \\Theta \\Theta is a strong notion than O O . And also can say: if f(n) = \\Theta(g(n)) f(n) = \\Theta(g(n)) , then f(n) = O(g(n)) f(n) = O(g(n)) \\Theta(g(n)) \\subseteq O(g(n)) \\Theta(g(n)) \\subseteq O(g(n)) However, using \\Omega \\Omega to denote the worst-case is acceptable but doesn't makes sense. We can say something like: the longest time this algorithm can take will always be more than or equal to 5 minutes. Non sense. Not much of use. So, the suitable asymptotic notations for this case could be O O & \\Theta \\Theta .","title":"Worst Case"},{"location":"Computer-Science/algo/#best-case","text":"This considers the size of input for which the algorithm may take lowest time (or algorithm runs fastest). Thus it defines the Rate of Growth (the influencing factor in the Running Time) for such input. Suitable asymptotic notations for this case could be \\Omega \\Omega & \\Theta \\Theta .","title":"Best Case"},{"location":"Computer-Science/algo/#average-case","text":"This considers a random size of input tries to define the Rate of Growth (the influencing factor in the Running Time) for such input. Suitable asymptotic notations for this case could be \\Theta \\Theta & O O .","title":"Average Case"},{"location":"Computer-Science/algo/#amortized-analysis","text":"Instead of analysis a data-structure operation and defining the time complexity/cost soley based on that single one, analyse a sequence of data-structure operations and average the cost of all those operations. So that it could be shown that the average cost of an operation is smaller, even though the cost of a single operation within the sequence is expensive.","title":"Amortized Analysis"},{"location":"Computer-Science/algo/#how-is-it-different-than-average-case-analyse","text":"Average case analysis is based on the input to the algorithm, as it assumes that the input provided in this case is a random sized. Thus it relies on a probablistic assumption about the input. While the amortized analysis does not rely on the input. It applies for all the inputs / any size of input. Amortized analysis guarantees on the worst-case cost of N N operations.","title":"How is it different than average-case analyse?"},{"location":"Computer-Science/algo/#techniques","text":"Aggregate Method Accounting Method Potential Method","title":"Techniques"},{"location":"Computer-Science/algo/#recurrence-relation","text":"","title":"Recurrence Relation"},{"location":"Computer-Science/algo/#ways-to-solve-recurrence-relation","text":"Substitution Method (aka Back substitution / Induction) Recursion Tree Master Theorem Let's understand deriving a recurrence relation and solving it using some examples. Example 1: Calculate the time complexity of finding an item in array using Binary Search ? Example 2: Calculate the time complexity of finding n^{th} n^{th} item of the Fibonacci Series ? A recurrsive solution says that: F(n) = \\left \\{ \\begin{array}{lcl} 0 & if & n=0 \\\\ 1 & if & n=1 \\\\ F(n-1) + F(n-2) & if & n>1 \\\\ \\end{array} \\right. F(n) = \\left \\{ \\begin{array}{lcl} 0 & if & n=0 \\\\ 1 & if & n=1 \\\\ F(n-1) + F(n-2) & if & n>1 \\\\ \\end{array} \\right. So, the recurrence relation should be: T(n) = \\left \\{ \\begin{array}{lcl} 1 & if & n \\le 1 \\\\ T(n-1) + T(n-2) & if & n>1 \\\\ \\end{array} \\right. T(n) = \\left \\{ \\begin{array}{lcl} 1 & if & n \\le 1 \\\\ T(n-1) + T(n-2) & if & n>1 \\\\ \\end{array} \\right. Solution using substitution method: \\begin{array}{r, c, l,l} T(n) & = & T(n-1) + T(n - 2) & \\text{(this will become harder to solve)} \\\\ T(n) & \\le & T(n-1) + T(n-1) & \\text{(so, approximate} \\space T(n-2) \\approx T(n-1) \\space \\text{for upper bound)} \\\\ \\therefore T(n) & \\le & 2 \\times T(n-1) & \\text{ eq. 1}\\\\ \\end{array} \\begin{array}{r, c, l,l} T(n) & = & T(n-1) + T(n - 2) & \\text{(this will become harder to solve)} \\\\ T(n) & \\le & T(n-1) + T(n-1) & \\text{(so, approximate} \\space T(n-2) \\approx T(n-1) \\space \\text{for upper bound)} \\\\ \\therefore T(n) & \\le & 2 \\times T(n-1) & \\text{ eq. 1}\\\\ \\end{array} Let's derive a few other terms using the eq. 1 \\begin{array}{r, c, l,l} T(n-1) & \\le & 2 \\times T(n-2) \\\\ T(n-2) & \\le & 2 \\times T(n-3) \\\\ ... \\\\ T(1) & = & 1 \\\\ T(0) & = & 1 \\\\ \\end{array} \\begin{array}{r, c, l,l} T(n-1) & \\le & 2 \\times T(n-2) \\\\ T(n-2) & \\le & 2 \\times T(n-3) \\\\ ... \\\\ T(1) & = & 1 \\\\ T(0) & = & 1 \\\\ \\end{array} Now do the substitution \\begin{array}{r, c, l,l} T(n) & \\le & 2 \\times T(n-1) \\\\ T(n) & \\le & 2 \\times [2 \\times T(n-2)] \\\\ T(n) & \\le & 2^3 \\times T(n-3) \\\\ T(n) & \\le & 2^4 \\times T(n-4) \\\\ ... \\\\ T(n) & \\le & 2^k \\times T(n-k) \\\\ \\end{array} \\begin{array}{r, c, l,l} T(n) & \\le & 2 \\times T(n-1) \\\\ T(n) & \\le & 2 \\times [2 \\times T(n-2)] \\\\ T(n) & \\le & 2^3 \\times T(n-3) \\\\ T(n) & \\le & 2^4 \\times T(n-4) \\\\ ... \\\\ T(n) & \\le & 2^k \\times T(n-k) \\\\ \\end{array} As we know \\begin{array}{r,c,l} T(0) & = & 1 \\\\ \\therefore T(n-k) & = & 1 \\\\ \\text{and } n-k & = & 0 \\\\ \\therefore k & = & n & \\text{(for } T(0) = 1 \\text{)} \\\\ \\end{array} \\begin{array}{r,c,l} T(0) & = & 1 \\\\ \\therefore T(n-k) & = & 1 \\\\ \\text{and } n-k & = & 0 \\\\ \\therefore k & = & n & \\text{(for } T(0) = 1 \\text{)} \\\\ \\end{array} Thus \\begin{array}{r,c,l} T(n) & \\le & 2^n \\times T(0) & \\text{(for } k=n \\text{)} \\\\ \\therefore T(n) & \\le & 2^n \\\\ \\therefore T(n) & = & O(2^n) & \\text{(this is upper bound of the worst case analysis)} \\\\ \\end{array} \\begin{array}{r,c,l} T(n) & \\le & 2^n \\times T(0) & \\text{(for } k=n \\text{)} \\\\ \\therefore T(n) & \\le & 2^n \\\\ \\therefore T(n) & = & O(2^n) & \\text{(this is upper bound of the worst case analysis)} \\\\ \\end{array} However, the tighter bound is \\Theta(\\phi^n) \\Theta(\\phi^n) where \\phi = \\frac{1+\\sqrt{5}}{2} \\approx 1.62 \\approx 2 \\phi = \\frac{1+\\sqrt{5}}{2} \\approx 1.62 \\approx 2 , called as Golden Ratio . Example 3: Calculate the time complexity of recursive solution of finding the number of ways to reach the top of a stair if allowed steps are 1, 2, and 3 ?","title":"Ways to solve recurrence relation"},{"location":"Computer-Science/algo/#master-theorem","text":"If the recurrence relation is of the form: T(n) = aT(n/b) + \\Theta(n^klog^pn) T(n) = aT(n/b) + \\Theta(n^klog^pn) TODO","title":"Master Theorem"},{"location":"Computer-Science/algo/#sorting-algorithms","text":"","title":"Sorting Algorithms"},{"location":"Computer-Science/algo/#types","text":"","title":"Types"},{"location":"Computer-Science/algo/#1-bubble-sort","text":"Description: Swapping adjacent elements if there are in wrong orders. Time Complexity: Always O(n^2) O(n^2) (even for sorted arrays) Can be optimized for sorted array","title":"1. Bubble Sort:"},{"location":"Computer-Science/algo/#2-selection-sort","text":"Description: Pulls a minimum element from unsorted-subarray and appends infront of the array (as a sorted sub-array) Time Complexity: O(n^2) O(n^2) (all cases) Auxilary Space: O(1) O(1)","title":"2. Selection Sort:"},{"location":"Computer-Science/algo/#3-insertion-sort","text":"Description: Keep first element on left side, start one by one from 2^{nd} 2^{nd} element (lets say ith element). Compare the picked element ( i^{th} i^{th} ) with all the elements in left sub-array (i.e. with i-1^{th} i-1^{th} to 0^{th} 0^{th} ). If LHS sub-array element is greater than i element, then shift that element right by one index. Repeat for all LHS elements. Fill the empty cell with picked element. Time Complexity: Best Case: O(n) O(n) ; if array is as small as n=1 or array is already sorted Avg Case: \\Theta(n^2) \\Theta(n^2) Worst Case: O(n^2) O(n^2) Auxilary Space: O(1) O(1)","title":"3. Insertion Sort:"},{"location":"Computer-Science/algo/#4-merge-sort","text":"Description: Split the array in Binary fashion, till you get minimum/single entity Start merging 2 single entities --> you'll get 2 sorted sub-array --> merge both Continue merging till end Merge Function: Merges two sorted sub-arrays by using an extra space of O(n). Begin from 0th index of both sub-array (using pointers like i,j), do comparision in temporary (un-touched) array Make changes in original array till both i i & j j reaches the end of the sub-array Time Complexity: Merge: O(n) O(n) Merge Sort: Best Case: O(nlogn) O(nlogn) Avg Case: O(nlogn) O(nlogn) Worst Case: O(nlogn) O(nlogn) Auxiliary Space: Merge: O(n) O(n) Merge Sort: O(n) + O(1) O(n) + O(1) Algorithmic Paradigm: Divide and Conquer Implementations: Recursive only Sorting In Place: Yes (No in a typical implementation) Applications: Sorting linkedlist in O(nlogn) O(nlogn) Inversion Count Problem (i.e. in an array E_i>E_j>E_k E_i>E_j>E_k where i<j<k i<j<k ) In External Sorting","title":"4. Merge Sort"},{"location":"Computer-Science/algo/#5-quick-sort-recursivearr-left-right","text":"Description: Partition the array about a pivot: re-arrange smaller elements in LHS & greater elements in RHS of pivot Return partitioned index (i.e. index of pivot after re-arrangement) Re-call quicksort for sub-array smaller than pivot Re-call quicksort for sub-array greater than pivot Partition Function(arr, left, right): Desc: Pick pivot is any preferable fashion: First element Last element Mean Randon Find index of pivot element in array by counting number of elements smaller than it + re-arranges elements smaller than pivot to left (and greater to right) in original Array. Swap pivot element with element at that index return the pivot/partioning index Returns: Partitioning Index (Re-calculated index of pivot) Time Complexity: O(n) O(n) Auxilary Space: O(1) O(1) Time Complexity: Best Case: O(nlogn) O(nlogn) (Occurs when pivot element is middle element value-wise) Avg Case: O(nlogn) O(nlogn) Worst Case: O(n^2) O(n^2) (Occurs when pivot element is either smaller or larger element) Auxilary Space: O(1) O(1) Algorithm Paradigm: Divide and Conquer Implementation: Recursive (Generally) and Iterative In-Place: Yes (because auxilary space O(n) O(n) ) Recurrence Relation: Best Case: T(n) = O(n) + 2*T(n/2) == O(nlogn) T(n) = O(n) + 2*T(n/2) == O(nlogn) Avg Case: T(n) = O(n) + 2*T(n/2) == O(nlogn) T(n) = O(n) + 2*T(n/2) == O(nlogn) Worst Case: T(n) = O(n) + T(n-1) == O(n^2) T(n) = O(n) + T(n-1) == O(n^2) Applications: In case of memory limitation this algo is used Advantages: One of the fastest algo for avg case Does not need additional memory, i.e. In-place processing/sorting Disadvantages: Worst case complexity O(n^2) O(n^2) speed is not guaranteed","title":"5. Quick Sort Recursive(arr, left, right):"},{"location":"Computer-Science/algo/#51-quick-sort-iterative","text":"Description: Use tha same Partition technique Instead of re-calling the quick_sort function, use stack to keep track of left and right indexes of the sub-arrays (LHS & RHS of partition_idx) found after partition. Keep doing this while stack is not empty. At the end partition() will arrange all the elements at their perfect index.","title":"5.1 Quick Sort Iterative:"},{"location":"Computer-Science/algo/#6-heap-sort","text":"Desc: Create a max-heap: Using HEAPIFY (aka BUILD_<MIN/MAX>_HEAP . To do so, start from last leaf and make max heap till root node. EXTRACT (Get and Delete) the Top/Root nodes from the heap one by one to get sorted items DELETE the top/root node Fill the empty root position by Swapping the last leaf with root node in max heap PERCOLATE_DOWN the (newly added) root node (by ignoring nodes added in step #3) till the loop covers all the nodes (no need to do anything with the last node in the heap) While doing step #2, why not add those items/nodes back again to the same array (start at the end, in right to left direction) so that we can avoid an auxilary space and utilize the same array (to store items in sorted order) Note: while doing step #2, make sure we're not considering the nodes added during step #3 Pre-requisites: Useful: When there is time (Quick's problem) and space (Merge's problem) bound Advantage: Worst case upper bound is O(nlogn) with only O(1) auxilary space Applications: Sort a nearly sorted (or K sorted) array k largest(or smallest) elements in an array Time Complexity: O(heapify) * O(n) = O(nlogn) O(heapify) * O(n) = O(nlogn) Auxilary Space: O(1) O(1) In-Place: Yes Implementation: Recursive (Heapify) Algorithm Paradigm: Comparision Based Data Structure: Array + Complete Binary Tree Stable: Not in general Note: Quick & Merge are better in practice.","title":"6. Heap Sort"},{"location":"Computer-Science/algo/#7-bucket-sort","text":"Pre-requisites: Standard: A uniform distributed input array in a range of [0,1) [0,1) . CLRS. Generalized: A uniform distributed input array in a range of non negative integers + floats. [0, k) [0, k) . Efficient Hash Function (specially in case of \"Generalized\" implementation. Desc: Hashing: hash_table_size or number of buckets: = size of input array; Standard OR = int(sqrt(size)) int(sqrt(size)) ; Generalized hash_func() = (element/MAX)* (hash_table_size) Condition: if i < k i < k then hash(i) < hash(k) hash(i) < hash(k) Partion inp array on the basis of hash function, store then in right bucket/array Sort each array using Insertionsort Merge all sorted arrays into one. Useful: When input is uniformly distributed over a positive range Advantage: Sorting in O(n) O(n) Applications: When input is uniformly distributed over a positive range Recurrence Equation: \\Theta(n) + nO(2 - 1/n) \\Theta(n) + nO(2 - 1/n) Time Complexity: Best: \\Omega(n) \\Omega(n) Avg: \\Theta(n) \\Theta(n) Worst: \\Theta(n^2) \\Theta(n^2) ; When all the elements fall under single bucket Auxilary Space: O(bucket size) == O(n) O(bucket size) == O(n) In-Place: No Implementation: Iterative Algorithm Paradigm: Hashing, Partion Data Structure: Hashtable, Array Stable: Yes Note: If input is not uniformally distributed, but also bucketsort may still run in linear time. CLRS. Why Insertion sort is used here? How overall time complexity of bucketsort is still O(n) then? Following Standard way: As, input is uniformly distributed, On avg each bucket/array will have 1 elements (k/k=1) (k/k=1) , some may have zero and some may have multiple with same value. And as insertionsort's best case is O(n) O(n) (if array size is 1). Hence overall its O(n) O(n)","title":"7. Bucket Sort"},{"location":"Computer-Science/algo/#8-counting-sort","text":"Pre-requisites (Standard): Elements should be Non Negative Integers Over a range of 0 to k where k < size of array to maintain O(n) O(n) Desc: For each element X X in the input array find the number of elements smaller than X X . Steps: Store counts of each element in a counting array Add previous count to current count, to find index of last occurence of that element Reverse Iterate over input array & pick index of the element from counting array Put the element in output array and decrement the count by 1 Useful: same as pre-requisites Advantage: Sorting in O(n + k) O(n + k) Applications: Same as pre-requisites As a subroutine in Radix Sort Time Complexity: Best: \\Omega(n + k) \\Omega(n + k) Avg: \\Theta(n + k) \\Theta(n + k) Worst: O(n + k) O(n + k) Auxilary Space: O(counting + output array) == O(n + k) O(counting + output array) == O(n + k) In-Place: No Implementation: Iterative Algorithm Paradigm: Partial Hashing Data Structure: Hashtable, Array Stable: Yes (order of elements with same value in input array maintains same order in output) Comparion Sort: No Note: Can be extended to sort negative integers also","title":"8. Counting Sort"},{"location":"Computer-Science/algo/#9-radix-sort","text":"Pre-requisites (Standard): input array have non negative integers range should be 0 0 to n^c n^c where c c is some constant & numbers are represented in base n n or each number takes only log_2n log_2n bits Desc: for 1 1 to d d : where d is most significant digit position of MAX element in inp array do counting sort on array (considering current digit of each iteration) Useful: same as prerequisites Advantage: Better worst case performance than bucket sort's Worst case Applications: Card sorting machine Recurrence Equation: n*O(n + k) == O(n + k) n*O(n + k) == O(n + k) Time Complexity: Best Case: \\Omega(n + k) \\Omega(n + k) Avg Case: \\Theta(n + k) \\Theta(n + k) Worst Case: O(n + k) O(n + k) Auxilary Space: d*O(counting array + output array) = d*O(n + k) d*O(counting array + output array) = d*O(n + k) In-Place: No Implementation: Iterative Algorithm Paradigm: Partial Hashing Data Structure: Hashtable, array Stable: Yes Comparion Sort: No Note:","title":"9. Radix Sort."},{"location":"Computer-Science/algo/#10-tim-sort","text":"","title":"10. Tim sort"},{"location":"Computer-Science/algo/#quick-sort-vs-merge-sort","text":"Merge sort is preferred over Quick sort when: Sorting a linkedlist: Quick sort is preferred over Merge sort when: There is memory limitation","title":"Quick Sort Vs Merge Sort"},{"location":"Computer-Science/algo/#searching-algorithms","text":"","title":"Searching Algorithms"},{"location":"Computer-Science/algo/#types_1","text":"","title":"Types"},{"location":"Computer-Science/algo/#1-linear","text":"","title":"1. Linear"},{"location":"Computer-Science/algo/#2-binary","text":"Pre-requisites: Sorted array of numbers (integers/floats) Method : Divide and search (conquer) Equation: T(n) = T(n/2) + O(1) T(n) = T(n/2) + O(1) Time Complexity: O(log n) O(log n) Implementation: Recursive Auxilary Space: O(log n) O(log n) recursion call stack space Iterative Auxilary Space: O(1) O(1)","title":"2. Binary"},{"location":"Computer-Science/algo/#3-jump","text":"Pre-requisites: Sorted array of numbers Desc: Enhancement over linear search. Jumps a step (of m m ) --by index-- and search, if it was searching value 52 and found 55 then will jump a step back and start linear search from that index. Similar to linear search. Recurrence Equation: Total number of comparision in worst case: (n/m) + (m-1) (n/m) + (m-1) i.e. (total number of jumps + linear search within one step) Best value of m = \\sqrt{n} m = \\sqrt{n} Time Complexity: O(\\sqrt{n}) O(\\sqrt{n}) i.e. between O(linear) O(linear) & O(binary) O(binary) Auxiliary Space : O(1) O(1)","title":"3. Jump"},{"location":"Computer-Science/algo/#4-interpolation","text":"Pre-requisites: Sorted array Desc: Enhancement over Binary search. Enhancement: Pivot is calculated based on interpolation, i.e. Pivot = left + \\frac{(right - left)} { (arr[right] - arr[left]) (e - arr[left])} Pivot = left + \\frac{(right - left)} { (arr[right] - arr[left]) (e - arr[left])} Time Complexity: O(log log n) O(log log n) (if elements are uniformaly distributed) Worst case: O(n) O(n) Auxilary space: O(1) O(1)","title":"4. Interpolation"},{"location":"Computer-Science/algo/#5-exponential","text":"Pre-requisites: Sorted array Desc: Enhancement over binary search. Enhancement: Find a range where element is present, and then do binary search found range. Range: Start with subarray of size 1 and chech if last element of subarray is greater than 'e'. If not, then increment size by two times Equation: Time Complexity: O(log n) O(log n) Auxilary Space: O(1) O(1) (if used iterative binary search) Advantage: Better to use if array is infinite (Unbounded Searches/ Unbounded Binary Search)","title":"5. Exponential"},{"location":"Computer-Science/algo/#6-ternary","text":"Pre-requisites: Sorted array Desc: Divide and Conquer + Linear search Same as binary search. When searching space gets shorter, use linear search according to precision = right-left Time Complexity: O(log_3 n) O(log_3 n) Auxilary Space: Iterative = O(1) O(1) Recursive = O(log_3 n) O(log_3 n) recursive stack space Application: Unimodel Functions","title":"6.  Ternary"},{"location":"Computer-Science/algo/#linkedlist-algorithms","text":"","title":"Linkedlist Algorithms"},{"location":"Computer-Science/algo/#remove-duplicates","text":"Remove duplicate nodes from an unsorted linkedlist (w/ auxilary space) Remove duplicate nodes from an unsorted linkedlist (w/o auxilary space)","title":"Remove duplicates"},{"location":"Computer-Science/algo/#reverse-the-linkedlist","text":"Reverse a singly linkedlist Reverse a portion of a singly linkedlist Reverse a doubly linkedlist Reverse a portion of a doubly linkedlist","title":"Reverse the Linkedlist"},{"location":"Computer-Science/algo/#cycle-loop-detection-algorithms","text":"Using Hashing/Mapping address of each visited nodes and lookup the map if current node is already mapped there By pointing all the visited nodes to A Dummy Node and check if current node already points to the Dummy node By adding a flag to each visited nodes and lookup the flag off current node if it is already flagged as visited - applicable only if the nodes are modifiable By assigning a special value to each visited nodes and lookup the value of the current node if it is already assigned that special value - applicable only if the range of the node values are given Using Floyd's Cycle Detection Algorithm","title":"Cycle / Loop Detection Algorithms"},{"location":"Computer-Science/algo/#floyds-cycle-detection-algorithm","text":"1 2 3 4 5 <------------------- S -------------------> <---- X ------- [a]-->[i]-->[j]-->[k]-->[l]-->[m]-------\u2192 [b] ------\u2192 [d] | \u2191 | | | \u2193 | [e] \u2190------ [c] \u2193 A. If a loop is suspected in a linkedlist, then run two pointers - one slow & another fast - starting from the head ( a a ). Run the faster pointer double the speed of the slower one. If they meet at some node ( c c ), then the loop exists & else not. B. If a loop has been detected using #A, then idetifying the start of the loop and removing the loop is also possible.","title":"Floyd's Cycle Detection Algorithm"},{"location":"Computer-Science/algo/#axiom-a","text":"If in a loop we're at some point i i , and if we finish K K complete (whole number) cycles/rounds of the loop, we'll be back to the same point i i . P_i \\cong P_i + K.L P_i \\cong P_i + K.L where: P_i P_i is a pointer at some i^{th} i^{th} node in the loop L L is the length (or number of nodes) of the loop K K is some non-negative integer","title":"Axiom A"},{"location":"Computer-Science/algo/#lemma-a","text":"If pointer P_s P_s and P_f P_f starts from the head ( a a ) of the linkedlist. P_s P_s moves 1 node at a time & P_f P_f moves 2 nodes at a time. Then the hypothesis is they will meet at some node ( c c ).","title":"Lemma A"},{"location":"Computer-Science/algo/#proof","text":"Let's suppose they met X X unit far from the start node ( b b ) of the loop. Before they met each other, P_s P_s might have completed K_1 K_1 whole rounds of the loop and P_f P_f might have completed K_2 K_2 whole rounds. So, D_s = S + K_1.L + X D_s = S + K_1.L + X - where K_1 K_1 is a whole number D_f = S + K_2.L + X D_f = S + K_2.L + X - where K_2 K_2 is a whole number Also to note that, the speed of the pointer P_f P_f is twice the speed of P_s P_s , and they are moving for the same time interval. So, suppose D_s = D D_s = D , then D_f = 2D D_f = 2D . Then we can derive the following relation: \\frac{ \\begin{array}{l,c,l} +(2D & = & S + K_2.L + X) \\\\ -(D & = & S + K_1.L + X) \\end{array} }{ \\begin{array}{r,c,l} D & = & (K_2 - K_1).L \\\\ \\therefore D & = & K.L \\end{array} } \\frac{ \\begin{array}{l,c,l} +(2D & = & S + K_2.L + X) \\\\ -(D & = & S + K_1.L + X) \\end{array} }{ \\begin{array}{r,c,l} D & = & (K_2 - K_1).L \\\\ \\therefore D & = & K.L \\end{array} } This equation must holds true for some appropriate values of D D & K K . Thus we can also conclude that our hypothesis that, both slow & fast pointer will meet at some point, is true as well.","title":"Proof"},{"location":"Computer-Science/algo/#corollary-inference-a1","text":"By Lemma-A, as we know that the speed of the pointer P_f P_f is twice the speed of P_s P_s , and they were moving for the same time interval. So, 2.S_s = S_f 2.S_s = S_f \\therefore 2.\\frac{D_s}{T} = \\frac{D_f}{T} \\therefore 2.\\frac{D_s}{T} = \\frac{D_f}{T} \\therefore 2.D_s = D_f \\therefore 2.D_s = D_f \\therefore 2(S + K_1.L + X) = S + K_2.L + X \\therefore 2(S + K_1.L + X) = S + K_2.L + X \\therefore 2S + 2K_1.L + 2X = S + K_2.L + X \\therefore 2S + 2K_1.L + 2X = S + K_2.L + X \\therefore S + X = K_2.L - 2K_1.L \\therefore S + X = K_2.L - 2K_1.L \\therefore S + X = (K_2 - 2K_1).L \\therefore S + X = (K_2 - 2K_1).L \\therefore S = K.L - X \\therefore S = K.L - X By this we can say that the moving K.L - X K.L - X units from the meeting point c c , within the loop, is exactly equal to S S , for some appropriate value of S, K, X S, K, X . (just remember this relationship)","title":"Corollary / Inference A.1"},{"location":"Computer-Science/algo/#lemma-b","text":"If move a pointer from the head a a of the linkedlist, and move another pointer from the meeting point c c , simultaniously, at the same speed, then the hypothesis is both the pointer will meet at the start node b b of the loop. (Note that both the pointers are moving in opposite direction towards each other, thus they are going to meet each other at some node.) In another word, the hypothesis is, the start node b b of the loop could be reached from the meeting point c c if we move a pointer S S units from the meeting point c c .","title":"Lemma B"},{"location":"Computer-Science/algo/#proof_1","text":"We know that both the pointers (slow an fast) met at node c c (meeting point) previously as per Lemma-A. Now, assume that on moving these two new pointers - one starts from head a a & another starts from meeting point c c - meets at the start node ( b b ) of the loop. That means P_s P_s reaches the start node b b by travelling D_s D_s distance from the head. That is S S units. And, P_f P_f reaches the start node b b by travelling D_f D_f distance from the meeting point c c . That is L - X L - X units. But, it is also possible that P_f P_f might have travelled some cycles of the loop before meeting P_s P_s . Lets assume that P_f P_f might have travelled J_1 J_1 cycles of the loop. So, P_f P_f reaches the start node b b by travelling J_1.L + (L - X) J_1.L + (L - X) units in total. i.e. (J_1 + 1).L - X (J_1 + 1).L - X As the speed of both the pointers were same & distance were travelled within the same time interval, we can relate it like: S_s = S_f S_s = S_f \\therefore \\frac{D_s}{T} = \\frac{D_f}{T} \\therefore \\frac{D_s}{T} = \\frac{D_f}{T} \\therefore D_s = D_f \\therefore D_s = D_f \\therefore S = (J_1 + 1).L - X \\therefore S = (J_1 + 1).L - X \\therefore S = J.L - X \\therefore S = J.L - X With the help of Lemma-A & Inference-A.1 we can conclude that this relation S = J.L - X S = J.L - X holds true, thus our hypothesis is true as well.","title":"Proof"},{"location":"Computer-Science/algo/#applications","text":"helpful in discovering infinite loop in a computer program more","title":"Applications"},{"location":"Computer-Science/algo/#pseudo-code","text":"Input := L L : a singly linkedlist Output := bool : Loop detected, L^{'} L^{'} : the same linkedlist with loop removed (if any) if L == \\varnothing L == \\varnothing ; then return ( False, L False, L ) if L.Head.Next == \\varnothing L.Head.Next == \\varnothing ; then return ( False, L False, L ) if L.Head.Next == L.Head L.Head.Next == L.Head ; then L.Head.Next = \\varnothing L.Head.Next = \\varnothing return ( True, L True, L ) P_s = L.Head P_s = L.Head ; P_f = L.Head P_f = L.Head meetingPoint = \\varnothing meetingPoint = \\varnothing while True True if P_s.Next == P_f.Next.Next P_s.Next == P_f.Next.Next ; then meetingPoint = P_f.Next meetingPoint = P_f.Next break if P_f.Next == \\varnothing P_f.Next == \\varnothing || P_f.Next.Next == \\varnothing P_f.Next.Next == \\varnothing ; then return ( False, L False, L ) P_s = P_s.Next P_s = P_s.Next P_f = P_f.Next.Next P_f = P_f.Next.Next if meetingPoint == L.Head meetingPoint == L.Head ; then meetingPoint.Next = \\varnothing meetingPoint.Next = \\varnothing P_f.Next = \\varnothing P_f.Next = \\varnothing return ( True, L True, L ) P_s = L.Head; P_f = meetingPoint P_s = L.Head; P_f = meetingPoint while True True if P_s.Next == P_f.Next P_s.Next == P_f.Next ; then break P_s = P_s.Next P_s = P_s.Next P_f = P_f.Next P_f = P_f.Next P_f.Next = \\varnothing P_f.Next = \\varnothing return ( True, L True, L )","title":"Pseudo Code"},{"location":"Computer-Science/algo/#analysis","text":"Time Complexity: Worst case: O(S + K.L + X) + O(S) = O(N) O(S + K.L + X) + O(S) = O(N) ; when S >> L S >> L Avg case: O(S + K.L + X) + O(S) = O(N) O(S + K.L + X) + O(S) = O(N) ; Best case: O(S + K.L + X) + O(S) = O(1) O(S + K.L + X) + O(S) = O(1) ; when S=0; X=0 L = 1 S=0; X=0 L = 1 Space Complexity: O(1) O(1)","title":"Analysis"},{"location":"Computer-Science/algo/#tree-algorithms","text":"","title":"Tree Algorithms"},{"location":"Computer-Science/algo/#depth-first-search-dfs","text":"","title":"Depth First Search (DFS)"},{"location":"Computer-Science/algo/#breadth-first-search-bfs-level-order-traversal","text":"","title":"Breadth First Search (BFS) / Level order traversal"},{"location":"Computer-Science/algo/#heap-algorithms","text":"","title":"Heap Algorithms"},{"location":"Computer-Science/algo/#heap-implemented-using-complete-binary-tree","text":"Basic Insert Top Extract Top Replace Internal percolate-down percolate-up heapify Creation Build","title":"Heap implemented using (Complete) Binary Tree"},{"location":"Computer-Science/algo/#heapify","text":"","title":"heapify()"},{"location":"Computer-Science/algo/#pseudo-code_1","text":"input = an array arr arr of numbers output = array ordered to follow min-heap properties return = none 1 2 for i in range ( arr . length - 1 to 0 ): percolate_down ( arr , i )","title":"Pseudo Code"},{"location":"Computer-Science/algo/#analysis_1","text":"","title":"Analysis"},{"location":"Computer-Science/algo/#asymptotically","text":"T(n) = n * O(log_2{n}) T(n) = n * O(log_2{n}) T(n) = O(nlog_2{n}) T(n) = O(nlog_2{n})","title":"Asymptotically"},{"location":"Computer-Science/algo/#amortized","text":"Worst case running time of heapify() heapify() = percolate down all the nodes at last/leaf ( h h ) level to 0 steps/levels (note: level start from top, i.e. 0 and, h h is total height of the heap) + percolate down all the nodes at second last ( h-1 h-1 ) level to 1 steps/levels + percolate down all the nodes at third last ( h-2 h-2 ) level to 2 steps/levels + ... + percolate down all the nodes at h - i h - i level to i i steps/levels + ... + percolate down all the nodes at zeroth ( 0 0 ) level to h h steps/levels i.e. T(n) = \\sum_{i=1}^{h} T(n) = \\sum_{i=1}^{h} percolate down all the nodes at level= h-i h-i to i i steps/levels \\therefore T(n) = \\sum_{i=0}^{h} \\therefore T(n) = \\sum_{i=0}^{h} (number of nodes at level h - i h - i to percolate down) \\times \\times (cost of percolate down a node to i i levels) \\therefore T(n) = \\sum_{i=0}^{h} 2^{h-i} \\times O(i) \\therefore T(n) = \\sum_{i=0}^{h} 2^{h-i} \\times O(i) \\because \\because number of nodes a level x = 2^{x} x = 2^{x} \\therefore T(n) = \\sum_{i=0}^{\\log_2{n}} 2^{h-i} \\times i \\therefore T(n) = \\sum_{i=0}^{\\log_2{n}} 2^{h-i} \\times i \\because \\because height h h of a heap = \\log_2{n} \\log_2{n} \\therefore T(n) = \\sum_{i=0}^{\\log_2{n}} 2^{\\log_2{n}-i} \\times i \\therefore T(n) = \\sum_{i=0}^{\\log_2{n}} 2^{\\log_2{n}-i} \\times i \\therefore T(n) = \\sum_{i=0}^{\\log_2{n}} \\frac{2^{\\log_2{n}}}{2^i} \\times i \\therefore T(n) = \\sum_{i=0}^{\\log_2{n}} \\frac{2^{\\log_2{n}}}{2^i} \\times i \\therefore T(n) = \\sum_{i=0}^{\\log_2{n}} \\frac{n}{2^i} \\times i \\therefore T(n) = \\sum_{i=0}^{\\log_2{n}} \\frac{n}{2^i} \\times i \\therefore T(n) = n \\times \\sum_{i=0}^{\\log_2{n}} \\frac{i}{2^i} \\therefore T(n) = n \\times \\sum_{i=0}^{\\log_2{n}} \\frac{i}{2^i} \\therefore T(n) \\le n \\times 2 \\because \\sum_{i=0}^{\\infty} \\frac{i}{2^i} = 2 \\therefore T(n) \\le n \\times 2 \\because \\sum_{i=0}^{\\infty} \\frac{i}{2^i} = 2 (upper bound) \\therefore T(n) \\le O(n) \\therefore T(n) \\le O(n)","title":"Amortized"},{"location":"Computer-Science/algo/#percolatedown","text":"","title":"percolateDown()"},{"location":"Computer-Science/algo/#pseudo-code_2","text":"input = an array arr arr of numbers, some index i i (index starting at 0 0 ) output = item at index i i in the array gets repositioned to follow min-heap properties return = none if the node at given index i i is a leaf node or non-existing node return leftChildPos := 2*i + 1 leftChildPos := 2*i + 1 rightChildPos := 2*i + 2 rightChildPos := 2*i + 2 minimumNodePos := i minimumNodePos := i if leftChildPos leftChildPos exists and arr[leftChildPos] \\lt arr[i] arr[leftChildPos] \\lt arr[i] : minimumNodePos := leftChildPos minimumNodePos := leftChildPos if rightChildPos rightChildPos exists and arr[rightChildPos] \\lt arr[i] arr[rightChildPos] \\lt arr[i] : minimumNodePos := rightChildPos minimumNodePos := rightChildPos if i i and minimumNodePos minimumNodePos are not same: swap item at index minimumNodePos minimumNodePos and i i percolateDown(arr, minimumNodePos) percolateDown(arr, minimumNodePos)","title":"Pseudo Code"},{"location":"Computer-Science/algo/#analysis_2","text":"At worst, a node might have to reposition/move from root to leaf position. Saying that, it might need tree/heap height / level steps i.e. \\log_2{n} \\log_2{n} . Where n n is number of nodes in the heap.","title":"Analysis"},{"location":"Computer-Science/algo/#recurrence-relation_1","text":"T(n) = T(n/2) + C T(n) = T(n/2) + C i.e. size of the problem is reducing by half (no. of nodes at a left or right sub-tree) each time. By substitution method: \\frac{ \\begin{array}{l,c,l} T(n) = T(n/2) + C \\\\ T(n/2) = T(n/2^2) + C \\\\ T(n/4) = T(n/2^3) + C \\\\ ... \\\\ T(1) = 1 \\\\ T(0) = 1 \\\\ \\end{array} }{ \\begin{array}{r,c,l} T(n) = T(n/2^k) + k*C \\end{array} } \\frac{ \\begin{array}{l,c,l} T(n) = T(n/2) + C \\\\ T(n/2) = T(n/2^2) + C \\\\ T(n/4) = T(n/2^3) + C \\\\ ... \\\\ T(1) = 1 \\\\ T(0) = 1 \\\\ \\end{array} }{ \\begin{array}{r,c,l} T(n) = T(n/2^k) + k*C \\end{array} } Suppose for some k k , n/2^k n/2^k becomes 1 so T(n/2^k) = 1 T(n/2^k) = 1 i.e. T(1) = 1 T(1) = 1 . Thus, n/2^k =1 n/2^k =1 n = 2^k n = 2^k log_2{n} = log_2{2^k} log_2{n} = log_2{2^k} log_2{n} = k log_2{n} = k i.e. k = log_2{n} k = log_2{n} by putting the value of k k in form of n n T(n) = T(1) + log_2{n}*C T(n) = T(1) + log_2{n}*C T(n) = 1 + log_2{n}*C T(n) = 1 + log_2{n}*C T(n) = O(1) + O(log_2{n}) T(n) = O(1) + O(log_2{n}) T(n) = O(log_2{n}) T(n) = O(log_2{n})","title":"Recurrence relation"},{"location":"Computer-Science/algo/#graph-algorithms","text":"","title":"Graph Algorithms"},{"location":"Computer-Science/algo/#depth-first-search-dfs_1","text":"DFS works in similar manner as pre-order travesal of a tree.","title":"Depth First Search (DFS)"},{"location":"Computer-Science/algo/#idea","text":"The idea is to randomly select a starting vertex and from there start traversing other vertices such that we go through a path at a time till its end depth and start printing the vertices from there while back tracking. Do the same for the rest of the untraversed/unvisited/unprinted paths/vertices. Imagine a person trying to figure out escape a maze. Trying to explore a path at once till its end (depth).","title":"Idea"},{"location":"Computer-Science/algo/#applications_1","text":"Finding the path To check if the graph is bipartite To detect cycles in the graph Topological sort Solving puzzles as maze Finding connected components Finding strongly connected components Finding \"cut vertices\"","title":"Applications"},{"location":"Computer-Science/algo/#implementation-standard-recursive","text":"Desc: TODO Approach: Recursive DS Used: (Internally Stack - for recursive function calls) Time Complexity: Best: O(V + E) if used adjacency list; O(V x V) if used adjacency matrix Avg: same Worst: same Auxilary Space: Best: O(V) (to track the visited/unvisited vertices) Avg: same Worst: same Disadvantage: TODO","title":"Implementation - Standard (Recursive)"},{"location":"Computer-Science/algo/#properties","text":"TODO","title":"Properties"},{"location":"Computer-Science/algo/#pseudo-code_3","text":"pick/choose a vertex as starting point - randomly find it adjacent vertices mark the the choosen vertex as visited & print it pick one of the adjacent vertices - randomly repeat [2-4] for this node as well; return repeat [4-5] for rest of the unvisited adjacent vertices","title":"Pseudo Code"},{"location":"Computer-Science/algo/#implementation-using-stack-iterative-which-is-same-as-recursive-one","text":"Desc: TODO Approach: Iterative DS Used: Stack Time Complexity: Best: O(V + E) if used adjacency list; O(V x V) if used adjacency matrix Avg: same Worst: same Auxilary Space: Best: O(V) (to track the visited/unvisited vertices + size of Stack) Avg: same Worst: same Disadvantage: TODO","title":"Implementation Using Stack (Iterative, Which is same as Recursive one)"},{"location":"Computer-Science/algo/#pseudo-code_4","text":"mark all the vertices as unvisited pick/choose a vertex as starting point - randomly put that into a stack; mark that as visited; print the vertex find any one of its (the vertex we printed recently) unvisited adjacent vertices put that into the stack; mark that as visited; print the vertex repeat [4-5] until we reach a vertex from where we can't move forward (i.e. it has no adjacent vertices or has no unvisited adjacent vertices) now start back tracking (using the stack) from current vertex pop the top element from the stack go back to that vertex repeat [4-6] for that vertex we may have some unvisited / disconncted graph (vertices) at this moment so, iterate through all the unvisited vertices (if any) repeat [2-6] for them","title":"Pseudo Code"},{"location":"Computer-Science/algo/#dfs-tree","text":"TODO","title":"DFS Tree"},{"location":"Computer-Science/algo/#breadth-first-search-bfs-level-order-traversal_1","text":"BFS works in similar manner as level-order travesal of a tree.","title":"Breadth First Search (BFS) / Level order traversal"},{"location":"Computer-Science/algo/#idea_1","text":"The idea is to randomly select a starting vertex and from there start traversing other vertices such that we visit the adjacent vertices first. Once we visited one level (adjacent vertices at a level), then find adjacent vertices of the previously visited ones, and repeat the step until we visited all the vertices.","title":"Idea"},{"location":"Computer-Science/algo/#applications_2","text":"Finding the path To check if the graph is bipartite To find the shortest path between two vetices Finding connected components","title":"Applications"},{"location":"Computer-Science/algo/#properties_1","text":"TODO","title":"Properties"},{"location":"Computer-Science/algo/#implementation-using-queue-iterative","text":"Desc: TODO Approach: Iterative DS Used: Queue Time Complexity: Best: O(V + E) if used adjacency list; O(V x V) if used adjacency matrix Avg: same Worst: same Auxilary Space: Best: O(V) (to track the visited/unvisited vertices + size of Queue) Avg: same Worst: same Disadvantage: TODO","title":"Implementation Using Queue (Iterative)"},{"location":"Computer-Science/algo/#pseudo-code_5","text":"mark all the vertices as unvisited pick/choose a vertex as starting point - randomly put it into a queue mark this as visited dequeue and get the item from the queue print the vertex find/explore its unvisited adjacent vertices mark all of them as visited put them into the queue repeat [3-5] until the queue is empty we may have some unvisited / disconncted graph (vertices) at this moment so, iterate through all the unvisited vertices (if any) repeat [2-6] for them","title":"Pseudo Code"},{"location":"Computer-Science/algo/#topological-sort","text":"Topological sort, is an ordering of vertices of a DAG in which each vertices[1] comes before, all vertices [2], to which[2] it[1] has outgoing edges. Saying that, [1] will come before [2], if there is an edge from [1] to [2] Note: There is no solution if the graph is a) Undirected, or b) Directed with cycle(s) - it will cause a deadlock.","title":"Topological Sort"},{"location":"Computer-Science/algo/#idea_2","text":"Topological sort, arranges the vertices of a DAG in an order of their dependecies in other vertices. Meaning that, A vertex which is not dependent on (i.e. no edge incidents to it) will be first in the list, and A vertex which is dependent on most of the vertices (or a series of dependecies has to be fullfilled before it) has to be at the end of the sorted list.","title":"Idea"},{"location":"Computer-Science/algo/#applications_3","text":"To execute some inter-dependent tasks/jobs Run pipeline of computing jobs To implement/evaluate formulae in Speadsheet cells serialization & deserialization To detect dead locks To check symbolic link loop (deadlock)","title":"Applications"},{"location":"Computer-Science/algo/#properties_2","text":"A DAG may have one or more Topological Order If all the consecutive vertices in a topological order, are connected by edges, then these edges forms Hamiltonian path If the Hamiltonian path exists, then the topological order is unique; else the DAG can have two or more topological order","title":"Properties"},{"location":"Computer-Science/algo/#implementation-using-stack-dfs","text":"Desc: TODO Approach: Iterative+Recursive DS Used: Stack Time Complexity: Best: O(V + E) if used adjacency list; O(V x V) if used adjacency matrix Avg: same Worst: same Auxilary Space: Best: O(V) (to track the visited/unvisited vertices + size of Stack) Avg: same Worst: same Disadvantage: TODO","title":"Implementation Using Stack / DFS"},{"location":"Computer-Science/algo/#pseudo-code_6","text":"start [perform a tweaked version of DFS of the graph] mark all the vertices unvisited create two stacks (backtrack-stack & topo-stack) to hold |V| number of vertices in each (one to help backtrack, another for topological order) arbitrarily choose a vertex as root to start DFS push that vertex into the backtrack-stack mark that vertex as visited find one of the unvisited adjacent vertices put that unvisted adjacent vertex into the backtrack-stack mark that vertex as visited repeat [7-9] for the current vertex if there is no way further (i.e. no unvisited adjacent vertices); then pop the top element from the backtrack-stack push that popped vertex to the topo-stack go to that popped vertex while backtrack-stack is not empty; repeat [7-14] we may have some unvisited / disconncted graph (vertices) at this moment so, iterate through all the unvisited vertices (if any) repeat [4-15] for them while topo-stack is not empty; pop the top element from the stack and print them end","title":"Pseudo Code"},{"location":"Computer-Science/algo/#implementation-using-queue-in-degree","text":"Desc: TODO Approach: Iterative DS Used: Queue Time Complexity: Best: O(V + E) if used adjacency list; O(V x V) if used adjacency matrix Avg: same Worst: same Auxilary Space: Best: O(V) (to track the visited/unvisited vertices + size of Queue) Avg: same Worst: same Disadvantage: TODO","title":"Implementation Using Queue / In-degree"},{"location":"Computer-Science/algo/#pseudo-code_7","text":"start traverse the graph (adjacency matrix/list) & calculate the in-degree of every vertices maintain an array to store in-degree of each vertices traverse the adjacency matrix/list increment the in-degree value corresponding to vertex if a new edge is known to incident on that if there are vertices with in-degree zero then goto step [4]; else goto step [10] create a queue & enqueue all those vertices with in-degree = 0 dequeue an element from the queue print that vertex find all the adjacent vertices of that vertex and decrement their in-degree if the in-degree of any of those adjacent vertices becomes zero, enqueue them while the queue is not empty; repeat [5-8] end","title":"Pseudo Code"},{"location":"Computer-Science/algo/#detect-cycle-in-graph","text":"Using DFS by maintaining the immediate call stack (i.e. before any backtrack) a.k.a. detecting back-edge Using BFS by maintaining 3 flags/colors/sets Using BFS by decreasing In-Degree of the nodes","title":"Detect Cycle in Graph"},{"location":"Computer-Science/algo/#single-source-shortest-path-in-unweighted-graph","text":"Given a unweighted graph (directed/undirected) G = (V, E) , and a source/distinguished vertex S find the shortest path from S to every other vertices in G .","title":"Single-Source Shortest Path in Unweighted Graph"},{"location":"Computer-Science/algo/#idea_3","text":"If the given graph is undirected, treat is as a directed graph by replacing an undirected edge with two directed edges. Cycles may exist in the graph. As the graph is unweighted, we can consider the cost of a path between 2 adjacent vertices are either zero or equal (suppose 1) - apply the same for all the V in the G . Start traversing the graph starting from S in level-order (BFS fashion), meaning explore all the adjacent vertices of the given vertex, then explore the adjacent vertices of them - while doing so, increment the distance of the adjacent vertices (from its parents) by 1. Interestingly - we'll be calculating the distance of each vertices only once (keep track of visited vertices), thus we'll not encounter such condition where we'll be updating/overwriting the existing distance, and we also don't need to bother about checking if the newly calculated distance is lesser than the existing one. Why so? Why the calculated distance/path will be the shortest? Think we started traversing from S parallely (round-robin?) . And a Queue & BFS helped us to achieve so. This is possible because of the fact that - the distance got calculated by traversing a path - which was shortest among others (think level order traversing) thats why this path brought us to that vertext first (among others) and we calculated the distance. There could be multiple shortest path as well. At the end, all the vertices will have shortest distance calculated from the vertex S .","title":"Idea"},{"location":"Computer-Science/algo/#applications_4","text":"finding fastest way to go from one place to another","title":"Applications"},{"location":"Computer-Science/algo/#properties_3","text":"","title":"Properties"},{"location":"Computer-Science/algo/#implementation-using-queue-bfs","text":"Desc: Approach: Iterative / Recursive DS Used: 1 Queue, 2 Arrays Time Complexity: O(V+E) if adjacency list is used; O(VxV) if adjacency matrix is used Best: -- Avg: -- Worst: -- Auxilary Space: O(V) Best: -- Avg: -- Worst: -- Disadvantage: can't be applied on a weighted graph","title":"Implementation Using Queue / BFS"},{"location":"Computer-Science/algo/#pseudo-code_8","text":"start create a queue create an array to store distances of the vertices from source S vertex initialize the distance array d with -1 (or INF infinite) mark all the vertices unvisited set the distance from S to S zero enqueue the source vertex S mark that vertex as visited dequeue an element explore the adjacent vertices of that vertex ( u - dequeued vertex) set the distance of the adjacent vertices to: d( u ) + 1 enqueue them mark them as visited while queue is not empty; repeat [9-13] end","title":"Pseudo Code"},{"location":"Computer-Science/algo/#single-source-shortest-path-in-weighted-directed-acyclic-graph-dag","text":"","title":"Single-Source Shortest Path in Weighted Directed Acyclic Graph (DAG)"},{"location":"Computer-Science/algo/#idea_4","text":"","title":"Idea"},{"location":"Computer-Science/algo/#applications_5","text":"","title":"Applications"},{"location":"Computer-Science/algo/#properties_4","text":"","title":"Properties"},{"location":"Computer-Science/algo/#implementation-using-stack-dfs-topological-sort","text":"Desc: Approach: Topological Sort DS Used: Time Complexity: Best: Avg: \\Theta(V + E) \\Theta(V + E) Worst: Auxilary Space: Best: Avg: Worst: Disadvantage:","title":"Implementation Using Stack / DFS (Topological Sort)"},{"location":"Computer-Science/algo/#pseudo-code_9","text":"","title":"Pseudo Code"},{"location":"Computer-Science/algo/#optimization","text":"","title":"Optimization"},{"location":"Computer-Science/algo/#dijkstras-algorithm-single-source-shortest-path-in-weighted-non-negative-graph","text":"Given a weighted graph (directed/undirected) G = (V, E) G = (V, E) , and a source/distinguished vertex s s find the shortest path from s s to every other vertices in G G .","title":"Dijkstra's Algorithm [Single-Source Shortest Path in Weighted (non-negative) Graph]"},{"location":"Computer-Science/algo/#idea_5","text":"If the given graph is undirected, treat it as a directed graph by replacing an undirected edge with two directed edges. And assigning the same given weight to both the edges. Cycles may exist in the graph. As the given graph is weighted in nature, simply following BFS and incrementing the distance (or just keep adding the prev. distance + weight) of a vertex may NOT lead us to find an optimal solution. Why so? Why wouldn't the same method help us here, as it did in the case of unweighted graph? As we'll start traversing the paths parallely (round-robin), the calculated distance (prev. distance + weight) would become the factor of comparision among candidate paths (to the same vertex from s s ). So we cannot rely on the closest/nearest (just based on number of edges inbetween) path because this may lead to an incorrect answer. The path with more edges could be the cheapest/shortest path. This has become an minimization/optimization problem. Which could either be solved by Greedy or Dynamic Programing approach. Dijkstra chose the Greedy one. So, this time need to explore all the paths from s s -> v v , visit the same vertex v v multiple times using all the possible (via adjacent vertices, say u u ) paths to that vertex from s s . And relax(update/overwrite) the existing distance of the vertex v v from s s when the newly calculated distance is lesser. Dijkstra says: Do this initial exercise: Calculate the distance of the immediate reachable (adjacent) vertices first, and set the distance of other vertices as \\infty \\infty Mark all the vertices as not done Then perform this sub task: Now pick the one (say u u ) from all vertices whose distance is minimal & is not done Find its adjacent vertices (say v v ) and Relax (recalculate the distance & update) them. No need to Relax the done marked adjacent vertices (not fruitful as per Dijkstra) . Relaxation: 1 2 if d[u] + w(u, v) < d[v]; then d[v] = d[u] + w(u, v) Now, mark that vertex u u as done . Then repeat the sub task until all the vertices are marked done .","title":"Idea"},{"location":"Computer-Science/algo/#applications_6","text":"finding cheapest way to go from one place to another","title":"Applications"},{"location":"Computer-Science/algo/#properties_5","text":"approach is Greedy in nature why/how? it's iteratively makes one greedy choice after another mark the distances of nearest vertices first choose the vertex with minimum distance the approach is also dynamic in nature because distances are updated using previously calculated values does not relax the edge going (incidenting) to already selected vertex thats why it may give in the cases of a graph with negative edges","title":"Properties"},{"location":"Computer-Science/algo/#pseudo-code_10","text":"start create a set/array S S to maintain the vertices whose shortest-path has been finalized/determined create a min-priority queue Q Q (i.e. keyed by distance u.d u.d of the vertex u u ) initialize the distances u.d u.d of all the vertices u \\in G.V u \\in G.V with \\infty \\infty , but set the distance of source vertex to zero i.e. s.d = 0 s.d = 0 insert (enqueue) all the vertices u \\in G.V u \\in G.V into the priority queue Q Q create an array P P to keep track of the path ( u u --> v v ) while Q \\ne \\emptyset Q \\ne \\emptyset (min-priority queue is not empty) u = u = extract-min (dequeue a vertex u u ) from the priority queue (the one with minimum distance from the source vertex) S = S \\cup \\{u\\} S = S \\cup \\{u\\} (add the current vertex u u into the determined set S S ) (as u u has already gone through the relaxation & has the minimum distance amongst others) (think about s s during the inital step, s.d == 0 s.d == 0 ) explore [BFS] those adjacent vertices G.adj[u] - S G.adj[u] - S (of the current vertex u u ) whose shortest-path are not yet determined for all adjacent vertices v \\in G.adj[u] - S v \\in G.adj[u] - S relax (calculate, compare, & decrease-key the distance) d.v d.v for the vertex v v ; where calculate: the new distance \\delta(s, u) + w(u, v) \\delta(s, u) + w(u, v) compare: the old distance \\delta(s, u) + w(u, v) < d.v \\delta(s, u) + w(u, v) < d.v decrease-key: v.d = \\delta(s, u) + w(u, v) v.d = \\delta(s, u) + w(u, v) if the distance v.d v.d got relaxed due to current vertex u u ; then store the current vertex u u against this vertex v v in the path-array P P (this will help track the shortest path) end","title":"Pseudo Code"},{"location":"Computer-Science/algo/#implementation-using-adjacency-list-priority-queue-bfs","text":"Approach: Greedy + BFS DS Used: 1 Priority Queue, 1 Array Time Complexity: Best: O(V) O(V) if no edges between the vertices (step-7 will run |V| times) Avg: O((V + E) * logV) O((V + E) * logV) if min-heap based priority queue is used DECREASE-KEY: O(log n) O(log n) EXTRACT-MIN: O(1) O(1) INSERT: O(log n) O(log n) step 5 = insert/enqueue |V| |V| vertices to a priority queue = O(V * log n) O(V * log n) step 7 * 7.d = loop over |E| |E| times (traverse through each edge exactly once) = O(E) O(E) step 7.d.i.iii = O(log n) O(log n) step 7 * 7.d * 7.d.i.iii = O(E * log n) O(E * log n) DOUBT: why the constraint \"If the graph is sufficiently sparse \u2014 in particular, E = O(V^2/log V) E = O(V^2/log V) \u2014 we can improve the algorithm by implementing the min-priority queue with a binary min-heap\" in CLRS? Why not always use min-heap based PQ? Worst: O(V + V^2)* logV) O(V + V^2)* logV) == O(E * logV) O(E * logV) if the graph is a complete graph, where |E| = |V|*(|V|-1)/2 |E| = |V|*(|V|-1)/2 Auxilary Space: O(V) O(V) Best: -- Avg: -- Worst: -- Disadvantage: does a blind search and wastes resources can't guarantee a solution for a negative weighted graph","title":"Implementation Using Adjacency List, Priority Queue, BFS"},{"location":"Computer-Science/algo/#optimization_1","text":"","title":"Optimization"},{"location":"Computer-Science/algo/#extra","text":"I personally don't like the way CLRS has mentioned the time complexity O(V^2 + E) O(V^2 + E) = O(V^2) O(V^2) when array/list based priority queue is used. It says EXTRACT-MIN would have time complexity O(n) O(n) , which is a bad implementation IMO. Rather, I'd implement EXTRACT-MIN in O(1) O(1) & INSERT in O(n) O(n) . Why the algorithm may give incorrect answer in the case of a graph with negative edges? because the algorithm does not relax edge pointing (going/incidenting) to the already selected vertex Why/how the algorithm is greedy? [see properties section] Can Dijkstra's algorithm handle negative edge & result correct answer if we add a positive constant to all the edges?","title":"Extra"},{"location":"Computer-Science/algo/#bellman-ford-algorithm-single-source-shortest-path-in-weighted-negative-graph","text":"Given a weighted graph (directed/undirected) G = (V, E) G = (V, E) , and a source/distinguished vertex s s find the shortest path from s s to every other vertices in G G . This algorithm can handle the negative weights in the graph which overcomes the drawbacks of Dijkstra's algorithm.","title":"Bellman-Ford Algorithm [Single-Source Shortest Path in Weighted (negative) Graph]"},{"location":"Computer-Science/algo/#idea_6","text":"If the given graph is undirected, treat is as a directed graph by replacing an undirected edge with two directed edges. And assigning the same given weight to both the edges. Cycles may exist in the graph. ( But not negative weight cycles ) As we've seen in previous simple/greedy algorithms that presence of negative weights may trip the approach and lead to incorrect answers. We need to be more careful and try all the possible paths to a vertex v v from source s s . And pick the shortest path (minimum distance) amongst them. This is again the same minimization/optimization problem, but approach is not Greedy this time. Bellman-Ford prefers to explore all the possibilies using Dynamic Programing approach - where the target would be to optimize the global solution rather just focusing on the local optimal solution. Bellman-Ford says: List down all the edges And start relaxing an edge u u -> v v for each edge pairs (u, v) (u, v) in G.E G.E Repeat the step-2 |V| - 1 |V| - 1 times, thus it will perform the relaxation of the each edge (u, v) (u, v) for |V| - 1 |V| - 1 times - because the vertex v v might be reachable from all the rest of the vertices at max Why such idea? Because in a worst-case, a simple shortest path from s s to v v might have maximum |V|-1 |V|-1 edges to pass by. Thus, to get a final correct/relaxed distance/answer, all the intermediate edges have to be relaxed as well. Thing to observe in step-2-3 is, each iteration guarantees relaxation/correctness of atleast one edge. (So, in first iteration it calculates the shortest path with atmost one edge in the path, in second iteration it calculates the shortest path with atmost two edges in the path, thus in i^{th} i^{th} iteration it calculates the shortest path with atmost i i edges in the path. So, in each of these repetitions, the number of vertices with correctly calculated distances grows, from which it follows that eventually all vertices will have their correct distances). So, a graph without cycle (assume) expects |V|-1 |V|-1 relaxation. Also, this idea has nothing to do with negative weights (till |V|-1 |V|-1 iterations) Illustration Repeat the step-2 one more time, and check if further relaxation is possible in |V| |V| ^{th}$ round; if so, then there exists a Negative Weight Cycle Relaxation: 1 2 if d[u] + w(u, v) < d[v]; then d[v] = d[u] + w(u, v)","title":"Idea"},{"location":"Computer-Science/algo/#applications_7","text":"Negative weights are found in various applications of graphs. For example, instead of paying cost for a path, we may get some advantage if we follow the path. in networks, in routing information protocol (RIP) finding cheapest way to go (or send something) from one place to another","title":"Applications"},{"location":"Computer-Science/algo/#properties_6","text":"follows Dynamic Programing approach works in bottom-up manner can detect a Negative Weight Cycle","title":"Properties"},{"location":"Computer-Science/algo/#pseudo-code_11","text":"input : graph G(V, E) G(V, E) , weight function w w , and source s s output : a) True, b) distance of all the v \\in V v \\in V , c) the shortest path; iff there in no negative weight cycle reachable from source s s ; else a) False start create a distance array d d & initialize d[u] \\forall u \\in V d[u] \\forall u \\in V with \\infty \\infty create an array P P to keep track of the path ( u u --> v v ) set d[s] = 0 d[s] = 0 for |V| -1 |V| -1 times: for each edge (u, v) \\in E (u, v) \\in E : relax (u, v, w) (u, v, w) i.e. if d[v] \\gt d[u] + w(u, v) d[v] \\gt d[u] + w(u, v) : d[v] = d[u] + w(u, v) d[v] = d[u] + w(u, v) P[v] = u P[v] = u for each edge (u, v) \\in E (u, v) \\in E : if d[v] \\gt d[u] + w(u, v) d[v] \\gt d[u] + w(u, v) : return False return True end","title":"Pseudo Code"},{"location":"Computer-Science/algo/#implementation-using-dynamic-programing","text":"Desc: Approach: Dynamic Programing DS Used: Time Complexity: Best: Avg: O(V * E) O(V * E) (at most |V|-1 |V|-1 number of edges might be between vertex s s & v v x # of edges to be relaxed) Worst: O(V^3) O(V^3) if the graph is a complete graph Auxilary Space: O(V) O(V) Best: Avg: Worst: Disadvantage: does not work if there is a negative weight cycle does not scale well","title":"Implementation Using Dynamic Programing"},{"location":"Computer-Science/algo/#optimization_2","text":"If no more relaxation happens in the Graph; then immediately stop/return. just maintain a flag & check the flag of the","title":"Optimization"},{"location":"Computer-Science/algo/#extra_1","text":"Can Bellman-Ford's algorithm handle negative edge cycle & result correct answer if we add a positive constant to all the edges? How negative weight cycle check works? A final scan of all the edges is performed and if any distance is updated, then a path of length |V| |V| edges has been found which can only occur if at least one negative cycle exists in the graph.","title":"Extra"},{"location":"Computer-Science/algo/#floyd-warshall-algorithm-all-pair-shortest-path-in-weighted-negative-graph","text":"Given a weighted graph (directed/undirected) G(V, E) G(V, E) with vertices V V numbered 1 1 to N N , and a weight-function w_{ij} w_{ij} to denote weight of the edge between vertext i i & j j , then find the shortest path from each vertex i \\in V i \\in V to every other vertex j \\in V j \\in V in G G . This can be solved using shortest-path single-source algorithm, by running them for |V| |V| times considering each vertex as source each time. If Floyd-Warshall have chosen Dijkstra's algorithm then it would have cost: O(V) * O((V + E)* log V) O(V) * O((V + E)* log V) (considering it cannot handle negative weights). And O(V^3 log V) O(V^3 log V) in case of a dense graph. If Floyd-Warshall have chosen Bellman-Ford's algorithm then it would have cost: O(V) * O(V * E) O(V) * O(V * E) , and O(V^4) O(V^4) in case of a dense graph. However, those two algorithms were based on using Adjacency List, while this algorithm uses Adjacency Matrix. And even with that it solves the problem in \\Theta(V^3) \\Theta(V^3) . Note: If the given graph is undirected, treat it as a directed graph by replacing an undirected edge with two directed edges. And assigning the same given weight to both the edges Negative weights may exist in the graph Cycles may exist in the graph The algorithm assumes that there are no negative cycles if there are negative cycles, the Floyd\u2013Warshall algorithm can be used to detect them How? Its true that the distance between same vertex i i should be zero If that distance changes to a negative value, then there is a negative weight cycle","title":"Floyd-Warshall Algorithm [All-Pair Shortest-Path in Weighted (negative) Graph]"},{"location":"Computer-Science/algo/#idea_7","text":"It's reasonable to think/assume/say that a shortest-path p p from vertex i i to j j could have (may not have) some intermediate vertices from a set of vertices say \\{1, 2, 3, ..., k \\} \\{1, 2, 3, ..., k \\} . Also, the shortest-path p p may go through vertex k k and may not. If we denote: a shortest-distance (length of a shortest-path ) from vertex i i to j j via k k as d_{ij}^k d_{ij}^k and, a shortest-distance from vertex i i to j j - if there is only 1 edge between them (i.e. no extra vertext between them) as d_{ij}^0 d_{ij}^0 (suppose we denote this case by k=0 k=0 ) So, for each pair i, j \\in V i, j \\in V , the observation could be: if the shortest-path p p does NOT even have more than 1 edge. Then we can denote the shortest-distance by d_{ij}^0 = w_{ij} d_{ij}^0 = w_{ij} (i.e. whatever weight is initially given in the G G ). if the shortest-path p p does NOT go through k k ; i.e. the intermediate vertices falls in set \\{1, 2, 3, ..., k-1\\} \\{1, 2, 3, ..., k-1\\} only. Then, we can denote the shortest-distance by d_{ij}^{k-1} d_{ij}^{k-1} if the shortest-path p p GOES through k k ; then the path could be broken into two parts, say: i i --> k k (say path p_1 p_1 ) k k --> j j (say path p_2 p_2 ) where both the path using intermediate vertices from set \\{1, 2, 3, ..., k-1\\} \\{1, 2, 3, ..., k-1\\} . And, if the shortest-path p p from i i to j j goes via k k , then it should definitely be the concatenation of a shortest-path from i i to k k ( p_1 p_1 , using intermediate vertices from set \\{1, 2, 3, ..., k-1\\} \\{1, 2, 3, ..., k-1\\} ), and a shortest-path from k k to j j ( p_2 p_2 , only using intermediate vertices from set \\{1, 2, 3, ..., k-1\\} \\{1, 2, 3, ..., k-1\\} ). Then, we can denote the shortest-distance (of shortest-path p p ) by d_{ik}^{k-1} + d_{kj}^{k-1} d_{ik}^{k-1} + d_{kj}^{k-1} Then, from the above observations, we can deduce a relationship between the: shortest-path p p (or its distance) from vertex i i to j j some intermediate vertex k k weight-function w_{ij} w_{ij} given in the graph G G Which can be recursively defined as: d_{ij}^k = \\left \\{ \\begin{array}{lcl} w_{ij} & if & k=0 \\\\ min( d_{ij}^{k-1}, \\space d_{ik}^{k-1} + d_{kj}^{k-1}) & if & k \\gt 0 \\\\ \\end{array} \\right. d_{ij}^k = \\left \\{ \\begin{array}{lcl} w_{ij} & if & k=0 \\\\ min( d_{ij}^{k-1}, \\space d_{ik}^{k-1} + d_{kj}^{k-1}) & if & k \\gt 0 \\\\ \\end{array} \\right.","title":"Idea"},{"location":"Computer-Science/algo/#applications_8","text":"find shortest path (from all the vertices) in directed graphs detect negative weight cycle in directed graphs many more real life applications (see this )","title":"Applications"},{"location":"Computer-Science/algo/#properties_7","text":"follows dynamic programing approach bottom up way (i.e. sub problems are already in indivisible state, so keep combining the results from sub problems till we get the final result) can detect negative weight cycle utilizes Adjacency Matrix","title":"Properties"},{"location":"Computer-Science/algo/#pseudo-code_12","text":"input : graph G(V, E) G(V, E) with weight function w w (or just weight matrix W W ) output : shortest distance between each vertices Typically Floyd\u2013Warshall algorithm calculates distances only, and does not reconstruct the shortest path (i.e. does not return predecessor map), also does not tell if a negative weight cycle exists. Though those could be achieved by some additional simple logic/code/steps. N = |V| N = |V| // number of vertices D = (d_{ij}) D = (d_{ij}) // initialize 2-d array of size N*N N*N with \\infty \\infty , to store shortest-distance between vertices for (i, j) \\in G.E (i, j) \\in G.E // for all edges from vertex i i to j j d_{ij} = w_{ij} d_{ij} = w_{ij} // set distance from vertex i i to j j using w_{ij} w_{ij} for i \\in V i \\in V d_{ii} = 0 d_{ii} = 0 // set distance between same vertex ( i i to i i , i.e. self loop) to zero for k k = 1 to N N for i \\in V i \\in V for j \\in V j \\in V d_{ij} = min(d_{ij}, \\space d_{ik} + d_{kj}) d_{ij} = min(d_{ij}, \\space d_{ik} + d_{kj}) // relaxation return D D","title":"Pseudo Code"},{"location":"Computer-Science/algo/#pseudo-code-detect-negative-weight-cycle","text":"input : final distance matrix D D from Floyd\u2013Warshall algorithm output : True if negative weight cycle exists; else False N = |D| N = |D| for i i = 1 to N N for j j = 1 to N N if i == j i == j // if diagonal coordinate of the matrix if d_{ij} < 0 d_{ij} < 0 return True return False","title":"Pseudo Code - detect negative weight cycle"},{"location":"Computer-Science/algo/#pseudo-code-reconstruct-shortest-path","text":"TODO","title":"Pseudo Code - reconstruct shortest-path"},{"location":"Computer-Science/algo/#implementation-using-adjacency-matrix","text":"Approach: Dynamic Programing DS Used: Matrix (2-d array) Time Complexity: \\Theta(V^3) \\Theta(V^3) Best: -- Avg: -- Worst: -- Auxilary Space: \\Theta(V^2) \\Theta(V^2) (if weight matrix and/or predecessor matrix has to be created) else \\Theta(1) \\Theta(1) Best: -- Avg: -- Worst: -- Disadvantage: --","title":"Implementation using Adjacency Matrix"},{"location":"Computer-Science/algo/#optimization_3","text":"","title":"Optimization"},{"location":"Computer-Science/algo/#extra_2","text":"","title":"Extra"},{"location":"Computer-Science/algo/#kruskal-minimum-cost-spanning-tree-algorithm","text":"","title":"Kruskal Minimum Cost Spanning Tree Algorithm"},{"location":"Computer-Science/algo/#idea_8","text":"","title":"Idea"},{"location":"Computer-Science/algo/#applications_9","text":"","title":"Applications"},{"location":"Computer-Science/algo/#properties_8","text":"","title":"Properties"},{"location":"Computer-Science/algo/#implementation-using-stack-dfs_1","text":"Desc: Approach: DS Used: Time Complexity: Best: Avg: Worst: Auxilary Space: Best: Avg: Worst: Disadvantage:","title":"Implementation Using Stack / DFS"},{"location":"Computer-Science/algo/#pseudo-code_13","text":"","title":"Pseudo Code"},{"location":"Computer-Science/algo/#prims-minumum-cost-spanning-tree","text":"","title":"Prim's Minumum Cost Spanning Tree"},{"location":"Computer-Science/algo/#knuth-morris-pratt-algorithm","text":"","title":"Knuth-Morris-Pratt Algorithm"},{"location":"Computer-Science/algo/#bipartite-matching","text":"","title":"Bipartite Matching"},{"location":"Computer-Science/algo/#iterative-deepening-depth-first-search-depth-limited-search","text":"","title":"Iterative Deepening Depth First Search (Depth Limited Search)"},{"location":"Computer-Science/algo/#a-search","text":"","title":"A* Search"},{"location":"Computer-Science/algo/#ternary-search","text":"","title":"Ternary Search"},{"location":"Computer-Science/algo/#meet-in-the-middle","text":"","title":"Meet in the middle"},{"location":"Computer-Science/algo/#strongly-connected-components-scc","text":"","title":"Strongly Connected Components (SCC)"},{"location":"Computer-Science/algo/#edmonds-karp-algorithm","text":"","title":"Edmonds-Karp Algorithm"},{"location":"Computer-Science/algo/#hungarian-algorithm","text":"","title":"Hungarian Algorithm"},{"location":"Computer-Science/algo/#sweep-line-algorithm","text":"","title":"Sweep Line Algorithm"},{"location":"Computer-Science/algo/#graham-scan","text":"","title":"Graham scan"},{"location":"Computer-Science/algo/#tarjans-algorithm","text":"","title":"Tarjan's Algorithm"},{"location":"Computer-Science/algo/#z-algorithm","text":"","title":"Z algorithm"},{"location":"Computer-Science/algo/#hill-climbing","text":"","title":"Hill Climbing"},{"location":"Computer-Science/algo/#number-theory","text":"","title":"Number Theory"},{"location":"Computer-Science/algo/#modular-arithmetic","text":"","title":"Modular Arithmetic"},{"location":"Computer-Science/algo/#fermats-theorem","text":"","title":"Fermat\u2019s Theorem"},{"location":"Computer-Science/algo/#chinese-remainder-theoremcrt","text":"","title":"Chinese Remainder Theorem(CRT)"},{"location":"Computer-Science/algo/#euclidian-method-for-gcd","text":"","title":"Euclidian Method for GCD"},{"location":"Computer-Science/algo/#logarithmic-exponentiation","text":"","title":"Logarithmic Exponentiation"},{"location":"Computer-Science/algo/#sieve-of-eratosthenes","text":"","title":"Sieve of Eratosthenes"},{"location":"Computer-Science/algo/#eulers-totient-function","text":"","title":"Euler\u2019s Totient Function"},{"location":"Computer-Science/algo/#geometric-algorithms","text":"","title":"Geometric Algorithms"},{"location":"Computer-Science/algo/#2d-rotation-and-scale-matrices","text":"","title":"2D Rotation and Scale Matrices"},{"location":"Computer-Science/algo/#2d-rotation-and-translation-matrices","text":"","title":"2D Rotation and Translation Matrices"},{"location":"Computer-Science/algo/#2d-changing-coordinate-systems","text":"","title":"2D Changing Coordinate Systems"},{"location":"Computer-Science/algo/#3d-rotation-and-scale-matrices","text":"","title":"3D Rotation and Scale Matrices"},{"location":"Computer-Science/algo/#3d-changing-coordinate-systems","text":"","title":"3D Changing Coordinate Systems"},{"location":"Computer-Science/algo/#greedy-algorithms","text":"The problem should be solved in stages, by consider one step a time and one input at a time. The each step should find a local optimal solution, and those local optimal should lead to global optimal solution. These algorithms provides a predefined procedure to be followed in each step. An optimization problem can be solved using greedy approach.","title":"Greedy Algorithms"},{"location":"Computer-Science/algo/#elementary-cases-fractional-knapsack-problem-task-scheduling","text":"","title":"Elementary cases : Fractional Knapsack Problem, Task Scheduling"},{"location":"Computer-Science/algo/#data-compression-using-huffman-trees","text":"","title":"Data Compression using Huffman Trees"},{"location":"Computer-Science/algo/#activity-selection","text":"","title":"Activity Selection"},{"location":"Computer-Science/algo/#dynamic-programing","text":"Dynamic programing is an approach to solve a problem which requires an optimal solutions amongst various other possible solutions. So, the goal is to get a global optimal solution. To do so, we analyze the problem and identify a repeated subtask (step) in that. Then we make decision at each step considering current problem and solution to previously solved sub problem to calculate optimal solution.","title":"Dynamic Programing"},{"location":"Computer-Science/algo/#intro","text":"","title":"Intro"},{"location":"Computer-Science/algo/#memoization","text":"Top - Down approach in a subproblem tree Intro divides a big problem into subproblems starts solving smallest problem first; approaching root of problem memoize/cache the solved subproblems for repeated/same subproblems, utilizes the cache/memoization table uses recursion Pros easy implementation Cons recusrsion stach could be deeper and that might create stack-overflow/memory issue Extra memo table fills up in bottom to top order","title":"Memoization"},{"location":"Computer-Science/algo/#tabulation","text":"Bottom - Up approach in a subproblem tree Intro Understands a root problem and finds subproblems to that starts solving subproblems (in a particular order which approaches the root problem, like smallest to bigger, 1 to 100 etc.) in iterative way store the answer to those subproblems in a table Pros avoids recursion stack issue suitable for extremly complicated problems suitable where optimization is concern because it gives flexibility of command over coding style Cons need more rigid thinking ahead of time to find the ordering of subproblems so that we do not need to backtrack and solve a smaller problem in order to solve a larger problem Extra table fills up in top to bottom order","title":"Tabulation"},{"location":"Computer-Science/algo/#examples","text":"","title":"Examples"},{"location":"Computer-Science/algo/#edit-distance","text":"","title":"Edit Distance"},{"location":"Computer-Science/algo/#knapsack-problem","text":"","title":"Knapsack problem"},{"location":"Computer-Science/algo/#01-knapsack-problem","text":"","title":"0/1 Knapsack Problem"},{"location":"Computer-Science/algo/#01-knapsack-problem-with-repetition-of-items","text":"","title":"0/1 Knapsack Problem (with repetition of items)"},{"location":"Computer-Science/algo/#knapsack-problem-with-fractional-items","text":"","title":"Knapsack Problem (with fractional items)"},{"location":"Computer-Science/algo/#matrix-chain-multiplication","text":"","title":"Matrix Chain Multiplication"},{"location":"Computer-Science/algo/#longest-common-substring","text":"","title":"Longest Common Substring"},{"location":"Computer-Science/algo/#longest-common-subsequence","text":"","title":"Longest Common Subsequence"},{"location":"Computer-Science/algo/#longest-increasing-monotonical-subsequence","text":"","title":"Longest Increasing Monotonical Subsequence"},{"location":"Computer-Science/algo/#rod-cutting","text":"","title":"Rod Cutting"},{"location":"Computer-Science/algo/#misc","text":"","title":"Misc"},{"location":"Computer-Science/algo/#recursion","text":"","title":"Recursion"},{"location":"Computer-Science/algo/#huffman-coding","text":"","title":"Huffman Coding"},{"location":"Computer-Science/algo/#regex-algorithm-pattern-matching-and-parsing","text":"","title":"Regex Algorithm (Pattern Matching and Parsing)"},{"location":"Computer-Science/algo/#hashing-hash-functions","text":"","title":"Hashing- Hash Functions"},{"location":"Computer-Science/algo/#monotone-chains-algorithm","text":"","title":"Monotone Chains Algorithm"},{"location":"Computer-Science/algo/#coordinate-compression","text":"","title":"Coordinate Compression"},{"location":"Computer-Science/algo/#ford-fulkerson-method","text":"","title":"Ford-Fulkerson Method"},{"location":"Computer-Science/algo/#preflow-push-algorithm","text":"","title":"Preflow-Push Algorithm"},{"location":"Computer-Science/algo/#dinics-algorithm","text":"","title":"Dinic's Algorithm"},{"location":"Computer-Science/algo/#monte-carlo-method-or-metropolis-algorithm","text":"","title":"Monte Carlo method or Metropolis Algorithm"},{"location":"Computer-Science/algo/#krylov-subspace-iteration-method","text":"","title":"Krylov Subspace Iteration Method"},{"location":"Computer-Science/algo/#householder-matrix-decomposition","text":"","title":"Householder Matrix Decomposition"},{"location":"Computer-Science/algo/#qr-algorithm","text":"","title":"QR Algorithm"},{"location":"Computer-Science/algo/#fast-fourier-transform","text":"","title":"Fast Fourier Transform"},{"location":"Computer-Science/algo/#integer-relation-detection-algorithm","text":"","title":"Integer Relation Detection Algorithm"},{"location":"Computer-Science/algo/#fast-multipole-algorithm","text":"","title":"Fast Multipole algorithm"},{"location":"Computer-Science/algo/#minmax-algorithm","text":"","title":"MinMax Algorithm"},{"location":"Computer-Science/algo/#divide-and-conquer-algorithm","text":"","title":"Divide and Conquer Algorithm"},{"location":"Computer-Science/algo/#nomenclature","text":"use uppercase letters to denote matrices and corresponding subscripted lowercase letters to denote their elements","title":"Nomenclature"},{"location":"Computer-Science/algo/#references","text":"CLRS 3rd Edition Algorithms Unlocked - 2013, by Dr. Thomas Cormen https://gist.github.com/toransahu/bb1c9f1cd6490ff29c42fa229e827a2a https://www.freetechbooks.com/algorithms-and-data-structures-f11.html LaTex Math Syntax https://www.caam.rice.edu/~heinken/latex/symbols.pdf https://en.wikibooks.org/wiki/LaTeX/Mathematics https://math.meta.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference https://latex.wikia.org/wiki/Array_(LaTeX_environment) https://tex.stackexchange.com/questions/77589/what-do-the-pieces-of-latex-left-and-right-respectively-mean","title":"References"},{"location":"Computer-Science/dbms/","text":"DBMS # Table of Contents DBMS Basics Transaction ACID Rollback Cascade Keys Primary Key UUID4 How to generate a custom UUID? Composite Key Foreign Key Candidate Key Super Key Dependencies Functional Dependency Partial Dependency Transitive Dependency Normalization Anamolies Types 0 NF: Un Normalized Form 1 NF (E. F. Codd - 1971) Index Why Type How Implementation Doubts/FAQs SQL Stored Procedure & Function How to optimize a query? a SP? a Func? Cursor? What? Why? How? When? Type? Trigger Jobs? Dynamic SQL? Generally Asked Basics # Transaction # represents any changes in database a unit of work performed in DBMS against a database independent of other transactions 2 Purposes: provide reliability, can be recovered on failure isolation from other programs accessing DB concurrently to avoid errors ACID # A set of properties of relational database transaction . Atomicity: All or nothing. Consistency: One state to another valid state (according to rulesets, constraints defined). Isolation: Concurrency control. Execution of multiple concurrent transaction should have the same result as executed sequentially. Durability: Changes should persist (permanent is a big word :D). Rollback # Cascade # Keys # Primary Key # A primary is a single column value used to identify a database record uniquely. It has following attributes: A primary key cannot be NULL A primary key value must be unique The primary key values cannot be changed The primary key must be given a value when a new record is inserted. Q. Which is better? Auto-incremental number vs UUID as primary key? Auto-incremental number pros good for internal use small in size (say 8 bytes) fast easy to sort by cons could reveal info/confidentiality a user can guess ID of other in long run, (i.e. very huge data, 10s of billions) integer takes more space than string UUID pros good for external use unique not only across table, but accross company, or even world does not reveal any info implicitly hence more secure system easier to merge data in sub-database/sub-table cons relatively more space (say 16 bytes) performace disaster for very large tables (say more than 200K) aer very random, so using them as unique/primary key (basically indexing) is inefficient on very large tables cannot sort could be adhoc (if we still need a number based unique key as well, say rollno, empid etc) so space for index for uuid as well as for sequence having both is waste Ref: - https://stackoverflow.com/questions/33274291/uuid-or-sequence-for-primary-key/33274393#33274393 - https://dba.stackexchange.com/questions/115766/should-i-use-uuid-as-well-as-id/119129 - http://compwron.github.io/2016/06/15/uuids-ids-and-primary-keys.html UUID4 # UUID version 4 128 bits 122 for data/randomization (i.e. could have 122 random bits) 6 bits for identification of UUID version How to generate a custom UUID? # machine id (MAC?) + timestamp (nano) + process id + thread id + counter may keep a lookup cache of 1 second Composite Key # A composite key is a primary key composed of multiple columns used to identify a record uniquely Foreign Key # A key which is referring to a primary key of another table Candidate Key # Candidate for the primary key. Means whichever combination of columns can uniquely identify a record are the candidate keys. Super Key # Superset of all the candidate key. Means a set which contains all the columns from all the candidate keys. Dependencies # Functional Dependency # Partial Dependency # Transitive Dependency # Normalization # A technique of organizing data in DB A systematic approach to decomposing tables to eliminate data redundancy & undesired characteristics like: Insertion Anamolies Update Anamolies Deletion Anamolies Multi step process Normalization is used for mainly two purposes: Eliminating redundant(useless) data. Ensuring data dependencies make sense i.e data is logically stored. Note: In most practical applications, normalization achieves its best in 3rd Normal Form. Anamolies # Insertion Anamoly Suppose for a new admission, we have a Student id, name, and address of a student but if the student has not opted for any subjects yet then we have to insert NULL there, leading to Insertion Anamoly. Update Anamoly To update the address of a student who occurs twice or more than twice in a table, we will have to update S_Address column in all the rows, else data will become inconsistent. Deletion Anamoly If a student has only one subject and temporarily he drops it. When we delete that row, entire student record will be deleted along with it. Types # 0 NF: Un Normalized Form # Multi Valued Cells like Unit Code 1 NF (E. F. Codd - 1971) # First normal form enforces these criteria: Eliminate repeating groups in individual tables. Create a separate table for each set of related data. Identify each set of related data with a primary/composite key Rules Atomic (i.e. indivisible) / single values in each cell. Means no set of values in a single cell. Values stored in a column should be of the same domain i.e. No repeating group/column/attribute. like Telephone 1, Telephone 2 All the columns in a table should have unique names. No repeating rows/records. In other way: For a set of values, create multiple rows/records for each individual values in the set. Or, Identify each row/records by a primary key/composite primary key Disadvantage: Using the First Normal Form, data redundancy increases, as there will be many columns with same data in multiple rows but each row as a whole will be unique. Table: 1-NF Table: Still in 1-NF 2 NF Be in 1 NF There must not be a Partial Dependency Table: 2-NF 3 NF BCNF Index # are used to quickly locate data without searching every row in the table Why # Type # How # Implementation # Doubts/FAQs # why not use hash table instead b-tree? https://stackoverflow.com/questions/7306316/b-tree-vs-hash-table https://www.quora.com/A-B-tree-index-is-better-than-a-hash-index-Why https://dev.mysql.com/doc/refman/8.0/en/index-btree-hash.html read sqlite code/doc as well https://hakibenita.com/postgresql-hash-index how a really huge b-tree fits in RAM? https://stackoverflow.com/questions/764221/larger-than-memory-data-structures-and-how-they-are-typically-handled what is paging? primary key uses indexing by default? Yes when not to use DB index? when table is small on columns on which majority of the values are null on columns which are frequently manipulated on columns which returns a high percentage of rows if use filter condition (like WHERE clause) on those indexing could perform slow on tables which get frequent bulk updates SQL # Structured Query Language (Please stop saying SEQUEL, i.e. Structured English QUEry Language, that was a trademark conflict) Stored Procedure & Function # Are schema level subprograms/program unit/commonly used codes, stored in database. Procedure Function Stored Procedures can call functions. Functions cannot call stored Procedures. Can have select statements as well as DML statements Cannot use DML statements Can use both table variables as well as temporary table in it. Cannot use temp tables Procedures cannot be utilized in a select statement Function can be embedded in a select statement. Procedure can return multiple OUT values(max. 1024) Function returns 1 value only however it can be collection datatype How to optimize a query? a SP? a Func? # Cursor? What? Why? How? When? Type? # Trigger # Jobs? # Dynamic SQL? # Generally Asked # second highest salary, query emp, dept, city import excel to table Employees with third highest salary @ address Pune? Tables: Employees, Address, Salary? delete vs drop vs truncate","title":"DBMS"},{"location":"Computer-Science/dbms/#dbms","text":"","title":"DBMS"},{"location":"Computer-Science/dbms/#basics","text":"","title":"Basics"},{"location":"Computer-Science/dbms/#transaction","text":"represents any changes in database a unit of work performed in DBMS against a database independent of other transactions 2 Purposes: provide reliability, can be recovered on failure isolation from other programs accessing DB concurrently to avoid errors","title":"Transaction"},{"location":"Computer-Science/dbms/#acid","text":"A set of properties of relational database transaction . Atomicity: All or nothing. Consistency: One state to another valid state (according to rulesets, constraints defined). Isolation: Concurrency control. Execution of multiple concurrent transaction should have the same result as executed sequentially. Durability: Changes should persist (permanent is a big word :D).","title":"ACID"},{"location":"Computer-Science/dbms/#rollback","text":"","title":"Rollback"},{"location":"Computer-Science/dbms/#cascade","text":"","title":"Cascade"},{"location":"Computer-Science/dbms/#keys","text":"","title":"Keys"},{"location":"Computer-Science/dbms/#primary-key","text":"A primary is a single column value used to identify a database record uniquely. It has following attributes: A primary key cannot be NULL A primary key value must be unique The primary key values cannot be changed The primary key must be given a value when a new record is inserted. Q. Which is better? Auto-incremental number vs UUID as primary key? Auto-incremental number pros good for internal use small in size (say 8 bytes) fast easy to sort by cons could reveal info/confidentiality a user can guess ID of other in long run, (i.e. very huge data, 10s of billions) integer takes more space than string UUID pros good for external use unique not only across table, but accross company, or even world does not reveal any info implicitly hence more secure system easier to merge data in sub-database/sub-table cons relatively more space (say 16 bytes) performace disaster for very large tables (say more than 200K) aer very random, so using them as unique/primary key (basically indexing) is inefficient on very large tables cannot sort could be adhoc (if we still need a number based unique key as well, say rollno, empid etc) so space for index for uuid as well as for sequence having both is waste Ref: - https://stackoverflow.com/questions/33274291/uuid-or-sequence-for-primary-key/33274393#33274393 - https://dba.stackexchange.com/questions/115766/should-i-use-uuid-as-well-as-id/119129 - http://compwron.github.io/2016/06/15/uuids-ids-and-primary-keys.html","title":"Primary Key"},{"location":"Computer-Science/dbms/#uuid4","text":"UUID version 4 128 bits 122 for data/randomization (i.e. could have 122 random bits) 6 bits for identification of UUID version","title":"UUID4"},{"location":"Computer-Science/dbms/#how-to-generate-a-custom-uuid","text":"machine id (MAC?) + timestamp (nano) + process id + thread id + counter may keep a lookup cache of 1 second","title":"How to generate a custom UUID?"},{"location":"Computer-Science/dbms/#composite-key","text":"A composite key is a primary key composed of multiple columns used to identify a record uniquely","title":"Composite Key"},{"location":"Computer-Science/dbms/#foreign-key","text":"A key which is referring to a primary key of another table","title":"Foreign Key"},{"location":"Computer-Science/dbms/#candidate-key","text":"Candidate for the primary key. Means whichever combination of columns can uniquely identify a record are the candidate keys.","title":"Candidate Key"},{"location":"Computer-Science/dbms/#super-key","text":"Superset of all the candidate key. Means a set which contains all the columns from all the candidate keys.","title":"Super Key"},{"location":"Computer-Science/dbms/#dependencies","text":"","title":"Dependencies"},{"location":"Computer-Science/dbms/#functional-dependency","text":"","title":"Functional Dependency"},{"location":"Computer-Science/dbms/#partial-dependency","text":"","title":"Partial Dependency"},{"location":"Computer-Science/dbms/#transitive-dependency","text":"","title":"Transitive Dependency"},{"location":"Computer-Science/dbms/#normalization","text":"A technique of organizing data in DB A systematic approach to decomposing tables to eliminate data redundancy & undesired characteristics like: Insertion Anamolies Update Anamolies Deletion Anamolies Multi step process Normalization is used for mainly two purposes: Eliminating redundant(useless) data. Ensuring data dependencies make sense i.e data is logically stored. Note: In most practical applications, normalization achieves its best in 3rd Normal Form.","title":"Normalization"},{"location":"Computer-Science/dbms/#anamolies","text":"Insertion Anamoly Suppose for a new admission, we have a Student id, name, and address of a student but if the student has not opted for any subjects yet then we have to insert NULL there, leading to Insertion Anamoly. Update Anamoly To update the address of a student who occurs twice or more than twice in a table, we will have to update S_Address column in all the rows, else data will become inconsistent. Deletion Anamoly If a student has only one subject and temporarily he drops it. When we delete that row, entire student record will be deleted along with it.","title":"Anamolies"},{"location":"Computer-Science/dbms/#types","text":"","title":"Types"},{"location":"Computer-Science/dbms/#0-nf-un-normalized-form","text":"Multi Valued Cells like Unit Code","title":"0 NF: Un Normalized Form"},{"location":"Computer-Science/dbms/#1-nf-e-f-codd-1971","text":"First normal form enforces these criteria: Eliminate repeating groups in individual tables. Create a separate table for each set of related data. Identify each set of related data with a primary/composite key Rules Atomic (i.e. indivisible) / single values in each cell. Means no set of values in a single cell. Values stored in a column should be of the same domain i.e. No repeating group/column/attribute. like Telephone 1, Telephone 2 All the columns in a table should have unique names. No repeating rows/records. In other way: For a set of values, create multiple rows/records for each individual values in the set. Or, Identify each row/records by a primary key/composite primary key Disadvantage: Using the First Normal Form, data redundancy increases, as there will be many columns with same data in multiple rows but each row as a whole will be unique. Table: 1-NF Table: Still in 1-NF 2 NF Be in 1 NF There must not be a Partial Dependency Table: 2-NF 3 NF BCNF","title":"1 NF (E. F. Codd - 1971)"},{"location":"Computer-Science/dbms/#index","text":"are used to quickly locate data without searching every row in the table","title":"Index"},{"location":"Computer-Science/dbms/#why","text":"","title":"Why"},{"location":"Computer-Science/dbms/#type","text":"","title":"Type"},{"location":"Computer-Science/dbms/#how","text":"","title":"How"},{"location":"Computer-Science/dbms/#implementation","text":"","title":"Implementation"},{"location":"Computer-Science/dbms/#doubtsfaqs","text":"why not use hash table instead b-tree? https://stackoverflow.com/questions/7306316/b-tree-vs-hash-table https://www.quora.com/A-B-tree-index-is-better-than-a-hash-index-Why https://dev.mysql.com/doc/refman/8.0/en/index-btree-hash.html read sqlite code/doc as well https://hakibenita.com/postgresql-hash-index how a really huge b-tree fits in RAM? https://stackoverflow.com/questions/764221/larger-than-memory-data-structures-and-how-they-are-typically-handled what is paging? primary key uses indexing by default? Yes when not to use DB index? when table is small on columns on which majority of the values are null on columns which are frequently manipulated on columns which returns a high percentage of rows if use filter condition (like WHERE clause) on those indexing could perform slow on tables which get frequent bulk updates","title":"Doubts/FAQs"},{"location":"Computer-Science/dbms/#sql","text":"Structured Query Language (Please stop saying SEQUEL, i.e. Structured English QUEry Language, that was a trademark conflict)","title":"SQL"},{"location":"Computer-Science/dbms/#stored-procedure-function","text":"Are schema level subprograms/program unit/commonly used codes, stored in database. Procedure Function Stored Procedures can call functions. Functions cannot call stored Procedures. Can have select statements as well as DML statements Cannot use DML statements Can use both table variables as well as temporary table in it. Cannot use temp tables Procedures cannot be utilized in a select statement Function can be embedded in a select statement. Procedure can return multiple OUT values(max. 1024) Function returns 1 value only however it can be collection datatype","title":"Stored Procedure &amp; Function"},{"location":"Computer-Science/dbms/#how-to-optimize-a-query-a-sp-a-func","text":"","title":"How to optimize a query? a SP? a Func?"},{"location":"Computer-Science/dbms/#cursor-what-why-how-when-type","text":"","title":"Cursor?  What? Why? How? When? Type?"},{"location":"Computer-Science/dbms/#trigger","text":"","title":"Trigger"},{"location":"Computer-Science/dbms/#jobs","text":"","title":"Jobs?"},{"location":"Computer-Science/dbms/#dynamic-sql","text":"","title":"Dynamic SQL?"},{"location":"Computer-Science/dbms/#generally-asked","text":"second highest salary, query emp, dept, city import excel to table Employees with third highest salary @ address Pune? Tables: Employees, Address, Salary? delete vs drop vs truncate","title":"Generally Asked"},{"location":"Computer-Science/ds/","text":"Data Structure # Table of Contents Data Structure Introduction Data Structure Vs Data Type Data Type Data Structure Abstract Data Type (ADT) Types Linear Data Structure Non Linear Data Structure Language Specific Array Linked List Doubly Linked List Circular Linked List Stack Queue Simple Queue Circular Queue Dequeue Priority Queue Desc Application Properties Operations Implementation Hash Desc Terminologies Why Dictionary Hash Table Hash Map Tree Desc Terminologies Why Application Hand Shaking Lemma Binary Tree Desc Representation Array Representation Properties Operations Depth First Traversal Breadth First Traversal (Level Order Traversal) Vertical Order Traversal Views Top View of a binary tree Bottom View of a binary tree Left View of a binary tree Right View of a binary tree Full Binary Tree Complete Binary Tree Properties Perfect Binary Tree Degenerated / Pathological Tree Skew (Binary) Tree Left Skew (Binary) Tree Right Skew (Binary) Tree Binary Search Tree (BST) Desc Operations Self Balancing Binary Tree AVL Tree Specification Rotation LL (left-left) Rotation RR (right-right) Rotation LR (left-right) Rotation RL (right-left) Rotation Exercise Complexity B-Tree Application Specification Complexity Red-Black Tree B+ Tree K-ary Tree M-way Search Tree Heap Binary Heap Applications Min Heap Operations Max Heap Operations Implementations Operations Trie Graph Desc Terminologies Why Application Representation Adjacency Matrix Adjacency List Adjacency Set Operations Undirected Graph Directed Graph Directed Acyclic Graph (DAG) Directed Cyclic Graph (DCG) Weighted Graph Subgraph Bipartite Graph Complete Graph Sparse Graph Dense Graph References TODO Introduction # Data structure (is a way) or (are the special format/structures in computer memory) to store & organize the data. To suit a specific purpose. So, that we can perform some operations on them like search, add, delete, insert Data Structure Vs Data Type # Data Type # It is class of objects sharing certain properties. It is a premitive element of a particular language like C, C++, Java, Python, Go Have predefined characteristics according to the language Contains data/information without any semantic Can't be reduced anymore e.g. integer, float, character, string, boolean Data Structure # It is an abstract description of organization of data & operations on them It uses data types Can be decomposed to smaller DS are data type as well, but not a premitive type Abstract Data Type (ADT) # Its visualization, thought, logical description, or a picture of a new data type which we are going to create while DS is a real & concrete thing, we can directly use them DS is a super set and Data type is sub set or building block for DS Types # Linear Data Structure # Traverses the data elements sequentially. Only one data element can directly be reached. Array Linked List Singly Linked List Circular Linked List Doubly Linked List Stack Queue Simple Queue Circular Queue Priority Queue Dequeue / Double Ended Queue Hash Matrix Non Linear Data Structure # Every data item is attached to several other data items with a relationships. The data items are not arranged in a sequential structure. Tree Binary Tree Full Binary Tree Complete Binary Tree Heap Perfect Binary Tree Binary Search Tree - BST Degenerated / Pathological Tree Skew Binary Tree Left Skew Tree Right Skew Tree Binary Search Tree - BST Self Balancing / Balanced / Height Balanced BST AVL Tree B-tree Red-Black Tree B+ tree Splay Tree Treap K-ary Tree Heap Binary Heap Max Heap Min Heap Bionomial Heap Fibbonacci Heap Leftist Heap Skew Heap Trie Misc Indexing with Trees Segment Tree Fenwick Tree Binary Indexed Tree (BIT) Binomial Queues Open Hash Tables (Closed Addressing) Closed Hash Tables (Open Addressing) Closed Hash Tables, using buckets Graph Undirected Graph Directed Graph (Digraph) Directed Acyclic Graph (DAG) Directed Cyclic Graph [having cycle(s)] (DCG) Weighted Weighted Undirected Graph Weighted Directed Graph (WITHOUT negative weights) Weighted DAG Weighted DCG Weighted Directed Graph (WITH negative weights) Weighted DAG Weighted DCG WITH Negative Weight Cycle WITHOUT Negative Weight Cycle Misc Finite Graph Infinite Graph Trivial Graph Simple Graph Multi Graph Null Graph (Empty Graph) Complete Graph Pseudo Graph Regular Graph Bipartite Graph Labelled Graph Connected & Disconnected Graph Cyclic Graph Vertex Labelled Graph Subgraph Rooted Graph Mixed Graph Path Graph Planar Graph Cycle Graph Petersen Graph Perfect Graph Cograph Chordal Graph Misc Disjoint-set Data Structures Suffix Arrays Language Specific # Python list tuple set dict built-in dict collections.OrderedDict remember insertion order of keys collections.defaultdict return default values for missing keys collections.ChainMap search multiple dicts as a single mapping types.MappingProxyType A wrapper for making read-only dictionaries Java C# Go Array # Linked List # Doubly Linked List # Circular Linked List # Stack # LIFO Queue # FIFO Simple Queue # Circular Queue # Dequeue # (aka Doubly eneded queue) Priority Queue # Desc # Application # job scheduling event-driven simulation Properties # Operations # INSERT MAXIMUM EXTRACT-MAX REPLACE Implementation # Array/Set/List(LinkedList) Binary Heap Fibbonacci Heap Hash # Desc # Linear data structure (ADT) to make lookup fast uses a hashing functions, a slot list and, a data list time complexity (lookup): best: O(1) worst: O(n) Terminologies # hash function folding method mid-squared method perfect hash functions slot/bucket load factor collision resolution techniques (rehashing) open addressing linear probing quadratic probing disadvantages clustered data chaining Why # to optimize the look-up speed after binary search and other varients Dictionary # a general concept/data structure that maps keys to values many ways to implement dictionary hashtable; O(1) - O(N) red-black tree; always O(longN) Hash Table # synchronized thread safe can be shared with many threads doesn't allows any null key or values Hash Map # non synchronized non-thread safe can't be shared between many threads without proper synchronization code allows one null key and multiple null values preferred over hash-table Tree # Desc # Are hierchical data structure made up of nodes or vertices and edges without having any cycle The tree with no nodes is called the null or empty tree Terminologies # Root: The top node in a tree Child: A node directly connected to another node when moving away from the Root Parent: The converse notion of a child Siblings: A group of nodes with the same parent Descendant: A node reachable by repeated proceeding from parent to child Ancestor: A node reachable by repeated proceeding from child to parent Leaf: (less commonly called External node) A node with no children Branch: Internal node, A node with at least one child Degree: The number of subtrees of a node Edge: The connection between one node and another Path: A sequence of nodes and edges connecting a node with a descendant Depth of node : The depth of a node is the number of edges from the tree's root node to the node Level of node : All nodes of depth d d are at level d d Height of node : The height of a node is the number of edges on the longest path between that node and a leaf Depth & Level of root node is zero (some may say 1 as well - no problem) Depth & Level are measured from top (root) to bottom (leaf) Height is measured from bottom (leaf) to top (root) Height of the leaf in last level is zero Depth of tree : The number of edges between root & deepest leaf + 1 Level of tree : The number of levels in the tree (i.e. number of edges between root & deepest leaf + 1; i.e. same as Depth of tree ) Height of tree : The height of a tree is the height of its root node using longest path Forest: A forest is a set of n \\ge 0 n \\ge 0 disjoint trees Why # need to store in hierchical way, e.g. computer filesystem provides quicker insertion/deletion than array but slower than unordered linked list No upper limit on number of nodes (like linkedlist & unlike array) Application # Easy to search (using traversal) Router Algorithm decision making Hand Shaking Lemma # In an undirected graph, Number of vertex of odd degree are always even e.g. Vertex of odd degree = Vertex connected to 3 edges. Binary Tree # Desc # tree whose each node have at most 2 children children typically known as left and right child Representation # a node consist of data, pointer to left child, pointer to right child Array Representation # Root at index 0 0 Left child at index 2 \\times i 2 \\times i Right child at index 2 \\times i + 1 2 \\times i + 1 Parent at index i / 2 i / 2 Note: - The array should be filled with nil value for non-existing child nodes - i.e. if traverse level-by-level, then if there are any missing nodes, then the index of those nodes should be filled with nil values Properties # Level(Root) = 0 Height of tree with only root node = 0 Maximum number of nodes possible at level i i is 2^{i} 2^{i} Maximum number of nodes possible in a binary tree of height h h is 2^{h+1}-1 2^{h+1}-1 no. of Levels in a Tree = Height of the Tree + 1 Minimum Possible Height of a tree having N nodes: h = \\lfloor \\log_2{(N+1)} \\rfloor h = \\lfloor \\log_2{(N+1)} \\rfloor A binary tree with L leaves is of at least height h = \\lceil \\log_2{L} \\rceil h = \\lceil \\log_2{L} \\rceil Number of leaves = Number of internal nodes having 2 children + 1 Operations # Depth First Traversal # Types: In Order: left, root, right Pre Order: root, left, right Post Order: left, right, root Ways: Standard (Recursive) Desc: Approach: Recursive DS Used: Using Stack (Iterative, Which is same as Recursive one) Desc: Apprach: Iterative DS Used: Stack Without Recursion, Without Stack: Morris Traversal: Based on Threaded Binary Tree Desc: Apprach: DS Used: Time Complexity: O(n) Auxilary Space: Avg: O(h) due to Recursive call stack; where h is max height of the tree Worst: When tree is skewed tree, i.e. h at last level = O(\\log_2{n}) O(\\log_2{n}) Dis-Advantage: Recursive solution Traversal starts from Leaf, unlike BFS. Breadth First Traversal (Level Order Traversal) # Way: Using Level By Level Looping: Desc: Find out total level of the tree : Traverse each sub-tree + Compare level of left & right Loop from first level to last For each level print all the nodes: If root is empty, return; If level reached 1, print the data Make Recursive call for each child by decreasing level by 1 Approach: Recursive DS Used: None Using Queue Desc: If given node is not none, visit & print given node, Then enqueue left & right child in the queue (if they are not none). Dequeue one node from the queue & make recursive call for that node. Approach: Recursive DS Used: Queue Time Complexity: \\theta{(n)} \\theta{(n)} Auxilary Space: Avg: O(w), where w is max width of the tree Worst: When tree is Perfect tree, i.e. w at last level = O(\\lceil {n/2} \\rceil) O(\\lceil {n/2} \\rceil) Advantage: Non recursive solution Traversal starts from root, unlike DFS. So, better if our finding is closer to root. Vertical Order Traversal # When nodes are traversed in vertical lines. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 1 / \\ 2 3 / \\ / \\ 4 5 6 7 / \\ 8 9 Vertical Order should be: 4 2 1 5 6 3 8 7 9 TODO Views # Top View of a binary tree # Top view of a binary tree is the set of nodes visible when the tree is viewed from the top . Imagine a real X-mas tree, and view it from sky. Note: Order of nodes doesn't matter. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 1 / \\ 2 3 / \\ / \\ 4 5 6 7 Top view: 4 2 1 3 7 1 / \\ 2 3 \\ 4 \\ 5 \\ 6 Top view: 2 1 3 6 Bottom View of a binary tree # Bottom view of a binary tree is the set of nodes visible when the tree is viewed from the bottom . Horizontal distance of node : The distance of a node from root, when measured horizontally (say in x-axis). And suppose root node lies on y-axis (i.e. x=0). So, another definition of bottom view: Set of bottommost nodes at their horizontal distance, i.e. For each horizontal distance unit, pick the bottom most node. Note: If there are two bottom most nodes at same horizontal distance, then pick the last/right one. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 20 / \\ 8 22 / \\ \\ 5 3 25 / \\ 10 14 Bottom view: 5, 10, 3, 14, 25 Horizontal Distance: -2, -1, 0, 1, 2 20 / \\ 8 22 / \\ / \\ 5 3 4 25 / \\ 10 14 5, 10, 4, 14, 25 Left View of a binary tree # Left view of a binary tree is the set of nodes visible when the tree is viewed from the left-side . Right View of a binary tree # Right view of a binary tree is the set of nodes visible when the tree is viewed from the right-side . Full Binary Tree # Every node at a particular level has 0 or 2 children Complete Binary Tree # All level are completely filled, except possibly last level and last level has all keys as left as possible. Practical e.g.: Binary Heap 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 18 / \\ 15 30 / \\ / \\ 40 50 100 40 18 / \\ / \\ 15 30 / \\ / \\ 40 50 100 40 / \\ / 8 7 9 Properties # height of a complete binary tree (having N nodes) = \\log_2{N} \\log_2{N} Perfect Binary Tree # All internal nodes have 2 children and all leaves are at same level. Degenerated / Pathological Tree # Every internal node has one child. Performance-wise same as linked list. Skew (Binary) Tree # A tree with every node having one child only Left Skew (Binary) Tree # A tree with every node having one Left child only Right Skew (Binary) Tree # A tree with every node having one Right child only Binary Search Tree (BST) # Desc # Operations # SEARCH, MINIMUM, MAXIMUM, PREDECESSOR, SUCCESSOR, INSERT, DELETE Self Balancing Binary Tree # (aka Balanced / Height Balanced BST) To over come the downside of the skewed BST, (when its search performance degrades to linear time), various self-balancing BST were proposed around 1962 - 1973. AVL Tree [1962] B-tree [1970] Red-Black Tree [1972] B+ tree [1972-73] Splay Tree Treap AVL Tree # First self-balancing BST to overcome the limitations of BST when its skewed. Invented By 1962 Georgy Adelson-Velsky & Evgenii Landis Specification # While creating/updating a BST, if the heights of the two child subtree of a node differs by more than 1, then rebalance that node Rebalancing is done by performing single or double-step rotation Note: Try to rebalance the BST as soon as possible. Rotation # LL (left-left) Rotation # 1 2 3 4 5 5 / 4 / 3 Here, the balance factor of the: node 3 = height of left subtree - height of right tree = 0 - 0 = 0 node 4 = height of left subtree - height of right tree = 1 - 0 = 1 node 5 = height of left subtree - height of right tree = 2 - 0 = 2 The balance factor of 5 is not in -1,0,1 (i.e. imbalanced by more than 1). Thus node 5 is imbalanced due to recent addition of node 3, to which we can reach by following \"Left-->Left\" i.e. LL. Thus node 5 is said to be LL imbalanced. To fix that, rotate the tree around 5 such that, 4 becomes parent of 3 & 5. (Assume putting a nail under node 5 and pulling 5 1-step towards right) i.e. 1 2 3 4 / \\ 3 5 Thus, it is called LL-rotation. RR (right-right) Rotation # 1 2 3 4 5 5 \\ 7 \\ 9 Similarly this is called RR imbalance. If we rotate the tree around node 5, such that node 7 become the parent of the 5 & 9. i.e. 1 2 3 7 / \\ 5 9 Then, this is called RR rotation. LR (left-right) Rotation # 1 2 3 4 5 7 / 5 \\ 6 Here, the balance factor of the: node 6 = height of left subtree - height of right tree = 0 - 0 = 0 node 5 = height of left subtree - height of right tree = 0 - 1 = -1 node 7 = height of left subtree - height of right tree = 2 - 0 = 2 The balance factor of 7 is not in -1,0,1 (i.e. imbalanced by more than 1). Thus node 7 is imbalanced due to recent addition of node 6, to which we can reach by following \"Left-->Right\" i.e. LR. Thus node 7 is said to be LR imbalanced. To fix that, follow a 2-step rotation: swap the node 5 & 6 (doing so, we achieved LL imbalanced state; Note: we changed the side as well) 1 2 3 4 5 7 / 6 / 5 now follow the LL rotation (i.e. rotate the tree around 7 such that, 6 becomes parent of 5 & 7. i.e. 1 2 3 6 / \\ 5 7 RL (right-left) Rotation # 1 2 3 4 5 5 \\ 7 / 6 Similarly this is called RL imbalance. If we: swap the node 7 and 6 1 2 3 4 5 5 \\ 6 \\ 7 rotate the tree around node 5, such that node 6 become the parent of the 5 & 7. i.e. 1 2 3 6 / \\ 5 7 Then, this is called RL rotation. The shorthand for this 2-step rotation could be: Replace Parent (node 5) with node L (node 6) and make the Parent (node 5) L child of node 6. Exercise # Q1. 1 2 3 4 5 7 / \\ 6 St1 / \\ 5 St2 A. 1 2 3 4 5 6 / \\ 5 7 / \\ St1 St2 Q2. 1 2 3 4 5 5 / \\ St1 7 / \\ 6 St2 A. 1 2 3 4 5 6 / \\ 5 7 / \\ St1 St2 Q3. 1 2 3 4 5 6 7 8 9 7 / \\ 6 St1 / \\ \\ 3 St2 St3 / \\ 1 5 / 4 (added 4) A. Addition of 4 causes LLRL (no just call it LL) imbalance of node 7. Now, just focus on node 3, 6, and 7 (as we do in LL rotation). 1 2 3 4 5 6 7 6 / \\ 3 7 / \\ / \\ 1 5 St1 St2 / / 4 St3 Complexity # Of Avg Worst Space \\Theta(n) \\Theta(n) O(n) O(n) Search \\Theta(log_2{n}) \\Theta(log_2{n}) O(log_2{n}) O(log_2{n}) Insert \\Theta(log_2{n}) \\Theta(log_2{n}) O(log_2{n}) O(log_2{n}) Delete \\Theta(log_2{n}) \\Theta(log_2{n}) O(log_2{n}) O(log_2{n}) B-Tree # (aka Balanced/Bayer/Boeing/Broad/Bushy Tree) First self-balancing m-way Search Tree (ST) to overcome the limitations of m-way ST when its skewed. Invented By 1970 Rudolf Bayer & Edward M. McCreight while working at Boeing Research Labs Idea: While creating/updating a BST, if the heights of the two child subtree of a node differs by more than 1, then rebalance that node. I see it as: generalization of the AVL tree self balanced version of m-way search tree Application # Indexing in Databases Filesystem Specification # defined by order of the tree order = number of children size of key = order - 1 nodes should be half full root can have less than half order keys are in sorted order for sequential traversal a B-tree of height h with all its nodes completely filled has n = m^{h+1}\u20131 n = m^{h+1}\u20131 entries minimum height of a b-tree: TODO maximum height of a b-tree: TODO Complexity # Of Avg Worst Space \\Theta(n) \\Theta(n) O(n) O(n) Search \\Theta(log_2{n}) \\Theta(log_2{n}) O(log_2{n}) O(log_2{n}) Insert \\Theta(log_2{n}) \\Theta(log_2{n}) O(log_2{n}) O(log_2{n}) Delete \\Theta(log_2{n}) \\Theta(log_2{n}) O(log_2{n}) O(log_2{n}) Red-Black Tree # B+ Tree # K-ary Tree # M-way Search Tree # Heap # Heap are nothing but a speacial form of tree The special condition is: the value of a node MUST be \\ge \\ge (or \\le \\le ) value of its children (heap property) notice its different than BST Another good to have condition is (not mandatory, but good for performance) all the levels should be fullfilled, except the possible last i.e. all the leaves should be in the level l l or l-1 l-1 (suppose l l is height of the tree/root, for l \\gt 0 l \\gt 0 ) i.e. the heap should be a complete binary tree reason : it will guarantee O(log_{2} N) O(log_{2} N) to build operation it will guarantee O(log_{2} N) O(log_{2} N) to insert operation because both depends on the height of the heap and height = log_{2} N height = log_{2} N , where N N is size of the heap Binary Heap # A heap with atmost 2 children Applications # Heap Sort: Heap Sort uses Binary Heap to sort an array in O(nLogn) time. Priority Queue: Priority queues can be efficiently implemented using Binary Heap because it supports insert(), delete() and extractmax(), decreaseKey() operations in O(logn) time. Binomoial Heap and Fibonacci Heap are variations of Binary Heap. These variations perform union also efficiently. Graph Algorithms: The priority queues are especially used in Graph Algorithms like Dijkstra\u2019s Shortest Path and Prim\u2019s Minimum Spanning Tree. Many problems likes K\u2019th Largest Element in an array. Sort an almost sorted array/ Merge K Sorted Arrays. Min Heap # A binary heap in which the value of a node MUST be \\le \\le value of its children Operations # Insertion Top/Min Delete-Top / Extract-Min (Get & Delete) Top Replace Max Heap # A binary heap in which the value of a node MUST be \\ge \\ge value of its children Operations # Insertion Top/Max Delete-Top / Extract-Max (Get & Delete) Top Replace Implementations # Implementation Insert Delete-Max Extract-Min Unordered Array 1 n n Unordered List 1 n n Ordered Array n 1 1 Ordered List n 1 1 Binary Search Tree log n log n log n log n log n log n Balanced Binary Search Tree log n log n log n log n log n log n Binary Heaps log n log n log n log n 1 Operations # Trie # Graph # G = (V, E) Desc # a data structure describing pairwise relations between objects made up of nodes/vertices and edges; with or without having any cycle sometimes called undirected graph for distinguishing from a directed graph, or simple graph for distinguishing from a multigraph Terminologies # Vertex (Node/Point): fundamental unit of the graph; Edge (Link/Line/Arc): The connection between one vertex and another Degree (of a Vertex): \ud835\udeff(v) in a graph is the number of edges incident to it In Degree: \ud835\udeff -(v) number of incoming edges Out Degree: \ud835\udeff +(v) number of outgoing edges Adjacent Vertex: vertices directly connected to the given vertex Adjacency Matrix: Size: VxV a matrix denoting (ordered/unordered) relations/edge between all the vertices Adjacency List: a map of size V of list/linkedlist denoting vertices connected to a particular vertex Isolated Vertex: is a vertex with degree zero Leaf Vertex (Pendant Vertex): is a vertex with degree one Source vertex: is a vertex with indegree zero Sink vertex: is a vertex with outdegree zero Simplicial vertex: is one whose neighbors form a clique: every two neighbors are adjacent Universal vertex: is a vertex that is adjacent to every other vertex in the graph Cut vertex: is a vertex the removal of which would disconnect the remaining graph Tree: A graph with no cycle; aka Acyclic Connected Graph Forest: is a disjoint set (non-overlapping elements in each set) iof trees Spanning Tree: A subset of a graph, such that it covers all the vertices of the graph but with minimal edges. Thus, it does NOT have any cycle It is NOT disconncted Can say, every connected & undirected graphs have atleast 1 spanning tree Connected components: A connected component or simply component of an undirected graph is a subgraph in which each pair of nodes is connected with each other via a path Strongly connected components (SCC): In a directed graph if every vertex is reachable from every other vertex Why # need to store network of objects / relations between objects e.g. a map of cities social n/w to describe such complicated thing in less space Application # transport networks computer networks database relationships relationships between electronic components Representation # Adjacency Matrix # A matrix (2-d array) denoting if there is an edge (directed/undirected) between 2 vertices or not. 1 2 3 4 5 6 | 0 1 2 3 --|-------- 0 | 0 1 1 1 1 | 0 0 1 0 2 | 1 0 0 1 3 | 0 0 0 0 Size: V*V Advantages Some operations are efficient and easy Disadvantages More space could be wastage if the graph is sparse (not dense) Adjacency List # A array of list (or a map of list) denoting adjacent vetices of a particular vertex. 1 2 3 4 [0]: [1]-->[2]--[3] [1]: [2] [2]: [0]-->[3] [3]: Size: E + V Advantages less space than adjacency matrix especially when the graph is not dense Disadvantages some operations are not efficient using adjacency list where lookup (whether there is an edge between 2 vertices or not) is required deleting a vertex deleting an edge Adjacency Set # TODO Operations # Addition Add Node Add Edge Removal Remove Node Remove Edge Search Contains - check if the graph contains a given value HasEdge - check if there is an edge between 2 given vertices Traversal - traverse the graph & its nodes/edges in various fashion Undirected Graph # Directed Graph # Directed Acyclic Graph (DAG) # Directed Cyclic Graph (DCG) # Weighted Graph # Subgraph # A graph whose edge & veritices are subset of the other graph. Bipartite Graph # A graph whose vertices could be divided/partitioned into 2 sets. Such that vertices of one set incidents an edge to verices on the other set. Complete Graph # A graph whose all the vertices are connected to each other. aka A graph whose all the edges are present. 1 |E| = |V|(|V|-1)/2 Sparse Graph # A graph with relatively less edges. 1 E < |V| * log|V| Dense Graph # A graph with relatively less edges are missing. References # https://gist.github.com/toransahu/bb1c9f1cd6490ff29c42fa229e827a2a TODO # avl m-way tree b tree b+ tree red-black tree trie","title":"Data Structure"},{"location":"Computer-Science/ds/#data-structure","text":"","title":"Data Structure"},{"location":"Computer-Science/ds/#introduction","text":"Data structure (is a way) or (are the special format/structures in computer memory) to store & organize the data. To suit a specific purpose. So, that we can perform some operations on them like search, add, delete, insert","title":"Introduction"},{"location":"Computer-Science/ds/#data-structure-vs-data-type","text":"","title":"Data Structure Vs Data Type"},{"location":"Computer-Science/ds/#data-type","text":"It is class of objects sharing certain properties. It is a premitive element of a particular language like C, C++, Java, Python, Go Have predefined characteristics according to the language Contains data/information without any semantic Can't be reduced anymore e.g. integer, float, character, string, boolean","title":"Data Type"},{"location":"Computer-Science/ds/#data-structure_1","text":"It is an abstract description of organization of data & operations on them It uses data types Can be decomposed to smaller DS are data type as well, but not a premitive type","title":"Data Structure"},{"location":"Computer-Science/ds/#abstract-data-type-adt","text":"Its visualization, thought, logical description, or a picture of a new data type which we are going to create while DS is a real & concrete thing, we can directly use them DS is a super set and Data type is sub set or building block for DS","title":"Abstract Data Type (ADT)"},{"location":"Computer-Science/ds/#types","text":"","title":"Types"},{"location":"Computer-Science/ds/#linear-data-structure","text":"Traverses the data elements sequentially. Only one data element can directly be reached. Array Linked List Singly Linked List Circular Linked List Doubly Linked List Stack Queue Simple Queue Circular Queue Priority Queue Dequeue / Double Ended Queue Hash Matrix","title":"Linear Data Structure"},{"location":"Computer-Science/ds/#non-linear-data-structure","text":"Every data item is attached to several other data items with a relationships. The data items are not arranged in a sequential structure. Tree Binary Tree Full Binary Tree Complete Binary Tree Heap Perfect Binary Tree Binary Search Tree - BST Degenerated / Pathological Tree Skew Binary Tree Left Skew Tree Right Skew Tree Binary Search Tree - BST Self Balancing / Balanced / Height Balanced BST AVL Tree B-tree Red-Black Tree B+ tree Splay Tree Treap K-ary Tree Heap Binary Heap Max Heap Min Heap Bionomial Heap Fibbonacci Heap Leftist Heap Skew Heap Trie Misc Indexing with Trees Segment Tree Fenwick Tree Binary Indexed Tree (BIT) Binomial Queues Open Hash Tables (Closed Addressing) Closed Hash Tables (Open Addressing) Closed Hash Tables, using buckets Graph Undirected Graph Directed Graph (Digraph) Directed Acyclic Graph (DAG) Directed Cyclic Graph [having cycle(s)] (DCG) Weighted Weighted Undirected Graph Weighted Directed Graph (WITHOUT negative weights) Weighted DAG Weighted DCG Weighted Directed Graph (WITH negative weights) Weighted DAG Weighted DCG WITH Negative Weight Cycle WITHOUT Negative Weight Cycle Misc Finite Graph Infinite Graph Trivial Graph Simple Graph Multi Graph Null Graph (Empty Graph) Complete Graph Pseudo Graph Regular Graph Bipartite Graph Labelled Graph Connected & Disconnected Graph Cyclic Graph Vertex Labelled Graph Subgraph Rooted Graph Mixed Graph Path Graph Planar Graph Cycle Graph Petersen Graph Perfect Graph Cograph Chordal Graph Misc Disjoint-set Data Structures Suffix Arrays","title":"Non Linear Data Structure"},{"location":"Computer-Science/ds/#language-specific","text":"Python list tuple set dict built-in dict collections.OrderedDict remember insertion order of keys collections.defaultdict return default values for missing keys collections.ChainMap search multiple dicts as a single mapping types.MappingProxyType A wrapper for making read-only dictionaries Java C# Go","title":"Language Specific"},{"location":"Computer-Science/ds/#array","text":"","title":"Array"},{"location":"Computer-Science/ds/#linked-list","text":"","title":"Linked List"},{"location":"Computer-Science/ds/#doubly-linked-list","text":"","title":"Doubly Linked List"},{"location":"Computer-Science/ds/#circular-linked-list","text":"","title":"Circular Linked List"},{"location":"Computer-Science/ds/#stack","text":"LIFO","title":"Stack"},{"location":"Computer-Science/ds/#queue","text":"FIFO","title":"Queue"},{"location":"Computer-Science/ds/#simple-queue","text":"","title":"Simple Queue"},{"location":"Computer-Science/ds/#circular-queue","text":"","title":"Circular Queue"},{"location":"Computer-Science/ds/#dequeue","text":"(aka Doubly eneded queue)","title":"Dequeue"},{"location":"Computer-Science/ds/#priority-queue","text":"","title":"Priority Queue"},{"location":"Computer-Science/ds/#desc","text":"","title":"Desc"},{"location":"Computer-Science/ds/#application","text":"job scheduling event-driven simulation","title":"Application"},{"location":"Computer-Science/ds/#properties","text":"","title":"Properties"},{"location":"Computer-Science/ds/#operations","text":"INSERT MAXIMUM EXTRACT-MAX REPLACE","title":"Operations"},{"location":"Computer-Science/ds/#implementation","text":"Array/Set/List(LinkedList) Binary Heap Fibbonacci Heap","title":"Implementation"},{"location":"Computer-Science/ds/#hash","text":"","title":"Hash"},{"location":"Computer-Science/ds/#desc_1","text":"Linear data structure (ADT) to make lookup fast uses a hashing functions, a slot list and, a data list time complexity (lookup): best: O(1) worst: O(n)","title":"Desc"},{"location":"Computer-Science/ds/#terminologies","text":"hash function folding method mid-squared method perfect hash functions slot/bucket load factor collision resolution techniques (rehashing) open addressing linear probing quadratic probing disadvantages clustered data chaining","title":"Terminologies"},{"location":"Computer-Science/ds/#why","text":"to optimize the look-up speed after binary search and other varients","title":"Why"},{"location":"Computer-Science/ds/#dictionary","text":"a general concept/data structure that maps keys to values many ways to implement dictionary hashtable; O(1) - O(N) red-black tree; always O(longN)","title":"Dictionary"},{"location":"Computer-Science/ds/#hash-table","text":"synchronized thread safe can be shared with many threads doesn't allows any null key or values","title":"Hash Table"},{"location":"Computer-Science/ds/#hash-map","text":"non synchronized non-thread safe can't be shared between many threads without proper synchronization code allows one null key and multiple null values preferred over hash-table","title":"Hash Map"},{"location":"Computer-Science/ds/#tree","text":"","title":"Tree"},{"location":"Computer-Science/ds/#desc_2","text":"Are hierchical data structure made up of nodes or vertices and edges without having any cycle The tree with no nodes is called the null or empty tree","title":"Desc"},{"location":"Computer-Science/ds/#terminologies_1","text":"Root: The top node in a tree Child: A node directly connected to another node when moving away from the Root Parent: The converse notion of a child Siblings: A group of nodes with the same parent Descendant: A node reachable by repeated proceeding from parent to child Ancestor: A node reachable by repeated proceeding from child to parent Leaf: (less commonly called External node) A node with no children Branch: Internal node, A node with at least one child Degree: The number of subtrees of a node Edge: The connection between one node and another Path: A sequence of nodes and edges connecting a node with a descendant Depth of node : The depth of a node is the number of edges from the tree's root node to the node Level of node : All nodes of depth d d are at level d d Height of node : The height of a node is the number of edges on the longest path between that node and a leaf Depth & Level of root node is zero (some may say 1 as well - no problem) Depth & Level are measured from top (root) to bottom (leaf) Height is measured from bottom (leaf) to top (root) Height of the leaf in last level is zero Depth of tree : The number of edges between root & deepest leaf + 1 Level of tree : The number of levels in the tree (i.e. number of edges between root & deepest leaf + 1; i.e. same as Depth of tree ) Height of tree : The height of a tree is the height of its root node using longest path Forest: A forest is a set of n \\ge 0 n \\ge 0 disjoint trees","title":"Terminologies"},{"location":"Computer-Science/ds/#why_1","text":"need to store in hierchical way, e.g. computer filesystem provides quicker insertion/deletion than array but slower than unordered linked list No upper limit on number of nodes (like linkedlist & unlike array)","title":"Why"},{"location":"Computer-Science/ds/#application_1","text":"Easy to search (using traversal) Router Algorithm decision making","title":"Application"},{"location":"Computer-Science/ds/#hand-shaking-lemma","text":"In an undirected graph, Number of vertex of odd degree are always even e.g. Vertex of odd degree = Vertex connected to 3 edges.","title":"Hand Shaking Lemma"},{"location":"Computer-Science/ds/#binary-tree","text":"","title":"Binary Tree"},{"location":"Computer-Science/ds/#desc_3","text":"tree whose each node have at most 2 children children typically known as left and right child","title":"Desc"},{"location":"Computer-Science/ds/#representation","text":"a node consist of data, pointer to left child, pointer to right child","title":"Representation"},{"location":"Computer-Science/ds/#array-representation","text":"Root at index 0 0 Left child at index 2 \\times i 2 \\times i Right child at index 2 \\times i + 1 2 \\times i + 1 Parent at index i / 2 i / 2 Note: - The array should be filled with nil value for non-existing child nodes - i.e. if traverse level-by-level, then if there are any missing nodes, then the index of those nodes should be filled with nil values","title":"Array Representation"},{"location":"Computer-Science/ds/#properties_1","text":"Level(Root) = 0 Height of tree with only root node = 0 Maximum number of nodes possible at level i i is 2^{i} 2^{i} Maximum number of nodes possible in a binary tree of height h h is 2^{h+1}-1 2^{h+1}-1 no. of Levels in a Tree = Height of the Tree + 1 Minimum Possible Height of a tree having N nodes: h = \\lfloor \\log_2{(N+1)} \\rfloor h = \\lfloor \\log_2{(N+1)} \\rfloor A binary tree with L leaves is of at least height h = \\lceil \\log_2{L} \\rceil h = \\lceil \\log_2{L} \\rceil Number of leaves = Number of internal nodes having 2 children + 1","title":"Properties"},{"location":"Computer-Science/ds/#operations_1","text":"","title":"Operations"},{"location":"Computer-Science/ds/#depth-first-traversal","text":"Types: In Order: left, root, right Pre Order: root, left, right Post Order: left, right, root Ways: Standard (Recursive) Desc: Approach: Recursive DS Used: Using Stack (Iterative, Which is same as Recursive one) Desc: Apprach: Iterative DS Used: Stack Without Recursion, Without Stack: Morris Traversal: Based on Threaded Binary Tree Desc: Apprach: DS Used: Time Complexity: O(n) Auxilary Space: Avg: O(h) due to Recursive call stack; where h is max height of the tree Worst: When tree is skewed tree, i.e. h at last level = O(\\log_2{n}) O(\\log_2{n}) Dis-Advantage: Recursive solution Traversal starts from Leaf, unlike BFS.","title":"Depth First Traversal"},{"location":"Computer-Science/ds/#breadth-first-traversal-level-order-traversal","text":"Way: Using Level By Level Looping: Desc: Find out total level of the tree : Traverse each sub-tree + Compare level of left & right Loop from first level to last For each level print all the nodes: If root is empty, return; If level reached 1, print the data Make Recursive call for each child by decreasing level by 1 Approach: Recursive DS Used: None Using Queue Desc: If given node is not none, visit & print given node, Then enqueue left & right child in the queue (if they are not none). Dequeue one node from the queue & make recursive call for that node. Approach: Recursive DS Used: Queue Time Complexity: \\theta{(n)} \\theta{(n)} Auxilary Space: Avg: O(w), where w is max width of the tree Worst: When tree is Perfect tree, i.e. w at last level = O(\\lceil {n/2} \\rceil) O(\\lceil {n/2} \\rceil) Advantage: Non recursive solution Traversal starts from root, unlike DFS. So, better if our finding is closer to root.","title":"Breadth First Traversal (Level Order Traversal)"},{"location":"Computer-Science/ds/#vertical-order-traversal","text":"When nodes are traversed in vertical lines. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 1 / \\ 2 3 / \\ / \\ 4 5 6 7 / \\ 8 9 Vertical Order should be: 4 2 1 5 6 3 8 7 9 TODO","title":"Vertical Order Traversal"},{"location":"Computer-Science/ds/#views","text":"","title":"Views"},{"location":"Computer-Science/ds/#top-view-of-a-binary-tree","text":"Top view of a binary tree is the set of nodes visible when the tree is viewed from the top . Imagine a real X-mas tree, and view it from sky. Note: Order of nodes doesn't matter. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 1 / \\ 2 3 / \\ / \\ 4 5 6 7 Top view: 4 2 1 3 7 1 / \\ 2 3 \\ 4 \\ 5 \\ 6 Top view: 2 1 3 6","title":"Top View of a binary tree"},{"location":"Computer-Science/ds/#bottom-view-of-a-binary-tree","text":"Bottom view of a binary tree is the set of nodes visible when the tree is viewed from the bottom . Horizontal distance of node : The distance of a node from root, when measured horizontally (say in x-axis). And suppose root node lies on y-axis (i.e. x=0). So, another definition of bottom view: Set of bottommost nodes at their horizontal distance, i.e. For each horizontal distance unit, pick the bottom most node. Note: If there are two bottom most nodes at same horizontal distance, then pick the last/right one. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 20 / \\ 8 22 / \\ \\ 5 3 25 / \\ 10 14 Bottom view: 5, 10, 3, 14, 25 Horizontal Distance: -2, -1, 0, 1, 2 20 / \\ 8 22 / \\ / \\ 5 3 4 25 / \\ 10 14 5, 10, 4, 14, 25","title":"Bottom View of a binary tree"},{"location":"Computer-Science/ds/#left-view-of-a-binary-tree","text":"Left view of a binary tree is the set of nodes visible when the tree is viewed from the left-side .","title":"Left View of a binary tree"},{"location":"Computer-Science/ds/#right-view-of-a-binary-tree","text":"Right view of a binary tree is the set of nodes visible when the tree is viewed from the right-side .","title":"Right View of a binary tree"},{"location":"Computer-Science/ds/#full-binary-tree","text":"Every node at a particular level has 0 or 2 children","title":"Full Binary Tree"},{"location":"Computer-Science/ds/#complete-binary-tree","text":"All level are completely filled, except possibly last level and last level has all keys as left as possible. Practical e.g.: Binary Heap 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 18 / \\ 15 30 / \\ / \\ 40 50 100 40 18 / \\ / \\ 15 30 / \\ / \\ 40 50 100 40 / \\ / 8 7 9","title":"Complete Binary Tree"},{"location":"Computer-Science/ds/#properties_2","text":"height of a complete binary tree (having N nodes) = \\log_2{N} \\log_2{N}","title":"Properties"},{"location":"Computer-Science/ds/#perfect-binary-tree","text":"All internal nodes have 2 children and all leaves are at same level.","title":"Perfect Binary Tree"},{"location":"Computer-Science/ds/#degenerated-pathological-tree","text":"Every internal node has one child. Performance-wise same as linked list.","title":"Degenerated / Pathological Tree"},{"location":"Computer-Science/ds/#skew-binary-tree","text":"A tree with every node having one child only","title":"Skew (Binary) Tree"},{"location":"Computer-Science/ds/#left-skew-binary-tree","text":"A tree with every node having one Left child only","title":"Left Skew (Binary) Tree"},{"location":"Computer-Science/ds/#right-skew-binary-tree","text":"A tree with every node having one Right child only","title":"Right Skew (Binary) Tree"},{"location":"Computer-Science/ds/#binary-search-tree-bst","text":"","title":"Binary Search Tree (BST)"},{"location":"Computer-Science/ds/#desc_4","text":"","title":"Desc"},{"location":"Computer-Science/ds/#operations_2","text":"SEARCH, MINIMUM, MAXIMUM, PREDECESSOR, SUCCESSOR, INSERT, DELETE","title":"Operations"},{"location":"Computer-Science/ds/#self-balancing-binary-tree","text":"(aka Balanced / Height Balanced BST) To over come the downside of the skewed BST, (when its search performance degrades to linear time), various self-balancing BST were proposed around 1962 - 1973. AVL Tree [1962] B-tree [1970] Red-Black Tree [1972] B+ tree [1972-73] Splay Tree Treap","title":"Self Balancing Binary Tree"},{"location":"Computer-Science/ds/#avl-tree","text":"First self-balancing BST to overcome the limitations of BST when its skewed. Invented By 1962 Georgy Adelson-Velsky & Evgenii Landis","title":"AVL Tree"},{"location":"Computer-Science/ds/#specification","text":"While creating/updating a BST, if the heights of the two child subtree of a node differs by more than 1, then rebalance that node Rebalancing is done by performing single or double-step rotation Note: Try to rebalance the BST as soon as possible.","title":"Specification"},{"location":"Computer-Science/ds/#rotation","text":"","title":"Rotation"},{"location":"Computer-Science/ds/#ll-left-left-rotation","text":"1 2 3 4 5 5 / 4 / 3 Here, the balance factor of the: node 3 = height of left subtree - height of right tree = 0 - 0 = 0 node 4 = height of left subtree - height of right tree = 1 - 0 = 1 node 5 = height of left subtree - height of right tree = 2 - 0 = 2 The balance factor of 5 is not in -1,0,1 (i.e. imbalanced by more than 1). Thus node 5 is imbalanced due to recent addition of node 3, to which we can reach by following \"Left-->Left\" i.e. LL. Thus node 5 is said to be LL imbalanced. To fix that, rotate the tree around 5 such that, 4 becomes parent of 3 & 5. (Assume putting a nail under node 5 and pulling 5 1-step towards right) i.e. 1 2 3 4 / \\ 3 5 Thus, it is called LL-rotation.","title":"LL (left-left) Rotation"},{"location":"Computer-Science/ds/#rr-right-right-rotation","text":"1 2 3 4 5 5 \\ 7 \\ 9 Similarly this is called RR imbalance. If we rotate the tree around node 5, such that node 7 become the parent of the 5 & 9. i.e. 1 2 3 7 / \\ 5 9 Then, this is called RR rotation.","title":"RR (right-right) Rotation"},{"location":"Computer-Science/ds/#lr-left-right-rotation","text":"1 2 3 4 5 7 / 5 \\ 6 Here, the balance factor of the: node 6 = height of left subtree - height of right tree = 0 - 0 = 0 node 5 = height of left subtree - height of right tree = 0 - 1 = -1 node 7 = height of left subtree - height of right tree = 2 - 0 = 2 The balance factor of 7 is not in -1,0,1 (i.e. imbalanced by more than 1). Thus node 7 is imbalanced due to recent addition of node 6, to which we can reach by following \"Left-->Right\" i.e. LR. Thus node 7 is said to be LR imbalanced. To fix that, follow a 2-step rotation: swap the node 5 & 6 (doing so, we achieved LL imbalanced state; Note: we changed the side as well) 1 2 3 4 5 7 / 6 / 5 now follow the LL rotation (i.e. rotate the tree around 7 such that, 6 becomes parent of 5 & 7. i.e. 1 2 3 6 / \\ 5 7","title":"LR (left-right) Rotation"},{"location":"Computer-Science/ds/#rl-right-left-rotation","text":"1 2 3 4 5 5 \\ 7 / 6 Similarly this is called RL imbalance. If we: swap the node 7 and 6 1 2 3 4 5 5 \\ 6 \\ 7 rotate the tree around node 5, such that node 6 become the parent of the 5 & 7. i.e. 1 2 3 6 / \\ 5 7 Then, this is called RL rotation. The shorthand for this 2-step rotation could be: Replace Parent (node 5) with node L (node 6) and make the Parent (node 5) L child of node 6.","title":"RL (right-left) Rotation"},{"location":"Computer-Science/ds/#exercise","text":"Q1. 1 2 3 4 5 7 / \\ 6 St1 / \\ 5 St2 A. 1 2 3 4 5 6 / \\ 5 7 / \\ St1 St2 Q2. 1 2 3 4 5 5 / \\ St1 7 / \\ 6 St2 A. 1 2 3 4 5 6 / \\ 5 7 / \\ St1 St2 Q3. 1 2 3 4 5 6 7 8 9 7 / \\ 6 St1 / \\ \\ 3 St2 St3 / \\ 1 5 / 4 (added 4) A. Addition of 4 causes LLRL (no just call it LL) imbalance of node 7. Now, just focus on node 3, 6, and 7 (as we do in LL rotation). 1 2 3 4 5 6 7 6 / \\ 3 7 / \\ / \\ 1 5 St1 St2 / / 4 St3","title":"Exercise"},{"location":"Computer-Science/ds/#complexity","text":"Of Avg Worst Space \\Theta(n) \\Theta(n) O(n) O(n) Search \\Theta(log_2{n}) \\Theta(log_2{n}) O(log_2{n}) O(log_2{n}) Insert \\Theta(log_2{n}) \\Theta(log_2{n}) O(log_2{n}) O(log_2{n}) Delete \\Theta(log_2{n}) \\Theta(log_2{n}) O(log_2{n}) O(log_2{n})","title":"Complexity"},{"location":"Computer-Science/ds/#b-tree","text":"(aka Balanced/Bayer/Boeing/Broad/Bushy Tree) First self-balancing m-way Search Tree (ST) to overcome the limitations of m-way ST when its skewed. Invented By 1970 Rudolf Bayer & Edward M. McCreight while working at Boeing Research Labs Idea: While creating/updating a BST, if the heights of the two child subtree of a node differs by more than 1, then rebalance that node. I see it as: generalization of the AVL tree self balanced version of m-way search tree","title":"B-Tree"},{"location":"Computer-Science/ds/#application_2","text":"Indexing in Databases Filesystem","title":"Application"},{"location":"Computer-Science/ds/#specification_1","text":"defined by order of the tree order = number of children size of key = order - 1 nodes should be half full root can have less than half order keys are in sorted order for sequential traversal a B-tree of height h with all its nodes completely filled has n = m^{h+1}\u20131 n = m^{h+1}\u20131 entries minimum height of a b-tree: TODO maximum height of a b-tree: TODO","title":"Specification"},{"location":"Computer-Science/ds/#complexity_1","text":"Of Avg Worst Space \\Theta(n) \\Theta(n) O(n) O(n) Search \\Theta(log_2{n}) \\Theta(log_2{n}) O(log_2{n}) O(log_2{n}) Insert \\Theta(log_2{n}) \\Theta(log_2{n}) O(log_2{n}) O(log_2{n}) Delete \\Theta(log_2{n}) \\Theta(log_2{n}) O(log_2{n}) O(log_2{n})","title":"Complexity"},{"location":"Computer-Science/ds/#red-black-tree","text":"","title":"Red-Black Tree"},{"location":"Computer-Science/ds/#b-tree_1","text":"","title":"B+ Tree"},{"location":"Computer-Science/ds/#k-ary-tree","text":"","title":"K-ary Tree"},{"location":"Computer-Science/ds/#m-way-search-tree","text":"","title":"M-way Search Tree"},{"location":"Computer-Science/ds/#heap","text":"Heap are nothing but a speacial form of tree The special condition is: the value of a node MUST be \\ge \\ge (or \\le \\le ) value of its children (heap property) notice its different than BST Another good to have condition is (not mandatory, but good for performance) all the levels should be fullfilled, except the possible last i.e. all the leaves should be in the level l l or l-1 l-1 (suppose l l is height of the tree/root, for l \\gt 0 l \\gt 0 ) i.e. the heap should be a complete binary tree reason : it will guarantee O(log_{2} N) O(log_{2} N) to build operation it will guarantee O(log_{2} N) O(log_{2} N) to insert operation because both depends on the height of the heap and height = log_{2} N height = log_{2} N , where N N is size of the heap","title":"Heap"},{"location":"Computer-Science/ds/#binary-heap","text":"A heap with atmost 2 children","title":"Binary Heap"},{"location":"Computer-Science/ds/#applications","text":"Heap Sort: Heap Sort uses Binary Heap to sort an array in O(nLogn) time. Priority Queue: Priority queues can be efficiently implemented using Binary Heap because it supports insert(), delete() and extractmax(), decreaseKey() operations in O(logn) time. Binomoial Heap and Fibonacci Heap are variations of Binary Heap. These variations perform union also efficiently. Graph Algorithms: The priority queues are especially used in Graph Algorithms like Dijkstra\u2019s Shortest Path and Prim\u2019s Minimum Spanning Tree. Many problems likes K\u2019th Largest Element in an array. Sort an almost sorted array/ Merge K Sorted Arrays.","title":"Applications"},{"location":"Computer-Science/ds/#min-heap","text":"A binary heap in which the value of a node MUST be \\le \\le value of its children","title":"Min Heap"},{"location":"Computer-Science/ds/#operations_3","text":"Insertion Top/Min Delete-Top / Extract-Min (Get & Delete) Top Replace","title":"Operations"},{"location":"Computer-Science/ds/#max-heap","text":"A binary heap in which the value of a node MUST be \\ge \\ge value of its children","title":"Max Heap"},{"location":"Computer-Science/ds/#operations_4","text":"Insertion Top/Max Delete-Top / Extract-Max (Get & Delete) Top Replace","title":"Operations"},{"location":"Computer-Science/ds/#implementations","text":"Implementation Insert Delete-Max Extract-Min Unordered Array 1 n n Unordered List 1 n n Ordered Array n 1 1 Ordered List n 1 1 Binary Search Tree log n log n log n log n log n log n Balanced Binary Search Tree log n log n log n log n log n log n Binary Heaps log n log n log n log n 1","title":"Implementations"},{"location":"Computer-Science/ds/#operations_5","text":"","title":"Operations"},{"location":"Computer-Science/ds/#trie","text":"","title":"Trie"},{"location":"Computer-Science/ds/#graph","text":"G = (V, E)","title":"Graph"},{"location":"Computer-Science/ds/#desc_5","text":"a data structure describing pairwise relations between objects made up of nodes/vertices and edges; with or without having any cycle sometimes called undirected graph for distinguishing from a directed graph, or simple graph for distinguishing from a multigraph","title":"Desc"},{"location":"Computer-Science/ds/#terminologies_2","text":"Vertex (Node/Point): fundamental unit of the graph; Edge (Link/Line/Arc): The connection between one vertex and another Degree (of a Vertex): \ud835\udeff(v) in a graph is the number of edges incident to it In Degree: \ud835\udeff -(v) number of incoming edges Out Degree: \ud835\udeff +(v) number of outgoing edges Adjacent Vertex: vertices directly connected to the given vertex Adjacency Matrix: Size: VxV a matrix denoting (ordered/unordered) relations/edge between all the vertices Adjacency List: a map of size V of list/linkedlist denoting vertices connected to a particular vertex Isolated Vertex: is a vertex with degree zero Leaf Vertex (Pendant Vertex): is a vertex with degree one Source vertex: is a vertex with indegree zero Sink vertex: is a vertex with outdegree zero Simplicial vertex: is one whose neighbors form a clique: every two neighbors are adjacent Universal vertex: is a vertex that is adjacent to every other vertex in the graph Cut vertex: is a vertex the removal of which would disconnect the remaining graph Tree: A graph with no cycle; aka Acyclic Connected Graph Forest: is a disjoint set (non-overlapping elements in each set) iof trees Spanning Tree: A subset of a graph, such that it covers all the vertices of the graph but with minimal edges. Thus, it does NOT have any cycle It is NOT disconncted Can say, every connected & undirected graphs have atleast 1 spanning tree Connected components: A connected component or simply component of an undirected graph is a subgraph in which each pair of nodes is connected with each other via a path Strongly connected components (SCC): In a directed graph if every vertex is reachable from every other vertex","title":"Terminologies"},{"location":"Computer-Science/ds/#why_2","text":"need to store network of objects / relations between objects e.g. a map of cities social n/w to describe such complicated thing in less space","title":"Why"},{"location":"Computer-Science/ds/#application_3","text":"transport networks computer networks database relationships relationships between electronic components","title":"Application"},{"location":"Computer-Science/ds/#representation_1","text":"","title":"Representation"},{"location":"Computer-Science/ds/#adjacency-matrix","text":"A matrix (2-d array) denoting if there is an edge (directed/undirected) between 2 vertices or not. 1 2 3 4 5 6 | 0 1 2 3 --|-------- 0 | 0 1 1 1 1 | 0 0 1 0 2 | 1 0 0 1 3 | 0 0 0 0 Size: V*V Advantages Some operations are efficient and easy Disadvantages More space could be wastage if the graph is sparse (not dense)","title":"Adjacency Matrix"},{"location":"Computer-Science/ds/#adjacency-list","text":"A array of list (or a map of list) denoting adjacent vetices of a particular vertex. 1 2 3 4 [0]: [1]-->[2]--[3] [1]: [2] [2]: [0]-->[3] [3]: Size: E + V Advantages less space than adjacency matrix especially when the graph is not dense Disadvantages some operations are not efficient using adjacency list where lookup (whether there is an edge between 2 vertices or not) is required deleting a vertex deleting an edge","title":"Adjacency List"},{"location":"Computer-Science/ds/#adjacency-set","text":"TODO","title":"Adjacency Set"},{"location":"Computer-Science/ds/#operations_6","text":"Addition Add Node Add Edge Removal Remove Node Remove Edge Search Contains - check if the graph contains a given value HasEdge - check if there is an edge between 2 given vertices Traversal - traverse the graph & its nodes/edges in various fashion","title":"Operations"},{"location":"Computer-Science/ds/#undirected-graph","text":"","title":"Undirected Graph"},{"location":"Computer-Science/ds/#directed-graph","text":"","title":"Directed Graph"},{"location":"Computer-Science/ds/#directed-acyclic-graph-dag","text":"","title":"Directed Acyclic Graph (DAG)"},{"location":"Computer-Science/ds/#directed-cyclic-graph-dcg","text":"","title":"Directed Cyclic Graph (DCG)"},{"location":"Computer-Science/ds/#weighted-graph","text":"","title":"Weighted Graph"},{"location":"Computer-Science/ds/#subgraph","text":"A graph whose edge & veritices are subset of the other graph.","title":"Subgraph"},{"location":"Computer-Science/ds/#bipartite-graph","text":"A graph whose vertices could be divided/partitioned into 2 sets. Such that vertices of one set incidents an edge to verices on the other set.","title":"Bipartite Graph"},{"location":"Computer-Science/ds/#complete-graph","text":"A graph whose all the vertices are connected to each other. aka A graph whose all the edges are present. 1 |E| = |V|(|V|-1)/2","title":"Complete Graph"},{"location":"Computer-Science/ds/#sparse-graph","text":"A graph with relatively less edges. 1 E < |V| * log|V|","title":"Sparse Graph"},{"location":"Computer-Science/ds/#dense-graph","text":"A graph with relatively less edges are missing.","title":"Dense Graph"},{"location":"Computer-Science/ds/#references","text":"https://gist.github.com/toransahu/bb1c9f1cd6490ff29c42fa229e827a2a","title":"References"},{"location":"Computer-Science/ds/#todo","text":"avl m-way tree b tree b+ tree red-black tree trie","title":"TODO"},{"location":"Computer-Science/faq/","text":"FAQs # Table of Contents FAQs Software Network FAQs DBMS OS Software # Stack vs heap memory Pointers how to go about debugging a piece of software. Design patterns OOP SOLID Network FAQs # What is a subnet TCP/IP OSI Layer & its working Routers L2 & L3 DBMS # joins second highest salary indexing multilevel, multi column indexing explain, analyze, debug function vs procedure aggregation vs analytical OS # Process vs Thread Kernel Thread vs User Space Thread How OS boots, list the operations Semaphores and mutexes Multiprocess and multithread questions deadlock prevention How does CPU caching work?","title":"FAQs"},{"location":"Computer-Science/faq/#faqs","text":"","title":"FAQs"},{"location":"Computer-Science/faq/#software","text":"Stack vs heap memory Pointers how to go about debugging a piece of software. Design patterns OOP SOLID","title":"Software"},{"location":"Computer-Science/faq/#network-faqs","text":"What is a subnet TCP/IP OSI Layer & its working Routers L2 & L3","title":"Network FAQs"},{"location":"Computer-Science/faq/#dbms","text":"joins second highest salary indexing multilevel, multi column indexing explain, analyze, debug function vs procedure aggregation vs analytical","title":"DBMS"},{"location":"Computer-Science/faq/#os","text":"Process vs Thread Kernel Thread vs User Space Thread How OS boots, list the operations Semaphores and mutexes Multiprocess and multithread questions deadlock prevention How does CPU caching work?","title":"OS"},{"location":"Computer-Science/keywords/","text":"Keywords # Table of Contents Keywords CS Dictionary General i368 amd64 CRUD module package library api framework sdk ide toolkit markup lang JSON YAML Web SaaS PaaS IaaS FaaS BaaS CaaS CSS LESS Sass CDN RWD (Responsive Web Design) Mobile First UI Design/Front-end Framework JS Framework HTTP HTTP Request Methods CSRF : Cross Site Request Forgery Web Server Web Apllication WSGI: Web Server Gateway Interface Web Service Web API Appication Without web API Appication Without web API Infrastructure Nginx: Gunicorn: Celery: Redis: Supervisor: CS Dictionary # General # i368 # genereally refers a 32bit processor Intel 80368: First 32bit CPU amd64 # genereally refers a 64bit processor 64bit CPU architecture invented by amd CRUD # create, read, update, delete SQL: insert, select, update, delete HTTP: Used in RESTful API: PUT/POST, GET, PUT/POST/PATCH, DELETE DDS: Data Distribution Service: write, read/take, write, despose module # a file containing codes package # collection of modules library # A bunch of code which simplifies functions/methods for quick use. to help you do things more quickly/easily offer one area of functionality api # application programming interface the interface to the library that you can call to ask it to do things for you framework # is a big library or group of libraries that provides many services supplies a complete base on which you build your own code sdk # software development kit is a library or group of libraries with extra tool applications, data files and sample code ide # integrated development environment text editor with additional support for developing,compiling and debugging applications. e.g Eclipse, Visual Studio. toolkit # is like an SDK with more focus on providing tools and applications than on just code libraries markup lang # Markup language is a paradigm/system to annotate a document in a way that is syntactically distinguishable from the text. e.g. (X)HTML: (Extensible)Hypertext ML, XML: Extensible ML, (La)TeX: (Lamport)TeX JSON # JavaScript object notation YAML # stands for \"YAML Ain't Markup Language\" YAML is to configuration what markdown is to markup YAML is a human-readable data serialization language commonly used for configuration files, but could be used in many applications where data is being stored (e.g. debugging output) data is being transmitted (e.g. document headers). Uses python style indentation, [], {} Superset of JSON YAML is case sensitive. Can contain unix/linux commands Web # SaaS # Software as a service Gmail, Google apps, Cisco WebEx PaaS # Platform as a service heroku, Travis CI, Circle CI IaaS # Infrastructure as a service AWS, MS Azure, FaaS # Function as a service BaaS # Backend as a service CaaS # Container as a service CSS # cascading style sheet a style sheet language used for describing presentation of a document written in a markup language. LESS # CSS preprocessor: scripting language that extends CSS Leaner CSS CSS Dry Implemented in JavaScript Extension: .less Sass # CSS preprocessor: scripting language that extends CSS Syntactically Awesome style sheet CSS Dry implemented in Ruby Extension: .scss CDN # Content Delivery Network a system of distributed servers/network that deliver pages and other web content to a user based on geographical locations of the user to provide highspeed delivery Working: When a user request a page that is a part of CDN, then request goes to the central server and redirects the request to nearby server RWD (Responsive Web Design) # A web designing approach crafting site to provide an optimal viewing experience easy reading and navigation with a minimum of resizing, panning, and scrolling across a wide range of device (from mobile phones, tablets to desktop screens) Mobile First # A paradigm for creating UX design UX for mobile devices first than other because users prefer mobiles nowadays UI Design/Front-end Framework # eg. Bootstrap, Foundation, Pure includes CSS, HTML for typography, icons, forms, buttons, tables, layout grids, navigation. includes JS also support for responsive JS Framework # e.g. AngularJS, ReactJS, JavaScript framework for building CRUD centric AJAX style web applications features 2-way data binding, deep linking, routing, transition animations and a lot lot more HTTP # Hyper text transfer protocol base protocol the internet is built on a request and response system client sends a request to endpoint and endpoint responds e.g. a browser accessing a web server, an app accessing an API HTTP Request Methods # Used for sending and retrieving HTML form data. GET Default sends request by enclosing all the data into url string The request/response have HTTP header only Parameters are visible Parameters remains in history Hackable Not Secure Restriction in data length, 2048 chars, depends on browser can be cached Should be used for case when state of system/data is not going to be changed unsuitable for sending password POST sends request by enclosing all the data into The request/response have HTTP header & HTTP body HTTP body contains message in URLEncoded format PUT DELETE CSRF : Cross Site Request Forgery # A Cross-site request forgery hole is when a malicious site can cause a visitor's browser to make a request to your server that causes a change on the server. The server thinks that because the request comes with the user's cookies, the user wanted to submit that form. Web Server # a software, not a machine which stores code recieves requests from client/browser returns response, but doesn't create response so, it talks to web application Web Apllication # creates response based on urls passes response to web server WSGI: Web Server Gateway Interface # an interface between web server & application contains some statements, set of rules its not a software/library/framework WSGI compliant server will able to communicate with a WSGI compliant web app in WSGI, WSGI application has to be callable & it needs o be given to web server, so web server can call web application whenever it receives a request Web Service # a piece of software available over internet and can be ustilized by some other softwares using standard messaging system XML. Web API # Application Programming Interface In web world its synonym to 'web services' used by client apps to retrieve and update data Appication Without web API # Appication Without web API # - Source: https://knpuniversity.com/screencast/rest/rest https://blogs.msdn.microsoft.com/martinkearn/2015/01/05/introduction-to-rest-and-net-web-api/ http://www.ics.uci.edu/~fielding/pubs/dissertation/rest_arch_style.htm Infrastructure # (Nginx, Gunicorn, RabbitMQ, Celery, Redis, memcached, apache, WSGI) (load balancer, web accelerator, cache, database, task queue, etc.) Nginx: # An HTTP and Reverse Proxy Server Gunicorn: # A WSGI HTTP server Celery: # A tool for asynchronous processing with Python Redis: # A message broker Supervisor: # A process control system for unix","title":"Keywords"},{"location":"Computer-Science/keywords/#keywords","text":"","title":"Keywords"},{"location":"Computer-Science/keywords/#cs-dictionary","text":"","title":"CS Dictionary"},{"location":"Computer-Science/keywords/#general","text":"","title":"General"},{"location":"Computer-Science/keywords/#i368","text":"genereally refers a 32bit processor Intel 80368: First 32bit CPU","title":"i368"},{"location":"Computer-Science/keywords/#amd64","text":"genereally refers a 64bit processor 64bit CPU architecture invented by amd","title":"amd64"},{"location":"Computer-Science/keywords/#crud","text":"create, read, update, delete SQL: insert, select, update, delete HTTP: Used in RESTful API: PUT/POST, GET, PUT/POST/PATCH, DELETE DDS: Data Distribution Service: write, read/take, write, despose","title":"CRUD"},{"location":"Computer-Science/keywords/#module","text":"a file containing codes","title":"module"},{"location":"Computer-Science/keywords/#package","text":"collection of modules","title":"package"},{"location":"Computer-Science/keywords/#library","text":"A bunch of code which simplifies functions/methods for quick use. to help you do things more quickly/easily offer one area of functionality","title":"library"},{"location":"Computer-Science/keywords/#api","text":"application programming interface the interface to the library that you can call to ask it to do things for you","title":"api"},{"location":"Computer-Science/keywords/#framework","text":"is a big library or group of libraries that provides many services supplies a complete base on which you build your own code","title":"framework"},{"location":"Computer-Science/keywords/#sdk","text":"software development kit is a library or group of libraries with extra tool applications, data files and sample code","title":"sdk"},{"location":"Computer-Science/keywords/#ide","text":"integrated development environment text editor with additional support for developing,compiling and debugging applications. e.g Eclipse, Visual Studio.","title":"ide"},{"location":"Computer-Science/keywords/#toolkit","text":"is like an SDK with more focus on providing tools and applications than on just code libraries","title":"toolkit"},{"location":"Computer-Science/keywords/#markup-lang","text":"Markup language is a paradigm/system to annotate a document in a way that is syntactically distinguishable from the text. e.g. (X)HTML: (Extensible)Hypertext ML, XML: Extensible ML, (La)TeX: (Lamport)TeX","title":"markup lang"},{"location":"Computer-Science/keywords/#json","text":"JavaScript object notation","title":"JSON"},{"location":"Computer-Science/keywords/#yaml","text":"stands for \"YAML Ain't Markup Language\" YAML is to configuration what markdown is to markup YAML is a human-readable data serialization language commonly used for configuration files, but could be used in many applications where data is being stored (e.g. debugging output) data is being transmitted (e.g. document headers). Uses python style indentation, [], {} Superset of JSON YAML is case sensitive. Can contain unix/linux commands","title":"YAML"},{"location":"Computer-Science/keywords/#web","text":"","title":"Web"},{"location":"Computer-Science/keywords/#saas","text":"Software as a service Gmail, Google apps, Cisco WebEx","title":"SaaS"},{"location":"Computer-Science/keywords/#paas","text":"Platform as a service heroku, Travis CI, Circle CI","title":"PaaS"},{"location":"Computer-Science/keywords/#iaas","text":"Infrastructure as a service AWS, MS Azure,","title":"IaaS"},{"location":"Computer-Science/keywords/#faas","text":"Function as a service","title":"FaaS"},{"location":"Computer-Science/keywords/#baas","text":"Backend as a service","title":"BaaS"},{"location":"Computer-Science/keywords/#caas","text":"Container as a service","title":"CaaS"},{"location":"Computer-Science/keywords/#css","text":"cascading style sheet a style sheet language used for describing presentation of a document written in a markup language.","title":"CSS"},{"location":"Computer-Science/keywords/#less","text":"CSS preprocessor: scripting language that extends CSS Leaner CSS CSS Dry Implemented in JavaScript Extension: .less","title":"LESS"},{"location":"Computer-Science/keywords/#sass","text":"CSS preprocessor: scripting language that extends CSS Syntactically Awesome style sheet CSS Dry implemented in Ruby Extension: .scss","title":"Sass"},{"location":"Computer-Science/keywords/#cdn","text":"Content Delivery Network a system of distributed servers/network that deliver pages and other web content to a user based on geographical locations of the user to provide highspeed delivery Working: When a user request a page that is a part of CDN, then request goes to the central server and redirects the request to nearby server","title":"CDN"},{"location":"Computer-Science/keywords/#rwd-responsive-web-design","text":"A web designing approach crafting site to provide an optimal viewing experience easy reading and navigation with a minimum of resizing, panning, and scrolling across a wide range of device (from mobile phones, tablets to desktop screens)","title":"RWD (Responsive Web Design)"},{"location":"Computer-Science/keywords/#mobile-first","text":"A paradigm for creating UX design UX for mobile devices first than other because users prefer mobiles nowadays","title":"Mobile First"},{"location":"Computer-Science/keywords/#ui-designfront-end-framework","text":"eg. Bootstrap, Foundation, Pure includes CSS, HTML for typography, icons, forms, buttons, tables, layout grids, navigation. includes JS also support for responsive","title":"UI Design/Front-end Framework"},{"location":"Computer-Science/keywords/#js-framework","text":"e.g. AngularJS, ReactJS, JavaScript framework for building CRUD centric AJAX style web applications features 2-way data binding, deep linking, routing, transition animations and a lot lot more","title":"JS Framework"},{"location":"Computer-Science/keywords/#http","text":"Hyper text transfer protocol base protocol the internet is built on a request and response system client sends a request to endpoint and endpoint responds e.g. a browser accessing a web server, an app accessing an API","title":"HTTP"},{"location":"Computer-Science/keywords/#http-request-methods","text":"Used for sending and retrieving HTML form data. GET Default sends request by enclosing all the data into url string The request/response have HTTP header only Parameters are visible Parameters remains in history Hackable Not Secure Restriction in data length, 2048 chars, depends on browser can be cached Should be used for case when state of system/data is not going to be changed unsuitable for sending password POST sends request by enclosing all the data into The request/response have HTTP header & HTTP body HTTP body contains message in URLEncoded format PUT DELETE","title":"HTTP Request Methods"},{"location":"Computer-Science/keywords/#csrf-cross-site-request-forgery","text":"A Cross-site request forgery hole is when a malicious site can cause a visitor's browser to make a request to your server that causes a change on the server. The server thinks that because the request comes with the user's cookies, the user wanted to submit that form.","title":"CSRF : Cross Site Request Forgery"},{"location":"Computer-Science/keywords/#web-server","text":"a software, not a machine which stores code recieves requests from client/browser returns response, but doesn't create response so, it talks to web application","title":"Web Server"},{"location":"Computer-Science/keywords/#web-apllication","text":"creates response based on urls passes response to web server","title":"Web Apllication"},{"location":"Computer-Science/keywords/#wsgi-web-server-gateway-interface","text":"an interface between web server & application contains some statements, set of rules its not a software/library/framework WSGI compliant server will able to communicate with a WSGI compliant web app in WSGI, WSGI application has to be callable & it needs o be given to web server, so web server can call web application whenever it receives a request","title":"WSGI: Web Server Gateway Interface"},{"location":"Computer-Science/keywords/#web-service","text":"a piece of software available over internet and can be ustilized by some other softwares using standard messaging system XML.","title":"Web Service"},{"location":"Computer-Science/keywords/#web-api","text":"Application Programming Interface In web world its synonym to 'web services' used by client apps to retrieve and update data","title":"Web API"},{"location":"Computer-Science/keywords/#appication-without-web-api","text":"","title":"Appication Without web API"},{"location":"Computer-Science/keywords/#appication-without-web-api_1","text":"- Source: https://knpuniversity.com/screencast/rest/rest https://blogs.msdn.microsoft.com/martinkearn/2015/01/05/introduction-to-rest-and-net-web-api/ http://www.ics.uci.edu/~fielding/pubs/dissertation/rest_arch_style.htm","title":"Appication Without web API"},{"location":"Computer-Science/keywords/#infrastructure","text":"(Nginx, Gunicorn, RabbitMQ, Celery, Redis, memcached, apache, WSGI) (load balancer, web accelerator, cache, database, task queue, etc.)","title":"Infrastructure"},{"location":"Computer-Science/keywords/#nginx","text":"An HTTP and Reverse Proxy Server","title":"Nginx:"},{"location":"Computer-Science/keywords/#gunicorn","text":"A WSGI HTTP server","title":"Gunicorn:"},{"location":"Computer-Science/keywords/#celery","text":"A tool for asynchronous processing with Python","title":"Celery:"},{"location":"Computer-Science/keywords/#redis","text":"A message broker","title":"Redis:"},{"location":"Computer-Science/keywords/#supervisor","text":"A process control system for unix","title":"Supervisor:"},{"location":"Computer-Science/maths/","text":"Mathematics # Table of Contents Mathematics Theory Terminology Arithmetic Progrssion Intro Geometric Progression Intro Theory Terminology # A theorem is a statement that has been proven to be true, either on the basis of generally accepted statements such as axioms, or on the basis of previously established statements. An axiom or postulate is a statement that is accepted without proof and regarded as fundamental to a subject. Historically axioms were regarded as \"self-evident\", but recently they have been considered assumptions that characterize the subject of study. In classical geometry, axioms are general statements, while postulates are statements about geometrical objects.[17] A definition is yet another form of statement that is also accepted without proof \u2014 since it simply gives the meaning of a word or phrase in terms of known concepts. A conjecture (or sometimes hypothesis, but with a different meaning from the one discussed above) is an unproved statement that is believed to be true. To be considered a conjecture, a statement must usually be proposed publicly, at which point the name of the proponent may be attached to the conjecture, as with Goldbach's conjecture. Other famous conjectures include the Collatz conjecture and the Riemann hypothesis. On the other hand, Fermat's Last Theorem has always been known by that name, even before it was proved; it was never known as \"Fermat's conjecture\". A proposition is a theorem of lesser importance, or considered so elementary that it may be stated with no proof. It is also the term for the statement-part of the theorem (If A then B) that precedes the proof-part, and particularly elementary theorems are often given only as a proposition, hence the name. This term connotes a statement with an especially simple or immediately obvious proof, while the term theorem is usually reserved for the most important results or those with long or difficult proofs. Some authors never use the word \"proposition\", while some others reserve the word \"theorem\" only for fundamental, central, or particularly important results.[a] A lemma is a \"helping theorem\", a proposition with little applicability except that it forms part of the proof of a larger theorem. In some cases, as the relative importance of different theorems becomes more clear, what was once considered a lemma is now considered a theorem, though the word \"lemma\" remains in the name. Examples include Gauss's lemma, Zorn's lemma, and the fundamental lemma. A corollary is a proposition that follows with little or no required proof from another theorem or definition.[18] Also a corollary can be a theorem restated in a simpler form, for a more restricted special case. For example, the theorem that all angles in a rectangle are right angles has as corollary that all angles in a square (a special case of a rectangle) are right angles. An Inference: A converse of a theorem is a statement formed by interchanging what is given in a theorem and what is to be proved. For example, the isosceles triangle theorem states that if two sides of a triangle are equal then two angles are equal. In the converse, the given (that two sides are equal) and what is to be proved (that two angles are equal) are swapped, so the converse is the statement that if two angles of a triangle are equal then two sides are equal. In this example, the converse can be proved as another theorem, but this is often not the case. For example, the converse to the theorem that two right angles are equal angles is the statement that two equal angles must be right angles, and this is clearly not always the case.[19] A generalization is a theorem which includes a previously proved theorem as a special case and hence as a corollary.[b] An identity is an equality, contained in a theorem, between two mathematical expressions that holds regardless of the values being used for any variables or parameters appearing in the expressions (as long as they are within the range of validity).[20] Examples include Euler's formula and Vandermonde's identity. A rule is a theorem, such as Bayes' rule and Cramer's rule, that establishes a useful formula. A law or a principle is a theorem that applies in a wide range of circumstances. Examples include the law of large numbers, the law of cosines, Kolmogorov's zero\u2013one law, Harnack's principle, the least-upper-bound principle, and the pigeonhole principle.[21] Arithmetic Progrssion # Intro # Lets take series 1, 3, 5, 9, ..... nth here 1, 1+2, 1 + (2+2), 1 + (2+2+2), ..... 1 i.e. a + a+d, a+2d, a+3d,...... for n series 1 a + a+d, a+2d, a+3d,......, a + (n-1)d So, 1 2 3 a = initial number == 1 d = difference == 2 nth or last term = l = a + (n-1)d l = a + (n-1)d Sum of AP = sum of a + a+d, a+2d, a+3d,......, a + (n-1)d = a + a+d + a+2d + a+3d + ......+ a+(n-1)d = a*n + d(1 + 2 + 3 +....+ n-1) i.e. S_n = an + \\frac{d(n-1)n}{2} S_n = an + \\frac{d(n-1)n}{2} S_n = \\frac{1}{2}n(2a+ (n-1)d) S_n = \\frac{1}{2}n(2a+ (n-1)d) S_n = \\frac{1}{2}n(a+l) S_n = \\frac{1}{2}n(a+l) Geometric Progression # Intro # Lets take a series 2, 6, 18, 54, .... 2, 6, 18, 54, .... i.e. 2, 2 \\times 3, 2 \\times 3^2, 2 \\times 3^3, ... 2, 2 \\times 3, 2 \\times 3^2, 2 \\times 3^3, ... a, a \\times r, 2 \\times r^2, 2 \\times r^3, ... a, a \\times r, 2 \\times r^2, 2 \\times r^3, ... where a a is first term, and r r is ratio So, n^{th} n^{th} term = a \\times r^{n-1} a \\times r^{n-1} Sum of the GP S_n = a + a r + 2 r^2 + 2 r^3 + ... + a r^{n-1} S_n = a + a r + 2 r^2 + 2 r^3 + ... + a r^{n-1} rS_n = a r + 2 r^2 + 2 r^3 + 2 r^4 + ... + a r^{n} rS_n = a r + 2 r^2 + 2 r^3 + 2 r^4 + ... + a r^{n} ------------------- ------------------- S_n - rS_n = a - a r^n S_n - rS_n = a - a r^n S_n (1 - r) = a (1 - r^n) S_n (1 - r) = a (1 - r^n) S_n = \\frac{a (1 - r^n)}{(1 - r)} S_n = \\frac{a (1 - r^n)}{(1 - r)} S_{\\infty} = \\frac{a} {(1 - r)} S_{\\infty} = \\frac{a} {(1 - r)}","title":"Mathematics"},{"location":"Computer-Science/maths/#mathematics","text":"","title":"Mathematics"},{"location":"Computer-Science/maths/#theory-terminology","text":"A theorem is a statement that has been proven to be true, either on the basis of generally accepted statements such as axioms, or on the basis of previously established statements. An axiom or postulate is a statement that is accepted without proof and regarded as fundamental to a subject. Historically axioms were regarded as \"self-evident\", but recently they have been considered assumptions that characterize the subject of study. In classical geometry, axioms are general statements, while postulates are statements about geometrical objects.[17] A definition is yet another form of statement that is also accepted without proof \u2014 since it simply gives the meaning of a word or phrase in terms of known concepts. A conjecture (or sometimes hypothesis, but with a different meaning from the one discussed above) is an unproved statement that is believed to be true. To be considered a conjecture, a statement must usually be proposed publicly, at which point the name of the proponent may be attached to the conjecture, as with Goldbach's conjecture. Other famous conjectures include the Collatz conjecture and the Riemann hypothesis. On the other hand, Fermat's Last Theorem has always been known by that name, even before it was proved; it was never known as \"Fermat's conjecture\". A proposition is a theorem of lesser importance, or considered so elementary that it may be stated with no proof. It is also the term for the statement-part of the theorem (If A then B) that precedes the proof-part, and particularly elementary theorems are often given only as a proposition, hence the name. This term connotes a statement with an especially simple or immediately obvious proof, while the term theorem is usually reserved for the most important results or those with long or difficult proofs. Some authors never use the word \"proposition\", while some others reserve the word \"theorem\" only for fundamental, central, or particularly important results.[a] A lemma is a \"helping theorem\", a proposition with little applicability except that it forms part of the proof of a larger theorem. In some cases, as the relative importance of different theorems becomes more clear, what was once considered a lemma is now considered a theorem, though the word \"lemma\" remains in the name. Examples include Gauss's lemma, Zorn's lemma, and the fundamental lemma. A corollary is a proposition that follows with little or no required proof from another theorem or definition.[18] Also a corollary can be a theorem restated in a simpler form, for a more restricted special case. For example, the theorem that all angles in a rectangle are right angles has as corollary that all angles in a square (a special case of a rectangle) are right angles. An Inference: A converse of a theorem is a statement formed by interchanging what is given in a theorem and what is to be proved. For example, the isosceles triangle theorem states that if two sides of a triangle are equal then two angles are equal. In the converse, the given (that two sides are equal) and what is to be proved (that two angles are equal) are swapped, so the converse is the statement that if two angles of a triangle are equal then two sides are equal. In this example, the converse can be proved as another theorem, but this is often not the case. For example, the converse to the theorem that two right angles are equal angles is the statement that two equal angles must be right angles, and this is clearly not always the case.[19] A generalization is a theorem which includes a previously proved theorem as a special case and hence as a corollary.[b] An identity is an equality, contained in a theorem, between two mathematical expressions that holds regardless of the values being used for any variables or parameters appearing in the expressions (as long as they are within the range of validity).[20] Examples include Euler's formula and Vandermonde's identity. A rule is a theorem, such as Bayes' rule and Cramer's rule, that establishes a useful formula. A law or a principle is a theorem that applies in a wide range of circumstances. Examples include the law of large numbers, the law of cosines, Kolmogorov's zero\u2013one law, Harnack's principle, the least-upper-bound principle, and the pigeonhole principle.[21]","title":"Theory Terminology"},{"location":"Computer-Science/maths/#arithmetic-progrssion","text":"","title":"Arithmetic Progrssion"},{"location":"Computer-Science/maths/#intro","text":"Lets take series 1, 3, 5, 9, ..... nth here 1, 1+2, 1 + (2+2), 1 + (2+2+2), ..... 1 i.e. a + a+d, a+2d, a+3d,...... for n series 1 a + a+d, a+2d, a+3d,......, a + (n-1)d So, 1 2 3 a = initial number == 1 d = difference == 2 nth or last term = l = a + (n-1)d l = a + (n-1)d Sum of AP = sum of a + a+d, a+2d, a+3d,......, a + (n-1)d = a + a+d + a+2d + a+3d + ......+ a+(n-1)d = a*n + d(1 + 2 + 3 +....+ n-1) i.e. S_n = an + \\frac{d(n-1)n}{2} S_n = an + \\frac{d(n-1)n}{2} S_n = \\frac{1}{2}n(2a+ (n-1)d) S_n = \\frac{1}{2}n(2a+ (n-1)d) S_n = \\frac{1}{2}n(a+l) S_n = \\frac{1}{2}n(a+l)","title":"Intro"},{"location":"Computer-Science/maths/#geometric-progression","text":"","title":"Geometric Progression"},{"location":"Computer-Science/maths/#intro_1","text":"Lets take a series 2, 6, 18, 54, .... 2, 6, 18, 54, .... i.e. 2, 2 \\times 3, 2 \\times 3^2, 2 \\times 3^3, ... 2, 2 \\times 3, 2 \\times 3^2, 2 \\times 3^3, ... a, a \\times r, 2 \\times r^2, 2 \\times r^3, ... a, a \\times r, 2 \\times r^2, 2 \\times r^3, ... where a a is first term, and r r is ratio So, n^{th} n^{th} term = a \\times r^{n-1} a \\times r^{n-1} Sum of the GP S_n = a + a r + 2 r^2 + 2 r^3 + ... + a r^{n-1} S_n = a + a r + 2 r^2 + 2 r^3 + ... + a r^{n-1} rS_n = a r + 2 r^2 + 2 r^3 + 2 r^4 + ... + a r^{n} rS_n = a r + 2 r^2 + 2 r^3 + 2 r^4 + ... + a r^{n} ------------------- ------------------- S_n - rS_n = a - a r^n S_n - rS_n = a - a r^n S_n (1 - r) = a (1 - r^n) S_n (1 - r) = a (1 - r^n) S_n = \\frac{a (1 - r^n)}{(1 - r)} S_n = \\frac{a (1 - r^n)}{(1 - r)} S_{\\infty} = \\frac{a} {(1 - r)} S_{\\infty} = \\frac{a} {(1 - r)}","title":"Intro"},{"location":"Computer-Science/network/","text":"Network # Table of Contents Network History Basics Network Usages todos Basic Story Protocols Why TCP (Transmission Control Protocol) IP (Internet Protocol) UDP ICMP Why? What? How? HTTP HTTPS SSL TLS WebSocket SMTP ARP (Address Resolution Protocol) OSI Model Intro Application Layer Protocols Presentation Layer Protocols Session Layer Protocols Transport Layer Protocols Network Layer Protocols Data Link Layer Protocols Physical Layer Protocols TCP/IP Model Application Layer Protocols Transport Layer Protocols Internet Layer Protocols Network Interface/Access Layer (Link Layer) Protocols Ethernet TODOs History # 18th century: age of mechanical systems - indutries 19th century: age of steam engine 20th century: age of information + telephone networks + radios, tv, computer, satellite earlier - a centralized computer - a big computer ina room for an org nowadays - many distributed computers, in large quantity with rise - need more robust, sophisticated system to monitor each part/layer/unit of the network end to end. large number of separate computers able to exchange info i.e. interconnected == Computer Network focus : design & organization of this network internet/www != computer network internet == collection of computer networks www == distributed system running on top of internet distributed system == computer network ? Differences - distributed system - a collection of independent computers appears to its users as a single coherent system - has a single model, paradigm - responsible for implementing these models -> middleware - built on top of a network - softwares gives it a high degree of cohesiveness and transparency 1 2 - e.g. - www: everything looks like a document, a **Web Page** computer network coherence, model, software are absent user are exposed to the actual machines The main differentiator is software or operating system. But no hardware. Middleware: software that acts as a bridge between an operating system or database and applications, especially on a network. Basics # Topologies Bus Star Ring Mesh Network Area Types PAN LAN CAN WAN MAN GAN XXX MoDem Repeater Amplifier? Network Equipments/Devices Hub 1 all data to all ports Bridge 2 Remember which MAC <-> Port Switch similar as Bride, but newer 2 MAC addr. table in memory 3 (optional support) Routing table VLANs Router 3 Routing table IP Wireless Access Point (WAP) Similar to Router, but wireless Modes Ad-hoc Enterprise Common Settings SSID SSID Broadcast Authentication methods WEP (Deprecated) WPA2 PSK (WiFi Protected Access) WPA2 Enterprise (RADIUS) RADIUS: Central Auth Server MAC Filtering Network Interface Card (NIC) NIC Types Network Usages # resource sharing (more general) business applications client-server model communication / e-mail e-commerce home aaplications internet remote information p2p - phone, instant msg-ing entertainment e-com fin mobile / wireless todos # wap1.0 wap2.0 Basic Story # Connect WiFi in laptop to your modem 2 ARP will happen Will follow DORA process Discovery: client mac will Protocols # Why # Given the importance of protocols to the Internet, it\u2019s important that everyone agree on what each and every protocol does, so that people can create systems and products that interoperate. This is where standards come into play. Internet stan- dards are developed by the Internet Engineering Task Force (IETF)[IETF 2012]. The IETF standards documents are called requests for comments (RFCs). RFCs started out as general requests for comments (hence the name) to resolve network and protocol design problems that faced the precursor to the Internet RFCs tend to be quite technical and detailed TCP (Transmission Control Protocol) # IP (Internet Protocol) # UDP # https://www.smashingmagazine.com/2017/06/guide-switching-http-https/ ICMP # (Internet Control Message Protocol) - https://erg.abdn.ac.uk/users/gorry/eg3567/inet-pages/icmp.html - https://cse.sc.edu/~wyxu/515Fall08/slides/IPRoutingtrace.pdf Why? # Because IP wasn't designed to be absolutely reliable & it doesn't have an inbuilt mechanism for sending control(errors & query) messages. ICMP comes in picture: - to provide feedback to the source IP address by network devices like routers (and may be switch, hub..) - that a router/service/host can't be reached for packet (datagram) delivery - to answer queries raised by ping & traceroute tools What? # ICMP is a supporting protocol ICMP is not a transport protocol ICMP is typlically not used to exchange data between systems (except in case of diagnostic tools ^) ICMP sits in N/W layer of OSI along with IP and hence port doesn't come in picture here ports are only used for protocols which work at the transport layer and above ICMP send multiple types of messages: ping (echo) ping reply (echo reply) destination unreachable etc.. ICMP structure ICMP Header (8 bytes) ICMP type ICMP code (sub-type) Chechsum Extra content (of 4 bytes) based on type & code ICMP message (variable, min 28 bytes) entire IP Header from IP datagram that resulted in error (min 20 bytes, variable) atleast (first) 8 bytes of data from IP datagram that resulted in error ICMP Header Structure How? # ICMP messages are encapsulated in an IP datagrams and transmitted along with them to the transport layer protocol in IP datagrams are set to ICMP so that receiving end can interprete it using ICMP client source address is set to the IP address of the device that generated the datagram (& ICMP message) destination address is set to the source address mentioned in the packet (datagram) that resulted in error (i.e. of which delivery failed) Structure of IP datagram having ICMP encapsulated within it IP header (20 bytes) IP Protocol=ICMP IP data ICMP message ICMP header (1+1+2+4 bytes) ICMP data (min 28 bytes, variable) IP Datagram Structure HTTP # HTTPS # SSL # Secure Socket Layer TLS # Transport Layer Security WebSocket # SMTP # ARP (Address Resolution Protocol) # OSI Model # (Open Systems Interconnection) https://www.webopedia.com/quick_ref/OSI_Layers.asp Intro # Open Systems Interconnection Model a conceptual model characterize & standarize the communication function of telecommunication/ compututing systems without any knowledge of its internal structure & technology partitioned in abstraction layers a layer serves the layer above it Application Layer # User apps, Network services Protocols # HTTP, HTTPS, SMTP, FTP, SSH Presentation Layer # Encryption, charsets Protocols # SSL, TLS, JPEG, MPEG Session Layer # Setup, maintain, tear-down sessions Protocols # NFS, SQL, RPC, Netbios Transport Layer # Datagram delivery (TCP/UDP), port number Protocols # TCP, UDP Network Layer # Routing, Software adrresses (IP) Protocols # IP, IPX, ICMP, ARP, RARP, IGMP Data Link Layer # Media access, Hardware addresses (MAC) Protocols # ARP, HDLC, PPP Physical Layer # Cable, connectors, electrical specs Protocols # no protocols at this layer TCP/IP Model # http://what-when-how.com/data-communications-and-networking/network-and-transport-layers-data-communications-and-networking/ https://www.hardwaresecrets.com/how-tcp-ip-protocol-works-part-1/6/ Application Layer # Protocols # HTTP, HTTPS, SMTP, FTP, JPEG, MPEG, NFS, SQL, RPC Transport Layer # Protocols # TCP, UDP Internet Layer # Protocols # TCP, UDP Network Interface/Access Layer (Link Layer) # Protocols # IP, ICMP, ARP, RARP, IGMP Ethernet # Note: TCP/IP is a set of protocols that deals with layers 3 to 7 from the OSI reference model, while Ethernet is a set of protocols that deals with layers 1 and 2 from the OSI reference model TODOs # Packets Packet Switch Internet API Internet Service Distributed App","title":"Network"},{"location":"Computer-Science/network/#network","text":"","title":"Network"},{"location":"Computer-Science/network/#history","text":"18th century: age of mechanical systems - indutries 19th century: age of steam engine 20th century: age of information + telephone networks + radios, tv, computer, satellite earlier - a centralized computer - a big computer ina room for an org nowadays - many distributed computers, in large quantity with rise - need more robust, sophisticated system to monitor each part/layer/unit of the network end to end. large number of separate computers able to exchange info i.e. interconnected == Computer Network focus : design & organization of this network internet/www != computer network internet == collection of computer networks www == distributed system running on top of internet distributed system == computer network ? Differences - distributed system - a collection of independent computers appears to its users as a single coherent system - has a single model, paradigm - responsible for implementing these models -> middleware - built on top of a network - softwares gives it a high degree of cohesiveness and transparency 1 2 - e.g. - www: everything looks like a document, a **Web Page** computer network coherence, model, software are absent user are exposed to the actual machines The main differentiator is software or operating system. But no hardware. Middleware: software that acts as a bridge between an operating system or database and applications, especially on a network.","title":"History"},{"location":"Computer-Science/network/#basics","text":"Topologies Bus Star Ring Mesh Network Area Types PAN LAN CAN WAN MAN GAN XXX MoDem Repeater Amplifier? Network Equipments/Devices Hub 1 all data to all ports Bridge 2 Remember which MAC <-> Port Switch similar as Bride, but newer 2 MAC addr. table in memory 3 (optional support) Routing table VLANs Router 3 Routing table IP Wireless Access Point (WAP) Similar to Router, but wireless Modes Ad-hoc Enterprise Common Settings SSID SSID Broadcast Authentication methods WEP (Deprecated) WPA2 PSK (WiFi Protected Access) WPA2 Enterprise (RADIUS) RADIUS: Central Auth Server MAC Filtering Network Interface Card (NIC) NIC Types","title":"Basics"},{"location":"Computer-Science/network/#network-usages","text":"resource sharing (more general) business applications client-server model communication / e-mail e-commerce home aaplications internet remote information p2p - phone, instant msg-ing entertainment e-com fin mobile / wireless","title":"Network Usages"},{"location":"Computer-Science/network/#todos","text":"wap1.0 wap2.0","title":"todos"},{"location":"Computer-Science/network/#basic-story","text":"Connect WiFi in laptop to your modem 2 ARP will happen Will follow DORA process Discovery: client mac will","title":"Basic Story"},{"location":"Computer-Science/network/#protocols","text":"","title":"Protocols"},{"location":"Computer-Science/network/#why","text":"Given the importance of protocols to the Internet, it\u2019s important that everyone agree on what each and every protocol does, so that people can create systems and products that interoperate. This is where standards come into play. Internet stan- dards are developed by the Internet Engineering Task Force (IETF)[IETF 2012]. The IETF standards documents are called requests for comments (RFCs). RFCs started out as general requests for comments (hence the name) to resolve network and protocol design problems that faced the precursor to the Internet RFCs tend to be quite technical and detailed","title":"Why"},{"location":"Computer-Science/network/#tcp-transmission-control-protocol","text":"","title":"TCP (Transmission Control Protocol)"},{"location":"Computer-Science/network/#ip-internet-protocol","text":"","title":"IP (Internet Protocol)"},{"location":"Computer-Science/network/#udp","text":"https://www.smashingmagazine.com/2017/06/guide-switching-http-https/","title":"UDP"},{"location":"Computer-Science/network/#icmp","text":"(Internet Control Message Protocol) - https://erg.abdn.ac.uk/users/gorry/eg3567/inet-pages/icmp.html - https://cse.sc.edu/~wyxu/515Fall08/slides/IPRoutingtrace.pdf","title":"ICMP"},{"location":"Computer-Science/network/#why_1","text":"Because IP wasn't designed to be absolutely reliable & it doesn't have an inbuilt mechanism for sending control(errors & query) messages. ICMP comes in picture: - to provide feedback to the source IP address by network devices like routers (and may be switch, hub..) - that a router/service/host can't be reached for packet (datagram) delivery - to answer queries raised by ping & traceroute tools","title":"Why?"},{"location":"Computer-Science/network/#what","text":"ICMP is a supporting protocol ICMP is not a transport protocol ICMP is typlically not used to exchange data between systems (except in case of diagnostic tools ^) ICMP sits in N/W layer of OSI along with IP and hence port doesn't come in picture here ports are only used for protocols which work at the transport layer and above ICMP send multiple types of messages: ping (echo) ping reply (echo reply) destination unreachable etc.. ICMP structure ICMP Header (8 bytes) ICMP type ICMP code (sub-type) Chechsum Extra content (of 4 bytes) based on type & code ICMP message (variable, min 28 bytes) entire IP Header from IP datagram that resulted in error (min 20 bytes, variable) atleast (first) 8 bytes of data from IP datagram that resulted in error ICMP Header Structure","title":"What?"},{"location":"Computer-Science/network/#how","text":"ICMP messages are encapsulated in an IP datagrams and transmitted along with them to the transport layer protocol in IP datagrams are set to ICMP so that receiving end can interprete it using ICMP client source address is set to the IP address of the device that generated the datagram (& ICMP message) destination address is set to the source address mentioned in the packet (datagram) that resulted in error (i.e. of which delivery failed) Structure of IP datagram having ICMP encapsulated within it IP header (20 bytes) IP Protocol=ICMP IP data ICMP message ICMP header (1+1+2+4 bytes) ICMP data (min 28 bytes, variable) IP Datagram Structure","title":"How?"},{"location":"Computer-Science/network/#http","text":"","title":"HTTP"},{"location":"Computer-Science/network/#https","text":"","title":"HTTPS"},{"location":"Computer-Science/network/#ssl","text":"Secure Socket Layer","title":"SSL"},{"location":"Computer-Science/network/#tls","text":"Transport Layer Security","title":"TLS"},{"location":"Computer-Science/network/#websocket","text":"","title":"WebSocket"},{"location":"Computer-Science/network/#smtp","text":"","title":"SMTP"},{"location":"Computer-Science/network/#arp-address-resolution-protocol","text":"","title":"ARP (Address Resolution Protocol)"},{"location":"Computer-Science/network/#osi-model","text":"(Open Systems Interconnection) https://www.webopedia.com/quick_ref/OSI_Layers.asp","title":"OSI Model"},{"location":"Computer-Science/network/#intro","text":"Open Systems Interconnection Model a conceptual model characterize & standarize the communication function of telecommunication/ compututing systems without any knowledge of its internal structure & technology partitioned in abstraction layers a layer serves the layer above it","title":"Intro"},{"location":"Computer-Science/network/#application-layer","text":"User apps, Network services","title":"Application Layer"},{"location":"Computer-Science/network/#protocols_1","text":"HTTP, HTTPS, SMTP, FTP, SSH","title":"Protocols"},{"location":"Computer-Science/network/#presentation-layer","text":"Encryption, charsets","title":"Presentation Layer"},{"location":"Computer-Science/network/#protocols_2","text":"SSL, TLS, JPEG, MPEG","title":"Protocols"},{"location":"Computer-Science/network/#session-layer","text":"Setup, maintain, tear-down sessions","title":"Session Layer"},{"location":"Computer-Science/network/#protocols_3","text":"NFS, SQL, RPC, Netbios","title":"Protocols"},{"location":"Computer-Science/network/#transport-layer","text":"Datagram delivery (TCP/UDP), port number","title":"Transport Layer"},{"location":"Computer-Science/network/#protocols_4","text":"TCP, UDP","title":"Protocols"},{"location":"Computer-Science/network/#network-layer","text":"Routing, Software adrresses (IP)","title":"Network Layer"},{"location":"Computer-Science/network/#protocols_5","text":"IP, IPX, ICMP, ARP, RARP, IGMP","title":"Protocols"},{"location":"Computer-Science/network/#data-link-layer","text":"Media access, Hardware addresses (MAC)","title":"Data Link Layer"},{"location":"Computer-Science/network/#protocols_6","text":"ARP, HDLC, PPP","title":"Protocols"},{"location":"Computer-Science/network/#physical-layer","text":"Cable, connectors, electrical specs","title":"Physical Layer"},{"location":"Computer-Science/network/#protocols_7","text":"no protocols at this layer","title":"Protocols"},{"location":"Computer-Science/network/#tcpip-model","text":"http://what-when-how.com/data-communications-and-networking/network-and-transport-layers-data-communications-and-networking/ https://www.hardwaresecrets.com/how-tcp-ip-protocol-works-part-1/6/","title":"TCP/IP Model"},{"location":"Computer-Science/network/#application-layer_1","text":"","title":"Application Layer"},{"location":"Computer-Science/network/#protocols_8","text":"HTTP, HTTPS, SMTP, FTP, JPEG, MPEG, NFS, SQL, RPC","title":"Protocols"},{"location":"Computer-Science/network/#transport-layer_1","text":"","title":"Transport Layer"},{"location":"Computer-Science/network/#protocols_9","text":"TCP, UDP","title":"Protocols"},{"location":"Computer-Science/network/#internet-layer","text":"","title":"Internet Layer"},{"location":"Computer-Science/network/#protocols_10","text":"TCP, UDP","title":"Protocols"},{"location":"Computer-Science/network/#network-interfaceaccess-layer-link-layer","text":"","title":"Network Interface/Access Layer (Link Layer)"},{"location":"Computer-Science/network/#protocols_11","text":"IP, ICMP, ARP, RARP, IGMP","title":"Protocols"},{"location":"Computer-Science/network/#ethernet","text":"Note: TCP/IP is a set of protocols that deals with layers 3 to 7 from the OSI reference model, while Ethernet is a set of protocols that deals with layers 1 and 2 from the OSI reference model","title":"Ethernet"},{"location":"Computer-Science/network/#todos_1","text":"Packets Packet Switch Internet API Internet Service Distributed App","title":"TODOs"},{"location":"Computer-Science/os/","text":"Operating Systems # Table of Contents Operating Systems Process Scheduling Preemptive Scheduling Non-Preemptive Scheduling Process Synchronization Methods used for Process Synchronization Semaphore AND Mutex Semaphore Types Mutex Types Pros Cons Mutex VS Semaphore Why 2 different synchronization premitives? Producer-Consumer Problem Critical Section Preemption Inter Process Communication Concurrency vs Parallelism IO bound, Memory bound, Cache bound, CPU bound operations Memory Management Virtual Memory Paging Demand Paging Page Fault Swapping Thrasing Process Scheduling # Preemptive Scheduling # is a kind of process scheduling which have facility to interrupt an on going process to proceed with other high priority process (and the resource, mainly CPU, is allocated to the new process) in such case, the running process moves to the ready state & the process in ready state moves to the running state Non-Preemptive Scheduling # Process Synchronization # The shared resources can be used by all the processes but the processes should make sure that at a particular time, only one (or, a finite number of) process should be using that shared resource. This is called process synchronization. Methods used for Process Synchronization # Semaphore Mutex (Mutual Exclusive object) Semaphore AND Mutex # Are kernel resources and provides synchronization service Also known as synchronization primitives both serves the same purpose but the mechanism, implementation, and use-cases are different Semaphore # is a Signalling Mechanism Types # Binary Semaphore Counting Semaphore Mutex # an object is a Locking Mechanism Ownership involved: The thread/process which have mutex and accessing data can only release mutex Types # Recursive mutex Pros # Cons # Mutex VS Semaphore # Mutex uses a locking mechanism i.e. if a process wants to use a resource then it locks the resource, uses it and then release it. But on the other hand, semaphore uses a signalling mechanism where wait() and signal() methods are used to show if a process is releasing a resource or taking a resource. A mutex is an object but semaphore is an integer variable. In semaphore, we have wait() and signal() functions. But in mutex, there is no such function. A mutex object allows multiple process threads to access a single shared resource but only one at a time. On the other hand, semaphore allows multiple process threads to access the finite instance of the resource until available. In mutex, the lock can be acquired and released by the same process at a time. But the value of the semaphore variable can be modified by any process that needs some resource but only one process can change the value at a time. Why 2 different synchronization premitives? # Producer-Consumer Problem # Critical Section # In conncurrent programming * A group of instructions/statements or region of code * that region to be executed atomically (i.e. all or nothing) A simple solution to critical section 1 2 3 acquireLock () executeCriticalSection () releaseLock () Preemption # Inter Process Communication # Concurrency vs Parallelism # IO bound, Memory bound, Cache bound, CPU bound operations # speed of IO bound \\lt \\lt Memory bound \\lt \\lt Cache bound \\lt \\lt CPU bound operations Memory Management # Virtual Memory # Virtual memory is a memory manage scheme as per which the secondary memory could act as a part of the primary memory implemented using demand paging or demand segmentation other insights a program does not refers to actual physical (primary) or machine (secondary) memory address rather it refers to the logical addresses logical addresses are translated into physical addresses during execution of the programs thus a process could be stored & executed in non-contiguos chunks Paging # Demand Paging # Page Fault # Swapping # Swapping a process out: marking its all pages as free/unloaded from memory Swapping a process in: bringing in all the pages of a process from secondary memory to primary memory Thrasing # When a process is busy swapping pages in and out then it is called thrasing","title":"Operating Systems"},{"location":"Computer-Science/os/#operating-systems","text":"","title":"Operating Systems"},{"location":"Computer-Science/os/#process-scheduling","text":"","title":"Process Scheduling"},{"location":"Computer-Science/os/#preemptive-scheduling","text":"is a kind of process scheduling which have facility to interrupt an on going process to proceed with other high priority process (and the resource, mainly CPU, is allocated to the new process) in such case, the running process moves to the ready state & the process in ready state moves to the running state","title":"Preemptive Scheduling"},{"location":"Computer-Science/os/#non-preemptive-scheduling","text":"","title":"Non-Preemptive Scheduling"},{"location":"Computer-Science/os/#process-synchronization","text":"The shared resources can be used by all the processes but the processes should make sure that at a particular time, only one (or, a finite number of) process should be using that shared resource. This is called process synchronization.","title":"Process Synchronization"},{"location":"Computer-Science/os/#methods-used-for-process-synchronization","text":"Semaphore Mutex (Mutual Exclusive object)","title":"Methods used for Process Synchronization"},{"location":"Computer-Science/os/#semaphore-and-mutex","text":"Are kernel resources and provides synchronization service Also known as synchronization primitives both serves the same purpose but the mechanism, implementation, and use-cases are different","title":"Semaphore AND Mutex"},{"location":"Computer-Science/os/#semaphore","text":"is a Signalling Mechanism","title":"Semaphore"},{"location":"Computer-Science/os/#types","text":"Binary Semaphore Counting Semaphore","title":"Types"},{"location":"Computer-Science/os/#mutex","text":"an object is a Locking Mechanism Ownership involved: The thread/process which have mutex and accessing data can only release mutex","title":"Mutex"},{"location":"Computer-Science/os/#types_1","text":"Recursive mutex","title":"Types"},{"location":"Computer-Science/os/#pros","text":"","title":"Pros"},{"location":"Computer-Science/os/#cons","text":"","title":"Cons"},{"location":"Computer-Science/os/#mutex-vs-semaphore","text":"Mutex uses a locking mechanism i.e. if a process wants to use a resource then it locks the resource, uses it and then release it. But on the other hand, semaphore uses a signalling mechanism where wait() and signal() methods are used to show if a process is releasing a resource or taking a resource. A mutex is an object but semaphore is an integer variable. In semaphore, we have wait() and signal() functions. But in mutex, there is no such function. A mutex object allows multiple process threads to access a single shared resource but only one at a time. On the other hand, semaphore allows multiple process threads to access the finite instance of the resource until available. In mutex, the lock can be acquired and released by the same process at a time. But the value of the semaphore variable can be modified by any process that needs some resource but only one process can change the value at a time.","title":"Mutex VS Semaphore"},{"location":"Computer-Science/os/#why-2-different-synchronization-premitives","text":"","title":"Why 2 different synchronization premitives?"},{"location":"Computer-Science/os/#producer-consumer-problem","text":"","title":"Producer-Consumer Problem"},{"location":"Computer-Science/os/#critical-section","text":"In conncurrent programming * A group of instructions/statements or region of code * that region to be executed atomically (i.e. all or nothing) A simple solution to critical section 1 2 3 acquireLock () executeCriticalSection () releaseLock ()","title":"Critical Section"},{"location":"Computer-Science/os/#preemption","text":"","title":"Preemption"},{"location":"Computer-Science/os/#inter-process-communication","text":"","title":"Inter Process Communication"},{"location":"Computer-Science/os/#concurrency-vs-parallelism","text":"","title":"Concurrency vs Parallelism"},{"location":"Computer-Science/os/#io-bound-memory-bound-cache-bound-cpu-bound-operations","text":"speed of IO bound \\lt \\lt Memory bound \\lt \\lt Cache bound \\lt \\lt CPU bound operations","title":"IO bound, Memory bound, Cache bound, CPU bound operations"},{"location":"Computer-Science/os/#memory-management","text":"","title":"Memory Management"},{"location":"Computer-Science/os/#virtual-memory","text":"Virtual memory is a memory manage scheme as per which the secondary memory could act as a part of the primary memory implemented using demand paging or demand segmentation other insights a program does not refers to actual physical (primary) or machine (secondary) memory address rather it refers to the logical addresses logical addresses are translated into physical addresses during execution of the programs thus a process could be stored & executed in non-contiguos chunks","title":"Virtual Memory"},{"location":"Computer-Science/os/#paging","text":"","title":"Paging"},{"location":"Computer-Science/os/#demand-paging","text":"","title":"Demand Paging"},{"location":"Computer-Science/os/#page-fault","text":"","title":"Page Fault"},{"location":"Computer-Science/os/#swapping","text":"Swapping a process out: marking its all pages as free/unloaded from memory Swapping a process in: bringing in all the pages of a process from secondary memory to primary memory","title":"Swapping"},{"location":"Computer-Science/os/#thrasing","text":"When a process is busy swapping pages in and out then it is called thrasing","title":"Thrasing"},{"location":"Computer-Science/se/","text":"Software Engineering # Table of Contents Software Engineering Programming Languages An Overview of Programs and Programming Languages Compiled, interpreted, or JIT-compiled High or Low Level Level Type System Paradigms Standardization Repository Structure History OS Open Source Guide Software Licensing Intro Apache-2.0 MIT BSD-2-Clause BSD-3-Clause GPL-2.0 GPL-3.0 AGPL-3.0 Software Security Obfuscation Obfuscation Tools Protect Code Protect .Net Code Decompile Tools DLLs Python PL/SQL Encryption Symmetric Key Encryption DES (Data Encryption Standard) AES (Advanced Encryption Standard) Asymmetric Encryption (Private/Public Key) Deffie-Hellman Key Exchange RSA (Rivest\u2013Shamir\u2013Adleman) Misc .Net DLL vs C++ DLL 32 bit vs 64 bit DLL Programming Languages # An Overview of Programs and Programming Languages # In order to better communicate to our computers what exactly it is we want them to do, we've developed a wide range of programming languages to make the communication process easier. Depending on the type of project, there are many factors that have to be considered when choosing a language. Here is a list of some of the more noteworthy ones: Compiled, interpreted, or JIT-compiled # Compiled languages are translated to the target machine's native language by a program called a compiler. This can result in very fast code, especially if the compiler is effective at optimizing, however the resulting code may not port well across operating systems and the compilation process may take a while. Interpreted languages are read by a program called an interpreter and are executed by that program. While they are as portable as their interpreter and have no long compile times, interpreted languages are usually much slower than an equivalent compiled program. Finally, just-in-time compiled (or JIT-compiled) languages are languages that are quickly compiled when programs written in them need to be run (usually with very little optimization), offering a balance between performance and portability. High or Low Level Level # In this case, refers to how much the nature of the language reflects the underlying system. In other words, a programming language's level refers to how similar the language is to a computer's native language. The higher the level, the less similar it is. A low-level language is generally quite similar to machine code, and thus is more suitable for programs like device drivers or very high performance programs that really need access to the hardware. Generally, the term is reserved for machine code itself and assembly languages, though many languages offer low-level elements. Since a low-level language is subject to all the nuances of the hardware it's accessing, however, a program written in a low-level language is generally difficult to port to other platforms. Low level languages are practically never interpreted, as this generally defeats the purpose. A high-level language focuses more on concepts that are easy to understand by the human mind, such as objects or mathematical functions. A high-level language usually is easier to understand than a low-level language, and it usually takes less time to develop a program in a high-level language than it does in a low-level language. As a trade-off one generally needs to sacrifice some degree of control over what the resulting program actually does. It is not, however, impossible to mix high-level and low-level functionality in a language. Type System # A type system refers to the rules that the different types of variables of a language have to follow. Some languages (including most assembly languages) do not have types and thus this section does not apply to them. However, as most languages (including C++) have types, this information is important. Type Strength: Strong or Weak A strong typing system puts restrictions on how different types of variables can be converted to each other without any converting statements. An ideal strong typing system would forbid implicit \"casts\" to types that do not make any sense, such as an integer to a Fruit object. A weak typing system would try to find some way to make the cast work. Type Expression: Manifest or Inferred This deals with how the compiler/interpreter for a language infers the types of variables. Many languages require variables' types to be explicitly defined, and thus rely on manifest typing. Some however, will infer the type of the variable based on the contexts in which it is used, and thus use inferred typing. Type Checking: Static or Dynamic If a language is statically typed, then the compiler/interpreter does the type checking once before the program runs/is compiled. If the language is dynamically type checked, then the types are checked at run-time. Type Safety: Safe or Unsafe These refer to the degree to which a language will prohibit operations on typed variables that might lead to undefined behavior or errors. A safe language will do more to ensure that such operations or conversions do not occur, while an unsafe language will give more responsibility to the user in this regard. These typing characteristics are not necessarily mutually exclusive, and some languages mix them. Paradigms # A programming paradigm is a methodology or way of programming that a programming language supports. Here is a summary of a few common paradigms: Declarative A declarative language will focus more on specifying what a language is supposed to accomplish rather than by what means it is supposed to accomplish it. Such a paradigm might be used to avoid undesired side-effects resulting from having to write one's own code. Functional Functional programming is a subset of declarative programming that tries to express problems in terms of mathematical equations and functions. It goes out of its way to avoid the concepts of states and mutable variables which are common in imperative languages. Generic Generic programming focuses on writing skeleton algorithms in terms of types that will be specified when the algorithm is actually used, thus allowing some leniency to programmers who wish to avoid strict strong typing rules. It can be a very powerful paradigm if well-implemented. Imperative Imperative languages allow programmers to give the computer ordered lists of instructions without necessarily having to explicitly state the task. It can be thought of being the opposite of declarative programming. Structured Structured programming languages aim to provide some form of noteworthy structure to a language, such as intuitive control over the order in which statements are executed (if X then do Y otherwise do Z, do X while Y is Z). Such languages generally deprecate \"jumps\", such as those provided by the goto statement in C and C++. Procedural Although it is sometimes used as a synonym for imperative programming, a procedural programming language can also refer to an imperative structured programming language which supports the concept of procedures and subroutines (also known as functions in C or C++). Object-Oriented Object-Oriented programming (sometimes abbreviated to OOP) is a subset of structured programming which expresses programs in the terms of \"objects\", which are meant to model objects in the real world. Such a paradigm allows code to be reused in remarkable ways and is meant to be easy to understand. Standardization # Does a language have a formal standard? This can be very important to ensure that programs written to work with one compiler/interpreter will work with another. Some languages are standardized by the American National Standards Institute (ANSI), some are standardized by the International Organization for Standardization (ISO), and some have an informal but de-facto standard not maintained by any standards organization. Repository Structure # 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 Repo/ | | ------- README.md | | ------- LICENSE | | ------- CONTRIBUTING.md | | ------- CODE_OF_CONDUCT.md | | ------- NOTICE.md | | | ------- configs/ | | ----- anyfile.json | | | | ------- docs/ | | ----- Index | | ----- Overview | | ----- API | | ----- Code Structure | | ----- Installation | | ----- Configuration | | ----- References | | ----- Future | | | | ------- src/ | | -----project/ | | --__init__.py | | --settings.py | | --urls.py | \u2514--wsgi.py | | -----app1/ | | | | -----app2-RESTful/ | | | | -----manage.py | | | \u2514-----db.sqlite3 Django - Code of Conduct Other - Code of Conduct History # OS # https://en.wikipedia.org/wiki/Timeline_of_operating_systems Open Source Guide # Software Licensing # Intro # https://opensource.org/licenses/alphabetical http://www.gnu.org/licenses/ https://opensource.org/licenses Apache-2.0 # TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION - free, reproduce, sublicense, re-distribute in any form - Redistribution - provide copy of license - mention you changed this file/code - retain copy of source code - can be monitized - Summary MIT # Permission is hereby granted, free of charge , to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction , including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so. Summary BSD-2-Clause # Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: 1 2 3 1. Redistributions of source code must `retain the above copyright notice`, this list of conditions and the following disclaimer. 2. Redistributions in binary form `must reproduce the above copyright notice`, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. also known as Simplified BSD or FreeBSD can be monitized Summary BSD-3-Clause # Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: 1 2 3 4 5 1. Redistributions of source code must `retain the above copyright notice`, this list of conditions and the following disclaimer. 2. Redistributions in binary form must `reproduce the above copyright notice`, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. 3. Neither the `name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived` from this software without specific prior written permission. also known as Modified BSD or New BSD can be monitized Summary GPL-2.0 # everyone is permitted to copy and distribute verbatim copies of this LICENSE document, but changing it is not allowed can use, modify, redistribute original licensor can relicense to GPL-3.0 also used as GPL version 2 or any later version gives option to follow the terms and conditions of any later version so, any other licensors can follow the the term & conditions of GPL-3.0 can be monitized modified versions of code/software be marked as changed with the date of the change any patent must be licensed for everyone's free use or not licensed at all Summary GPL-3.0 # everyone is permitted to copy and distribute verbatim copies of this LICENSE document, but changing it is not allowed can use, modify, redistribute else, similar to GPL-2.0 but re-phrased Summary AGPL-3.0 # everyone is permitted to copy and distribute verbatim copies of this LICENSE document, but changing it is not allowed built for network software can distribute modified versions if you keep track of the changes and the date you made them source code must be distributed along with web service publication the AGPL is the GPL of the web http://www.gnu.org/licenses/agpl.html Summary Software Security # Obfuscation # The process of protecting the protable executable files (EXE, DLL) from getting decompiled into the original source code is called Obfuscation. Obfuscation Tools # ConfuserEx: https://yck1509.github.io/ConfuserEx/ (EXE, DLLs) ConfuserEx Decoder: https://github.com/cawk/ConfuserEx-Unpacker dotfuscators: https://www.preemptive.com/products/dotfuscator/overview https://translate.google.it/translate?hl=it?sl=it&tl=en&u=https%3A//gianmarcocastagna.blogspot.it/ Protect Code # https://stackoverflow.com/questions/805461/how-to-protect-dlls https://stackoverflow.com/questions/109997/how-do-you-protect-your-software-from-illegal-distribution https://stackoverflow.com/questions/506282/protect-net-code-from-reverse-engineering https://www.codeproject.com/Articles/1139773/Protect-Your-Source-Code-from-Decompiling-or-Rever https://stackoverflow.com/questions/7849620/what-is-the-best-way-to-protect-sensitive-data-in-the-code Protect .Net Code # https://stackoverflow.com/questions/7669684/encrypt-app-config-file-automatically-during-build-process Decompile Tools # https://stackoverflow.com/questions/130058/how-are-serial-generators-cracks-developed DLLs # dumpbin (c/c+) ILSpy dotPeek: https://www.jetbrains.com/decompiler/ ReSharper Python # uncompyle6 (.pyc/.pyo to .py) A native Python cross-version decompiler and fragment decompiler. The successor to decompyle, uncompyle, and uncompyle2. pyinstallerextractor (.exe to .py) PL/SQL # UnwrapIt Encryption # Symmetric Key Encryption # Intro use the same cryptographic keys for both encryption of plaintext and decryption of ciphertext The keys may be identical or there may be a simple transformation to go between the two keys Insecure DES (Data Encryption Standard) # Intro Insecure mainly due to the 56-bit key size being too small Once hacked in 22hrs AES (Advanced Encryption Standard) # Intro a subset of Rijndael superseded DES uses block cipher block cipher is a method of encrypting text (to produce ciphertext) a cryptographic key and algorithm are applied to a block of data (for example, 64 contiguous bits) at once as a group rather than to one bit at a time Design Principle substitution\u2013permutation network a combination of both substitution and permutation are applied to the plain text fixed block size of 128 bits, and a key size of 128, 192, or 256 bits Variaties MARS RC6 Rijndael Serpent Twofish Asymmetric Encryption (Private/Public Key) # Intro pair of keys made up of long random strings public key: known to every one private key: known to owner only Functions Authentication public key verifies that message received is sent by paired private key holder only Encryption only paired private key can decrypt the message encrypted by the public key Deffie-Hellman Key Exchange # http://www.omnisecu.com/security/public-key-infrastructure/asymmetric-encryption-algorithms.php Intro RSA (Rivest\u2013Shamir\u2013Adleman) # Misc # .Net DLL vs C++ DLL # https://stackoverflow.com/questions/10634743/net-dll-vs-c-dll 32 bit vs 64 bit DLL # https://stackoverflow.com/questions/495244/how-can-i-test-a-windows-dll-file-to-determine-if-it-is-32-bit-or-64-bit","title":"Software Engineering"},{"location":"Computer-Science/se/#software-engineering","text":"","title":"Software Engineering"},{"location":"Computer-Science/se/#programming-languages","text":"","title":"Programming Languages"},{"location":"Computer-Science/se/#an-overview-of-programs-and-programming-languages","text":"In order to better communicate to our computers what exactly it is we want them to do, we've developed a wide range of programming languages to make the communication process easier. Depending on the type of project, there are many factors that have to be considered when choosing a language. Here is a list of some of the more noteworthy ones:","title":"An Overview of Programs and Programming Languages"},{"location":"Computer-Science/se/#compiled-interpreted-or-jit-compiled","text":"Compiled languages are translated to the target machine's native language by a program called a compiler. This can result in very fast code, especially if the compiler is effective at optimizing, however the resulting code may not port well across operating systems and the compilation process may take a while. Interpreted languages are read by a program called an interpreter and are executed by that program. While they are as portable as their interpreter and have no long compile times, interpreted languages are usually much slower than an equivalent compiled program. Finally, just-in-time compiled (or JIT-compiled) languages are languages that are quickly compiled when programs written in them need to be run (usually with very little optimization), offering a balance between performance and portability.","title":"Compiled, interpreted, or JIT-compiled"},{"location":"Computer-Science/se/#high-or-low-level-level","text":"In this case, refers to how much the nature of the language reflects the underlying system. In other words, a programming language's level refers to how similar the language is to a computer's native language. The higher the level, the less similar it is. A low-level language is generally quite similar to machine code, and thus is more suitable for programs like device drivers or very high performance programs that really need access to the hardware. Generally, the term is reserved for machine code itself and assembly languages, though many languages offer low-level elements. Since a low-level language is subject to all the nuances of the hardware it's accessing, however, a program written in a low-level language is generally difficult to port to other platforms. Low level languages are practically never interpreted, as this generally defeats the purpose. A high-level language focuses more on concepts that are easy to understand by the human mind, such as objects or mathematical functions. A high-level language usually is easier to understand than a low-level language, and it usually takes less time to develop a program in a high-level language than it does in a low-level language. As a trade-off one generally needs to sacrifice some degree of control over what the resulting program actually does. It is not, however, impossible to mix high-level and low-level functionality in a language.","title":"High or Low Level Level"},{"location":"Computer-Science/se/#type-system","text":"A type system refers to the rules that the different types of variables of a language have to follow. Some languages (including most assembly languages) do not have types and thus this section does not apply to them. However, as most languages (including C++) have types, this information is important. Type Strength: Strong or Weak A strong typing system puts restrictions on how different types of variables can be converted to each other without any converting statements. An ideal strong typing system would forbid implicit \"casts\" to types that do not make any sense, such as an integer to a Fruit object. A weak typing system would try to find some way to make the cast work. Type Expression: Manifest or Inferred This deals with how the compiler/interpreter for a language infers the types of variables. Many languages require variables' types to be explicitly defined, and thus rely on manifest typing. Some however, will infer the type of the variable based on the contexts in which it is used, and thus use inferred typing. Type Checking: Static or Dynamic If a language is statically typed, then the compiler/interpreter does the type checking once before the program runs/is compiled. If the language is dynamically type checked, then the types are checked at run-time. Type Safety: Safe or Unsafe These refer to the degree to which a language will prohibit operations on typed variables that might lead to undefined behavior or errors. A safe language will do more to ensure that such operations or conversions do not occur, while an unsafe language will give more responsibility to the user in this regard. These typing characteristics are not necessarily mutually exclusive, and some languages mix them.","title":"Type System"},{"location":"Computer-Science/se/#paradigms","text":"A programming paradigm is a methodology or way of programming that a programming language supports. Here is a summary of a few common paradigms: Declarative A declarative language will focus more on specifying what a language is supposed to accomplish rather than by what means it is supposed to accomplish it. Such a paradigm might be used to avoid undesired side-effects resulting from having to write one's own code. Functional Functional programming is a subset of declarative programming that tries to express problems in terms of mathematical equations and functions. It goes out of its way to avoid the concepts of states and mutable variables which are common in imperative languages. Generic Generic programming focuses on writing skeleton algorithms in terms of types that will be specified when the algorithm is actually used, thus allowing some leniency to programmers who wish to avoid strict strong typing rules. It can be a very powerful paradigm if well-implemented. Imperative Imperative languages allow programmers to give the computer ordered lists of instructions without necessarily having to explicitly state the task. It can be thought of being the opposite of declarative programming. Structured Structured programming languages aim to provide some form of noteworthy structure to a language, such as intuitive control over the order in which statements are executed (if X then do Y otherwise do Z, do X while Y is Z). Such languages generally deprecate \"jumps\", such as those provided by the goto statement in C and C++. Procedural Although it is sometimes used as a synonym for imperative programming, a procedural programming language can also refer to an imperative structured programming language which supports the concept of procedures and subroutines (also known as functions in C or C++). Object-Oriented Object-Oriented programming (sometimes abbreviated to OOP) is a subset of structured programming which expresses programs in the terms of \"objects\", which are meant to model objects in the real world. Such a paradigm allows code to be reused in remarkable ways and is meant to be easy to understand.","title":"Paradigms"},{"location":"Computer-Science/se/#standardization","text":"Does a language have a formal standard? This can be very important to ensure that programs written to work with one compiler/interpreter will work with another. Some languages are standardized by the American National Standards Institute (ANSI), some are standardized by the International Organization for Standardization (ISO), and some have an informal but de-facto standard not maintained by any standards organization.","title":"Standardization"},{"location":"Computer-Science/se/#repository-structure","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 Repo/ | | ------- README.md | | ------- LICENSE | | ------- CONTRIBUTING.md | | ------- CODE_OF_CONDUCT.md | | ------- NOTICE.md | | | ------- configs/ | | ----- anyfile.json | | | | ------- docs/ | | ----- Index | | ----- Overview | | ----- API | | ----- Code Structure | | ----- Installation | | ----- Configuration | | ----- References | | ----- Future | | | | ------- src/ | | -----project/ | | --__init__.py | | --settings.py | | --urls.py | \u2514--wsgi.py | | -----app1/ | | | | -----app2-RESTful/ | | | | -----manage.py | | | \u2514-----db.sqlite3 Django - Code of Conduct Other - Code of Conduct","title":"Repository Structure"},{"location":"Computer-Science/se/#history","text":"","title":"History"},{"location":"Computer-Science/se/#os","text":"https://en.wikipedia.org/wiki/Timeline_of_operating_systems","title":"OS"},{"location":"Computer-Science/se/#open-source-guide","text":"","title":"Open Source Guide"},{"location":"Computer-Science/se/#software-licensing","text":"","title":"Software Licensing"},{"location":"Computer-Science/se/#intro","text":"https://opensource.org/licenses/alphabetical http://www.gnu.org/licenses/ https://opensource.org/licenses","title":"Intro"},{"location":"Computer-Science/se/#apache-20","text":"TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION - free, reproduce, sublicense, re-distribute in any form - Redistribution - provide copy of license - mention you changed this file/code - retain copy of source code - can be monitized - Summary","title":"Apache-2.0"},{"location":"Computer-Science/se/#mit","text":"Permission is hereby granted, free of charge , to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction , including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so. Summary","title":"MIT"},{"location":"Computer-Science/se/#bsd-2-clause","text":"Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: 1 2 3 1. Redistributions of source code must `retain the above copyright notice`, this list of conditions and the following disclaimer. 2. Redistributions in binary form `must reproduce the above copyright notice`, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. also known as Simplified BSD or FreeBSD can be monitized Summary","title":"BSD-2-Clause"},{"location":"Computer-Science/se/#bsd-3-clause","text":"Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: 1 2 3 4 5 1. Redistributions of source code must `retain the above copyright notice`, this list of conditions and the following disclaimer. 2. Redistributions in binary form must `reproduce the above copyright notice`, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. 3. Neither the `name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived` from this software without specific prior written permission. also known as Modified BSD or New BSD can be monitized Summary","title":"BSD-3-Clause"},{"location":"Computer-Science/se/#gpl-20","text":"everyone is permitted to copy and distribute verbatim copies of this LICENSE document, but changing it is not allowed can use, modify, redistribute original licensor can relicense to GPL-3.0 also used as GPL version 2 or any later version gives option to follow the terms and conditions of any later version so, any other licensors can follow the the term & conditions of GPL-3.0 can be monitized modified versions of code/software be marked as changed with the date of the change any patent must be licensed for everyone's free use or not licensed at all Summary","title":"GPL-2.0"},{"location":"Computer-Science/se/#gpl-30","text":"everyone is permitted to copy and distribute verbatim copies of this LICENSE document, but changing it is not allowed can use, modify, redistribute else, similar to GPL-2.0 but re-phrased Summary","title":"GPL-3.0"},{"location":"Computer-Science/se/#agpl-30","text":"everyone is permitted to copy and distribute verbatim copies of this LICENSE document, but changing it is not allowed built for network software can distribute modified versions if you keep track of the changes and the date you made them source code must be distributed along with web service publication the AGPL is the GPL of the web http://www.gnu.org/licenses/agpl.html Summary","title":"AGPL-3.0"},{"location":"Computer-Science/se/#software-security","text":"","title":"Software Security"},{"location":"Computer-Science/se/#obfuscation","text":"The process of protecting the protable executable files (EXE, DLL) from getting decompiled into the original source code is called Obfuscation.","title":"Obfuscation"},{"location":"Computer-Science/se/#obfuscation-tools","text":"ConfuserEx: https://yck1509.github.io/ConfuserEx/ (EXE, DLLs) ConfuserEx Decoder: https://github.com/cawk/ConfuserEx-Unpacker dotfuscators: https://www.preemptive.com/products/dotfuscator/overview https://translate.google.it/translate?hl=it?sl=it&tl=en&u=https%3A//gianmarcocastagna.blogspot.it/","title":"Obfuscation Tools"},{"location":"Computer-Science/se/#protect-code","text":"https://stackoverflow.com/questions/805461/how-to-protect-dlls https://stackoverflow.com/questions/109997/how-do-you-protect-your-software-from-illegal-distribution https://stackoverflow.com/questions/506282/protect-net-code-from-reverse-engineering https://www.codeproject.com/Articles/1139773/Protect-Your-Source-Code-from-Decompiling-or-Rever https://stackoverflow.com/questions/7849620/what-is-the-best-way-to-protect-sensitive-data-in-the-code","title":"Protect Code"},{"location":"Computer-Science/se/#protect-net-code","text":"https://stackoverflow.com/questions/7669684/encrypt-app-config-file-automatically-during-build-process","title":"Protect .Net Code"},{"location":"Computer-Science/se/#decompile-tools","text":"https://stackoverflow.com/questions/130058/how-are-serial-generators-cracks-developed","title":"Decompile Tools"},{"location":"Computer-Science/se/#dlls","text":"dumpbin (c/c+) ILSpy dotPeek: https://www.jetbrains.com/decompiler/ ReSharper","title":"DLLs"},{"location":"Computer-Science/se/#python","text":"uncompyle6 (.pyc/.pyo to .py) A native Python cross-version decompiler and fragment decompiler. The successor to decompyle, uncompyle, and uncompyle2. pyinstallerextractor (.exe to .py)","title":"Python"},{"location":"Computer-Science/se/#plsql","text":"UnwrapIt","title":"PL/SQL"},{"location":"Computer-Science/se/#encryption","text":"","title":"Encryption"},{"location":"Computer-Science/se/#symmetric-key-encryption","text":"Intro use the same cryptographic keys for both encryption of plaintext and decryption of ciphertext The keys may be identical or there may be a simple transformation to go between the two keys Insecure","title":"Symmetric Key Encryption"},{"location":"Computer-Science/se/#des-data-encryption-standard","text":"Intro Insecure mainly due to the 56-bit key size being too small Once hacked in 22hrs","title":"DES (Data Encryption Standard)"},{"location":"Computer-Science/se/#aes-advanced-encryption-standard","text":"Intro a subset of Rijndael superseded DES uses block cipher block cipher is a method of encrypting text (to produce ciphertext) a cryptographic key and algorithm are applied to a block of data (for example, 64 contiguous bits) at once as a group rather than to one bit at a time Design Principle substitution\u2013permutation network a combination of both substitution and permutation are applied to the plain text fixed block size of 128 bits, and a key size of 128, 192, or 256 bits Variaties MARS RC6 Rijndael Serpent Twofish","title":"AES (Advanced Encryption Standard)"},{"location":"Computer-Science/se/#asymmetric-encryption-privatepublic-key","text":"Intro pair of keys made up of long random strings public key: known to every one private key: known to owner only Functions Authentication public key verifies that message received is sent by paired private key holder only Encryption only paired private key can decrypt the message encrypted by the public key","title":"Asymmetric Encryption (Private/Public Key)"},{"location":"Computer-Science/se/#deffie-hellman-key-exchange","text":"http://www.omnisecu.com/security/public-key-infrastructure/asymmetric-encryption-algorithms.php Intro","title":"Deffie-Hellman Key Exchange"},{"location":"Computer-Science/se/#rsa-rivestshamiradleman","text":"","title":"RSA (Rivest\u2013Shamir\u2013Adleman)"},{"location":"Computer-Science/se/#misc","text":"","title":"Misc"},{"location":"Computer-Science/se/#net-dll-vs-c-dll","text":"https://stackoverflow.com/questions/10634743/net-dll-vs-c-dll","title":".Net DLL vs C++ DLL"},{"location":"Computer-Science/se/#32-bit-vs-64-bit-dll","text":"https://stackoverflow.com/questions/495244/how-can-i-test-a-windows-dll-file-to-determine-if-it-is-32-bit-or-64-bit","title":"32 bit vs 64 bit DLL"},{"location":"Computer-Science/toc/","text":"Theory of Computation # Table of Contents Theory of Computation Automata Deterministinc Finite Non-Detereministic Finite Decision Problems (H) P (Polynomial Time) NP (Non-Deterministic Polynomial Time) NP-Complete NP-Hard Automata # Deterministinc # Finite # Non-Detereministic Finite # Decision Problems (H) # P (Polynomial Time) # NP (Non-Deterministic Polynomial Time) # NP-Complete # NP-Hard #","title":"Theory of Computation"},{"location":"Computer-Science/toc/#theory-of-computation","text":"","title":"Theory of Computation"},{"location":"Computer-Science/toc/#automata","text":"","title":"Automata"},{"location":"Computer-Science/toc/#deterministinc","text":"","title":"Deterministinc"},{"location":"Computer-Science/toc/#finite","text":"","title":"Finite"},{"location":"Computer-Science/toc/#non-detereministic-finite","text":"","title":"Non-Detereministic Finite"},{"location":"Computer-Science/toc/#decision-problems-h","text":"","title":"Decision Problems (H)"},{"location":"Computer-Science/toc/#p-polynomial-time","text":"","title":"P (Polynomial Time)"},{"location":"Computer-Science/toc/#np-non-deterministic-polynomial-time","text":"","title":"NP (Non-Deterministic Polynomial Time)"},{"location":"Computer-Science/toc/#np-complete","text":"","title":"NP-Complete"},{"location":"Computer-Science/toc/#np-hard","text":"","title":"NP-Hard"},{"location":"Data-Science/ml_azure/","text":"Azure Machine Learning # Table of Contents Azure Machine Learning Azure Machine Learning Sources Requirement Module 1: Intro to Data Science Introduction Data ---> What happened? --> Why did it happen? --> What will happen? ---> Decision Steps Types of Analytics Predictive vs Prescriptive Historical Notes Big Data Process CCC KDD Module 1 Chapter 4: Regression Intro Simple Linear Regression Ridge Regression Support Vector Machine Regression (SVM) Cross-Validation Nested Cross-Validation Chapter 5: Classification Intro Decision Boundary Classification Error Loss Functions Different ML Techniques & LFs Logistic Regression SVM Regression AdaBoost Regression Decision Tree Boosted Decision Tree Imbalanced Dataset Minority Class Data (Excess amount, Weight) ROC (Receiver Operating Characteristic) Curve FPR & TPR (False Positive Rate & True Positive Rate) Chapter 6: Clustering Intro Unsuperwised Learning K- Means Clustering Hierarchical Agglomerative Clustering Distance metrics are important Chapter 7: Recommender Systems & Matrix Factorization Intro Example: Options Matrix-Factorization Chapter 8: Intro to Data Science Technologies Why Azure ML? Supports? Cortana Analytics Suite Azure ML Studio Module 2: Working with Data Chapter 9 Chapter 10 Chapter 11: Data Sampling and Quantization Azure ML Table Data Types: Continuous Vs Catergorial Variables Quantization What? Sampling? Example Quantization of Continuous Variable Extra Chapter 12: Data Cleansing and Transformation Missing & Repeated Values Clean Missing & Repeated values Errors & Outliers Visualizing Outliers Clean Errors & Outliers Scaling Data Module 3: Visualizing Data & EXploring Models Chapter 13: Data Exploration & Visualization Exploratory Data Analysis View of data Types of plots Azure Machine Learning # Sources # https://github.com/MicrosoftLearning/Data-Science-and-ML-Essentials/tree/master/Labs Requirement # Anaconda OR Spyder (or IPython console) scikit-learn matplotlib numpy Module 1: Intro to Data Science # Introduction # Evolving subject, no single definition Requires a range of skills Exploration and quantitative analysis of all available structured or unstructured data to develop understanding, extract knowledge, and formulate actionable results. Data --> Decisions --> Actions Data ---> What happened? --> Why did it happen? --> What will happen? ---> Decision # Accidents like plane crashes etc Areas of interest: Automatic Trading, Bidding Steps # Finding data sources Acquiring data Cleaning and transforming data, Reshaping (99% work) Relationship finding Decision Types of Analytics # Retrospective Real-time Predictive (Most ML falls under) Prescriptive Intelligent Saas apps (Cortana, ..) Predictive vs Prescriptive # Predictive analysis calibrated on past data, tells us what to expect Prescriptive analysis tells what actions to take Historical Notes # Big Data by astronomers Cox & Ellsworth in 1997 By CCC in 2012 By KDD in 1996 Big Data Process # CCC # A KDD # A Module 1 # Chapter 4: Regression # Intro # Simple Linear Regression # Ridge Regression # Support Vector Machine Regression (SVM) # Cross-Validation # Nested Cross-Validation # Popular evalution technique in ML Divide data set into 10 folds, pich one for test, reserve 1 for validation, and rest 8 as test data. Chapter 5: Classification # Intro # Prediction of labels/predictable data - X (true/false or 1/-1) using independent variable/Feature/ - Y .. Decision Boundary # Classification Error # Loss Functions # Different ML Techniques & LFs # Logistic Regression # SVM Regression # AdaBoost Regression # Decision Tree # Boosted Decision Tree # Imbalanced Dataset # Minority Class Data (Excess amount, Weight) # ROC (Receiver Operating Characteristic) Curve # FPR & TPR (False Positive Rate & True Positive Rate) # Chapter 6: Clustering # Intro # Unsuperwised label prediction Unsuperwised Learning # Means training data has no ground truth labels to learn from K- Means Clustering # Input K = number of clusterss Randomly initialize centers Assign all the points to the closest centers Repeat till convergence Hierarchical Agglomerative Clustering # Start with each point in its own cluster Repeatedly merge the clusters of the closest two points Distance metrics are important # Large impact on the solution Some algos uses \"Adaptive\" distance measures Chapter 7: Recommender Systems & Matrix Factorization # Intro # \\left(\\begin{array}{cc} 5 & * & 1 & 1\\\\ 5 & * & 1 & 1 \\end{array}\\right) \\left(\\begin{array}{cc} 5 & * & 1 & 1\\\\ 5 & * & 1 & 1 \\end{array}\\right) Example: # Netflix contest Options # User-Based Collaborative Filtering Item-Based Collaborative Filtering Matrix-Factorization # Carmen_1 \\left (\\begin{array}{cc} 5 & 1 \\end{array}\\right) Carmen_1 \\left (\\begin{array}{cc} 5 & 1 \\end{array}\\right) Chapter 8: Intro to Data Science Technologies # Why Azure ML? # Easy to deploy services on production Supports? # Sql R Python Cortana Analytics Suite # https://www.microsoft.com/cortanaanalytics Preconfigures Solutions Dashboard & Visualization Machine Learning & Analytics Azure Bigdata (Hadoop Implementation) Information Management Azure ML Studio # https://account.azure.com Experiments contain workflow Experiments constructed of modules Modules: Transform Data Compute Models Score Models Evaluate Models Create custom modules with SQL, R & Python Module 2: Working with Data # Chapter 9 # Chapter 10 # Chapter 11: Data Sampling and Quantization # Azure ML Table Data Types: # Numeric: Integer, Floating points Boolean String Date time Time span Categorial Image Continuous Vs Catergorial Variables # Continuous: Countable, e.g. Time, Temperature, Counts* Categorial: Classifiable, e.g. Gender, Type, City * * descrete continuous Quantization # A range with sampled data. What? # Continuous variables must be sampled Sampling? # Digitizing the domain. * Time stamped * Precision Example # Temperature every minute Count over 1 hour Quantization of Continuous Variable # Convert continuous variables into categorial using binning/categorizing. Binning: Allocating each value into one category/bin. Example: * Small, Medium & Large Module to use: Quantize Module Extra # Metadata Editor Chapter 12: Data Cleansing and Transformation # (Data Munging) Deals with Missing & repeated values Outliers and errors Scaling Filtering with custom code Iterative process Example: Forest-Fire Data Missing & Repeated Values # are common many ML algos don't deal with missing values repeated values bias results, so search for them make estimation treat them Clean Missing & Repeated values # remove rows substitute a specific value Interpolate values - Linear/polynomial on the basis of growth/trend of the data forward/backword fill With Azure ML Module: Clean Missing Data, Remove Duplicate Rows With R Missing data: is.na() Repeated data: duplicated() With Python Missing data: pandas.isnull() Repeated data: DataFrame.drop_duplicates() Errors & Outliers # can bias model training, so search for them validate treat them Visualizing Outliers # Scatter plot matrix R - pairs plot Python - pandas.tools.plotting.scatter_matrix Bar chart or graph histogram Clean Errors & Outliers # Error treatment Censor: remove entire row Trim: trim the value inbetween a range Interpolate: Linear or polynomial on the basis of growth/trend of the data Substitute With Azure ML Module: Clip values (select column--> set lower/upper threshold) With R data.frame = data.fram[filter.expression,] With Python frame1 = frame1[(frame1[\"col1\"] > 40.0) & (frame1[\"col2\"] < 30.0) & (frame1[\"col3\"] < 23.0)] Scaling Data # (aka Normalization, Transformation) * Why: * to put all the numerical data into same range line -1 to 1 or 0 to 10 other than a:0-1, b:0-100, c:500:1000 * not doing so: * will make adverse effect on training model * will get biased training model What: looking at numerical features/columns numerical features/variable/columns needs similar scale Scaling methods: zero mean & unit variance min-max: all numeric values in range 0 to 1 logrithmic: does distributional changes (good for classification) LogNormal: Hyperbolic tangent scaling: distribution transformation ordered data like time-series may need to de-trend scale after treating outliers How: Azure ML Module: Normalize Data R: Python: Doubts: How to make such transformations? Module 3: Visualizing Data & EXploring Models # Chapter 13: Data Exploration & Visualization # Exploratory Data Analysis # What: Explore the data with visualization Understand the relationships in the data How: Create multiple views of data Data conditioning: Poweful plotting method to project multiple dimension on two dimension page/screen View of data # Relationships in data can be complex Data exploration requires multiple views Conditioned (aka faceted, trellis, lattice) plots are ideal project multiple dimension onto two plots of subsets (group by) Types of plots # Scatter and line plots Bar: like histogram but Used for categorical & factor data like disease, blood grp Types: ordered, un-ordered Histogram: used for continuos variable like time, temp density or count are plotted on vertical axis widely used Violin Q-Q Box: Shows 4 quartiles, i.e. a box divided in two half (by median), one upper vertical line, one lower and dot as outliers Line: connecting dot--> Polynomial regression--> curve 1 1","title":"Azure Machine Learning"},{"location":"Data-Science/ml_azure/#azure-machine-learning","text":"","title":"Azure Machine Learning"},{"location":"Data-Science/ml_azure/#azure-machine-learning_1","text":"","title":"Azure Machine Learning"},{"location":"Data-Science/ml_azure/#sources","text":"https://github.com/MicrosoftLearning/Data-Science-and-ML-Essentials/tree/master/Labs","title":"Sources"},{"location":"Data-Science/ml_azure/#requirement","text":"Anaconda OR Spyder (or IPython console) scikit-learn matplotlib numpy","title":"Requirement"},{"location":"Data-Science/ml_azure/#module-1-intro-to-data-science","text":"","title":"Module 1: Intro to Data Science"},{"location":"Data-Science/ml_azure/#introduction","text":"Evolving subject, no single definition Requires a range of skills Exploration and quantitative analysis of all available structured or unstructured data to develop understanding, extract knowledge, and formulate actionable results. Data --> Decisions --> Actions","title":"Introduction"},{"location":"Data-Science/ml_azure/#data-what-happened-why-did-it-happen-what-will-happen-decision","text":"Accidents like plane crashes etc Areas of interest: Automatic Trading, Bidding","title":"Data ---&gt; What happened? --&gt;  Why did it happen? --&gt; What will happen? ---&gt; Decision"},{"location":"Data-Science/ml_azure/#steps","text":"Finding data sources Acquiring data Cleaning and transforming data, Reshaping (99% work) Relationship finding Decision","title":"Steps"},{"location":"Data-Science/ml_azure/#types-of-analytics","text":"Retrospective Real-time Predictive (Most ML falls under) Prescriptive Intelligent Saas apps (Cortana, ..)","title":"Types of Analytics"},{"location":"Data-Science/ml_azure/#predictive-vs-prescriptive","text":"Predictive analysis calibrated on past data, tells us what to expect Prescriptive analysis tells what actions to take","title":"Predictive vs Prescriptive"},{"location":"Data-Science/ml_azure/#historical-notes","text":"Big Data by astronomers Cox & Ellsworth in 1997 By CCC in 2012 By KDD in 1996","title":"Historical Notes"},{"location":"Data-Science/ml_azure/#big-data-process","text":"","title":"Big Data Process"},{"location":"Data-Science/ml_azure/#ccc","text":"A","title":"CCC"},{"location":"Data-Science/ml_azure/#kdd","text":"A","title":"KDD"},{"location":"Data-Science/ml_azure/#module-1","text":"","title":"Module 1"},{"location":"Data-Science/ml_azure/#chapter-4-regression","text":"","title":"Chapter 4: Regression"},{"location":"Data-Science/ml_azure/#intro","text":"","title":"Intro"},{"location":"Data-Science/ml_azure/#simple-linear-regression","text":"","title":"Simple Linear Regression"},{"location":"Data-Science/ml_azure/#ridge-regression","text":"","title":"Ridge Regression"},{"location":"Data-Science/ml_azure/#support-vector-machine-regression-svm","text":"","title":"Support Vector Machine Regression (SVM)"},{"location":"Data-Science/ml_azure/#cross-validation","text":"","title":"Cross-Validation"},{"location":"Data-Science/ml_azure/#nested-cross-validation","text":"Popular evalution technique in ML Divide data set into 10 folds, pich one for test, reserve 1 for validation, and rest 8 as test data.","title":"Nested Cross-Validation"},{"location":"Data-Science/ml_azure/#chapter-5-classification","text":"","title":"Chapter 5: Classification"},{"location":"Data-Science/ml_azure/#intro_1","text":"Prediction of labels/predictable data - X (true/false or 1/-1) using independent variable/Feature/ - Y ..","title":"Intro"},{"location":"Data-Science/ml_azure/#decision-boundary","text":"","title":"Decision Boundary"},{"location":"Data-Science/ml_azure/#classification-error","text":"","title":"Classification Error"},{"location":"Data-Science/ml_azure/#loss-functions","text":"","title":"Loss Functions"},{"location":"Data-Science/ml_azure/#different-ml-techniques-lfs","text":"","title":"Different ML Techniques &amp; LFs"},{"location":"Data-Science/ml_azure/#logistic-regression","text":"","title":"Logistic Regression"},{"location":"Data-Science/ml_azure/#svm-regression","text":"","title":"SVM Regression"},{"location":"Data-Science/ml_azure/#adaboost-regression","text":"","title":"AdaBoost Regression"},{"location":"Data-Science/ml_azure/#decision-tree","text":"","title":"Decision Tree"},{"location":"Data-Science/ml_azure/#boosted-decision-tree","text":"","title":"Boosted Decision Tree"},{"location":"Data-Science/ml_azure/#imbalanced-dataset","text":"","title":"Imbalanced Dataset"},{"location":"Data-Science/ml_azure/#minority-class-data-excess-amount-weight","text":"","title":"Minority Class Data (Excess amount, Weight)"},{"location":"Data-Science/ml_azure/#roc-receiver-operating-characteristic-curve","text":"","title":"ROC (Receiver Operating Characteristic) Curve"},{"location":"Data-Science/ml_azure/#fpr-tpr-false-positive-rate-true-positive-rate","text":"","title":"FPR &amp; TPR (False Positive Rate &amp; True Positive Rate)"},{"location":"Data-Science/ml_azure/#chapter-6-clustering","text":"","title":"Chapter 6: Clustering"},{"location":"Data-Science/ml_azure/#intro_2","text":"Unsuperwised label prediction","title":"Intro"},{"location":"Data-Science/ml_azure/#unsuperwised-learning","text":"Means training data has no ground truth labels to learn from","title":"Unsuperwised Learning"},{"location":"Data-Science/ml_azure/#k-means-clustering","text":"Input K = number of clusterss Randomly initialize centers Assign all the points to the closest centers Repeat till convergence","title":"K- Means Clustering"},{"location":"Data-Science/ml_azure/#hierarchical-agglomerative-clustering","text":"Start with each point in its own cluster Repeatedly merge the clusters of the closest two points","title":"Hierarchical Agglomerative Clustering"},{"location":"Data-Science/ml_azure/#distance-metrics-are-important","text":"Large impact on the solution Some algos uses \"Adaptive\" distance measures","title":"Distance metrics are important"},{"location":"Data-Science/ml_azure/#chapter-7-recommender-systems-matrix-factorization","text":"","title":"Chapter 7: Recommender Systems &amp; Matrix Factorization"},{"location":"Data-Science/ml_azure/#intro_3","text":"\\left(\\begin{array}{cc} 5 & * & 1 & 1\\\\ 5 & * & 1 & 1 \\end{array}\\right) \\left(\\begin{array}{cc} 5 & * & 1 & 1\\\\ 5 & * & 1 & 1 \\end{array}\\right)","title":"Intro"},{"location":"Data-Science/ml_azure/#example","text":"Netflix contest","title":"Example:"},{"location":"Data-Science/ml_azure/#options","text":"User-Based Collaborative Filtering Item-Based Collaborative Filtering","title":"Options"},{"location":"Data-Science/ml_azure/#matrix-factorization","text":"Carmen_1 \\left (\\begin{array}{cc} 5 & 1 \\end{array}\\right) Carmen_1 \\left (\\begin{array}{cc} 5 & 1 \\end{array}\\right)","title":"Matrix-Factorization"},{"location":"Data-Science/ml_azure/#chapter-8-intro-to-data-science-technologies","text":"","title":"Chapter 8: Intro to Data Science Technologies"},{"location":"Data-Science/ml_azure/#why-azure-ml","text":"Easy to deploy services on production","title":"Why Azure ML?"},{"location":"Data-Science/ml_azure/#supports","text":"Sql R Python","title":"Supports?"},{"location":"Data-Science/ml_azure/#cortana-analytics-suite","text":"https://www.microsoft.com/cortanaanalytics Preconfigures Solutions Dashboard & Visualization Machine Learning & Analytics Azure Bigdata (Hadoop Implementation) Information Management","title":"Cortana Analytics Suite"},{"location":"Data-Science/ml_azure/#azure-ml-studio","text":"https://account.azure.com Experiments contain workflow Experiments constructed of modules Modules: Transform Data Compute Models Score Models Evaluate Models Create custom modules with SQL, R & Python","title":"Azure ML Studio"},{"location":"Data-Science/ml_azure/#module-2-working-with-data","text":"","title":"Module 2: Working with Data"},{"location":"Data-Science/ml_azure/#chapter-9","text":"","title":"Chapter 9"},{"location":"Data-Science/ml_azure/#chapter-10","text":"","title":"Chapter 10"},{"location":"Data-Science/ml_azure/#chapter-11-data-sampling-and-quantization","text":"","title":"Chapter 11: Data Sampling and Quantization"},{"location":"Data-Science/ml_azure/#azure-ml-table-data-types","text":"Numeric: Integer, Floating points Boolean String Date time Time span Categorial Image","title":"Azure ML Table Data Types:"},{"location":"Data-Science/ml_azure/#continuous-vs-catergorial-variables","text":"Continuous: Countable, e.g. Time, Temperature, Counts* Categorial: Classifiable, e.g. Gender, Type, City * * descrete continuous","title":"Continuous Vs Catergorial Variables"},{"location":"Data-Science/ml_azure/#quantization","text":"A range with sampled data.","title":"Quantization"},{"location":"Data-Science/ml_azure/#what","text":"Continuous variables must be sampled","title":"What?"},{"location":"Data-Science/ml_azure/#sampling","text":"Digitizing the domain. * Time stamped * Precision","title":"Sampling?"},{"location":"Data-Science/ml_azure/#example_1","text":"Temperature every minute Count over 1 hour","title":"Example"},{"location":"Data-Science/ml_azure/#quantization-of-continuous-variable","text":"Convert continuous variables into categorial using binning/categorizing. Binning: Allocating each value into one category/bin. Example: * Small, Medium & Large Module to use: Quantize Module","title":"Quantization of Continuous Variable"},{"location":"Data-Science/ml_azure/#extra","text":"Metadata Editor","title":"Extra"},{"location":"Data-Science/ml_azure/#chapter-12-data-cleansing-and-transformation","text":"(Data Munging) Deals with Missing & repeated values Outliers and errors Scaling Filtering with custom code Iterative process Example: Forest-Fire Data","title":"Chapter 12: Data Cleansing and Transformation"},{"location":"Data-Science/ml_azure/#missing-repeated-values","text":"are common many ML algos don't deal with missing values repeated values bias results, so search for them make estimation treat them","title":"Missing &amp; Repeated Values"},{"location":"Data-Science/ml_azure/#clean-missing-repeated-values","text":"remove rows substitute a specific value Interpolate values - Linear/polynomial on the basis of growth/trend of the data forward/backword fill With Azure ML Module: Clean Missing Data, Remove Duplicate Rows With R Missing data: is.na() Repeated data: duplicated() With Python Missing data: pandas.isnull() Repeated data: DataFrame.drop_duplicates()","title":"Clean Missing &amp; Repeated values"},{"location":"Data-Science/ml_azure/#errors-outliers","text":"can bias model training, so search for them validate treat them","title":"Errors &amp; Outliers"},{"location":"Data-Science/ml_azure/#visualizing-outliers","text":"Scatter plot matrix R - pairs plot Python - pandas.tools.plotting.scatter_matrix Bar chart or graph histogram","title":"Visualizing Outliers"},{"location":"Data-Science/ml_azure/#clean-errors-outliers","text":"Error treatment Censor: remove entire row Trim: trim the value inbetween a range Interpolate: Linear or polynomial on the basis of growth/trend of the data Substitute With Azure ML Module: Clip values (select column--> set lower/upper threshold) With R data.frame = data.fram[filter.expression,] With Python frame1 = frame1[(frame1[\"col1\"] > 40.0) & (frame1[\"col2\"] < 30.0) & (frame1[\"col3\"] < 23.0)]","title":"Clean Errors &amp; Outliers"},{"location":"Data-Science/ml_azure/#scaling-data","text":"(aka Normalization, Transformation) * Why: * to put all the numerical data into same range line -1 to 1 or 0 to 10 other than a:0-1, b:0-100, c:500:1000 * not doing so: * will make adverse effect on training model * will get biased training model What: looking at numerical features/columns numerical features/variable/columns needs similar scale Scaling methods: zero mean & unit variance min-max: all numeric values in range 0 to 1 logrithmic: does distributional changes (good for classification) LogNormal: Hyperbolic tangent scaling: distribution transformation ordered data like time-series may need to de-trend scale after treating outliers How: Azure ML Module: Normalize Data R: Python: Doubts: How to make such transformations?","title":"Scaling Data"},{"location":"Data-Science/ml_azure/#module-3-visualizing-data-exploring-models","text":"","title":"Module 3: Visualizing Data &amp; EXploring Models"},{"location":"Data-Science/ml_azure/#chapter-13-data-exploration-visualization","text":"","title":"Chapter 13: Data Exploration &amp; Visualization"},{"location":"Data-Science/ml_azure/#exploratory-data-analysis","text":"What: Explore the data with visualization Understand the relationships in the data How: Create multiple views of data Data conditioning: Poweful plotting method to project multiple dimension on two dimension page/screen","title":"Exploratory Data Analysis"},{"location":"Data-Science/ml_azure/#view-of-data","text":"Relationships in data can be complex Data exploration requires multiple views Conditioned (aka faceted, trellis, lattice) plots are ideal project multiple dimension onto two plots of subsets (group by)","title":"View of data"},{"location":"Data-Science/ml_azure/#types-of-plots","text":"Scatter and line plots Bar: like histogram but Used for categorical & factor data like disease, blood grp Types: ordered, un-ordered Histogram: used for continuos variable like time, temp density or count are plotted on vertical axis widely used Violin Q-Q Box: Shows 4 quartiles, i.e. a box divided in two half (by median), one upper vertical line, one lower and dot as outliers Line: connecting dot--> Polynomial regression--> curve 1 1","title":"Types of plots"},{"location":"Engineering/Network-Engineering/FAQ/","text":"FAQs # FAQs Networking - Wireless Networking - Wireless # Ref: https://networkproxy.wordpress.com/tag/cisco-wireless-interview-questions-and-answers/","title":"FAQs"},{"location":"Engineering/Network-Engineering/FAQ/#faqs","text":"FAQs Networking - Wireless","title":"FAQs"},{"location":"Engineering/Network-Engineering/FAQ/#networking-wireless","text":"Ref: https://networkproxy.wordpress.com/tag/cisco-wireless-interview-questions-and-answers/","title":"Networking - Wireless"},{"location":"Engineering/Network-Engineering/Keywords/","text":"Keywords # Keywords - https://www.cloudflare.com/en-in/learning/dns/dns-records/ - Point-to-Point Protocol over Ethernet DNS Record Types A The record that holds the IP address of a domain CNAME Forwards one domain or subdomain to another domain, does NOT provide an IP address. AAAA Same as \u201cA\u201d, but for IPv6 addresses MX Directs emails to an email server TXT Lets an admin store text notes in the record NS Stores the name server for a DNS entry Etc - https://www.cloudflare.com/en-in/learning/dns/dns-records/ # NAT - Network Address Translation https://www.geeksforgeeks.org/network-address-translation-nat/ https://www.cisco.com/c/en/us/support/docs/ip/network-address-translation-nat/26704-nat-faq-00.html PAT - Port Address Translation This overloads NAT\u2019s feature https://www.geeksforgeeks.org/network-address-translation-nat/ Masquerade (TBD) MPLS, Broadband, LTE, Satellite (TBD) PPPoE - Point-to-Point Protocol over Ethernet # SSL Secure socket layer protocol L6 (layer 6 - presentation layer thing) 1995, at Netscape establish encrypted communications between web servers/clients encrypts the data received from L7 and passes encrypted data to L5 decrypts the data received from L5 and passes decrypted data to L7 leveraged by HTTPS protocol provides privacy and data integrity uses public-key exchange version 1, 2, 3, all deprecated TLS transport layer security protocol successor of SSL 1999, by IETF L6 (layer 6 - presentation layer thing) establish encrypted communications between web servers/clients encrypts the data received from L7 and passes encrypted data to L5 decrypts the data received from L5 and passes decrypted data to L7 leveraged by HTTPS protocol, can be used provides privacy and data integrity version 1.0, 1.1 are deprecated and 1.2 & 1.3 are available SSL vs TLS In short, a better version of SSL https://www.ssl2buy.com/wiki/ssl-vs-tls There is no SSL certificate and TLS certificates, they're just certificates which works for both the protocols both are X.509 digital certificates that helps authenticate the server and facilitate the handshake process TLS SNI: Server Name Indication, is an addition to the TLS encryption protocol that enables a client device to specify the domain name it is trying to reach in the first step of the TLS handshake, preventing common name mismatch errors. DSCP Differentiated Services Code Point Is a packet header value That can be used to request (for example) high priority or best effort delivery of the traffic Related Devices: TBD Layer: TBD DHCP Dynamic Hosting Configuration Protocol L2-L3 (Network Layer to Transport Layer) Is a NMP (N/W management protocol), helps automate the process of configuring devices on IP networks Thus assigning devices IP address so that IPs don't need to be manually assigned by an admin each time a device connects Thus allowing them to use network service like NAT, DNS and any communication protocols based on UDP or TCP DHCP is also responsible for the configuration of domain name server (DNS) and subnet masks, as well as default gateways Related Devices: Router/Gateway OSI Layer: 2-3 NAT Network Address Translation Is a method of remapping an IP address space into another by modifying the network address information in IP header of packets while they are in transit across a traffic routing device Means, it enables private IP networks that uses unregistered IP address to connect to internet by translating the unregistered (not globally unique) IP addresses in internal network into legal address before packets are forwarded to another network Related Devices: Router Layer: TBD PFE Packet Forwarding Engine (Juniper routers?) Related Devices: Router Layer: TBD SD-WAN Software-Driven Wide Area Network Related Devices: Router/Gateway Layer: TBD RPM Real-time Performance Monitoring (Switch?) Related Devices: Switch Layer: TBD TWAMP Two Way Active Measurement Protocol (Switch?) Related Devices: Switch Layer: TBD QoS Quality of service QoE Quality of experience AP, Router, Modem, Switch, Bridge, Hub Node, Hop Gateway, Catenet LAN WLAN WxLAN WAN VLAN VxLAN MAC Media access control IP IPSec protocol suite Port SSID BSSID Roaming Hash(), HMAC OSI application layer (7) presentation layer (6) session layer (5) data layer (4) transport layer (3) network layer (2) physical layer (1) RFC CNA - Captive N/W Assistance / Captive Portal WiFi Auth Mechanism LE , BLE (Bluetooth Low Energy), Beacon, vBLE SLE (Service Level Expectations), SLA (Service Level Agreements) Host .local files e.g. hostname.local RADIUS Remote Authentication Dial-In User Service NAS Network access server MAC Auth CoA Change of authorization RadSec RADIUS security Traceroute, ARP, Ping, TCP ping, TCP traceroute SPA Source protocol address TPA target protocol address Connection-oriented, Connectionless protocol TCP Transmission Control Protocol defines how to establish connection to exchange info how computers send packets of data to each other works on top of IP ensures reliable transmission of packets have a feedback/ack/validation mechanism which ensures if the packet has reached the destination or not thus is lossless transmission called as connection-oriented Related Device: OSI Layer: L3 (Transport Layer) UDP User Datagram Protocol defines how to establish connection to exchange info how computers send packets of data to each other works on top of IP ensures low-latency transmission of datagrams re-transmission of lost datagrams are not possible connection-less protocol ICMP Internet Control Message Protocol one of the protocol of TCP/IP suite generally used for network diagnostic or control purpose e.g. PING, Traceroute etc Packet Data, Datagram, Error-Message, Segment, IP datagram, IP fragmentation Ethernet Websocket Control Plane ACL/Policy Rogue, Neighbor, Honeypot Honeypot SSID Rogue AP, Rogue Clients Neighbour APs https://www.mist.com/documentation/rogue-neighbor-honeypot-aps/ Wifi spoof Ssid injection Xss (script in ssid name) DFS (dynamic frequency selection) WiFi bandwidth channels FCC, CE, Other certifications 2.4 GHz 5 GHz 802.11ac and all other RRM: Radio resource management SLE: Service level experience BGP: Border Gateway Protocol SSH VPN Tunnel Split Tunnel PPTP: Point-to-Point Tunneling Protocol used by an Internet service provider (ISP) to enable the operation of a virtual private network (VPN) over the Internet L2TP: Layer Two Tunneling Protocol an extension of the PPTP SSTP: Secure Socket Tunneling Protocol Layer Two: Devices e.g. Network interface cards, hubs, bridges, and switches OSI layers: layer 2 Layer Three?? Device e.g. Advanced Switch Combines the functionality of a switch and a router OSI layers: layer 2 and layer 3 Ref: https://documentation.meraki.com/MS/Layer_3_Switching/Layer_3_vs_Layer_2_Switching http://techgenix.com/layer-3-switch/ LLDP: Link Layer Discovery Protocol wifi authentication types WPA, WEP or WPA2, WPA3 (personal/Enterprise) 802.1x, EAP PPSK PSK Pre-shared key multi PSK PMK Pair-wise master key OKC Opportunistic key caching WIDS - wireless intrusion detection system EAP/802.1X NTP, NTP config, http://www.ntp.org/ ZTP: Zero touch provisioning is a switch feature that allows the devices to be provisioned and configured automatically, eliminating most of the manual labor involved with adding them to a network MDNS (multicast DNS) DNS Domain Name System translates Name to IP address Is a hierarchical and decentralized naming system For computers, services or other resources connected to internet or private network Related Devices: TBD Layer: L3 (Transport Layer thing) process hit a URL (say api.example.com) DNS lookup starts queries recursive resolver which can be ISP, wireless carrier, etc. recursive resolver redirects the query to root server root server keeps map of all the top level domain (e.g. .com, .net) with top level domain (TLD) server's address root server redirects the query for .com part to a TLD server TLD server keeps map of all the top level domains with domain's name server TLD server finds the domain's name server for example.com and redirects the query there domain's name server keeps map of all the subdomains with IP address request reaches to domain's DNS server with unsolved puzzle part (i.e. api.example.com ) domain's DNS server returns the IP address to recursive resolver server recursive resolver server shares the IP address with client/browser webpage appears query resolution happen in 10th of the seconds https://d1.awsstatic.com/Route53/how-route-53-routes-traffic.8d313c7da075c3c7303aaef32e89b5d0b7885e7c.png DOT1X dBm - decibels with milliwatt (mW) reference A decibel is a logarithmic unit that is a ratio of the power of the system to some reference 10 dBm (1mW) is 10 times powerful than 0 dBm 20 dBm (100mW) is 10 times powerful than 10 dBm RX (Receive) vs. TX (Transmit) Ionizing vs non-ionizing radiation 5G vs WiFi6 WiFi naming convention for past and current generation tech has been simplified Wi-Fi 6 means 802.11ax technology \u2013 the new generation of Wi-Fi, present in many new routers you'll buy from now on - but not many devices as yet. enhancement: at PHY layer, better traffic management, better multi-client mgnt Wi-Fi 5 means 802.11ac technology \u2013 effectively the current generation Wi-Fi 4 means 802.11n technology \u2013 many people will have networking gear based on 802.11n, but it was replaced by 802.11ac in many new routers from 2013 on. WiFi6 > 5G OEM original equipment manufacturer OUI organizationally unique identifier a 24-bit number that uniquely identifies a vendor, manufacturer, or other organization MAC physical address EUI Ref: https://www.intel.com/content/www/us/en/wireless-network/5g-technology/5g-vs-wifi.html https://www.cisco.com/c/m/en_us/solutions/enterprise-networks/802-11ax-solution/nb-06-5-things-WiFi6-5G-infograph-cte-en.html Delay vs jitter vs latency Latency: time to reach a packet to destination Delay: time wasted/spent before sending the packet Jitter: intermittent network issue/failure/package drop baud","title":"Keywords"},{"location":"Engineering/Network-Engineering/Keywords/#keywords","text":"Keywords - https://www.cloudflare.com/en-in/learning/dns/dns-records/ - Point-to-Point Protocol over Ethernet DNS Record Types A The record that holds the IP address of a domain CNAME Forwards one domain or subdomain to another domain, does NOT provide an IP address. AAAA Same as \u201cA\u201d, but for IPv6 addresses MX Directs emails to an email server TXT Lets an admin store text notes in the record NS Stores the name server for a DNS entry Etc","title":"Keywords"},{"location":"Engineering/Network-Engineering/Keywords/#-httpswwwcloudflarecomen-inlearningdnsdns-records","text":"NAT - Network Address Translation https://www.geeksforgeeks.org/network-address-translation-nat/ https://www.cisco.com/c/en/us/support/docs/ip/network-address-translation-nat/26704-nat-faq-00.html PAT - Port Address Translation This overloads NAT\u2019s feature https://www.geeksforgeeks.org/network-address-translation-nat/ Masquerade (TBD) MPLS, Broadband, LTE, Satellite (TBD) PPPoE","title":"- https://www.cloudflare.com/en-in/learning/dns/dns-records/"},{"location":"Engineering/Network-Engineering/Keywords/#-point-to-point-protocol-over-ethernet","text":"SSL Secure socket layer protocol L6 (layer 6 - presentation layer thing) 1995, at Netscape establish encrypted communications between web servers/clients encrypts the data received from L7 and passes encrypted data to L5 decrypts the data received from L5 and passes decrypted data to L7 leveraged by HTTPS protocol provides privacy and data integrity uses public-key exchange version 1, 2, 3, all deprecated TLS transport layer security protocol successor of SSL 1999, by IETF L6 (layer 6 - presentation layer thing) establish encrypted communications between web servers/clients encrypts the data received from L7 and passes encrypted data to L5 decrypts the data received from L5 and passes decrypted data to L7 leveraged by HTTPS protocol, can be used provides privacy and data integrity version 1.0, 1.1 are deprecated and 1.2 & 1.3 are available SSL vs TLS In short, a better version of SSL https://www.ssl2buy.com/wiki/ssl-vs-tls There is no SSL certificate and TLS certificates, they're just certificates which works for both the protocols both are X.509 digital certificates that helps authenticate the server and facilitate the handshake process TLS SNI: Server Name Indication, is an addition to the TLS encryption protocol that enables a client device to specify the domain name it is trying to reach in the first step of the TLS handshake, preventing common name mismatch errors. DSCP Differentiated Services Code Point Is a packet header value That can be used to request (for example) high priority or best effort delivery of the traffic Related Devices: TBD Layer: TBD DHCP Dynamic Hosting Configuration Protocol L2-L3 (Network Layer to Transport Layer) Is a NMP (N/W management protocol), helps automate the process of configuring devices on IP networks Thus assigning devices IP address so that IPs don't need to be manually assigned by an admin each time a device connects Thus allowing them to use network service like NAT, DNS and any communication protocols based on UDP or TCP DHCP is also responsible for the configuration of domain name server (DNS) and subnet masks, as well as default gateways Related Devices: Router/Gateway OSI Layer: 2-3 NAT Network Address Translation Is a method of remapping an IP address space into another by modifying the network address information in IP header of packets while they are in transit across a traffic routing device Means, it enables private IP networks that uses unregistered IP address to connect to internet by translating the unregistered (not globally unique) IP addresses in internal network into legal address before packets are forwarded to another network Related Devices: Router Layer: TBD PFE Packet Forwarding Engine (Juniper routers?) Related Devices: Router Layer: TBD SD-WAN Software-Driven Wide Area Network Related Devices: Router/Gateway Layer: TBD RPM Real-time Performance Monitoring (Switch?) Related Devices: Switch Layer: TBD TWAMP Two Way Active Measurement Protocol (Switch?) Related Devices: Switch Layer: TBD QoS Quality of service QoE Quality of experience AP, Router, Modem, Switch, Bridge, Hub Node, Hop Gateway, Catenet LAN WLAN WxLAN WAN VLAN VxLAN MAC Media access control IP IPSec protocol suite Port SSID BSSID Roaming Hash(), HMAC OSI application layer (7) presentation layer (6) session layer (5) data layer (4) transport layer (3) network layer (2) physical layer (1) RFC CNA - Captive N/W Assistance / Captive Portal WiFi Auth Mechanism LE , BLE (Bluetooth Low Energy), Beacon, vBLE SLE (Service Level Expectations), SLA (Service Level Agreements) Host .local files e.g. hostname.local RADIUS Remote Authentication Dial-In User Service NAS Network access server MAC Auth CoA Change of authorization RadSec RADIUS security Traceroute, ARP, Ping, TCP ping, TCP traceroute SPA Source protocol address TPA target protocol address Connection-oriented, Connectionless protocol TCP Transmission Control Protocol defines how to establish connection to exchange info how computers send packets of data to each other works on top of IP ensures reliable transmission of packets have a feedback/ack/validation mechanism which ensures if the packet has reached the destination or not thus is lossless transmission called as connection-oriented Related Device: OSI Layer: L3 (Transport Layer) UDP User Datagram Protocol defines how to establish connection to exchange info how computers send packets of data to each other works on top of IP ensures low-latency transmission of datagrams re-transmission of lost datagrams are not possible connection-less protocol ICMP Internet Control Message Protocol one of the protocol of TCP/IP suite generally used for network diagnostic or control purpose e.g. PING, Traceroute etc Packet Data, Datagram, Error-Message, Segment, IP datagram, IP fragmentation Ethernet Websocket Control Plane ACL/Policy Rogue, Neighbor, Honeypot Honeypot SSID Rogue AP, Rogue Clients Neighbour APs https://www.mist.com/documentation/rogue-neighbor-honeypot-aps/ Wifi spoof Ssid injection Xss (script in ssid name) DFS (dynamic frequency selection) WiFi bandwidth channels FCC, CE, Other certifications 2.4 GHz 5 GHz 802.11ac and all other RRM: Radio resource management SLE: Service level experience BGP: Border Gateway Protocol SSH VPN Tunnel Split Tunnel PPTP: Point-to-Point Tunneling Protocol used by an Internet service provider (ISP) to enable the operation of a virtual private network (VPN) over the Internet L2TP: Layer Two Tunneling Protocol an extension of the PPTP SSTP: Secure Socket Tunneling Protocol Layer Two: Devices e.g. Network interface cards, hubs, bridges, and switches OSI layers: layer 2 Layer Three?? Device e.g. Advanced Switch Combines the functionality of a switch and a router OSI layers: layer 2 and layer 3 Ref: https://documentation.meraki.com/MS/Layer_3_Switching/Layer_3_vs_Layer_2_Switching http://techgenix.com/layer-3-switch/ LLDP: Link Layer Discovery Protocol wifi authentication types WPA, WEP or WPA2, WPA3 (personal/Enterprise) 802.1x, EAP PPSK PSK Pre-shared key multi PSK PMK Pair-wise master key OKC Opportunistic key caching WIDS - wireless intrusion detection system EAP/802.1X NTP, NTP config, http://www.ntp.org/ ZTP: Zero touch provisioning is a switch feature that allows the devices to be provisioned and configured automatically, eliminating most of the manual labor involved with adding them to a network MDNS (multicast DNS) DNS Domain Name System translates Name to IP address Is a hierarchical and decentralized naming system For computers, services or other resources connected to internet or private network Related Devices: TBD Layer: L3 (Transport Layer thing) process hit a URL (say api.example.com) DNS lookup starts queries recursive resolver which can be ISP, wireless carrier, etc. recursive resolver redirects the query to root server root server keeps map of all the top level domain (e.g. .com, .net) with top level domain (TLD) server's address root server redirects the query for .com part to a TLD server TLD server keeps map of all the top level domains with domain's name server TLD server finds the domain's name server for example.com and redirects the query there domain's name server keeps map of all the subdomains with IP address request reaches to domain's DNS server with unsolved puzzle part (i.e. api.example.com ) domain's DNS server returns the IP address to recursive resolver server recursive resolver server shares the IP address with client/browser webpage appears query resolution happen in 10th of the seconds https://d1.awsstatic.com/Route53/how-route-53-routes-traffic.8d313c7da075c3c7303aaef32e89b5d0b7885e7c.png DOT1X dBm - decibels with milliwatt (mW) reference A decibel is a logarithmic unit that is a ratio of the power of the system to some reference 10 dBm (1mW) is 10 times powerful than 0 dBm 20 dBm (100mW) is 10 times powerful than 10 dBm RX (Receive) vs. TX (Transmit) Ionizing vs non-ionizing radiation 5G vs WiFi6 WiFi naming convention for past and current generation tech has been simplified Wi-Fi 6 means 802.11ax technology \u2013 the new generation of Wi-Fi, present in many new routers you'll buy from now on - but not many devices as yet. enhancement: at PHY layer, better traffic management, better multi-client mgnt Wi-Fi 5 means 802.11ac technology \u2013 effectively the current generation Wi-Fi 4 means 802.11n technology \u2013 many people will have networking gear based on 802.11n, but it was replaced by 802.11ac in many new routers from 2013 on. WiFi6 > 5G OEM original equipment manufacturer OUI organizationally unique identifier a 24-bit number that uniquely identifies a vendor, manufacturer, or other organization MAC physical address EUI Ref: https://www.intel.com/content/www/us/en/wireless-network/5g-technology/5g-vs-wifi.html https://www.cisco.com/c/m/en_us/solutions/enterprise-networks/802-11ax-solution/nb-06-5-things-WiFi6-5G-infograph-cte-en.html Delay vs jitter vs latency Latency: time to reach a packet to destination Delay: time wasted/spent before sending the packet Jitter: intermittent network issue/failure/package drop baud","title":"- Point-to-Point Protocol over Ethernet"},{"location":"Engineering/Network-Engineering/MAC/","text":"MAC # MAC - The IEEE manages MAC addresses How many combinations / Sufficient? OUI Are MAC addresses really unique? MAC Randomization List of MAC series vs Vendors Read More Media Access Control - The IEEE manages MAC addresses # How many combinations / Sufficient? # 12 digits (0-F i.e. 16 possible chars) 12 hex digits 6 bytes 46 bits 16^12=2.81E14 281 trillion 281, 474, 980, 000, 000 two hundred and eighty-one trillion, four hundred and seventy-four billion, nine hundred and eighty million 40k MAC per person on earth OUI # Organization Unique Identifier first 6 digits 3 bytes 24 bits identifies the vendor/manufacturer purchased from IEEE registration authority Are MAC addresses really unique? # Or are there any (maybe cloned?) network interface cards that have the same MAC address as another NIC? What is the probability of having two identical MAC addresses within one network? The hardware identification addresses that the IEEE distributes are unique. That makes the probability of matching MAC addresses zero. On the other hand, some hardware MAC addresses are programmable, which makes them spoofable. This means that it is possible for two machines in the same network to have the same MAC address. MAC Randomization # Apple release with iOS 8 in 2014 Android followed the same and introduced with android 8, in 2017; made it default ON with android 10+ Earlier it was used for probing purpose, later adapted it for association as well Random MAC could be identified by looking at the 2nd digit of the MAC, it should be 2, 6, a, e Affects N/W vendors and service providers Ref: https://www.mist.com/get-to-know-mac-address-randomization-in-2020/ https://securityaffairs.co/wordpress/57076/uncategorized/mac-address-randomization-flaws.html https://hsc.com/Resources/Blog/Effect-of-Apples-MAC-Randomization-Feature-on-Enterprises https://globalreachtech.com/blog-mac-randomisation-apple/ List of MAC series vs Vendors # https://regauth.standards.ieee.org/standards-ra-web/pub/view.html#registries https://devtools360.com/en/macaddress/vendorMacs.xml https://gist.github.com/aallan/b4bb86db86079509e6159810ae9bd3e4 Read More # IEEE Standards IEEE MAC Addresss","title":"MAC"},{"location":"Engineering/Network-Engineering/MAC/#mac","text":"MAC - The IEEE manages MAC addresses How many combinations / Sufficient? OUI Are MAC addresses really unique? MAC Randomization List of MAC series vs Vendors Read More Media Access Control","title":"MAC"},{"location":"Engineering/Network-Engineering/MAC/#-the-ieee-manages-mac-addresses","text":"","title":"- The IEEE manages MAC addresses"},{"location":"Engineering/Network-Engineering/MAC/#how-many-combinations-sufficient","text":"12 digits (0-F i.e. 16 possible chars) 12 hex digits 6 bytes 46 bits 16^12=2.81E14 281 trillion 281, 474, 980, 000, 000 two hundred and eighty-one trillion, four hundred and seventy-four billion, nine hundred and eighty million 40k MAC per person on earth","title":"How many combinations / Sufficient?"},{"location":"Engineering/Network-Engineering/MAC/#oui","text":"Organization Unique Identifier first 6 digits 3 bytes 24 bits identifies the vendor/manufacturer purchased from IEEE registration authority","title":"OUI"},{"location":"Engineering/Network-Engineering/MAC/#are-mac-addresses-really-unique","text":"Or are there any (maybe cloned?) network interface cards that have the same MAC address as another NIC? What is the probability of having two identical MAC addresses within one network? The hardware identification addresses that the IEEE distributes are unique. That makes the probability of matching MAC addresses zero. On the other hand, some hardware MAC addresses are programmable, which makes them spoofable. This means that it is possible for two machines in the same network to have the same MAC address.","title":"Are MAC addresses really unique?"},{"location":"Engineering/Network-Engineering/MAC/#mac-randomization","text":"Apple release with iOS 8 in 2014 Android followed the same and introduced with android 8, in 2017; made it default ON with android 10+ Earlier it was used for probing purpose, later adapted it for association as well Random MAC could be identified by looking at the 2nd digit of the MAC, it should be 2, 6, a, e Affects N/W vendors and service providers Ref: https://www.mist.com/get-to-know-mac-address-randomization-in-2020/ https://securityaffairs.co/wordpress/57076/uncategorized/mac-address-randomization-flaws.html https://hsc.com/Resources/Blog/Effect-of-Apples-MAC-Randomization-Feature-on-Enterprises https://globalreachtech.com/blog-mac-randomisation-apple/","title":"MAC Randomization"},{"location":"Engineering/Network-Engineering/MAC/#list-of-mac-series-vs-vendors","text":"https://regauth.standards.ieee.org/standards-ra-web/pub/view.html#registries https://devtools360.com/en/macaddress/vendorMacs.xml https://gist.github.com/aallan/b4bb86db86079509e6159810ae9bd3e4","title":"List of MAC series vs Vendors"},{"location":"Engineering/Network-Engineering/MAC/#read-more","text":"IEEE Standards IEEE MAC Addresss","title":"Read More"},{"location":"Engineering/Network-Engineering/Tools/","text":"Networking Tools # Networking Tools Netstat Telnet Dig Ping Arp Traceroute Nslookup Nmap Nmcli Ngrok (tunelling) Netstat # https://www.geeksforgeeks.org/netstat-command-linux/ Telnet # bidirectional interactive text-oriented communication facility using a virtual terminal connection Dig # Ping # Arp # Traceroute # Nslookup # Nmap # Nmcli # Ngrok (tunelling) #","title":"Networking Tools"},{"location":"Engineering/Network-Engineering/Tools/#networking-tools","text":"Networking Tools Netstat Telnet Dig Ping Arp Traceroute Nslookup Nmap Nmcli Ngrok (tunelling)","title":"Networking Tools"},{"location":"Engineering/Network-Engineering/Tools/#netstat","text":"https://www.geeksforgeeks.org/netstat-command-linux/","title":"Netstat"},{"location":"Engineering/Network-Engineering/Tools/#telnet","text":"bidirectional interactive text-oriented communication facility using a virtual terminal connection","title":"Telnet"},{"location":"Engineering/Network-Engineering/Tools/#dig","text":"","title":"Dig"},{"location":"Engineering/Network-Engineering/Tools/#ping","text":"","title":"Ping"},{"location":"Engineering/Network-Engineering/Tools/#arp","text":"","title":"Arp"},{"location":"Engineering/Network-Engineering/Tools/#traceroute","text":"","title":"Traceroute"},{"location":"Engineering/Network-Engineering/Tools/#nslookup","text":"","title":"Nslookup"},{"location":"Engineering/Network-Engineering/Tools/#nmap","text":"","title":"Nmap"},{"location":"Engineering/Network-Engineering/Tools/#nmcli","text":"","title":"Nmcli"},{"location":"Engineering/Network-Engineering/Tools/#ngrok-tunelling","text":"","title":"Ngrok (tunelling)"},{"location":"Engineering/Network-Engineering/OpenConfig/openconfig/","text":"OpenConfig # OpenConfig OpenConfig Access Point OpenConfig Access Point # joined-aps/ joined-ap/ provision-aps/ provision-aps/ access-points/ access-point/ Ref: http://ops.openconfig.net/branches/models/master/docs/openconfig-access-points.html","title":"OpenConfig"},{"location":"Engineering/Network-Engineering/OpenConfig/openconfig/#openconfig","text":"OpenConfig OpenConfig Access Point","title":"OpenConfig"},{"location":"Engineering/Network-Engineering/OpenConfig/openconfig/#openconfig-access-point","text":"joined-aps/ joined-ap/ provision-aps/ provision-aps/ access-points/ access-point/ Ref: http://ops.openconfig.net/branches/models/master/docs/openconfig-access-points.html","title":"OpenConfig Access Point"},{"location":"Engineering/Network-Engineering/Wireless/wifi_basics_1/","text":"Wireless Basics - 1 # Wireless Basics - 1 IEEE & Wi-Fi Alliance IEEE Wi-Fi Alliance Wi-Fi Frequencies 2.4 GHz & 5 GHz 2.4 GHz Band 5 GHz Band Wi-Fi History Major 802.11 Amendments 2.4 GHz Band Channels Wi-Fi 2.4 Ghz and Interference Wi-Fi Adjacent Channel Interference Half Duplex Wi-Fi Co-channel interference 2.4 GHz Channel Re-Use Wi-Fi 5 Ghz Wi-Fi 5 Ghz Channels Wi-Fi DFS Wi-Fi Channel Bonding 5 GHz Channel Re-Use Wi-Fi Signal Strength BSSID, SSID and ESSIDs Other Wireless Technologies Bluetooth Low Energy (BLE) Wi-Fi NICs vs Ethernet SNR and Data Rates SNR Data Rate MCS Network Packets vs Frame Wi-Fi Frame Types and Timing Airtime Arbitration Process Wi-Fi Roaming Wi-Fi Security Roaming Wi-Fi Design Determine requirements Client channel support Further Define Metrics Wi-Fi Ekahau Design Sources: https://courses.mist.com/dashboard https://www.mist.com/documentation/wi-fi-basics-1/ IEEE & Wi-Fi Alliance # IEEE # Institute of Electrical & Electronics Engineers creates standard like: 802.15.1 (BlueTooth) 802.15.4 (Zigbee) IEEE 1394 (FireWire) 802.3 (Ethernet) 802.11 (Wi-Fi) Wi-Fi Alliance # Composed of major players in the wireless industry/space Apple Comcast Sony Motorola Intel Qualcomm T-Mobile Provides branding for Wi-Fi \"Wi-Fi\" WPA/WPA2/WPA3 802.11i WMM Wi-Fi 6 Ensures interoperability between diff vendors Wi-Fi Frequencies # 2.4 GHz & 5 GHz # unlicensed spectrum 1 2 .-. .-. .-. .-. .-. .-. .-. .-. .-. '-' '-' '-' '-' '-' '-' '-' '-' '-' 2.4 GHz Band # low freq punch stuffs tend to attenuate longer wavelength 4.9 inch long more range 300 ft ? more crowded more non Wi-Fi interference 802.11b/g/n/ax 5 GHz Band # higher freq shorter wavelength 2.5 inch long less range 90 ft ? less crowded less non Wi-Fi interference 802.11a/n/ac/ax Q. Can a Wi-Fi adapter support multiple bands? Yes, depends on capabilities. Its called Dual-band. Wi-Fi History # Major 802.11 Amendments # Q. Band vs Channel? Within these (2.4/5) Wi-Fi frequency bands, we have smaller bands which are referred to as Wi-Fi channels. A Wi-Fi channel is the medium through which our wireless networks can send and receive data. For routers made in the U.S., the 2.4 GHz band has 11 channels and the 5 GHz band has 45 channels. Q. Why should I care what Wi-Fi channel I'm on? The reason that certain channels aren't the best choice to use is because they have interference. There are a couple different ways this interference is caused: Co-Channel interference results when there are numerous devices all competing for time to talk on the same channel. Adjacent-Channel interference occurs when devices from overlapping channels are trying to talk over each other. Channels that have interference from other devices are considered to be 'crowded'. The time it takes to transmit data is increased and you are left waiting for your Internet request to be made. The channels with the most interference are those that overlap with each other. To further explain channel overlapping, let's look at the 2.4 GHz band, where each channel is allotted 20 MHz and separated by 5 MHz. Considering the 2.4 GHz band is only 100 MHz wide, the 11 channels of 20 MHz overlap with one another. This is what causes the interference on your network and and a lag in your Wi-Fi's performance. Certain channels yield better Wi-Fi performance than others because they are non-overlapping. Yes, there are some channels in the 2.4 GHz spectrum that don't overlap with the other channels. These are the channels you ought to look for, especially if experiencing Wi-Fi problems: Channels 1, 6, and 11. Read more: 1. https://www.minim.com/blog/wifi-channels-explained 1. https://www.electronics-notes.com/articles/connectivity/wifi-ieee-802-11/channels-frequencies-bands-bandwidth.php Note: There are also 24 non-overlapping channels in the 5 GHz band spectrum. 2.4 GHz Band Channels # 20 MHz wide 11 channels Q. Different Country vs Channels? Possible? Yes. Europe has 12,13 as well. Japan has 14 as well. Wi-Fi 2.4 Ghz and Interference # Wi-Fi Adjacent Channel Interference # STAs on overlapping channels will corrupt each other's transmission. Half Duplex Wi-Fi # Only one device can transmit on a channel at a time. Ethernet cables are made up of multiple twisted pair of copper, they MUX. They can talk & listen at the same time. Co-channel interference # Slow devices on the channel consume more airtime. 2.4 GHz Channel Re-Use # But avoiding interference in 2.4GHz is hard due to high range. NOTE: Mist use channels 1,6,11 world-wide. (btw, they support all) Wi-Fi 5 Ghz # Wi-Fi 5 Ghz Channels # lot more channels thus lot more throughput Wi-Fi DFS # Dynamic frequency selection If detected radar, move to other channel. Announce to move other APs as well. Wi-Fi Channel Bonding # haha typo. haha typo again. 5 GHz Channel Re-Use # Channel reuse is much easier in 5 GHz band. Wi-Fi Signal Strength # Unit/repr: dB and dBm for absolute values. dB quantifies the ratio between two values, whereas dBm expresses the absolute power level. dBm is an absolute unit, whereas dB is a dimensionless unit. dBm is always relative to 1mW, while dB is expressed in watts and can be relative to other powers. the unit is logarithmic & not linear, so lets see 3 & 10 rule for simiplicty: Isn't there something in positive scale? Yes, mW . :D LOL! see the figures in mW. 1 2 3 4 iwconfig # for streaming results watch -n1 iwconfig BSSID, SSID and ESSIDs # Other Wireless Technologies # Bluetooth Low Energy (BLE) # Wi-Fi NICs vs Ethernet # SNR and Data Rates # SNR # How much signal above the background noise. Unit: dB Data Rate # The same \"Maximum Signaling Rate\". Defined per standards 802.11xyz. Unit: Mbps Good the SNR, Can choose/set higher Data rates. Q. Signaling Rate vs Throughput? TBD MCS # Modulation and Coding scheme. Ref: http://mcsindex.com/ https://www.wlanpros.com/mcs-index-charts/ Network Packets vs Frame # TBD Wi-Fi Frame Types and Timing # STA (Station): In IEEE 802.11 (Wi-Fi) terminology, a station (abbreviated as STA) is a device that has the capability to use the 802.11 protocol. For example, a station may be a laptop, a desktop PC, PDA, access point or Wi-Fi phone. Airtime Arbitration Process # How wireless devices decides, who's going to talk next? Wi-Fi Roaming # Wi-Fi Security # Ethernet is a bounded medium, inheritently & physically secure. What about security of unbounded Wi-Fi? A Wi-Fi adapter in monitor mode can eavesdrop.. can capture all the packets nearby.. hmm.. What Mist supports today: WPA-2/PSK with passphrase WPA-2/EAP (802.1X) Open Access WPA-2/PSK with multiple passphrases WPA-PSK/TKIP WPA2-PSK/TKIP WEP Multi-mode/PSK with passphrase Multi-mode/EAP (802.1X) OWE Transition OWE Opportunistic Wireless Encryption MAC address authentication by RADIUS lookup Roaming # Every client device has its own green diamond. :D Some can roam, some are not designed to roam. Wi-Fi Design # How to design a wireless network? Probably for a customer.. :p Determine requirements # Client channel support # Some devices might not support some channels, so plan accordingly. Ref: https://clients.mikealbano.com/ (Mike - Our OpenConfig guy, from Wireless N/W Team, Google) ;) Further Define Metrics # Wi-Fi Ekahau Design # Software to design a Wireless network.","title":"Wireless Basics - 1"},{"location":"Engineering/Network-Engineering/Wireless/wifi_basics_1/#wireless-basics-1","text":"Wireless Basics - 1 IEEE & Wi-Fi Alliance IEEE Wi-Fi Alliance Wi-Fi Frequencies 2.4 GHz & 5 GHz 2.4 GHz Band 5 GHz Band Wi-Fi History Major 802.11 Amendments 2.4 GHz Band Channels Wi-Fi 2.4 Ghz and Interference Wi-Fi Adjacent Channel Interference Half Duplex Wi-Fi Co-channel interference 2.4 GHz Channel Re-Use Wi-Fi 5 Ghz Wi-Fi 5 Ghz Channels Wi-Fi DFS Wi-Fi Channel Bonding 5 GHz Channel Re-Use Wi-Fi Signal Strength BSSID, SSID and ESSIDs Other Wireless Technologies Bluetooth Low Energy (BLE) Wi-Fi NICs vs Ethernet SNR and Data Rates SNR Data Rate MCS Network Packets vs Frame Wi-Fi Frame Types and Timing Airtime Arbitration Process Wi-Fi Roaming Wi-Fi Security Roaming Wi-Fi Design Determine requirements Client channel support Further Define Metrics Wi-Fi Ekahau Design Sources: https://courses.mist.com/dashboard https://www.mist.com/documentation/wi-fi-basics-1/","title":"Wireless Basics - 1"},{"location":"Engineering/Network-Engineering/Wireless/wifi_basics_1/#ieee-wi-fi-alliance","text":"","title":"IEEE &amp; Wi-Fi Alliance"},{"location":"Engineering/Network-Engineering/Wireless/wifi_basics_1/#ieee","text":"Institute of Electrical & Electronics Engineers creates standard like: 802.15.1 (BlueTooth) 802.15.4 (Zigbee) IEEE 1394 (FireWire) 802.3 (Ethernet) 802.11 (Wi-Fi)","title":"IEEE"},{"location":"Engineering/Network-Engineering/Wireless/wifi_basics_1/#wi-fi-alliance","text":"Composed of major players in the wireless industry/space Apple Comcast Sony Motorola Intel Qualcomm T-Mobile Provides branding for Wi-Fi \"Wi-Fi\" WPA/WPA2/WPA3 802.11i WMM Wi-Fi 6 Ensures interoperability between diff vendors","title":"Wi-Fi Alliance"},{"location":"Engineering/Network-Engineering/Wireless/wifi_basics_1/#wi-fi-frequencies","text":"","title":"Wi-Fi Frequencies"},{"location":"Engineering/Network-Engineering/Wireless/wifi_basics_1/#24-ghz-5-ghz","text":"unlicensed spectrum 1 2 .-. .-. .-. .-. .-. .-. .-. .-. .-. '-' '-' '-' '-' '-' '-' '-' '-' '-'","title":"2.4 GHz &amp; 5 GHz"},{"location":"Engineering/Network-Engineering/Wireless/wifi_basics_1/#24-ghz-band","text":"low freq punch stuffs tend to attenuate longer wavelength 4.9 inch long more range 300 ft ? more crowded more non Wi-Fi interference 802.11b/g/n/ax","title":"2.4 GHz Band"},{"location":"Engineering/Network-Engineering/Wireless/wifi_basics_1/#5-ghz-band","text":"higher freq shorter wavelength 2.5 inch long less range 90 ft ? less crowded less non Wi-Fi interference 802.11a/n/ac/ax Q. Can a Wi-Fi adapter support multiple bands? Yes, depends on capabilities. Its called Dual-band.","title":"5 GHz Band"},{"location":"Engineering/Network-Engineering/Wireless/wifi_basics_1/#wi-fi-history","text":"","title":"Wi-Fi History"},{"location":"Engineering/Network-Engineering/Wireless/wifi_basics_1/#major-80211-amendments","text":"Q. Band vs Channel? Within these (2.4/5) Wi-Fi frequency bands, we have smaller bands which are referred to as Wi-Fi channels. A Wi-Fi channel is the medium through which our wireless networks can send and receive data. For routers made in the U.S., the 2.4 GHz band has 11 channels and the 5 GHz band has 45 channels. Q. Why should I care what Wi-Fi channel I'm on? The reason that certain channels aren't the best choice to use is because they have interference. There are a couple different ways this interference is caused: Co-Channel interference results when there are numerous devices all competing for time to talk on the same channel. Adjacent-Channel interference occurs when devices from overlapping channels are trying to talk over each other. Channels that have interference from other devices are considered to be 'crowded'. The time it takes to transmit data is increased and you are left waiting for your Internet request to be made. The channels with the most interference are those that overlap with each other. To further explain channel overlapping, let's look at the 2.4 GHz band, where each channel is allotted 20 MHz and separated by 5 MHz. Considering the 2.4 GHz band is only 100 MHz wide, the 11 channels of 20 MHz overlap with one another. This is what causes the interference on your network and and a lag in your Wi-Fi's performance. Certain channels yield better Wi-Fi performance than others because they are non-overlapping. Yes, there are some channels in the 2.4 GHz spectrum that don't overlap with the other channels. These are the channels you ought to look for, especially if experiencing Wi-Fi problems: Channels 1, 6, and 11. Read more: 1. https://www.minim.com/blog/wifi-channels-explained 1. https://www.electronics-notes.com/articles/connectivity/wifi-ieee-802-11/channels-frequencies-bands-bandwidth.php Note: There are also 24 non-overlapping channels in the 5 GHz band spectrum.","title":"Major 802.11 Amendments"},{"location":"Engineering/Network-Engineering/Wireless/wifi_basics_1/#24-ghz-band-channels","text":"20 MHz wide 11 channels Q. Different Country vs Channels? Possible? Yes. Europe has 12,13 as well. Japan has 14 as well.","title":"2.4 GHz Band Channels"},{"location":"Engineering/Network-Engineering/Wireless/wifi_basics_1/#wi-fi-24-ghz-and-interference","text":"","title":"Wi-Fi 2.4 Ghz and Interference"},{"location":"Engineering/Network-Engineering/Wireless/wifi_basics_1/#wi-fi-adjacent-channel-interference","text":"STAs on overlapping channels will corrupt each other's transmission.","title":"Wi-Fi Adjacent Channel Interference"},{"location":"Engineering/Network-Engineering/Wireless/wifi_basics_1/#half-duplex-wi-fi","text":"Only one device can transmit on a channel at a time. Ethernet cables are made up of multiple twisted pair of copper, they MUX. They can talk & listen at the same time.","title":"Half Duplex Wi-Fi"},{"location":"Engineering/Network-Engineering/Wireless/wifi_basics_1/#co-channel-interference","text":"Slow devices on the channel consume more airtime.","title":"Co-channel interference"},{"location":"Engineering/Network-Engineering/Wireless/wifi_basics_1/#24-ghz-channel-re-use","text":"But avoiding interference in 2.4GHz is hard due to high range. NOTE: Mist use channels 1,6,11 world-wide. (btw, they support all)","title":"2.4 GHz Channel Re-Use"},{"location":"Engineering/Network-Engineering/Wireless/wifi_basics_1/#wi-fi-5-ghz","text":"","title":"Wi-Fi 5 Ghz"},{"location":"Engineering/Network-Engineering/Wireless/wifi_basics_1/#wi-fi-5-ghz-channels","text":"lot more channels thus lot more throughput","title":"Wi-Fi 5 Ghz Channels"},{"location":"Engineering/Network-Engineering/Wireless/wifi_basics_1/#wi-fi-dfs","text":"Dynamic frequency selection If detected radar, move to other channel. Announce to move other APs as well.","title":"Wi-Fi DFS"},{"location":"Engineering/Network-Engineering/Wireless/wifi_basics_1/#wi-fi-channel-bonding","text":"haha typo. haha typo again.","title":"Wi-Fi Channel Bonding"},{"location":"Engineering/Network-Engineering/Wireless/wifi_basics_1/#5-ghz-channel-re-use","text":"Channel reuse is much easier in 5 GHz band.","title":"5 GHz Channel Re-Use"},{"location":"Engineering/Network-Engineering/Wireless/wifi_basics_1/#wi-fi-signal-strength","text":"Unit/repr: dB and dBm for absolute values. dB quantifies the ratio between two values, whereas dBm expresses the absolute power level. dBm is an absolute unit, whereas dB is a dimensionless unit. dBm is always relative to 1mW, while dB is expressed in watts and can be relative to other powers. the unit is logarithmic & not linear, so lets see 3 & 10 rule for simiplicty: Isn't there something in positive scale? Yes, mW . :D LOL! see the figures in mW. 1 2 3 4 iwconfig # for streaming results watch -n1 iwconfig","title":"Wi-Fi Signal Strength"},{"location":"Engineering/Network-Engineering/Wireless/wifi_basics_1/#bssid-ssid-and-essids","text":"","title":"BSSID, SSID and ESSIDs"},{"location":"Engineering/Network-Engineering/Wireless/wifi_basics_1/#other-wireless-technologies","text":"","title":"Other Wireless Technologies"},{"location":"Engineering/Network-Engineering/Wireless/wifi_basics_1/#bluetooth-low-energy-ble","text":"","title":"Bluetooth Low Energy (BLE)"},{"location":"Engineering/Network-Engineering/Wireless/wifi_basics_1/#wi-fi-nics-vs-ethernet","text":"","title":"Wi-Fi NICs vs Ethernet"},{"location":"Engineering/Network-Engineering/Wireless/wifi_basics_1/#snr-and-data-rates","text":"","title":"SNR and Data Rates"},{"location":"Engineering/Network-Engineering/Wireless/wifi_basics_1/#snr","text":"How much signal above the background noise. Unit: dB","title":"SNR"},{"location":"Engineering/Network-Engineering/Wireless/wifi_basics_1/#data-rate","text":"The same \"Maximum Signaling Rate\". Defined per standards 802.11xyz. Unit: Mbps Good the SNR, Can choose/set higher Data rates. Q. Signaling Rate vs Throughput? TBD","title":"Data Rate"},{"location":"Engineering/Network-Engineering/Wireless/wifi_basics_1/#mcs","text":"Modulation and Coding scheme. Ref: http://mcsindex.com/ https://www.wlanpros.com/mcs-index-charts/","title":"MCS"},{"location":"Engineering/Network-Engineering/Wireless/wifi_basics_1/#network-packets-vs-frame","text":"TBD","title":"Network Packets vs Frame"},{"location":"Engineering/Network-Engineering/Wireless/wifi_basics_1/#wi-fi-frame-types-and-timing","text":"STA (Station): In IEEE 802.11 (Wi-Fi) terminology, a station (abbreviated as STA) is a device that has the capability to use the 802.11 protocol. For example, a station may be a laptop, a desktop PC, PDA, access point or Wi-Fi phone.","title":"Wi-Fi Frame Types and Timing"},{"location":"Engineering/Network-Engineering/Wireless/wifi_basics_1/#airtime-arbitration-process","text":"How wireless devices decides, who's going to talk next?","title":"Airtime Arbitration Process"},{"location":"Engineering/Network-Engineering/Wireless/wifi_basics_1/#wi-fi-roaming","text":"","title":"Wi-Fi Roaming"},{"location":"Engineering/Network-Engineering/Wireless/wifi_basics_1/#wi-fi-security","text":"Ethernet is a bounded medium, inheritently & physically secure. What about security of unbounded Wi-Fi? A Wi-Fi adapter in monitor mode can eavesdrop.. can capture all the packets nearby.. hmm.. What Mist supports today: WPA-2/PSK with passphrase WPA-2/EAP (802.1X) Open Access WPA-2/PSK with multiple passphrases WPA-PSK/TKIP WPA2-PSK/TKIP WEP Multi-mode/PSK with passphrase Multi-mode/EAP (802.1X) OWE Transition OWE Opportunistic Wireless Encryption MAC address authentication by RADIUS lookup","title":"Wi-Fi Security"},{"location":"Engineering/Network-Engineering/Wireless/wifi_basics_1/#roaming","text":"Every client device has its own green diamond. :D Some can roam, some are not designed to roam.","title":"Roaming"},{"location":"Engineering/Network-Engineering/Wireless/wifi_basics_1/#wi-fi-design","text":"How to design a wireless network? Probably for a customer.. :p","title":"Wi-Fi Design"},{"location":"Engineering/Network-Engineering/Wireless/wifi_basics_1/#determine-requirements","text":"","title":"Determine requirements"},{"location":"Engineering/Network-Engineering/Wireless/wifi_basics_1/#client-channel-support","text":"Some devices might not support some channels, so plan accordingly. Ref: https://clients.mikealbano.com/ (Mike - Our OpenConfig guy, from Wireless N/W Team, Google) ;)","title":"Client channel support"},{"location":"Engineering/Network-Engineering/Wireless/wifi_basics_1/#further-define-metrics","text":"","title":"Further Define Metrics"},{"location":"Engineering/Network-Engineering/Wireless/wifi_basics_1/#wi-fi-ekahau-design","text":"Software to design a Wireless network.","title":"Wi-Fi Ekahau Design"},{"location":"Engineering/Network-Engineering/Wireless/wifi_basics_2/","text":"Wireless Basics - 2 # Wireless Basics - 2 Physical Carrier Sense Virtual Carrier Sense Duty Cycle and Spectrum Air-time Usage Multi Pre-shared Keys Interesting Site Survey SLEs Sources: https://courses.mist.com/dashboard https://www.mist.com/documentation/wi-fi-basics-1/ Physical Carrier Sense # Virtual Carrier Sense # Duty Cycle and Spectrum # Air-time Usage # Multi Pre-shared Keys # Interesting Site Survey # SLEs #","title":"Wireless Basics - 2"},{"location":"Engineering/Network-Engineering/Wireless/wifi_basics_2/#wireless-basics-2","text":"Wireless Basics - 2 Physical Carrier Sense Virtual Carrier Sense Duty Cycle and Spectrum Air-time Usage Multi Pre-shared Keys Interesting Site Survey SLEs Sources: https://courses.mist.com/dashboard https://www.mist.com/documentation/wi-fi-basics-1/","title":"Wireless Basics - 2"},{"location":"Engineering/Network-Engineering/Wireless/wifi_basics_2/#physical-carrier-sense","text":"","title":"Physical Carrier Sense"},{"location":"Engineering/Network-Engineering/Wireless/wifi_basics_2/#virtual-carrier-sense","text":"","title":"Virtual Carrier Sense"},{"location":"Engineering/Network-Engineering/Wireless/wifi_basics_2/#duty-cycle-and-spectrum","text":"","title":"Duty Cycle and Spectrum"},{"location":"Engineering/Network-Engineering/Wireless/wifi_basics_2/#air-time-usage","text":"","title":"Air-time Usage"},{"location":"Engineering/Network-Engineering/Wireless/wifi_basics_2/#multi-pre-shared-keys","text":"","title":"Multi Pre-shared Keys"},{"location":"Engineering/Network-Engineering/Wireless/wifi_basics_2/#interesting-site-survey","text":"","title":"Interesting Site Survey"},{"location":"Engineering/Network-Engineering/Wireless/wifi_basics_2/#sles","text":"","title":"SLEs"},{"location":"Engineering/Network-Engineering/Wireless/wifi_basics_3/","text":"Wireless Basics - 3 # Wireless Basics - 3 Data Rates Modulation SNR OFDM Mimo Controllers Mist Config Templates Mist RRM Data Rates # Modulation # SNR # OFDM # Mimo # Controllers # Mist Config Templates # Mist RRM #","title":"Wireless Basics - 3"},{"location":"Engineering/Network-Engineering/Wireless/wifi_basics_3/#wireless-basics-3","text":"Wireless Basics - 3 Data Rates Modulation SNR OFDM Mimo Controllers Mist Config Templates Mist RRM","title":"Wireless Basics - 3"},{"location":"Engineering/Network-Engineering/Wireless/wifi_basics_3/#data-rates","text":"","title":"Data Rates"},{"location":"Engineering/Network-Engineering/Wireless/wifi_basics_3/#modulation","text":"","title":"Modulation"},{"location":"Engineering/Network-Engineering/Wireless/wifi_basics_3/#snr","text":"","title":"SNR"},{"location":"Engineering/Network-Engineering/Wireless/wifi_basics_3/#ofdm","text":"","title":"OFDM"},{"location":"Engineering/Network-Engineering/Wireless/wifi_basics_3/#mimo","text":"","title":"Mimo"},{"location":"Engineering/Network-Engineering/Wireless/wifi_basics_3/#controllers","text":"","title":"Controllers"},{"location":"Engineering/Network-Engineering/Wireless/wifi_basics_3/#mist-config-templates","text":"","title":"Mist Config Templates"},{"location":"Engineering/Network-Engineering/Wireless/wifi_basics_3/#mist-rrm","text":"","title":"Mist RRM"},{"location":"Engineering/Software-Engineering/android/","text":"Android # Android Debug adb Issue: no permission Root Magisk Debug # adb # Issue: no permission # When i try: 1 adb devices i get the result: 1 2 List of devices attached ???????????? no permissions Then I tried to restart the Adb server 1 sudo adb kill-server and then 1 sudo adb start-server then connected the device, turn Debugging on and type 1 adb devices Still any issue? then follow this Root # Magisk # Source: - https://www.didgeridoohan.com/magisk if failing to flash .zip (due to signature verification) --> uncheck sign verification before flash","title":"Android"},{"location":"Engineering/Software-Engineering/android/#android","text":"Android Debug adb Issue: no permission Root Magisk","title":"Android"},{"location":"Engineering/Software-Engineering/android/#debug","text":"","title":"Debug"},{"location":"Engineering/Software-Engineering/android/#adb","text":"","title":"adb"},{"location":"Engineering/Software-Engineering/android/#issue-no-permission","text":"When i try: 1 adb devices i get the result: 1 2 List of devices attached ???????????? no permissions Then I tried to restart the Adb server 1 sudo adb kill-server and then 1 sudo adb start-server then connected the device, turn Debugging on and type 1 adb devices Still any issue? then follow this","title":"Issue: no permission"},{"location":"Engineering/Software-Engineering/android/#root","text":"","title":"Root"},{"location":"Engineering/Software-Engineering/android/#magisk","text":"Source: - https://www.didgeridoohan.com/magisk if failing to flash .zip (due to signature verification) --> uncheck sign verification before flash","title":"Magisk"},{"location":"Engineering/Software-Engineering/blockchain/","text":"Blockchain # Blockchain Introduction Trade-offs Terminologies Components Block Features Applications Facts Frameworks Workflow FAQ Multichain Installation Getting Started Nodes On Different Servers Node 1 Initial Setup on Server 1 Node 2 Connection with Node 1 on Server 2 Back on Server 1 Back on Server 2 Delete a Chain Nodes On Same Server Node 1 Initial Setup Node 2 Connection with Node 1 Introduction # Trade-offs # Public vs Private Security vs Performance Terminologies # Genesis Consensus Proof-of-Work Proof-of-Stake Proof of Byzantine Fault Tollerance Black Node/Block Orphan Block (Purple) Immutable distributed Ledger Double Spending Node Any computer that connects to the Bitcoin network is called a node every mining computer, every computer that has a bitcoin wallet and by special sites that give blockchain monitor services Full Node Nodes that fully verify all of the rules of Bitcoin Full nodes download every block and transaction and check them against Bitcoin's consensus rules Merkle Tree Hash of transaction in a block Keys Source Private Key (64): Assiged to node on joining network Public Key (34): same as above but can be publiced, same as bitcoin QR code balances in wallet are fetched from bitcoin address, which is public key of the wallet Components # Transactions commits changes to the blockchain contains signature of owner of address many transactions inside a block Blocks conatins an ordered bunch of transactions timestamps the transactions, are immutable each block references the previous block Block # Hash Data Previous Hash Features # Encryption of data Proof-of-work History Decentralized Data Trust in Data No Intermediaries Network Type Public Private Authentication peers signs the transaction with their private key Applications # Banking & Money Transfer Healthcare All types of document management Land / Real State Owner's Doc Transfer Deeds Land History Certificates Birth Caste Marriage Death Contracts Governance All the govt. data Voting Voter Registration Voter Identification Electronic Voting Cloud Storage Digital Twin Energy management Online Music Retail Shops Crowdfunding Startups Govt. & Contractor Avoid Intermediaters Advocate Bank Other 3rd Parties Blockchain + AI Decentralized AI platform Facts # data commited to ledger cann't be changed Frameworks # Source1: https://medium.com/hyperlegendary/6-blockchain-frameworks-to-build-enterprise-blockchain-how-to-choose-them-2b7d50ba275c Source2: https://www.blockchain-council.org/blockchain/list-of-best-open-source-blockchain-platforms/ Hyperledger (active Consumer): https://github.com/hyperledger/composer/ Ethereum: https://github.com/ethereum MultiChain: https://github.com/MultiChain , https://www.multichain.com OpenBlockChain : https://github.com/openblockchain Eris Workflow # genesis few transactions by few peoples transactions stored temporary in transaction pool many miners try to solve for candidate block candidate block is the next block going to be added to the blockchain a block can store a limited amount of transactions (1MB, in Bitcoin Hard Cash 8MB, 500 transactions etc.) they will take the all transaction data and create hash from that by making chnages in nonce any one miner won the race he will broadcast the hash/new to the network rest of the miners will validate the calculated hash there are some math like value of hash should be less than target source: http://learnmeabitcoin.com/guide/blocks) or a simple math for example, hash should start with 0000 if validations succeed, all the FAQ # how data changes reaches to the leader how leader'd PoW are verified where the new block will get added if one leader solved the problem what is the mathematical problem to be solved how many transactions in single block when is new block created and why? how creation/updation in doc will be validated, before audit? how to avoid false data entry by doctors? how to add real time data like, pulse, heartbit directly from device? include IoT? who are PoW validators? Are the other miners? who are ledger keepers? are they miners? Multichain # Installation # 1 2 3 4 5 6 7 8 9 su ( enter root password ) cd /tmp wget https://www.multichain.com/download/multichain-1.0.5.tar.gz tar -xvzf multichain-1.0.5.tar.gz cd multichain-1.0.5 mv multichaind multichain-cli multichain-util /usr/local/bin ( to make easily accessible on the command line ) exit ( to return to your regular user ) Getting Started # Nodes On Different Servers # Node 1 Initial Setup on Server 1 # Run 1 2 #Create chain1 multichain-util create chain1 Output 1 2 3 4 5 6 MultiChain 1 .0.5 Utilities ( latest protocol 10011 ) Blockchain parameter set was successfully generated. You can edit it in /home/toran/node1/chain1/params.dat before running multichaind for the first time. To generate blockchain please run \"multichaind chain1 -daemon\" . Run 1 2 #Initialize Blockchain with genesis block multichaind chain1 -daemon Output 1 2 3 4 5 6 7 8 9 10 11 12 13 MultiChain 1 .0.5 Daemon ( latest protocol 10011 ) Starting up node... Looking for genesis block... Genesis block found Other nodes can connect to this node using: multichaind chain1@192.168.1.100:6725 Listening for API requests on port 6724 ( local only - see rpcallowip setting ) Node ready. Node 2 Connection with Node 1 on Server 2 # Run 1 2 3 4 multichaind chain1@192.168.1.100:6725 # aws node multichaind -datadir = /home/toran/node2 -port = 10255 -rpcport = 10254 mychain@34.254.186.152:6821 -daemon Output 1 2 3 4 5 6 7 8 MultiChain 1 .0.5 Daemon ( latest protocol 10011 ) Retrieving blockchain parameters from the seed node 192 .168.1.100:6725 ... Blockchain successfully initialized. Please ask blockchain admin or user having activate permission to let you connect and/or transact: multichain-cli chain1 grant 1UcDevL9XAeEeHu5qtjBqcdXTsLfELW3nHt5YR connect multichain-cli chain1 grant 1UcDevL9XAeEeHu5qtjBqcdXTsLfELW3nHt5YR connect,send,receive Back on Server 1 # Run 1 multichain-cli chain1 grant 1UcDevL9XAeEeHu5qtjBqcdXTsLfELW3nHt5YR connect Back on Server 2 # Run 1 2 #Now try reconnecting again multichaind chain1 -daemon Delete a Chain # 1 2 3 4 5 #Stop it running multichain-cli chain1 stop #Delete its directory sudo rm -r ~/.multichain/chain1 Nodes On Same Server # Node 1 Initial Setup # Run 1 2 mkdir /home/toran/node1 multichain-util create chain1 -datadir = /home/toran/node1 Output 1 2 3 4 5 6 MultiChain 1 .0.5 Utilities ( latest protocol 10011 ) Blockchain parameter set was successfully generated. You can edit it in /home/toran/node1/chain1/params.dat before running multichaind for the first time. To generate blockchain please run \"multichaind chain1 -daemon\" . Run 1 multichaind chain1 -daemon -datadir = /home/toran/node1 Output 1 2 3 4 5 6 7 8 9 10 11 12 13 MultiChain 1 .0.5 Daemon ( latest protocol 10011 ) Starting up node... Looking for genesis block... Genesis block found Other nodes can connect to this node using: multichaind chain1@192.168.1.100:6725 Listening for API requests on port 6724 ( local only - see rpcallowip setting ) Node ready. Node 2 Connection with Node 1 # Syntax: multichaind -datadir=<your_path_to_multichain2nd_directory> -port=10255 -rpcport=10254 chain0 -daemon Run 1 2 mkdir /home/toran/node2 multichaind chain1@<chain1-ip>:<chain1-port> -datadir = /home/toran/node2 Output 1 2 3 4 5 6 7 8 MultiChain 1 .0.5 Daemon ( latest protocol 10011 ) Retrieving blockchain parameters from the seed node 192 .168.1.100:6725 ... Blockchain successfully initialized. Please ask blockchain admin or user having activate permission to let you connect and/or transact: multichain-cli chain1 grant 1UcDevL9XAeEeHu5qtjBqcdXTsLfELW3nHt5YR connect multichain-cli chain1 grant 1UcDevL9XAeEeHu5qtjBqcdXTsLfELW3nHt5YR connect,send,receive Run 1 multichain-cli chain1 -datadir = /home/toran/node1 grant 1UcDevL9XAeEeHu5qtjBqcdXTsLfELW3nHt5YR connect Commands in Interactive Mode sudo mkdir ~/.multichain/chain1 multichain-cli chain1 -datadir=/home/toran/node1 multichaind -datadir=/home/toran/node2 -port=10255 -rpcport=10254 mychain@34.254.186.152:6821 -daemon multichain-cli mychain -datadir=/home/toran/node2 allow port range in aws instance, like 1000-9999 in all multichain nodes aws instace while running the node chain@ip:port, this port is used in cli while API port is used in python/code multichain-cli -datadir=/home/ubuntu/toran-aws block-chain create stream \"scheme\" true connect multichaind -datadir=/home/ubuntu/vin-aws-new block-chain@13.232.100.99:2657 IP: vin-new: 34.255.8.211 tom: 13.232.130.184 my: 13.232.100.99","title":"Blockchain"},{"location":"Engineering/Software-Engineering/blockchain/#blockchain","text":"Blockchain Introduction Trade-offs Terminologies Components Block Features Applications Facts Frameworks Workflow FAQ Multichain Installation Getting Started Nodes On Different Servers Node 1 Initial Setup on Server 1 Node 2 Connection with Node 1 on Server 2 Back on Server 1 Back on Server 2 Delete a Chain Nodes On Same Server Node 1 Initial Setup Node 2 Connection with Node 1","title":"Blockchain"},{"location":"Engineering/Software-Engineering/blockchain/#introduction","text":"","title":"Introduction"},{"location":"Engineering/Software-Engineering/blockchain/#trade-offs","text":"Public vs Private Security vs Performance","title":"Trade-offs"},{"location":"Engineering/Software-Engineering/blockchain/#terminologies","text":"Genesis Consensus Proof-of-Work Proof-of-Stake Proof of Byzantine Fault Tollerance Black Node/Block Orphan Block (Purple) Immutable distributed Ledger Double Spending Node Any computer that connects to the Bitcoin network is called a node every mining computer, every computer that has a bitcoin wallet and by special sites that give blockchain monitor services Full Node Nodes that fully verify all of the rules of Bitcoin Full nodes download every block and transaction and check them against Bitcoin's consensus rules Merkle Tree Hash of transaction in a block Keys Source Private Key (64): Assiged to node on joining network Public Key (34): same as above but can be publiced, same as bitcoin QR code balances in wallet are fetched from bitcoin address, which is public key of the wallet","title":"Terminologies"},{"location":"Engineering/Software-Engineering/blockchain/#components","text":"Transactions commits changes to the blockchain contains signature of owner of address many transactions inside a block Blocks conatins an ordered bunch of transactions timestamps the transactions, are immutable each block references the previous block","title":"Components"},{"location":"Engineering/Software-Engineering/blockchain/#block","text":"Hash Data Previous Hash","title":"Block"},{"location":"Engineering/Software-Engineering/blockchain/#features","text":"Encryption of data Proof-of-work History Decentralized Data Trust in Data No Intermediaries Network Type Public Private Authentication peers signs the transaction with their private key","title":"Features"},{"location":"Engineering/Software-Engineering/blockchain/#applications","text":"Banking & Money Transfer Healthcare All types of document management Land / Real State Owner's Doc Transfer Deeds Land History Certificates Birth Caste Marriage Death Contracts Governance All the govt. data Voting Voter Registration Voter Identification Electronic Voting Cloud Storage Digital Twin Energy management Online Music Retail Shops Crowdfunding Startups Govt. & Contractor Avoid Intermediaters Advocate Bank Other 3rd Parties Blockchain + AI Decentralized AI platform","title":"Applications"},{"location":"Engineering/Software-Engineering/blockchain/#facts","text":"data commited to ledger cann't be changed","title":"Facts"},{"location":"Engineering/Software-Engineering/blockchain/#frameworks","text":"Source1: https://medium.com/hyperlegendary/6-blockchain-frameworks-to-build-enterprise-blockchain-how-to-choose-them-2b7d50ba275c Source2: https://www.blockchain-council.org/blockchain/list-of-best-open-source-blockchain-platforms/ Hyperledger (active Consumer): https://github.com/hyperledger/composer/ Ethereum: https://github.com/ethereum MultiChain: https://github.com/MultiChain , https://www.multichain.com OpenBlockChain : https://github.com/openblockchain Eris","title":"Frameworks"},{"location":"Engineering/Software-Engineering/blockchain/#workflow","text":"genesis few transactions by few peoples transactions stored temporary in transaction pool many miners try to solve for candidate block candidate block is the next block going to be added to the blockchain a block can store a limited amount of transactions (1MB, in Bitcoin Hard Cash 8MB, 500 transactions etc.) they will take the all transaction data and create hash from that by making chnages in nonce any one miner won the race he will broadcast the hash/new to the network rest of the miners will validate the calculated hash there are some math like value of hash should be less than target source: http://learnmeabitcoin.com/guide/blocks) or a simple math for example, hash should start with 0000 if validations succeed, all the","title":"Workflow"},{"location":"Engineering/Software-Engineering/blockchain/#faq","text":"how data changes reaches to the leader how leader'd PoW are verified where the new block will get added if one leader solved the problem what is the mathematical problem to be solved how many transactions in single block when is new block created and why? how creation/updation in doc will be validated, before audit? how to avoid false data entry by doctors? how to add real time data like, pulse, heartbit directly from device? include IoT? who are PoW validators? Are the other miners? who are ledger keepers? are they miners?","title":"FAQ"},{"location":"Engineering/Software-Engineering/blockchain/#multichain","text":"","title":"Multichain"},{"location":"Engineering/Software-Engineering/blockchain/#installation","text":"1 2 3 4 5 6 7 8 9 su ( enter root password ) cd /tmp wget https://www.multichain.com/download/multichain-1.0.5.tar.gz tar -xvzf multichain-1.0.5.tar.gz cd multichain-1.0.5 mv multichaind multichain-cli multichain-util /usr/local/bin ( to make easily accessible on the command line ) exit ( to return to your regular user )","title":"Installation"},{"location":"Engineering/Software-Engineering/blockchain/#getting-started","text":"","title":"Getting Started"},{"location":"Engineering/Software-Engineering/blockchain/#nodes-on-different-servers","text":"","title":"Nodes On Different Servers"},{"location":"Engineering/Software-Engineering/blockchain/#node-1-initial-setup-on-server-1","text":"Run 1 2 #Create chain1 multichain-util create chain1 Output 1 2 3 4 5 6 MultiChain 1 .0.5 Utilities ( latest protocol 10011 ) Blockchain parameter set was successfully generated. You can edit it in /home/toran/node1/chain1/params.dat before running multichaind for the first time. To generate blockchain please run \"multichaind chain1 -daemon\" . Run 1 2 #Initialize Blockchain with genesis block multichaind chain1 -daemon Output 1 2 3 4 5 6 7 8 9 10 11 12 13 MultiChain 1 .0.5 Daemon ( latest protocol 10011 ) Starting up node... Looking for genesis block... Genesis block found Other nodes can connect to this node using: multichaind chain1@192.168.1.100:6725 Listening for API requests on port 6724 ( local only - see rpcallowip setting ) Node ready.","title":"Node 1 Initial Setup on Server 1"},{"location":"Engineering/Software-Engineering/blockchain/#node-2-connection-with-node-1-on-server-2","text":"Run 1 2 3 4 multichaind chain1@192.168.1.100:6725 # aws node multichaind -datadir = /home/toran/node2 -port = 10255 -rpcport = 10254 mychain@34.254.186.152:6821 -daemon Output 1 2 3 4 5 6 7 8 MultiChain 1 .0.5 Daemon ( latest protocol 10011 ) Retrieving blockchain parameters from the seed node 192 .168.1.100:6725 ... Blockchain successfully initialized. Please ask blockchain admin or user having activate permission to let you connect and/or transact: multichain-cli chain1 grant 1UcDevL9XAeEeHu5qtjBqcdXTsLfELW3nHt5YR connect multichain-cli chain1 grant 1UcDevL9XAeEeHu5qtjBqcdXTsLfELW3nHt5YR connect,send,receive","title":"Node 2 Connection with Node 1 on Server 2"},{"location":"Engineering/Software-Engineering/blockchain/#back-on-server-1","text":"Run 1 multichain-cli chain1 grant 1UcDevL9XAeEeHu5qtjBqcdXTsLfELW3nHt5YR connect","title":"Back on Server 1"},{"location":"Engineering/Software-Engineering/blockchain/#back-on-server-2","text":"Run 1 2 #Now try reconnecting again multichaind chain1 -daemon","title":"Back on Server 2"},{"location":"Engineering/Software-Engineering/blockchain/#delete-a-chain","text":"1 2 3 4 5 #Stop it running multichain-cli chain1 stop #Delete its directory sudo rm -r ~/.multichain/chain1","title":"Delete  a Chain"},{"location":"Engineering/Software-Engineering/blockchain/#nodes-on-same-server","text":"","title":"Nodes On Same Server"},{"location":"Engineering/Software-Engineering/blockchain/#node-1-initial-setup","text":"Run 1 2 mkdir /home/toran/node1 multichain-util create chain1 -datadir = /home/toran/node1 Output 1 2 3 4 5 6 MultiChain 1 .0.5 Utilities ( latest protocol 10011 ) Blockchain parameter set was successfully generated. You can edit it in /home/toran/node1/chain1/params.dat before running multichaind for the first time. To generate blockchain please run \"multichaind chain1 -daemon\" . Run 1 multichaind chain1 -daemon -datadir = /home/toran/node1 Output 1 2 3 4 5 6 7 8 9 10 11 12 13 MultiChain 1 .0.5 Daemon ( latest protocol 10011 ) Starting up node... Looking for genesis block... Genesis block found Other nodes can connect to this node using: multichaind chain1@192.168.1.100:6725 Listening for API requests on port 6724 ( local only - see rpcallowip setting ) Node ready.","title":"Node 1 Initial Setup"},{"location":"Engineering/Software-Engineering/blockchain/#node-2-connection-with-node-1","text":"Syntax: multichaind -datadir=<your_path_to_multichain2nd_directory> -port=10255 -rpcport=10254 chain0 -daemon Run 1 2 mkdir /home/toran/node2 multichaind chain1@<chain1-ip>:<chain1-port> -datadir = /home/toran/node2 Output 1 2 3 4 5 6 7 8 MultiChain 1 .0.5 Daemon ( latest protocol 10011 ) Retrieving blockchain parameters from the seed node 192 .168.1.100:6725 ... Blockchain successfully initialized. Please ask blockchain admin or user having activate permission to let you connect and/or transact: multichain-cli chain1 grant 1UcDevL9XAeEeHu5qtjBqcdXTsLfELW3nHt5YR connect multichain-cli chain1 grant 1UcDevL9XAeEeHu5qtjBqcdXTsLfELW3nHt5YR connect,send,receive Run 1 multichain-cli chain1 -datadir = /home/toran/node1 grant 1UcDevL9XAeEeHu5qtjBqcdXTsLfELW3nHt5YR connect Commands in Interactive Mode sudo mkdir ~/.multichain/chain1 multichain-cli chain1 -datadir=/home/toran/node1 multichaind -datadir=/home/toran/node2 -port=10255 -rpcport=10254 mychain@34.254.186.152:6821 -daemon multichain-cli mychain -datadir=/home/toran/node2 allow port range in aws instance, like 1000-9999 in all multichain nodes aws instace while running the node chain@ip:port, this port is used in cli while API port is used in python/code multichain-cli -datadir=/home/ubuntu/toran-aws block-chain create stream \"scheme\" true connect multichaind -datadir=/home/ubuntu/vin-aws-new block-chain@13.232.100.99:2657 IP: vin-new: 34.255.8.211 tom: 13.232.130.184 my: 13.232.100.99","title":"Node 2 Connection with Node 1"},{"location":"Engineering/Software-Engineering/code/","text":"Code # Code Introduction Readability Maintainability Testability Measurements & Metrics Cognitive Complexity Cyclomatic Complexity Introduction # Readability # Maintainability # Testability # Measurements & Metrics # Cognitive Complexity # Thomas J. McCabe Ref: 1. https://blog.sonarsource.com/cognitive-complexity-because-testability-understandability 1. https://www.sonarsource.com/docs/CognitiveComplexity.pdf 1. https://medium.com/takeaway-tech/insights-in-cyclomatic-and-cognitive-complexity-in-your-application-58922ae59e80 Cyclomatic Complexity # Map Factory Pattern Strategy Pattern Command Pattern Repository Pattern Rule Engine","title":"Code"},{"location":"Engineering/Software-Engineering/code/#code","text":"Code Introduction Readability Maintainability Testability Measurements & Metrics Cognitive Complexity Cyclomatic Complexity","title":"Code"},{"location":"Engineering/Software-Engineering/code/#introduction","text":"","title":"Introduction"},{"location":"Engineering/Software-Engineering/code/#readability","text":"","title":"Readability"},{"location":"Engineering/Software-Engineering/code/#maintainability","text":"","title":"Maintainability"},{"location":"Engineering/Software-Engineering/code/#testability","text":"","title":"Testability"},{"location":"Engineering/Software-Engineering/code/#measurements-metrics","text":"","title":"Measurements &amp; Metrics"},{"location":"Engineering/Software-Engineering/code/#cognitive-complexity","text":"Thomas J. McCabe Ref: 1. https://blog.sonarsource.com/cognitive-complexity-because-testability-understandability 1. https://www.sonarsource.com/docs/CognitiveComplexity.pdf 1. https://medium.com/takeaway-tech/insights-in-cyclomatic-and-cognitive-complexity-in-your-application-58922ae59e80","title":"Cognitive Complexity"},{"location":"Engineering/Software-Engineering/code/#cyclomatic-complexity","text":"Map Factory Pattern Strategy Pattern Command Pattern Repository Pattern Rule Engine","title":"Cyclomatic Complexity"},{"location":"Engineering/Software-Engineering/docker/","text":"Docker # Dockerfile # RUN # CMD # ENTRYPOINT # COPY # Pull # Push # Build # build outside docker context https://www.jamestharpe.com/docker-include-files-outside-build-context/ https://stackoverflow.com/questions/60713751/where-to-put-dockerignore Run # exec # mount volume # mount/bind executables? # networks # Docker at GCP # https://cloud.google.com/build/docs/build-push-docker-image#build_using_dockerfile FAQ # Q. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 > [9/9] RUN apt-get update && apt-get install -y libffi-dev git && rm -rf /var/lib/apt/lists/* && pip install --no-cache-dir --upgrade pip && pip install --no-cache-dir -r /dataflow/plombier/src/requirements.txt && pip download --no-cache-dir --dest /tmp/dataflow-requirements-cache -r /dataflow/plombier/src/requirements.txt: #13 0.931 Get:1 http://security.debian.org/debian-security bullseye-security InRelease [44.1 kB] #13 0.931 Get:2 http://deb.debian.org/debian bullseye InRelease [116 kB] #13 1.324 Err:1 http://security.debian.org/debian-security bullseye-security InRelease #13 1.324 At least one invalid signature was encountered. #13 1.485 Err:2 http://deb.debian.org/debian bullseye InRelease #13 1.485 At least one invalid signature was encountered. #13 1.715 Get:3 http://deb.debian.org/debian bullseye-updates InRelease [39.4 kB] #13 1.815 Err:3 http://deb.debian.org/debian bullseye-updates InRelease #13 1.815 At least one invalid signature was encountered. #13 1.821 Reading package lists... #13 1.833 W: GPG error: http://security.debian.org/debian-security bullseye-security InRelease: At least one invalid signature was encountered. #13 1.833 E: The repository 'http://security.debian.org/debian-security bullseye-security InRelease' is not signed. #13 1.833 W: GPG error: http://deb.debian.org/debian bullseye InRelease: At least one invalid signature was encountered. #13 1.833 E: The repository 'http://deb.debian.org/debian bullseye InRelease' is not signed. #13 1.833 W: GPG error: http://deb.debian.org/debian bullseye-updates InRelease: At least one invalid signature was encountered. #13 1.833 E: The repository 'http://deb.debian.org/debian bullseye-updates InRelease' is not signed. ------ executor failed running [/bin/sh -c apt-get update && apt-get install -y libffi-dev git && rm -rf /var/lib/apt/lists/* && pip install --no-cache-dir --upgrade pip && pip install --no-cache-dir -r $PYTHON_REQUIREMENTS_FILE && pip download --no-cache-dir --dest /tmp/dataflow-requirements-cache -r $PYTHON_REQUIREMENTS_FILE]: exit code: 100 A: https://stackoverflow.com/questions/62473932/atleast-one-invalid-signature-was-encountered 1 docker builder prune","title":"Docker"},{"location":"Engineering/Software-Engineering/docker/#docker","text":"","title":"Docker"},{"location":"Engineering/Software-Engineering/docker/#dockerfile","text":"","title":"Dockerfile"},{"location":"Engineering/Software-Engineering/docker/#run","text":"","title":"RUN"},{"location":"Engineering/Software-Engineering/docker/#cmd","text":"","title":"CMD"},{"location":"Engineering/Software-Engineering/docker/#entrypoint","text":"","title":"ENTRYPOINT"},{"location":"Engineering/Software-Engineering/docker/#copy","text":"","title":"COPY"},{"location":"Engineering/Software-Engineering/docker/#pull","text":"","title":"Pull"},{"location":"Engineering/Software-Engineering/docker/#push","text":"","title":"Push"},{"location":"Engineering/Software-Engineering/docker/#build","text":"build outside docker context https://www.jamestharpe.com/docker-include-files-outside-build-context/ https://stackoverflow.com/questions/60713751/where-to-put-dockerignore","title":"Build"},{"location":"Engineering/Software-Engineering/docker/#run_1","text":"","title":"Run"},{"location":"Engineering/Software-Engineering/docker/#exec","text":"","title":"exec"},{"location":"Engineering/Software-Engineering/docker/#mount-volume","text":"","title":"mount volume"},{"location":"Engineering/Software-Engineering/docker/#mountbind-executables","text":"","title":"mount/bind executables?"},{"location":"Engineering/Software-Engineering/docker/#networks","text":"","title":"networks"},{"location":"Engineering/Software-Engineering/docker/#docker-at-gcp","text":"https://cloud.google.com/build/docs/build-push-docker-image#build_using_dockerfile","title":"Docker at GCP"},{"location":"Engineering/Software-Engineering/docker/#faq","text":"Q. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 > [9/9] RUN apt-get update && apt-get install -y libffi-dev git && rm -rf /var/lib/apt/lists/* && pip install --no-cache-dir --upgrade pip && pip install --no-cache-dir -r /dataflow/plombier/src/requirements.txt && pip download --no-cache-dir --dest /tmp/dataflow-requirements-cache -r /dataflow/plombier/src/requirements.txt: #13 0.931 Get:1 http://security.debian.org/debian-security bullseye-security InRelease [44.1 kB] #13 0.931 Get:2 http://deb.debian.org/debian bullseye InRelease [116 kB] #13 1.324 Err:1 http://security.debian.org/debian-security bullseye-security InRelease #13 1.324 At least one invalid signature was encountered. #13 1.485 Err:2 http://deb.debian.org/debian bullseye InRelease #13 1.485 At least one invalid signature was encountered. #13 1.715 Get:3 http://deb.debian.org/debian bullseye-updates InRelease [39.4 kB] #13 1.815 Err:3 http://deb.debian.org/debian bullseye-updates InRelease #13 1.815 At least one invalid signature was encountered. #13 1.821 Reading package lists... #13 1.833 W: GPG error: http://security.debian.org/debian-security bullseye-security InRelease: At least one invalid signature was encountered. #13 1.833 E: The repository 'http://security.debian.org/debian-security bullseye-security InRelease' is not signed. #13 1.833 W: GPG error: http://deb.debian.org/debian bullseye InRelease: At least one invalid signature was encountered. #13 1.833 E: The repository 'http://deb.debian.org/debian bullseye InRelease' is not signed. #13 1.833 W: GPG error: http://deb.debian.org/debian bullseye-updates InRelease: At least one invalid signature was encountered. #13 1.833 E: The repository 'http://deb.debian.org/debian bullseye-updates InRelease' is not signed. ------ executor failed running [/bin/sh -c apt-get update && apt-get install -y libffi-dev git && rm -rf /var/lib/apt/lists/* && pip install --no-cache-dir --upgrade pip && pip install --no-cache-dir -r $PYTHON_REQUIREMENTS_FILE && pip download --no-cache-dir --dest /tmp/dataflow-requirements-cache -r $PYTHON_REQUIREMENTS_FILE]: exit code: 100 A: https://stackoverflow.com/questions/62473932/atleast-one-invalid-signature-was-encountered 1 docker builder prune","title":"FAQ"},{"location":"Engineering/Software-Engineering/git/","text":"Git # Git Intro What? Why? Terminologies master origin remote url Setup & Config config CRLF, LF, CR Windows Only Linux/Mac Only If Both Inspection & Comparision diff Compare local with remote branch diff-tree List all the files in a commit show List all the files in a commit Commands init --bare clone pull add rm commit push checkout status log reflog reset Uses Branching Merging merge Vs rebase merge rebase: rebase one branch on the top of another Reverting back to old commits Reseting Modify last commit Message Modify a particular commit Squashing last x commits into one Misc Travis CI (Continue Integration) Prerequisites Steps to Integrate CI command to install dependencies command to run tests Setup SSH key Generate or Get ssh key Generate Set paraphrase Get Add the key to the ssh-agent Start Add Provide paraphrase Add the public key to your VCS portal settings Copy hash of public key Paste in settings Update remotes with ssh url Intro # Book Ref Cheatsheet What? # Why? # As a collaboration tool As a version control tool Terminologies # master # origin # remote # url # Setup & Config # config # CRLF, LF, CR # Windows Only # Then turn off automatical convertions, and stick to CRLF only 1 git config --global core.autocrlf false Linux/Mac Only # If on LF system, then do this to convert any CRLF to LF but not LF to CRLF 1 git config --global core.autocrlf input If Both # Then set this in Windows system only auto-converting CRLF line endings into LF when you add a file to the index and vice versa when it checks out code onto your filesystem hence Git will be having LF but windows filesystem will always have CRLF 1 git config --global core.autocrlf true Inspection & Comparision # diff # Compare local with remote branch # 1 2 3 4 5 git diff <masterbranch_path> <remotebranch_path> #or git diff master origin/master diff-tree # List all the files in a commit # 1 git diff-tree --no-commit-id --name-only -r bd61ad98 show # List all the files in a commit # 1 git show --pretty=\"\" --name-only bd61ad98 Commands # init # --bare # Source: http://www.saintsjd.com/2011/01/what-is-a-bare-git-repository/ clone # Normal Way git clone url:repo Clone a specific release git clone url:repo --branch <tag#> pull # git pull origin master git pull origin master -f git pull --rebase origin master add # rm # commit # commit -m \"message\" commit --amend to re-phrase the commit message; iff commit has not been pushed push # checkout # status # log # To see in descriptive format 1 git log --graph --decorate --oneline where: --graph: shows flow --decorate: shows branch names --oneline: compact description in single line reflog # To see in short reset # Uses # Branching # Lets say you have 2 branches: master dev Options: create a new branch dev 1 git checkout -b dev dev is active 1 git checkout dev master is active 1 git checkout master dev is active & incorporate master in it 1 2 git checkout dev git merge master master is active & incorporate dev in it 1 2 git checkout master git merge dev delete a local branch if completely merged 1 git branch -d dev delete a local branch if not merged 1 git branch -D dev delete a remote branch 1 git push origin :dev Rename your local branch. If you are on the branch you want to rename: 1 git branch -m new-name If you are on a different branch: 1 git branch -m old-name new-name Delete the old-name remote branch and push the new-name local branch. 1 git push origin :old-name new-name Reset the upstream branch for the new-name local branch. Switch to the branch and then: 1 git push origin -u new-name Golden rules first incorporate master into dev check compatibility/bugs/conflicts & resolve them now incorporate/merge dev in master Merging # merge Vs rebase # Both are used for same purpose but with different approach Integrates changes from one branch to another merge # Lets say, if you're woring on a feature in a dedicated branch. And someone makes a commit in master branch and you want to pull those changes to your feature branch, i.e. 1 2 3 4 5 A---B---C feature / D---E---F---G master here A,B,C are commits in feature. F,G are new commits in master then: 1 2 git checkout feature git merge master Or in one line 1 git merge master feature This will create a new commit in feature branch called as \"merge commit\" and feature branch will have same history as master (but not vice-versa) i.e. 1 2 3 4 5 A---B---C---(*) feature / / D---E---F---G---H master here (*) is merge commit Very active master branch can pollute the feature branch. rebase: # Alternative to merging rebases feature branch onto master branch 1 2 git checkout feature git rebase master shifts the entire feature branch to tip/(latest node) of the master branch. i.e. 1 2 3 4 5 A'--B'--C' feature / D---E---F---G master here A',B',C' are brand new commits instead of creating \"merge commit\", it creates brand new commits in feature branch for all the commits created earlier on the feature branch (the changes will same, but with new commit infos) i.e. re-writes branch history Benifits over merge gives cleaner history log by avoiding un-necessary \"merge commits\" results in perfectly linear project history Golden rules to use rebase : * never use rebase on any public branch: Means, don't rebase the master branch onto feature branch (it is opposite of rebasing feature branch onto master branch). It will create a brand new commit/history in master branch which will affect other developers. rebase one branch on the top of another # 1 2 3 4 5 6 7 8 9 10 11 12 13 Branch-1 Branch-2 X--Y A--B--C / / D---E---F---G---H master After rebaseing Branch-1 on the top of Branch-2 A--B--C--X--Y Branch-1 / A--B--C Branch-2 / D---E---F---G---H master 1 2 git checkout branch1 git rebase branch2 Reverting back to old commits # https://stackoverflow.com/questions/4114095/how-to-revert-a-git-repository-to-a-previous-commit Reseting # 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 git checkout <branch> git reflog pick the commit_sha one prior to the the incident git reset --hard <commit_sha> --hard Matches the working tree and index to that of the tree being switched to. Any changes to tracked files in the working tree since <commit> are lost. git reset --merge <commit_sha> or git reset --merge <head_count> --merge Resets the index to match the tree recorded by the named commit, and updates the files that are different between the named commit and the current commit in the working tree. Modify last commit Message # 1 git commit -a -m Modify a particular commit # If you want to modify files at commit 1 git rebase -i <SHA> You will get a editor opened up with list of all the commits prior to that. Change the pick to edit for that particular commit . Save & close the editor. Make changes to your files. After that add or rm files. If you want to change the commit msg also then 1 git commit -a Then continue rebasing 1 git rebase --continue Squashing last x commits into one # If you want to squash last X commits into single commit then 1 git rebase -i HEAD~X You will get a editor opened up with list of all the X commits. Change the pick to s for all the commits except the oldest one into which you want to merge all the lastest. Save & close the editor. Again you'll get an editor with list & order of all the squashed commits. (if you want you can change the commit msgs at this moment) Then continue rebasing 1 git rebase --continue Done. Misc # Travis CI (Continue Integration) # Continuous Integration is the practice of merging in small code changes frequently - rather than merging in a large change at the end of a development cycle. The goal is to build healthier software by developing and testing in smaller increments. Source Prerequisites # To start using Travis CI, make sure you have all of the following: GitHub login Project hosted as a repository on GitHub Working code in your project Working build or test script Steps to Integrate CI # Using github login to TravisCI.org for public repositories TravisCI.com for private repositories Enable the repo in CI portal Add .travis.yml file to the repo to tell Travis CI what to do e.g. ```yaml language: python python: - \"3.6.4\" command to install dependencies # install: # - pip install -r requirements.pip - pip install pipenv - \"pipenv install --dev\" command to run tests # script: # - pytest src/test.py # or py.test for Python versions 3.5 and below - pipenv run pytest src/test.py # or py.test for Python versions 3.5 and below ``` Infrastructure options ```yaml os: linux dist: trusty sudo: enabled ``` Add the .travis.yml file to git, commit and push, to trigger a Travis CI build Travis only runs builds on the commits you push after you\u2019ve enabled the repository in Travis CI. check the build page embbed build status on github page Setup SSH key # Generate or Get ssh key # Generate # 1 ssh-keygen Set paraphrase # Get # 1 ls ~/.ssh Add the key to the ssh-agent # Start # 1 eval ` ssh-agent ` Add # 1 ssh-add ~/.ssh/<private_key_file> where is generally id_rsa Provide paraphrase # Add the public key to your VCS portal settings # Copy hash of public key # 1 2 3 4 5 cat ~/.ssh/id_rsa.pub or xclip -sel clip ~/.ssh/id_rsa.pub Paste in settings # Update remotes with ssh url # 1 2 3 git remote rm origin git remote add origin git@bitbucket.org:toransahu/ethereal-machines-backend.git","title":"Git"},{"location":"Engineering/Software-Engineering/git/#git","text":"Git Intro What? Why? Terminologies master origin remote url Setup & Config config CRLF, LF, CR Windows Only Linux/Mac Only If Both Inspection & Comparision diff Compare local with remote branch diff-tree List all the files in a commit show List all the files in a commit Commands init --bare clone pull add rm commit push checkout status log reflog reset Uses Branching Merging merge Vs rebase merge rebase: rebase one branch on the top of another Reverting back to old commits Reseting Modify last commit Message Modify a particular commit Squashing last x commits into one Misc Travis CI (Continue Integration) Prerequisites Steps to Integrate CI command to install dependencies command to run tests Setup SSH key Generate or Get ssh key Generate Set paraphrase Get Add the key to the ssh-agent Start Add Provide paraphrase Add the public key to your VCS portal settings Copy hash of public key Paste in settings Update remotes with ssh url","title":"Git"},{"location":"Engineering/Software-Engineering/git/#intro","text":"Book Ref Cheatsheet","title":"Intro"},{"location":"Engineering/Software-Engineering/git/#what","text":"","title":"What?"},{"location":"Engineering/Software-Engineering/git/#why","text":"As a collaboration tool As a version control tool","title":"Why?"},{"location":"Engineering/Software-Engineering/git/#terminologies","text":"","title":"Terminologies"},{"location":"Engineering/Software-Engineering/git/#master","text":"","title":"master"},{"location":"Engineering/Software-Engineering/git/#origin","text":"","title":"origin"},{"location":"Engineering/Software-Engineering/git/#remote","text":"","title":"remote"},{"location":"Engineering/Software-Engineering/git/#url","text":"","title":"url"},{"location":"Engineering/Software-Engineering/git/#setup-config","text":"","title":"Setup &amp; Config"},{"location":"Engineering/Software-Engineering/git/#config","text":"","title":"config"},{"location":"Engineering/Software-Engineering/git/#crlf-lf-cr","text":"","title":"CRLF, LF, CR"},{"location":"Engineering/Software-Engineering/git/#windows-only","text":"Then turn off automatical convertions, and stick to CRLF only 1 git config --global core.autocrlf false","title":"Windows Only"},{"location":"Engineering/Software-Engineering/git/#linuxmac-only","text":"If on LF system, then do this to convert any CRLF to LF but not LF to CRLF 1 git config --global core.autocrlf input","title":"Linux/Mac Only"},{"location":"Engineering/Software-Engineering/git/#if-both","text":"Then set this in Windows system only auto-converting CRLF line endings into LF when you add a file to the index and vice versa when it checks out code onto your filesystem hence Git will be having LF but windows filesystem will always have CRLF 1 git config --global core.autocrlf true","title":"If Both"},{"location":"Engineering/Software-Engineering/git/#inspection-comparision","text":"","title":"Inspection &amp; Comparision"},{"location":"Engineering/Software-Engineering/git/#diff","text":"","title":"diff"},{"location":"Engineering/Software-Engineering/git/#compare-local-with-remote-branch","text":"1 2 3 4 5 git diff <masterbranch_path> <remotebranch_path> #or git diff master origin/master","title":"Compare local with remote branch"},{"location":"Engineering/Software-Engineering/git/#diff-tree","text":"","title":"diff-tree"},{"location":"Engineering/Software-Engineering/git/#list-all-the-files-in-a-commit","text":"1 git diff-tree --no-commit-id --name-only -r bd61ad98","title":"List all the files in a commit"},{"location":"Engineering/Software-Engineering/git/#show","text":"","title":"show"},{"location":"Engineering/Software-Engineering/git/#list-all-the-files-in-a-commit_1","text":"1 git show --pretty=\"\" --name-only bd61ad98","title":"List all the files in a commit"},{"location":"Engineering/Software-Engineering/git/#commands","text":"","title":"Commands"},{"location":"Engineering/Software-Engineering/git/#init","text":"","title":"init"},{"location":"Engineering/Software-Engineering/git/#-bare","text":"Source: http://www.saintsjd.com/2011/01/what-is-a-bare-git-repository/","title":"--bare"},{"location":"Engineering/Software-Engineering/git/#clone","text":"Normal Way git clone url:repo Clone a specific release git clone url:repo --branch <tag#>","title":"clone"},{"location":"Engineering/Software-Engineering/git/#pull","text":"git pull origin master git pull origin master -f git pull --rebase origin master","title":"pull"},{"location":"Engineering/Software-Engineering/git/#add","text":"","title":"add"},{"location":"Engineering/Software-Engineering/git/#rm","text":"","title":"rm"},{"location":"Engineering/Software-Engineering/git/#commit","text":"commit -m \"message\" commit --amend to re-phrase the commit message; iff commit has not been pushed","title":"commit"},{"location":"Engineering/Software-Engineering/git/#push","text":"","title":"push"},{"location":"Engineering/Software-Engineering/git/#checkout","text":"","title":"checkout"},{"location":"Engineering/Software-Engineering/git/#status","text":"","title":"status"},{"location":"Engineering/Software-Engineering/git/#log","text":"To see in descriptive format 1 git log --graph --decorate --oneline where: --graph: shows flow --decorate: shows branch names --oneline: compact description in single line","title":"log"},{"location":"Engineering/Software-Engineering/git/#reflog","text":"To see in short","title":"reflog"},{"location":"Engineering/Software-Engineering/git/#reset","text":"","title":"reset"},{"location":"Engineering/Software-Engineering/git/#uses","text":"","title":"Uses"},{"location":"Engineering/Software-Engineering/git/#branching","text":"Lets say you have 2 branches: master dev Options: create a new branch dev 1 git checkout -b dev dev is active 1 git checkout dev master is active 1 git checkout master dev is active & incorporate master in it 1 2 git checkout dev git merge master master is active & incorporate dev in it 1 2 git checkout master git merge dev delete a local branch if completely merged 1 git branch -d dev delete a local branch if not merged 1 git branch -D dev delete a remote branch 1 git push origin :dev Rename your local branch. If you are on the branch you want to rename: 1 git branch -m new-name If you are on a different branch: 1 git branch -m old-name new-name Delete the old-name remote branch and push the new-name local branch. 1 git push origin :old-name new-name Reset the upstream branch for the new-name local branch. Switch to the branch and then: 1 git push origin -u new-name Golden rules first incorporate master into dev check compatibility/bugs/conflicts & resolve them now incorporate/merge dev in master","title":"Branching"},{"location":"Engineering/Software-Engineering/git/#merging","text":"","title":"Merging"},{"location":"Engineering/Software-Engineering/git/#merge-vs-rebase","text":"Both are used for same purpose but with different approach Integrates changes from one branch to another","title":"merge Vs rebase"},{"location":"Engineering/Software-Engineering/git/#merge","text":"Lets say, if you're woring on a feature in a dedicated branch. And someone makes a commit in master branch and you want to pull those changes to your feature branch, i.e. 1 2 3 4 5 A---B---C feature / D---E---F---G master here A,B,C are commits in feature. F,G are new commits in master then: 1 2 git checkout feature git merge master Or in one line 1 git merge master feature This will create a new commit in feature branch called as \"merge commit\" and feature branch will have same history as master (but not vice-versa) i.e. 1 2 3 4 5 A---B---C---(*) feature / / D---E---F---G---H master here (*) is merge commit Very active master branch can pollute the feature branch.","title":"merge"},{"location":"Engineering/Software-Engineering/git/#rebase","text":"Alternative to merging rebases feature branch onto master branch 1 2 git checkout feature git rebase master shifts the entire feature branch to tip/(latest node) of the master branch. i.e. 1 2 3 4 5 A'--B'--C' feature / D---E---F---G master here A',B',C' are brand new commits instead of creating \"merge commit\", it creates brand new commits in feature branch for all the commits created earlier on the feature branch (the changes will same, but with new commit infos) i.e. re-writes branch history Benifits over merge gives cleaner history log by avoiding un-necessary \"merge commits\" results in perfectly linear project history Golden rules to use rebase : * never use rebase on any public branch: Means, don't rebase the master branch onto feature branch (it is opposite of rebasing feature branch onto master branch). It will create a brand new commit/history in master branch which will affect other developers.","title":"rebase:"},{"location":"Engineering/Software-Engineering/git/#rebase-one-branch-on-the-top-of-another","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 Branch-1 Branch-2 X--Y A--B--C / / D---E---F---G---H master After rebaseing Branch-1 on the top of Branch-2 A--B--C--X--Y Branch-1 / A--B--C Branch-2 / D---E---F---G---H master 1 2 git checkout branch1 git rebase branch2","title":"rebase one branch on the top of another"},{"location":"Engineering/Software-Engineering/git/#reverting-back-to-old-commits","text":"https://stackoverflow.com/questions/4114095/how-to-revert-a-git-repository-to-a-previous-commit","title":"Reverting back to old commits"},{"location":"Engineering/Software-Engineering/git/#reseting","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 git checkout <branch> git reflog pick the commit_sha one prior to the the incident git reset --hard <commit_sha> --hard Matches the working tree and index to that of the tree being switched to. Any changes to tracked files in the working tree since <commit> are lost. git reset --merge <commit_sha> or git reset --merge <head_count> --merge Resets the index to match the tree recorded by the named commit, and updates the files that are different between the named commit and the current commit in the working tree.","title":"Reseting"},{"location":"Engineering/Software-Engineering/git/#modify-last-commit-message","text":"1 git commit -a -m","title":"Modify last commit Message"},{"location":"Engineering/Software-Engineering/git/#modify-a-particular-commit","text":"If you want to modify files at commit 1 git rebase -i <SHA> You will get a editor opened up with list of all the commits prior to that. Change the pick to edit for that particular commit . Save & close the editor. Make changes to your files. After that add or rm files. If you want to change the commit msg also then 1 git commit -a Then continue rebasing 1 git rebase --continue","title":"Modify a particular commit"},{"location":"Engineering/Software-Engineering/git/#squashing-last-x-commits-into-one","text":"If you want to squash last X commits into single commit then 1 git rebase -i HEAD~X You will get a editor opened up with list of all the X commits. Change the pick to s for all the commits except the oldest one into which you want to merge all the lastest. Save & close the editor. Again you'll get an editor with list & order of all the squashed commits. (if you want you can change the commit msgs at this moment) Then continue rebasing 1 git rebase --continue Done.","title":"Squashing last x commits into one"},{"location":"Engineering/Software-Engineering/git/#misc","text":"","title":"Misc"},{"location":"Engineering/Software-Engineering/git/#travis-ci-continue-integration","text":"Continuous Integration is the practice of merging in small code changes frequently - rather than merging in a large change at the end of a development cycle. The goal is to build healthier software by developing and testing in smaller increments. Source","title":"Travis CI (Continue Integration)"},{"location":"Engineering/Software-Engineering/git/#prerequisites","text":"To start using Travis CI, make sure you have all of the following: GitHub login Project hosted as a repository on GitHub Working code in your project Working build or test script","title":"Prerequisites"},{"location":"Engineering/Software-Engineering/git/#steps-to-integrate-ci","text":"Using github login to TravisCI.org for public repositories TravisCI.com for private repositories Enable the repo in CI portal Add .travis.yml file to the repo to tell Travis CI what to do e.g. ```yaml language: python python: - \"3.6.4\"","title":"Steps to Integrate CI"},{"location":"Engineering/Software-Engineering/git/#command-to-install-dependencies","text":"install: # - pip install -r requirements.pip - pip install pipenv - \"pipenv install --dev\"","title":"command to install dependencies"},{"location":"Engineering/Software-Engineering/git/#command-to-run-tests","text":"script: # - pytest src/test.py # or py.test for Python versions 3.5 and below - pipenv run pytest src/test.py # or py.test for Python versions 3.5 and below ``` Infrastructure options ```yaml os: linux dist: trusty sudo: enabled ``` Add the .travis.yml file to git, commit and push, to trigger a Travis CI build Travis only runs builds on the commits you push after you\u2019ve enabled the repository in Travis CI. check the build page embbed build status on github page","title":"command to run tests"},{"location":"Engineering/Software-Engineering/git/#setup-ssh-key","text":"","title":"Setup SSH key"},{"location":"Engineering/Software-Engineering/git/#generate-or-get-ssh-key","text":"","title":"Generate or Get ssh key"},{"location":"Engineering/Software-Engineering/git/#generate","text":"1 ssh-keygen","title":"Generate"},{"location":"Engineering/Software-Engineering/git/#set-paraphrase","text":"","title":"Set paraphrase"},{"location":"Engineering/Software-Engineering/git/#get","text":"1 ls ~/.ssh","title":"Get"},{"location":"Engineering/Software-Engineering/git/#add-the-key-to-the-ssh-agent","text":"","title":"Add the key to the ssh-agent"},{"location":"Engineering/Software-Engineering/git/#start","text":"1 eval ` ssh-agent `","title":"Start"},{"location":"Engineering/Software-Engineering/git/#add_1","text":"1 ssh-add ~/.ssh/<private_key_file> where is generally id_rsa","title":"Add"},{"location":"Engineering/Software-Engineering/git/#provide-paraphrase","text":"","title":"Provide paraphrase"},{"location":"Engineering/Software-Engineering/git/#add-the-public-key-to-your-vcs-portal-settings","text":"","title":"Add the public key to your VCS portal settings"},{"location":"Engineering/Software-Engineering/git/#copy-hash-of-public-key","text":"1 2 3 4 5 cat ~/.ssh/id_rsa.pub or xclip -sel clip ~/.ssh/id_rsa.pub","title":"Copy hash of public key"},{"location":"Engineering/Software-Engineering/git/#paste-in-settings","text":"","title":"Paste in settings"},{"location":"Engineering/Software-Engineering/git/#update-remotes-with-ssh-url","text":"1 2 3 git remote rm origin git remote add origin git@bitbucket.org:toransahu/ethereal-machines-backend.git","title":"Update remotes with ssh url"},{"location":"Engineering/Software-Engineering/k8s/","text":"Kubernetes # Kubernetes TODO TODO # learn https://www.youtube.com/watch?v=zeS6OyDoy78 https://github.com/janakiramm/kubernetes-101","title":"Kubernetes"},{"location":"Engineering/Software-Engineering/k8s/#kubernetes","text":"Kubernetes TODO","title":"Kubernetes"},{"location":"Engineering/Software-Engineering/k8s/#todo","text":"learn https://www.youtube.com/watch?v=zeS6OyDoy78 https://github.com/janakiramm/kubernetes-101","title":"TODO"},{"location":"Engineering/Software-Engineering/linux/","text":"Linux # Linux Bash Scripting AWK include/source/import String Comparison Sub-String Comparison If, elif, else, fi Switch Case UNIX Commands mv all files/dir to a subdir Get System Informations Set deafult shell xclip ls prefixed commands find Set hostname 1. at /etc/hosts 2. at /etc/hostname 3. then tell the machine CPU Info CPU Temperature IP Related ssh In case of AWS EC2 SSH sftp sshfs Device connected to the Network dd (Disk Dump) commands Create a backup Restore a backup Clone a hard disk Transfer a disk image Create an iso image of a CD/DVD Burn an iso image of a CD/DVD Rescue a file that contains bad blocks Create your own bootloader Create a backup of your MBR Restore a backup of your MBR Mount dd image of and entire disk When the hard disk has errors Network Clone Network speed test Links Task Autorun/Automation Daemons Shells Graphical Cron Jobs Intro Syntax Operations Type Operators misc Services .desktop file using /etc/rc.local using /etc/init.d Disk Related fdisk lsblk blkid df du GRUB Edit grub configs grub update Reboot into other OS Problems & Solutions Make Anaconda Python Default If cinnamon freezes sudo: unable to resolve host Ray: Connection timed out nemo context menu \"open in terminal\" not working Speedup mouse scroll Utilities Multitouch Gesture Battery Monitor Battery Optimization SATA Power Management PowerTop by Intel (Recommanded) TLP (Emergency) Mono Install Verify Thinkpad Touchpad Middle Button Install Config Finger Print Reader Installation Uninstallation ZSH oh-my-zsh VIM Vundle Using Vundle VIM as Python IDE Tmux Tmux Package Manager Tmux Plugins: Commands Load .bashrc with tmux KeyBoard Custom Shortcuts Bindings Emulate right click from keyboard Backup & Restore Cinnamon settings Raspberry Pi OS Raspbian with RPD Desktop Raspbian Lite (without GUI) Install Desktop for Lite Enable ssh & enter SSID/WiFi details without keyboard Ubuntu Core/Snap Windows IoT Display Touch Screeen Config Backup Image from Burnt Image On a different linux PC using USB. In Raspberry Itself using rsync Network Drive Mount (sshfs) Remote Audio bluetooth Way 1 Way 2 wifi enable ssh from card set wifi/ssid from card wifi to ethernet Bash Scripting # AWK # https://www.gnu.org/software/gawk/manual/gawk.html include/source/import # https://gist.github.com/toransahu/0a816af786b7d0b98ab1e7079bf0426f String Comparison # use = or == keep proper square spacing between operators & brackets (most imp) use double quote for strings and variables sometime we need to use brackets to access env variables https://gist.github.com/toransahu/4fd3abc369bb5c8a1ee424af07cb1563 Sub-String Comparison # 1 2 3 string = \" $( hostname ) \" substr = \"mint\" if [ -z \" ${ string ##* $substr * } \" ] ; then https://gist.github.com/toransahu/f812260a37947a299c5c26adddfa4cfa If, elif, else, fi # Source: https://ryanstutorials.net/bash-scripting-tutorial/bash-if-statements.php Switch Case # https://www.shellscript.sh/case.html UNIX Commands # mv all files/dir to a subdir # BASH https://askubuntu.com/questions/91740/how-to-move-all-files-in-current-folder-to-subfolder 1 TODO: zsh 1 2 setopt extended_glob mv Get System Informations # Set deafult shell # 1 sudo chsh -s /bin/zsh then logout In Github:linux-tweaks/guides xclip # 1 xclip -sel clip <file> ls prefixed commands # ls lsA lsblk lscpu lshw lsipc lslogins lsof lspcmcia lss16toppm lsusb lsa lsattr lsb_release lsdiff lsinitramfs lslocks lsmod lspci lspgpot lss3 find # 1 2 3 find <dir> <type> <test> <expression> <action> find /Ray/ -type f -name \"*.pyc\" -delete Set hostname # 1. at /etc/hosts # 127.0.1.1 mint-ThinkPad-L440 2. at /etc/hostname # write mint-ThinkPad-L440 3. then tell the machine # 1 sudo hostname mint-ThinkPad-L440 CPU Info # 1 cat flags /proc/cpuinfo CPU Temperature # 1 sudo apt-get install lm-sensors After installation type the following in terminal 1 sudo sensors-detect You may also need to run 1 sudo service kmod start It will ask you few questions. Answer Yes for all of them. Finally to get your CPU temperature type sensors in your terminal. 1 sensors IP Related # ifconfig ip addr hostname -I ssh # 1 ssh pi@192.168.1.108 In case of AWS EC2 SSH # Connection freezes frequently on inactivity - source https://www.quora.com/Why-does-ssh-to-amazon-ec2-instance-gets-frozen-after-sometime - Solution - sudo vim /etc/ssh/ssh_config - add TCPKeepAlive yes - add ServerAliveInterval 5 where 5 is minute - sudo service sshd restart sftp # 1 2 3 4 5 6 7 8 sftp pi@192.168.1.108 put local_file get remote_file files will upload/download @ home sshfs # 1 2 3 4 sudo mkdir /mnt/pi sudo sshfs -o allow_other pi@192.168.1.108:/ /mnt/pi/ Device connected to the Network # 1 nmap -sP 192 .168.1.0/24 dd (Disk Dump) commands # Create a backup # dd if=/dev/sda of=/opt/backup_sda.img Restore a backup # dd if=/opt/backup_sda.img of=/dev/sda Clone a hard disk # dd if=/dev/sdb of=/dev/sdc Transfer a disk image # dd if=/dev/sdb | ssh root@target \"(cat > backup.img)\" Create an iso image of a CD/DVD # dd if=/dev/cdrom of=cdimage.iso Burn an iso image of a CD/DVD # dd if=cdimage.iso of=/dev/cdrom obs=32k seek=0 Rescue a file that contains bad blocks # dd if=movie.avi of=rescued_movie.avi conv=noerror Create your own bootloader # dd conv=notrunc if=bootloader of=qemu.img Create a backup of your MBR # dd if=/dev/sdb of=mbr_backup bs=512 count=1 Restore a backup of your MBR # dd if=mbr_backup of=/dev/sdb bs=512 count=1 Mount dd image of and entire disk # You must use the start number of the partition. fdisk -u -l disk_image 1 2 3 4 5 6 7 8 Disk /mnt/storage/disk_image: 0 MB, 0 bytes255 heads, 63 sectors/track, 0 cylinders, total 0 sectors Units = sectors of 1 * 512 = 512 bytesDisk identifier: 0x41172ba5 Device Boot Start End Blocks Id System /mnt/storage/disk_image1 63 64259 32098 + de Dell Utility /mnt/storage/disk_image2 * 64260 78108029 39021885 7 HPFS/NTFS Partition 2 has different physical/logical endings:phys =( 1023 , 254 , 63 ) logical =( 4861 , 254 , 63 ) Then take the start of the partition that you want to edit, 64260 (disk_image2) in this case, and multiply it by 512 Ex: 512 * 64260 = 32901120 mount -o loop,offset=32901120 -t auto /mnt/storage/disk_image /mnt/image_partition_2 When the hard disk has errors # Get the dd_rescue tool dd_rescue /dev/sdb /opt/backup_sdb.img Network Clone # Destination: nc -l -p 2222 | dd of=/dev/sda bs=16M Source: dd if=/dev/sda bs=16M | nc Destination 2222 Network speed test # dd if=/dev/zero bs=1M count=100 | ssh user@machine 'cat > /dev/null' Links # Symbolic link ln -s original link Hard Link ln original link Task Autorun/Automation # src1 : https://developer.toradex.com/knowledge-base/how-to-autorun-application-at-the-start-up-in-linux Daemons # Shells # Graphical # Cron Jobs # Intro # The cron service (daemon) runs in the background and constantly checks the /etc/crontab file, and /etc/cron.*/ directories. It also checks the /var/spool/cron/ directory. Source create cron job crontab -e and edit the opened file add your task (in cron format) or, create a new cron job under /etc/cron.*/ add your task (in cron format) Syntax # 1 2 3 4 5 6 SHELL = /bin/bash PATH = /usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin MAILTO = \"\" 1 2 3 4 5 /path/to/command arg1 arg2 # or 1 2 3 4 5 /root/backup.sh ` to automate tasks based on time like on boot: @reboot hourly: @hourly or \u201c0 * * \u201d daily : @daily or @midnight or \u201c0 0 * \u201d weekly: @weekly or \u201c0 0 * * 0\u201d monthly: @monthly or \u201c0 0 1 * *\u201d yearly: @yearly or @annually or \u201c0 0 1 1 *\u201d command to be executed | | | | | | | | | ----- Day of week (0 - 7) (Sunday=0 or 7) | | | ------- Month (1 - 12) | | --------- Day of month (1 - 31) | ----------- Hour (0 - 23) ------------- Minute (0 - 59) Operations # start: sudo service cron start stop: sudo service cron stop restart: sudo service cron restart status: sudo service cron status list: crontab -l or crontab -u USERNAME -l delete all: crontab -r or crontab -u USERNAME -r Type # System Level need to define USERNAME 1 2 3 4 5 USERNAME /path/to/command arg1 arg2 User Level by default takes USERNAME of creator 1 2 3 4 5 /path/to/command arg1 arg2 Operators # asterisk (*) every [minute/hour/day/month/day of week] comma (,) list of values like, on this days: 1,2,3,4,5 dash (-) range like, on this days: 1-5 seperator (/) stepper like, */2 in hour field means: every 2 hours misc # print names of all valid files: run-parts --list /etc/cron.d print script names which would run, but don't run them: run-parts --test /etc/cron.d Services # .desktop file # Works on boot put .desktop file with some script in ~/.config/autostart/ write script like 1 2 3 4 5 6 7 8 9 [ Desktop Entry ] Type = Application Exec = sh /home/toran/Applications/linux_tweaks/LowBatterySound/LowBattery.sh X-GNOME-Autostart-enabled = false NoDisplay = false Hidden = false Name [ en_IN ]= Low Battery Warning Comment [ en_IN ]= Warning sound on battery less than 15 % X-GNOME-Autostart-Delay = 20 using /etc/rc.local # using /etc/init.d # Disk Related # fdisk # manupulate disk partition table list devices: sudo fdisk -l lsblk # lists block devices blkid # locate/ print block device attributes df # reports filesystem disk space usage du # estimate file space usage Summarize disk usage of the set of FILEs, recursively for directories. GRUB # Edit grub configs # 1 vim /etc/default/grub grub update # 1 sudo update-grub Reboot into other OS # 1 sudo grub-reboot 2 Problems & Solutions # Make Anaconda Python Default # put following in .bashrc or .zshrc 1 2 # added by Anaconda3 installer export PATH=\"/home/toran/anaconda3/bin:$PATH\" If cinnamon freezes # go to tty0 or tty1 using Alt + Ctrl + F1 or F2 login type w and enter look under FROM column, where row is related to cinnamon session note the value for that cell, in my case its :0 (with colon) note type export DISPLAY=:0 cinnamon & and enter sudo: unable to resolve host Ray: Connection timed out # Two things to check (assuming your machine is called my-machine, you can change this as appropriate): That the /etc/hostname file contains just the name of the machine. That /etc/hosts has an entry for localhost. It should have something like: 1 2 127 .0.0.1 localhost.localdomain localhost 127 .0.1.1 my-machine - If either of these files aren't correct (since you can't sudo), you may have to reboot the machine into recovery mode and make the modifications, then reboot to your usual environment. nemo context menu \"open in terminal\" not working # source: https://forums.linuxmint.com/viewtopic.php?t=204933 1 gsettings set org.cinnamon.desktop.default-applications.terminal exec mate-terminal Speedup mouse scroll # Source: https://forums.linuxmint.com/viewtopic.php?t=221526 1 sudo apt install imwheel then use mousewheel.sh to adjust the speed. Utilities # Multitouch Gesture # src: https://github.com/toransahu/libinput-gestures - by bulletmark 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 sudo gpasswd -a toran input sudo apt-get install xdotool wmctrl libinput-tools git clone http://github.com/toransahu/libinput-gestures cd libinput-gestures sudo ./libinput-gestures-setup install cd ~/libinput-gestures #edit the created libinput-gestures.conf: vim libinput-gestures.conf #set following: gesture swipe down xdotool key ctrl+alt+Up gesture swipe up xdotool key ctrl+alt+Down gesture swipe right xdotool key ctrl+super+Left gesture swipe left xdotool key ctrl+super+Right sudo make install (or sudo ./libinput-gestures-setup install) libinput-gestures-setup autostart libinput-gestures-setup start Battery Monitor # Source: http://battery-monitor.maateen.me/ Battery Optimization # SATA Power Management # source: https://askubuntu.com/questions/809127/how-to-turn-off-one-of-sata-hdds-installed-in-the-computer-to-save-power Using hdparm utility. It allows you to control your hard drives' power settings, apart from benchmarking and other stuff. put the disk into standby mode whenever you access the disk, it should automatically wake up and work back again 1 hdparm -y /dev/sdX deeper resting mode may need to restart your computer in order to make the HDD work again 1 hdparm -Y /dev/sdX PowerTop by Intel (Recommanded) # 1 2 3 4 sudo apt-get update sudo apt-get install powertop sudo powertop --auto-tune sudo powertop --calibrate TLP (Emergency) # Source: https://linrunner.de/en/tlp/docs/tlp-linux-advanced-power-management.html 1 2 3 4 5 6 sudo apt install tlp #restart #or sudo tlp start Mono # Install # https://www.mono-project.com/download/stable/#download-lin Verify # Source: https://www.mono-project.com/docs/getting-started/mono-basics/ Scripts: Thinkpad Touchpad Middle Button # Install # 1 sudo apt install xserver-xorg-input-libinput Config # restart alter touchpad setting and choose use multiple fingers in click actions Finger Print Reader # src: https://launchpad.net/~fingerprint/+archive/ubuntu/fingerprint-gui Installation # First of all, if you have installed Fingerprint GUI manually before, get rid of it completely. Remove all binaries, shared libraries, any other files and undo all the changes you have made to your system config files (especially to files under /etc/pam.d/). Add this PPA to your sources: 1 2 sudo add-apt-repository ppa:fingerprint/fingerprint-gui sudo apt-get update Install the packages: 1 sudo apt-get install libbsapi policykit-1-fingerprint-gui fingerprint-gui Log out of your session and log back in (we need the new session defaults to be picked up). Uninstallation # 1 2 sudo apt-get install policykit-1-gnome sudo apt-get remove fingerprint-gui ZSH # 1 sudo apt install zsh oh-my-zsh # 1 2 3 4 5 sh -c \" $( curl -fsSL https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh ) \" #OR sh -c \" $( wget https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh -O - ) \" VIM # Vundle # VIM plugin manager (VIM + Bundle) install from : https://github.com/VundleVim/Vundle.vim i.e. git clone https://github.com/VundleVim/Vundle.vim.git ~/.vim/bundle/Vundle.vim Using Vundle # Mention bundle names (as a github repo) in .vimrc file 1 2 3 4 5 6 7 8 9 Plugin 'VundleVim/Vundle.vim' Plugin 'morhetz/gruvbox' Plugin 'itchyny/lightline.vim' Plugin 'itchyny/vim-gitbranch' Plugin 'scrooloose/nerdtree.git' Plugin 'aperezdc/vim-template' Plugin 'davidhalter/jedi-vim' Plugin 'editorconfig/editorconfig-vim' Plugin 'tpope/vim-abolish' Install using command 1 :PluginInstall VIM as Python IDE # Source: http://chrisstrelioff.ws/sandbox/2016/09/21/vim_and_vundle_on_ubuntu_16_04.html ~/.vimrc & ~/.editorconfig config files @ https://gist.github.com/toransahu/66c69903649f3d417d8ba4dc78324f60 cheat sheet: https://gist.github.com/toransahu/4a921c6fb73274479dbdfbcfe6dda483 Tmux # Tmux Package Manager # Source: https://github.com/tmux-plugins/tpm ~/.tmux.conf : https://gist.github.com/toransahu/53a523d973a212f52ce53474417e01b1 Tmux Plugins: # Sidebar: https://tmuxcheatsheet.com/tmux-plugins-tools/?full_name=tmux-plugins%2Ftmux-sidebar Commands # abbr. prefix: ctrl + b help: ctrl + b ? select, copy, paste within tmux enter scroll mode: ctrl + b [ enter select mode: ctrl + space move cursor to select text copy to tmux buffer: alt + w copy to system clip: ctrl + b - y paste to any tmux: ctrl + b ] create/split pane vertical split: prefix % hor split: prefix \" convert a pane to window: prefix ! move to pane: prefix arrows close current pane: prefix x Load .bashrc with tmux # By default tmux runs a login shell. When bash is invoked as an interactive login shell, it looks for ~/.bash_profile, ~/.bash_login, and ~/.profile. So you have to put source ~/.bashrc in one of those files. Another way to solve this issue is to put in your file .tmux.conf the line: set-option -g default-shell \"/bin/bash\" KeyBoard Custom Shortcuts Bindings # Emulate right click from keyboard # install xdotool 1 sudo apt install xdotool bind keyboard shortcut with command: xdotool click 3 Backup & Restore Cinnamon settings # 1 2 3 4 5 6 7 8 9 10 11 sudo apt install dconf-cli #backup dconf dump /org/cinnamon/ > cinnamon_backup #restore dconf load /org/cinnamon/ < cinnamon_backup #reset to default dconf reset -f /org/cinnamon/ Raspberry Pi # OS # Raspbian with RPD Desktop # prepare https://www.raspberrypi.org/forums/viewtopic.php?t=135316 Raspbian Lite (without GUI) # Install Desktop for Lite # Source: https://www.raspberrypi.org/forums/viewtopic.php?t=133691 Enable ssh & enter SSID/WiFi details without keyboard # Source: https://medium.com/@danidudas/install-raspbian-jessie-lite-and-setup-wi-fi-without-access-to-command-line-or-using-the-network-97f065af722e Ubuntu Core/Snap # Windows IoT # Display # Touch Screeen Config # 5 inch capacitive 800*480 /boot/config.txt 1 2 3 4 5 6 framebuffer_width = 800 framebuffer_height = 480 hdmi_force_hotplug = 1 hdmi_group = 2 hdmi_mode = 87 hdmi_cvt 800 480 60 6 0 0 0 Backup Image from Burnt Image # On a different linux PC using USB. # It won't work on the raspi!, it will get trapped in infinit loop 1 2 3 4 5 6 7 8 #list disk sudo fdisk -l #backup SD card sudo dd bs = 4M if = /dev/sdb | gzip > /home/your_username/image ` date +%d%m%y ` .gz #restore the backup on SD card sudo gzip -dc /home/your_username/image.gz | dd bs = 4M of = /dev/sdb In Raspberry Itself using rsync # Network Drive Mount (sshfs) # 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 #Install SSHFS sudo apt-get install sshfs #First, create a directory on your host computer: mkdir pi #Then mount the Raspberry Pi's filesystem to this location: sudo sshfs -o allow_other pi@192.168.1.124:/ pi #Now enter this directory as if it is a regular folder; you should be able to see and access the contents of the Raspberry Pi: cd pi/ ls Remote Audio # bluetooth # Way 1 # 1 2 3 4 sudo apt-get install bluetooth blueman bluez python-gobject python-gobject-2 pulseaudio-module-bluetooth sudo systemctl status bluetooth pulseaudio --start https://gist.github.com/mill1000/74c7473ee3b4a5b13f6325e9994ff84c https://raspberrypi.stackexchange.com/questions/48140/raspberry-pi-3-connecting-to-bluetooth-audio-device-on-raspbian-jessie https://markus.jarvisalo.dy.fi/2017/12/making-the-raspberry-pi-3-a-bluetooth-audio-receiver/ Way 2 # https://www.instructables.com/id/Turn-your-Raspberry-Pi-into-a-Portable-Bluetooth-A/ 1 $ sudo vim /var/lib/bluetooth/00:1A:7D:DA:71:11/config wifi # shareport https://thepi.io/how-to-set-up-a-raspberry-pi-airplay-receiver/ enable ssh from card # put empty ssh file inside /boot/ set wifi/ssid from card # change inside /etc/network/interface 1 2 3 4 5 6 7 8 9 10 11 12 auto lo iface lo inet loopback iface eth0 inet dhcp allow-hotplug wlan0 auto wlan0 iface wlan0 inet dhcp wpa-ssid \"Connecting...\" wpa-psk \"PrideValencia_A704\" wifi to ethernet # src: https://www.instructables.com/id/Share-WiFi-With-Ethernet-Port-on-a-Raspberry-Pi/ 1 2 3 4 5 6 sudo apt install dnsmasq -y cd wget https://raw.githubusercontent.com/arpitjindal97/raspbian-recipes/master/wifi-to-eth-route.sh chmod +x ~/wifi-to-eth-route.sh sudo crontab -e @reboot bash /home/pi/wifi-to-eth-route.sh & # add it to crontab file","title":"Linux"},{"location":"Engineering/Software-Engineering/linux/#linux","text":"Linux Bash Scripting AWK include/source/import String Comparison Sub-String Comparison If, elif, else, fi Switch Case UNIX Commands mv all files/dir to a subdir Get System Informations Set deafult shell xclip ls prefixed commands find Set hostname 1. at /etc/hosts 2. at /etc/hostname 3. then tell the machine CPU Info CPU Temperature IP Related ssh In case of AWS EC2 SSH sftp sshfs Device connected to the Network dd (Disk Dump) commands Create a backup Restore a backup Clone a hard disk Transfer a disk image Create an iso image of a CD/DVD Burn an iso image of a CD/DVD Rescue a file that contains bad blocks Create your own bootloader Create a backup of your MBR Restore a backup of your MBR Mount dd image of and entire disk When the hard disk has errors Network Clone Network speed test Links Task Autorun/Automation Daemons Shells Graphical Cron Jobs Intro Syntax Operations Type Operators misc Services .desktop file using /etc/rc.local using /etc/init.d Disk Related fdisk lsblk blkid df du GRUB Edit grub configs grub update Reboot into other OS Problems & Solutions Make Anaconda Python Default If cinnamon freezes sudo: unable to resolve host Ray: Connection timed out nemo context menu \"open in terminal\" not working Speedup mouse scroll Utilities Multitouch Gesture Battery Monitor Battery Optimization SATA Power Management PowerTop by Intel (Recommanded) TLP (Emergency) Mono Install Verify Thinkpad Touchpad Middle Button Install Config Finger Print Reader Installation Uninstallation ZSH oh-my-zsh VIM Vundle Using Vundle VIM as Python IDE Tmux Tmux Package Manager Tmux Plugins: Commands Load .bashrc with tmux KeyBoard Custom Shortcuts Bindings Emulate right click from keyboard Backup & Restore Cinnamon settings Raspberry Pi OS Raspbian with RPD Desktop Raspbian Lite (without GUI) Install Desktop for Lite Enable ssh & enter SSID/WiFi details without keyboard Ubuntu Core/Snap Windows IoT Display Touch Screeen Config Backup Image from Burnt Image On a different linux PC using USB. In Raspberry Itself using rsync Network Drive Mount (sshfs) Remote Audio bluetooth Way 1 Way 2 wifi enable ssh from card set wifi/ssid from card wifi to ethernet","title":"Linux"},{"location":"Engineering/Software-Engineering/linux/#bash-scripting","text":"","title":"Bash Scripting"},{"location":"Engineering/Software-Engineering/linux/#awk","text":"https://www.gnu.org/software/gawk/manual/gawk.html","title":"AWK"},{"location":"Engineering/Software-Engineering/linux/#includesourceimport","text":"https://gist.github.com/toransahu/0a816af786b7d0b98ab1e7079bf0426f","title":"include/source/import"},{"location":"Engineering/Software-Engineering/linux/#string-comparison","text":"use = or == keep proper square spacing between operators & brackets (most imp) use double quote for strings and variables sometime we need to use brackets to access env variables https://gist.github.com/toransahu/4fd3abc369bb5c8a1ee424af07cb1563","title":"String Comparison"},{"location":"Engineering/Software-Engineering/linux/#sub-string-comparison","text":"1 2 3 string = \" $( hostname ) \" substr = \"mint\" if [ -z \" ${ string ##* $substr * } \" ] ; then https://gist.github.com/toransahu/f812260a37947a299c5c26adddfa4cfa","title":"Sub-String Comparison"},{"location":"Engineering/Software-Engineering/linux/#if-elif-else-fi","text":"Source: https://ryanstutorials.net/bash-scripting-tutorial/bash-if-statements.php","title":"If, elif, else, fi"},{"location":"Engineering/Software-Engineering/linux/#switch-case","text":"https://www.shellscript.sh/case.html","title":"Switch Case"},{"location":"Engineering/Software-Engineering/linux/#unix-commands","text":"","title":"UNIX Commands"},{"location":"Engineering/Software-Engineering/linux/#mv-all-filesdir-to-a-subdir","text":"BASH https://askubuntu.com/questions/91740/how-to-move-all-files-in-current-folder-to-subfolder 1 TODO: zsh 1 2 setopt extended_glob mv","title":"mv all files/dir to a subdir"},{"location":"Engineering/Software-Engineering/linux/#get-system-informations","text":"","title":"Get System Informations"},{"location":"Engineering/Software-Engineering/linux/#set-deafult-shell","text":"1 sudo chsh -s /bin/zsh then logout In Github:linux-tweaks/guides","title":"Set deafult shell"},{"location":"Engineering/Software-Engineering/linux/#xclip","text":"1 xclip -sel clip <file>","title":"xclip"},{"location":"Engineering/Software-Engineering/linux/#ls-prefixed-commands","text":"ls lsA lsblk lscpu lshw lsipc lslogins lsof lspcmcia lss16toppm lsusb lsa lsattr lsb_release lsdiff lsinitramfs lslocks lsmod lspci lspgpot lss3","title":"ls prefixed commands"},{"location":"Engineering/Software-Engineering/linux/#find","text":"1 2 3 find <dir> <type> <test> <expression> <action> find /Ray/ -type f -name \"*.pyc\" -delete","title":"find"},{"location":"Engineering/Software-Engineering/linux/#set-hostname","text":"","title":"Set hostname"},{"location":"Engineering/Software-Engineering/linux/#1-at-etchosts","text":"127.0.1.1 mint-ThinkPad-L440","title":"1. at /etc/hosts"},{"location":"Engineering/Software-Engineering/linux/#2-at-etchostname","text":"write mint-ThinkPad-L440","title":"2. at /etc/hostname"},{"location":"Engineering/Software-Engineering/linux/#3-then-tell-the-machine","text":"1 sudo hostname mint-ThinkPad-L440","title":"3. then tell the machine"},{"location":"Engineering/Software-Engineering/linux/#cpu-info","text":"1 cat flags /proc/cpuinfo","title":"CPU Info"},{"location":"Engineering/Software-Engineering/linux/#cpu-temperature","text":"1 sudo apt-get install lm-sensors After installation type the following in terminal 1 sudo sensors-detect You may also need to run 1 sudo service kmod start It will ask you few questions. Answer Yes for all of them. Finally to get your CPU temperature type sensors in your terminal. 1 sensors","title":"CPU Temperature"},{"location":"Engineering/Software-Engineering/linux/#ip-related","text":"ifconfig ip addr hostname -I","title":"IP Related"},{"location":"Engineering/Software-Engineering/linux/#ssh","text":"1 ssh pi@192.168.1.108","title":"ssh"},{"location":"Engineering/Software-Engineering/linux/#in-case-of-aws-ec2-ssh","text":"Connection freezes frequently on inactivity - source https://www.quora.com/Why-does-ssh-to-amazon-ec2-instance-gets-frozen-after-sometime - Solution - sudo vim /etc/ssh/ssh_config - add TCPKeepAlive yes - add ServerAliveInterval 5 where 5 is minute - sudo service sshd restart","title":"In case of AWS EC2 SSH"},{"location":"Engineering/Software-Engineering/linux/#sftp","text":"1 2 3 4 5 6 7 8 sftp pi@192.168.1.108 put local_file get remote_file files will upload/download @ home","title":"sftp"},{"location":"Engineering/Software-Engineering/linux/#sshfs","text":"1 2 3 4 sudo mkdir /mnt/pi sudo sshfs -o allow_other pi@192.168.1.108:/ /mnt/pi/","title":"sshfs"},{"location":"Engineering/Software-Engineering/linux/#device-connected-to-the-network","text":"1 nmap -sP 192 .168.1.0/24","title":"Device connected to the Network"},{"location":"Engineering/Software-Engineering/linux/#dd-disk-dump-commands","text":"","title":"dd (Disk Dump) commands"},{"location":"Engineering/Software-Engineering/linux/#create-a-backup","text":"dd if=/dev/sda of=/opt/backup_sda.img","title":"Create a backup"},{"location":"Engineering/Software-Engineering/linux/#restore-a-backup","text":"dd if=/opt/backup_sda.img of=/dev/sda","title":"Restore a backup"},{"location":"Engineering/Software-Engineering/linux/#clone-a-hard-disk","text":"dd if=/dev/sdb of=/dev/sdc","title":"Clone a hard disk"},{"location":"Engineering/Software-Engineering/linux/#transfer-a-disk-image","text":"dd if=/dev/sdb | ssh root@target \"(cat > backup.img)\"","title":"Transfer a disk image"},{"location":"Engineering/Software-Engineering/linux/#create-an-iso-image-of-a-cddvd","text":"dd if=/dev/cdrom of=cdimage.iso","title":"Create an iso image of a CD/DVD"},{"location":"Engineering/Software-Engineering/linux/#burn-an-iso-image-of-a-cddvd","text":"dd if=cdimage.iso of=/dev/cdrom obs=32k seek=0","title":"Burn an iso image of a CD/DVD"},{"location":"Engineering/Software-Engineering/linux/#rescue-a-file-that-contains-bad-blocks","text":"dd if=movie.avi of=rescued_movie.avi conv=noerror","title":"Rescue a file that contains bad blocks"},{"location":"Engineering/Software-Engineering/linux/#create-your-own-bootloader","text":"dd conv=notrunc if=bootloader of=qemu.img","title":"Create your own bootloader"},{"location":"Engineering/Software-Engineering/linux/#create-a-backup-of-your-mbr","text":"dd if=/dev/sdb of=mbr_backup bs=512 count=1","title":"Create a backup of your MBR"},{"location":"Engineering/Software-Engineering/linux/#restore-a-backup-of-your-mbr","text":"dd if=mbr_backup of=/dev/sdb bs=512 count=1","title":"Restore a backup of your MBR"},{"location":"Engineering/Software-Engineering/linux/#mount-dd-image-of-and-entire-disk","text":"You must use the start number of the partition. fdisk -u -l disk_image 1 2 3 4 5 6 7 8 Disk /mnt/storage/disk_image: 0 MB, 0 bytes255 heads, 63 sectors/track, 0 cylinders, total 0 sectors Units = sectors of 1 * 512 = 512 bytesDisk identifier: 0x41172ba5 Device Boot Start End Blocks Id System /mnt/storage/disk_image1 63 64259 32098 + de Dell Utility /mnt/storage/disk_image2 * 64260 78108029 39021885 7 HPFS/NTFS Partition 2 has different physical/logical endings:phys =( 1023 , 254 , 63 ) logical =( 4861 , 254 , 63 ) Then take the start of the partition that you want to edit, 64260 (disk_image2) in this case, and multiply it by 512 Ex: 512 * 64260 = 32901120 mount -o loop,offset=32901120 -t auto /mnt/storage/disk_image /mnt/image_partition_2","title":"Mount dd image of and entire disk"},{"location":"Engineering/Software-Engineering/linux/#when-the-hard-disk-has-errors","text":"Get the dd_rescue tool dd_rescue /dev/sdb /opt/backup_sdb.img","title":"When the hard disk has errors"},{"location":"Engineering/Software-Engineering/linux/#network-clone","text":"Destination: nc -l -p 2222 | dd of=/dev/sda bs=16M Source: dd if=/dev/sda bs=16M | nc Destination 2222","title":"Network Clone"},{"location":"Engineering/Software-Engineering/linux/#network-speed-test","text":"dd if=/dev/zero bs=1M count=100 | ssh user@machine 'cat > /dev/null'","title":"Network speed test"},{"location":"Engineering/Software-Engineering/linux/#links","text":"Symbolic link ln -s original link Hard Link ln original link","title":"Links"},{"location":"Engineering/Software-Engineering/linux/#task-autorunautomation","text":"src1 : https://developer.toradex.com/knowledge-base/how-to-autorun-application-at-the-start-up-in-linux","title":"Task Autorun/Automation"},{"location":"Engineering/Software-Engineering/linux/#daemons","text":"","title":"Daemons"},{"location":"Engineering/Software-Engineering/linux/#shells","text":"","title":"Shells"},{"location":"Engineering/Software-Engineering/linux/#graphical","text":"","title":"Graphical"},{"location":"Engineering/Software-Engineering/linux/#cron-jobs","text":"","title":"Cron Jobs"},{"location":"Engineering/Software-Engineering/linux/#intro","text":"The cron service (daemon) runs in the background and constantly checks the /etc/crontab file, and /etc/cron.*/ directories. It also checks the /var/spool/cron/ directory. Source create cron job crontab -e and edit the opened file add your task (in cron format) or, create a new cron job under /etc/cron.*/ add your task (in cron format)","title":"Intro"},{"location":"Engineering/Software-Engineering/linux/#syntax","text":"1 2 3 4 5 6 SHELL = /bin/bash PATH = /usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin MAILTO = \"\" 1 2 3 4 5 /path/to/command arg1 arg2 # or 1 2 3 4 5 /root/backup.sh ` to automate tasks based on time like on boot: @reboot hourly: @hourly or \u201c0 * * \u201d daily : @daily or @midnight or \u201c0 0 * \u201d weekly: @weekly or \u201c0 0 * * 0\u201d monthly: @monthly or \u201c0 0 1 * *\u201d yearly: @yearly or @annually or \u201c0 0 1 1 *\u201d command to be executed | | | | | | | | | ----- Day of week (0 - 7) (Sunday=0 or 7) | | | ------- Month (1 - 12) | | --------- Day of month (1 - 31) | ----------- Hour (0 - 23) ------------- Minute (0 - 59)","title":"Syntax"},{"location":"Engineering/Software-Engineering/linux/#operations","text":"start: sudo service cron start stop: sudo service cron stop restart: sudo service cron restart status: sudo service cron status list: crontab -l or crontab -u USERNAME -l delete all: crontab -r or crontab -u USERNAME -r","title":"Operations"},{"location":"Engineering/Software-Engineering/linux/#type","text":"System Level need to define USERNAME 1 2 3 4 5 USERNAME /path/to/command arg1 arg2 User Level by default takes USERNAME of creator 1 2 3 4 5 /path/to/command arg1 arg2","title":"Type"},{"location":"Engineering/Software-Engineering/linux/#operators","text":"asterisk (*) every [minute/hour/day/month/day of week] comma (,) list of values like, on this days: 1,2,3,4,5 dash (-) range like, on this days: 1-5 seperator (/) stepper like, */2 in hour field means: every 2 hours","title":"Operators"},{"location":"Engineering/Software-Engineering/linux/#misc","text":"print names of all valid files: run-parts --list /etc/cron.d print script names which would run, but don't run them: run-parts --test /etc/cron.d","title":"misc"},{"location":"Engineering/Software-Engineering/linux/#services","text":"","title":"Services"},{"location":"Engineering/Software-Engineering/linux/#desktop-file","text":"Works on boot put .desktop file with some script in ~/.config/autostart/ write script like 1 2 3 4 5 6 7 8 9 [ Desktop Entry ] Type = Application Exec = sh /home/toran/Applications/linux_tweaks/LowBatterySound/LowBattery.sh X-GNOME-Autostart-enabled = false NoDisplay = false Hidden = false Name [ en_IN ]= Low Battery Warning Comment [ en_IN ]= Warning sound on battery less than 15 % X-GNOME-Autostart-Delay = 20","title":".desktop file"},{"location":"Engineering/Software-Engineering/linux/#using-etcrclocal","text":"","title":"using /etc/rc.local"},{"location":"Engineering/Software-Engineering/linux/#using-etcinitd","text":"","title":"using /etc/init.d"},{"location":"Engineering/Software-Engineering/linux/#disk-related","text":"","title":"Disk Related"},{"location":"Engineering/Software-Engineering/linux/#fdisk","text":"manupulate disk partition table list devices: sudo fdisk -l","title":"fdisk"},{"location":"Engineering/Software-Engineering/linux/#lsblk","text":"lists block devices","title":"lsblk"},{"location":"Engineering/Software-Engineering/linux/#blkid","text":"locate/ print block device attributes","title":"blkid"},{"location":"Engineering/Software-Engineering/linux/#df","text":"reports filesystem disk space usage","title":"df"},{"location":"Engineering/Software-Engineering/linux/#du","text":"estimate file space usage Summarize disk usage of the set of FILEs, recursively for directories.","title":"du"},{"location":"Engineering/Software-Engineering/linux/#grub","text":"","title":"GRUB"},{"location":"Engineering/Software-Engineering/linux/#edit-grub-configs","text":"1 vim /etc/default/grub","title":"Edit grub configs"},{"location":"Engineering/Software-Engineering/linux/#grub-update","text":"1 sudo update-grub","title":"grub update"},{"location":"Engineering/Software-Engineering/linux/#reboot-into-other-os","text":"1 sudo grub-reboot 2","title":"Reboot into other OS"},{"location":"Engineering/Software-Engineering/linux/#problems-solutions","text":"","title":"Problems &amp; Solutions"},{"location":"Engineering/Software-Engineering/linux/#make-anaconda-python-default","text":"put following in .bashrc or .zshrc 1 2 # added by Anaconda3 installer export PATH=\"/home/toran/anaconda3/bin:$PATH\"","title":"Make Anaconda Python Default"},{"location":"Engineering/Software-Engineering/linux/#if-cinnamon-freezes","text":"go to tty0 or tty1 using Alt + Ctrl + F1 or F2 login type w and enter look under FROM column, where row is related to cinnamon session note the value for that cell, in my case its :0 (with colon) note type export DISPLAY=:0 cinnamon & and enter","title":"If cinnamon freezes"},{"location":"Engineering/Software-Engineering/linux/#sudo-unable-to-resolve-host-ray-connection-timed-out","text":"Two things to check (assuming your machine is called my-machine, you can change this as appropriate): That the /etc/hostname file contains just the name of the machine. That /etc/hosts has an entry for localhost. It should have something like: 1 2 127 .0.0.1 localhost.localdomain localhost 127 .0.1.1 my-machine - If either of these files aren't correct (since you can't sudo), you may have to reboot the machine into recovery mode and make the modifications, then reboot to your usual environment.","title":"sudo: unable to resolve host Ray: Connection timed out"},{"location":"Engineering/Software-Engineering/linux/#nemo-context-menu-open-in-terminal-not-working","text":"source: https://forums.linuxmint.com/viewtopic.php?t=204933 1 gsettings set org.cinnamon.desktop.default-applications.terminal exec mate-terminal","title":"nemo context menu \"open in terminal\" not working"},{"location":"Engineering/Software-Engineering/linux/#speedup-mouse-scroll","text":"Source: https://forums.linuxmint.com/viewtopic.php?t=221526 1 sudo apt install imwheel then use mousewheel.sh to adjust the speed.","title":"Speedup mouse scroll"},{"location":"Engineering/Software-Engineering/linux/#utilities","text":"","title":"Utilities"},{"location":"Engineering/Software-Engineering/linux/#multitouch-gesture","text":"src: https://github.com/toransahu/libinput-gestures - by bulletmark 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 sudo gpasswd -a toran input sudo apt-get install xdotool wmctrl libinput-tools git clone http://github.com/toransahu/libinput-gestures cd libinput-gestures sudo ./libinput-gestures-setup install cd ~/libinput-gestures #edit the created libinput-gestures.conf: vim libinput-gestures.conf #set following: gesture swipe down xdotool key ctrl+alt+Up gesture swipe up xdotool key ctrl+alt+Down gesture swipe right xdotool key ctrl+super+Left gesture swipe left xdotool key ctrl+super+Right sudo make install (or sudo ./libinput-gestures-setup install) libinput-gestures-setup autostart libinput-gestures-setup start","title":"Multitouch Gesture"},{"location":"Engineering/Software-Engineering/linux/#battery-monitor","text":"Source: http://battery-monitor.maateen.me/","title":"Battery Monitor"},{"location":"Engineering/Software-Engineering/linux/#battery-optimization","text":"","title":"Battery Optimization"},{"location":"Engineering/Software-Engineering/linux/#sata-power-management","text":"source: https://askubuntu.com/questions/809127/how-to-turn-off-one-of-sata-hdds-installed-in-the-computer-to-save-power Using hdparm utility. It allows you to control your hard drives' power settings, apart from benchmarking and other stuff. put the disk into standby mode whenever you access the disk, it should automatically wake up and work back again 1 hdparm -y /dev/sdX deeper resting mode may need to restart your computer in order to make the HDD work again 1 hdparm -Y /dev/sdX","title":"SATA Power Management"},{"location":"Engineering/Software-Engineering/linux/#powertop-by-intel-recommanded","text":"1 2 3 4 sudo apt-get update sudo apt-get install powertop sudo powertop --auto-tune sudo powertop --calibrate","title":"PowerTop by Intel (Recommanded)"},{"location":"Engineering/Software-Engineering/linux/#tlp-emergency","text":"Source: https://linrunner.de/en/tlp/docs/tlp-linux-advanced-power-management.html 1 2 3 4 5 6 sudo apt install tlp #restart #or sudo tlp start","title":"TLP (Emergency)"},{"location":"Engineering/Software-Engineering/linux/#mono","text":"","title":"Mono"},{"location":"Engineering/Software-Engineering/linux/#install","text":"https://www.mono-project.com/download/stable/#download-lin","title":"Install"},{"location":"Engineering/Software-Engineering/linux/#verify","text":"Source: https://www.mono-project.com/docs/getting-started/mono-basics/ Scripts:","title":"Verify"},{"location":"Engineering/Software-Engineering/linux/#thinkpad-touchpad-middle-button","text":"","title":"Thinkpad Touchpad Middle Button"},{"location":"Engineering/Software-Engineering/linux/#install_1","text":"1 sudo apt install xserver-xorg-input-libinput","title":"Install"},{"location":"Engineering/Software-Engineering/linux/#config","text":"restart alter touchpad setting and choose use multiple fingers in click actions","title":"Config"},{"location":"Engineering/Software-Engineering/linux/#finger-print-reader","text":"src: https://launchpad.net/~fingerprint/+archive/ubuntu/fingerprint-gui","title":"Finger Print Reader"},{"location":"Engineering/Software-Engineering/linux/#installation","text":"First of all, if you have installed Fingerprint GUI manually before, get rid of it completely. Remove all binaries, shared libraries, any other files and undo all the changes you have made to your system config files (especially to files under /etc/pam.d/). Add this PPA to your sources: 1 2 sudo add-apt-repository ppa:fingerprint/fingerprint-gui sudo apt-get update Install the packages: 1 sudo apt-get install libbsapi policykit-1-fingerprint-gui fingerprint-gui Log out of your session and log back in (we need the new session defaults to be picked up).","title":"Installation"},{"location":"Engineering/Software-Engineering/linux/#uninstallation","text":"1 2 sudo apt-get install policykit-1-gnome sudo apt-get remove fingerprint-gui","title":"Uninstallation"},{"location":"Engineering/Software-Engineering/linux/#zsh","text":"1 sudo apt install zsh","title":"ZSH"},{"location":"Engineering/Software-Engineering/linux/#oh-my-zsh","text":"1 2 3 4 5 sh -c \" $( curl -fsSL https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh ) \" #OR sh -c \" $( wget https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh -O - ) \"","title":"oh-my-zsh"},{"location":"Engineering/Software-Engineering/linux/#vim","text":"","title":"VIM"},{"location":"Engineering/Software-Engineering/linux/#vundle","text":"VIM plugin manager (VIM + Bundle) install from : https://github.com/VundleVim/Vundle.vim i.e. git clone https://github.com/VundleVim/Vundle.vim.git ~/.vim/bundle/Vundle.vim","title":"Vundle"},{"location":"Engineering/Software-Engineering/linux/#using-vundle","text":"Mention bundle names (as a github repo) in .vimrc file 1 2 3 4 5 6 7 8 9 Plugin 'VundleVim/Vundle.vim' Plugin 'morhetz/gruvbox' Plugin 'itchyny/lightline.vim' Plugin 'itchyny/vim-gitbranch' Plugin 'scrooloose/nerdtree.git' Plugin 'aperezdc/vim-template' Plugin 'davidhalter/jedi-vim' Plugin 'editorconfig/editorconfig-vim' Plugin 'tpope/vim-abolish' Install using command 1 :PluginInstall","title":"Using Vundle"},{"location":"Engineering/Software-Engineering/linux/#vim-as-python-ide","text":"Source: http://chrisstrelioff.ws/sandbox/2016/09/21/vim_and_vundle_on_ubuntu_16_04.html ~/.vimrc & ~/.editorconfig config files @ https://gist.github.com/toransahu/66c69903649f3d417d8ba4dc78324f60 cheat sheet: https://gist.github.com/toransahu/4a921c6fb73274479dbdfbcfe6dda483","title":"VIM as Python IDE"},{"location":"Engineering/Software-Engineering/linux/#tmux","text":"","title":"Tmux"},{"location":"Engineering/Software-Engineering/linux/#tmux-package-manager","text":"Source: https://github.com/tmux-plugins/tpm ~/.tmux.conf : https://gist.github.com/toransahu/53a523d973a212f52ce53474417e01b1","title":"Tmux Package Manager"},{"location":"Engineering/Software-Engineering/linux/#tmux-plugins","text":"Sidebar: https://tmuxcheatsheet.com/tmux-plugins-tools/?full_name=tmux-plugins%2Ftmux-sidebar","title":"Tmux Plugins:"},{"location":"Engineering/Software-Engineering/linux/#commands","text":"abbr. prefix: ctrl + b help: ctrl + b ? select, copy, paste within tmux enter scroll mode: ctrl + b [ enter select mode: ctrl + space move cursor to select text copy to tmux buffer: alt + w copy to system clip: ctrl + b - y paste to any tmux: ctrl + b ] create/split pane vertical split: prefix % hor split: prefix \" convert a pane to window: prefix ! move to pane: prefix arrows close current pane: prefix x","title":"Commands"},{"location":"Engineering/Software-Engineering/linux/#load-bashrc-with-tmux","text":"By default tmux runs a login shell. When bash is invoked as an interactive login shell, it looks for ~/.bash_profile, ~/.bash_login, and ~/.profile. So you have to put source ~/.bashrc in one of those files. Another way to solve this issue is to put in your file .tmux.conf the line: set-option -g default-shell \"/bin/bash\"","title":"Load .bashrc with tmux"},{"location":"Engineering/Software-Engineering/linux/#keyboard-custom-shortcuts-bindings","text":"","title":"KeyBoard Custom Shortcuts Bindings"},{"location":"Engineering/Software-Engineering/linux/#emulate-right-click-from-keyboard","text":"install xdotool 1 sudo apt install xdotool bind keyboard shortcut with command: xdotool click 3","title":"Emulate right click from keyboard"},{"location":"Engineering/Software-Engineering/linux/#backup-restore-cinnamon-settings","text":"1 2 3 4 5 6 7 8 9 10 11 sudo apt install dconf-cli #backup dconf dump /org/cinnamon/ > cinnamon_backup #restore dconf load /org/cinnamon/ < cinnamon_backup #reset to default dconf reset -f /org/cinnamon/","title":"Backup &amp; Restore Cinnamon settings"},{"location":"Engineering/Software-Engineering/linux/#raspberry-pi","text":"","title":"Raspberry Pi"},{"location":"Engineering/Software-Engineering/linux/#os","text":"","title":"OS"},{"location":"Engineering/Software-Engineering/linux/#raspbian-with-rpd-desktop","text":"prepare https://www.raspberrypi.org/forums/viewtopic.php?t=135316","title":"Raspbian with RPD Desktop"},{"location":"Engineering/Software-Engineering/linux/#raspbian-lite-without-gui","text":"","title":"Raspbian Lite (without GUI)"},{"location":"Engineering/Software-Engineering/linux/#install-desktop-for-lite","text":"Source: https://www.raspberrypi.org/forums/viewtopic.php?t=133691","title":"Install Desktop for Lite"},{"location":"Engineering/Software-Engineering/linux/#enable-ssh-enter-ssidwifi-details-without-keyboard","text":"Source: https://medium.com/@danidudas/install-raspbian-jessie-lite-and-setup-wi-fi-without-access-to-command-line-or-using-the-network-97f065af722e","title":"Enable ssh &amp; enter SSID/WiFi details without keyboard"},{"location":"Engineering/Software-Engineering/linux/#ubuntu-coresnap","text":"","title":"Ubuntu Core/Snap"},{"location":"Engineering/Software-Engineering/linux/#windows-iot","text":"","title":"Windows IoT"},{"location":"Engineering/Software-Engineering/linux/#display","text":"","title":"Display"},{"location":"Engineering/Software-Engineering/linux/#touch-screeen-config","text":"5 inch capacitive 800*480 /boot/config.txt 1 2 3 4 5 6 framebuffer_width = 800 framebuffer_height = 480 hdmi_force_hotplug = 1 hdmi_group = 2 hdmi_mode = 87 hdmi_cvt 800 480 60 6 0 0 0","title":"Touch Screeen Config"},{"location":"Engineering/Software-Engineering/linux/#backup-image-from-burnt-image","text":"","title":"Backup Image from Burnt Image"},{"location":"Engineering/Software-Engineering/linux/#on-a-different-linux-pc-using-usb","text":"It won't work on the raspi!, it will get trapped in infinit loop 1 2 3 4 5 6 7 8 #list disk sudo fdisk -l #backup SD card sudo dd bs = 4M if = /dev/sdb | gzip > /home/your_username/image ` date +%d%m%y ` .gz #restore the backup on SD card sudo gzip -dc /home/your_username/image.gz | dd bs = 4M of = /dev/sdb","title":"On a different linux PC using USB."},{"location":"Engineering/Software-Engineering/linux/#in-raspberry-itself-using-rsync","text":"","title":"In Raspberry Itself using rsync"},{"location":"Engineering/Software-Engineering/linux/#network-drive-mount-sshfs","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 #Install SSHFS sudo apt-get install sshfs #First, create a directory on your host computer: mkdir pi #Then mount the Raspberry Pi's filesystem to this location: sudo sshfs -o allow_other pi@192.168.1.124:/ pi #Now enter this directory as if it is a regular folder; you should be able to see and access the contents of the Raspberry Pi: cd pi/ ls","title":"Network Drive Mount (sshfs)"},{"location":"Engineering/Software-Engineering/linux/#remote-audio","text":"","title":"Remote Audio"},{"location":"Engineering/Software-Engineering/linux/#bluetooth","text":"","title":"bluetooth"},{"location":"Engineering/Software-Engineering/linux/#way-1","text":"1 2 3 4 sudo apt-get install bluetooth blueman bluez python-gobject python-gobject-2 pulseaudio-module-bluetooth sudo systemctl status bluetooth pulseaudio --start https://gist.github.com/mill1000/74c7473ee3b4a5b13f6325e9994ff84c https://raspberrypi.stackexchange.com/questions/48140/raspberry-pi-3-connecting-to-bluetooth-audio-device-on-raspbian-jessie https://markus.jarvisalo.dy.fi/2017/12/making-the-raspberry-pi-3-a-bluetooth-audio-receiver/","title":"Way 1"},{"location":"Engineering/Software-Engineering/linux/#way-2","text":"https://www.instructables.com/id/Turn-your-Raspberry-Pi-into-a-Portable-Bluetooth-A/ 1 $ sudo vim /var/lib/bluetooth/00:1A:7D:DA:71:11/config","title":"Way 2"},{"location":"Engineering/Software-Engineering/linux/#wifi","text":"shareport https://thepi.io/how-to-set-up-a-raspberry-pi-airplay-receiver/","title":"wifi"},{"location":"Engineering/Software-Engineering/linux/#enable-ssh-from-card","text":"put empty ssh file inside /boot/","title":"enable ssh from card"},{"location":"Engineering/Software-Engineering/linux/#set-wifissid-from-card","text":"change inside /etc/network/interface 1 2 3 4 5 6 7 8 9 10 11 12 auto lo iface lo inet loopback iface eth0 inet dhcp allow-hotplug wlan0 auto wlan0 iface wlan0 inet dhcp wpa-ssid \"Connecting...\" wpa-psk \"PrideValencia_A704\"","title":"set wifi/ssid  from card"},{"location":"Engineering/Software-Engineering/linux/#wifi-to-ethernet","text":"src: https://www.instructables.com/id/Share-WiFi-With-Ethernet-Port-on-a-Raspberry-Pi/ 1 2 3 4 5 6 sudo apt install dnsmasq -y cd wget https://raw.githubusercontent.com/arpitjindal97/raspbian-recipes/master/wifi-to-eth-route.sh chmod +x ~/wifi-to-eth-route.sh sudo crontab -e @reboot bash /home/pi/wifi-to-eth-route.sh & # add it to crontab file","title":"wifi to ethernet"},{"location":"Engineering/Software-Engineering/markup/","text":"Markup Languages # Markup Languages XML HTML JSON YAML JSON vs YAML TOML XML # extensible markup language HTML # hyper text markup language JSON # javascript object notion YAML # YAML ain't a markup language JSON vs YAML # Ref - https://www.lucidchart.com/techblog/2018/07/16/why-json-isnt-a-good-configuration-language/ TOML # Tom's obvious, minimal language","title":"Markup Languages"},{"location":"Engineering/Software-Engineering/markup/#markup-languages","text":"Markup Languages XML HTML JSON YAML JSON vs YAML TOML","title":"Markup Languages"},{"location":"Engineering/Software-Engineering/markup/#xml","text":"extensible markup language","title":"XML"},{"location":"Engineering/Software-Engineering/markup/#html","text":"hyper text markup language","title":"HTML"},{"location":"Engineering/Software-Engineering/markup/#json","text":"javascript object notion","title":"JSON"},{"location":"Engineering/Software-Engineering/markup/#yaml","text":"YAML ain't a markup language","title":"YAML"},{"location":"Engineering/Software-Engineering/markup/#json-vs-yaml","text":"Ref - https://www.lucidchart.com/techblog/2018/07/16/why-json-isnt-a-good-configuration-language/","title":"JSON vs YAML"},{"location":"Engineering/Software-Engineering/markup/#toml","text":"Tom's obvious, minimal language","title":"TOML"},{"location":"Engineering/Software-Engineering/practice_problems/","text":"Practice Problems # Practice Problems Numbers Classic Algorithms Graph Data Structures Text Networking Classes Threading Web Files Databases Graphics and Multimedia Security Numbers # Find PI to the Nth Digit - Enter a number and have the program generate PI up to that many decimal places. Keep a limit to how far the program will go. Find e to the Nth Digit - Just like the previous problem, but with e instead of PI. Enter a number and have the program generate e up to that many decimal places. Keep a limit to how far the program will go. Fibonacci Sequence - Enter a number and have the program generate the Fibonacci sequence to that number or to the Nth number. Prime Factorization - Have the user enter a number and find all Prime Factors (if there are any) and display them. Next Prime Number - Have the program find prime numbers until the user chooses to stop asking for the next one. Find Cost of Tile to Cover W x H Floor - Calculate the total cost of tile it would take to cover a floor plan of width and height, using a cost entered by the user. Mortgage Calculator - Calculate the monthly payments of a fixed term mortgage over given Nth terms at a given interest rate. Also figure out how long it will take the user to pay back the loan. For added complexity, add an option for users to select the compounding interval (Monthly, Weekly, Daily, Continually). Change Return Program - The user enters a cost and then the amount of money given. The program will figure out the change and the number of quarters, dimes, nickels, pennies needed for the change. Binary to Decimal and Back Converter - Develop a converter to convert a decimal number to binary or a binary number to its decimal equivalent. Calculator - A simple calculator to do basic operators. Make it a scientific calculator for added complexity. Unit Converter (temp, currency, volume, mass and more) - Converts various units between one another. The user enters the type of unit being entered, the type of unit they want to convert to and then the value. The program will then make the conversion. Alarm Clock - A simple clock where it plays a sound after X number of minutes/seconds or at a particular time. Distance Between Two Cities - Calculates the distance between two cities and allows the user to specify a unit of distance. This program may require finding coordinates for the cities like latitude and longitude. Credit Card Validator - Takes in a credit card number from a common credit card vendor (Visa, MasterCard, American Express, Discoverer) and validates it to make sure that it is a valid number (look into how credit cards use a checksum). Tax Calculator - Asks the user to enter a cost and either a country or state tax. It then returns the tax plus the total cost with tax. Factorial Finder - The Factorial of a positive integer, n, is defined as the product of the sequence n, n-1, n-2, ...1 and the factorial of zero, 0, is defined as being 1. Solve this using both loops and recursion. Complex Number Algebra - Show addition, multiplication, negation, and inversion of complex numbers in separate functions. (Subtraction and division operations can be made with pairs of these operations.) Print the results for each operation tested. Happy Numbers - A happy number is defined by the following process. Starting with any positive integer, replace the number by the sum of the squares of its digits, and repeat the process until the number equals 1 (where it will stay), or it loops endlessly in a cycle which does not include 1. Those numbers for which this process ends in 1 are happy numbers, while those that do not end in 1 are unhappy numbers. Display an example of your output here. Find first 8 happy numbers. Number Names - Show how to spell out a number in English. You can use a preexisting implementation or roll your own, but you should support inputs up to at least one million (or the maximum value of your language's default bounded integer type, if that's less). Optional: Support for inputs other than positive integers (like zero, negative integers, and floating-point numbers). Coin Flip Simulation - Write some code that simulates flipping a single coin however many times the user decides. The code should record the outcomes and count the number of tails and heads. Limit Calculator - Ask the user to enter f(x) and the limit value, then return the value of the limit statement Optional: Make the calculator capable of supporting infinite limits. Fast Exponentiation - Ask the user to enter 2 integers a and b and output a^b (i.e. pow(a,b)) in O(lg n) time complexity. Classic Algorithms # Collatz Conjecture - Start with a number n > 1. Find the number of steps it takes to reach one using the following process: If n is even, divide it by 2. If n is odd, multiply it by 3 and add 1. Sorting - Implement two types of sorting algorithms: Merge sort and bubble sort. Closest pair problem - The closest pair of points problem or closest pair problem is a problem of computational geometry: given n points in metric space, find a pair of points with the smallest distance between them. Sieve of Eratosthenes - The sieve of Eratosthenes is one of the most efficient ways to find all of the smaller primes (below 10 million or so). Graph # Graph from links - Create a program that will create a graph or network from a series of links. Eulerian Path - Create a program which will take as an input a graph and output either a Eulerian path or a Eulerian cycle, or state that it is not possible. A Eulerian Path starts at one node and traverses every edge of a graph through every node and finishes at another node. A Eulerian cycle is a eulerian Path that starts and finishes at the same node. Connected Graph - Create a program which takes a graph as an input and outputs whether every node is connected or not. Dijkstra\u00e2\u0080\u0099s Algorithm - Create a program that finds the shortest path through a graph using its edges. Minimum Spanning Tree - Create a program which takes a connected, undirected graph with weights and outputs the minimum spanning tree of the graph i.e., a subgraph that is a tree, contains all the vertices, and the sum of its weights is the least possible. Data Structures # Inverted index - An Inverted Index is a data structure used to create full text search. Given a set of text files, implement a program to create an inverted index. Also create a user interface to do a search using that inverted index which returns a list of files that contain the query term / terms. The search index can be in memory. Text # Fizz Buzz - Write a program that prints the numbers from 1 to 100. But for multiples of three print \u00e2\u0080\u009cFizz\u00e2\u0080\u009d instead of the number and for the multiples of five print \u00e2\u0080\u009cBuzz\u00e2\u0080\u009d. For numbers which are multiples of both three and five print \u00e2\u0080\u009cFizzBuzz\u00e2\u0080\u009d. Reverse a String - Enter a string and the program will reverse it and print it out. Pig Latin - Pig Latin is a game of alterations played on the English language game. To create the Pig Latin form of an English word the initial consonant sound is transposed to the end of the word and an ay is affixed (Ex.: \"banana\" would yield anana-bay). Read Wikipedia for more information on rules. Count Vowels - Enter a string and the program counts the number of vowels in the text. For added complexity have it report a sum of each vowel found. Check if Palindrome - Checks if the string entered by the user is a palindrome. That is that it reads the same forwards as backwards like \u00e2\u0080\u009cracecar\u00e2\u0080\u009d Count Words in a String - Counts the number of individual words in a string. For added complexity read these strings in from a text file and generate a summary. Text Editor - Notepad style application that can open, edit, and save text documents. Optional: Add syntax highlighting and other features. RSS Feed Creator - Given a link to RSS/Atom Feed, get all posts and display them. Quote Tracker (market symbols etc) - A program which can go out and check the current value of stocks for a list of symbols entered by the user. The user can set how often the stocks are checked. For CLI, show whether the stock has moved up or down. Optional: If GUI, the program can show green up and red down arrows to show which direction the stock value has moved. Guestbook / Journal - A simple application that allows people to add comments or write journal entries. It can allow comments or not and timestamps for all entries. Could also be made into a shout box. Optional: Deploy it on Google App Engine or Heroku or any other PaaS (if possible, of course). Vigenere / Vernam / Ceasar Ciphers - Functions for encrypting and decrypting data messages. Then send them to a friend. Regex Query Tool - A tool that allows the user to enter a text string and then in a separate control enter a regex pattern. It will run the regular expression against the source text and return any matches or flag errors in the regular expression. Networking # FTP Program - A file transfer program which can transfer files back and forth from a remote web sever. Bandwidth Monitor - A small utility program that tracks how much data you have uploaded and downloaded from the net during the course of your current online session. See if you can find out what periods of the day you use more and less and generate a report or graph that shows it. Port Scanner - Enter an IP address and a port range where the program will then attempt to find open ports on the given computer by connecting to each of them. On any successful connections mark the port as open. Mail Checker (POP3 / IMAP) - The user enters various account information include web server and IP, protocol type (POP3 or IMAP) and the application will check for email at a given interval. Country from IP Lookup - Enter an IP address and find the country that IP is registered in. Optional: Find the Ip automatically. Whois Search Tool - Enter an IP or host address and have it look it up through whois and return the results to you. Site Checker with Time Scheduling - An application that attempts to connect to a website or server every so many minutes or a given time and check if it is up. If it is down, it will notify you by email or by posting a notice on screen. Classes # Product Inventory Project - Create an application which manages an inventory of products. Create a product class which has a price, id, and quantity on hand. Then create an inventory class which keeps track of various products and can sum up the inventory value. Airline / Hotel Reservation System - Create a reservation system which books airline seats or hotel rooms. It charges various rates for particular sections of the plane or hotel. Example, first class is going to cost more than coach. Hotel rooms have penthouse suites which cost more. Keep track of when rooms will be available and can be scheduled. Company Manager - Create an hierarchy of classes - abstract class Employee and subclasses HourlyEmployee, SalariedEmployee, Manager and Executive. Every one's pay is calculated differently, research a bit about it. After you've established an employee hierarchy, create a Company class that allows you to manage the employees. You should be able to hire, fire and raise employees. Bank Account Manager - Create a class called Account which will be an abstract class for three other classes called CheckingAccount, SavingsAccount and BusinessAccount. Manage credits and debits from these accounts through an ATM style program. Patient / Doctor Scheduler - Create a patient class and a doctor class. Have a doctor that can handle multiple patients and setup a scheduling program where a doctor can only handle 16 patients during an 8 hr work day. Recipe Creator and Manager - Create a recipe class with ingredients and a put them in a recipe manager program that organizes them into categories like deserts, main courses or by ingredients like chicken, beef, soups, pies etc. Image Gallery - Create an image abstract class and then a class that inherits from it for each image type. Put them in a program which displays them in a gallery style format for viewing. Shape Area and Perimeter Classes - Create an abstract class called Shape and then inherit from it other shapes like diamond, rectangle, circle, triangle etc. Then have each class override the area and perimeter functionality to handle each shape type. Flower Shop Ordering To Go - Create a flower shop application which deals in flower objects and use those flower objects in a bouquet object which can then be sold. Keep track of the number of objects and when you may need to order more. Family Tree Creator - Create a class called Person which will have a name, when they were born and when (and if) they died. Allow the user to create these Person classes and put them into a family tree structure. Print out the tree to the screen. Threading # Create A Progress Bar for Downloads - Create a progress bar for applications that can keep track of a download in progress. The progress bar will be on a separate thread and will communicate with the main thread using delegates. Bulk Thumbnail Creator - Picture processing can take a bit of time for some transformations. Especially if the image is large. Create an image program which can take hundreds of images and converts them to a specified size in the background thread while you do other things. For added complexity, have one thread handling re-sizing, have another bulk renaming of thumbnails etc. Web # Page Scraper - Create an application which connects to a site and pulls out all links, or images, and saves them to a list. Optional: Organize the indexed content and don\u00e2\u0080\u0099t allow duplicates. Have it put the results into an easily searchable index file. Online White Board - Create an application which allows you to draw pictures, write notes and use various colors to flesh out ideas for projects. Optional: Add feature to invite friends to collaborate on a white board online. Get Atomic Time from Internet Clock - This program will get the true atomic time from an atomic time clock on the Internet. Use any one of the atomic clocks returned by a simple Google search. Fetch Current Weather - Get the current weather for a given zip/postal code. Optional: Try locating the user automatically. Scheduled Auto Login and Action - Make an application which logs into a given site on a schedule and invokes a certain action and then logs out. This can be useful for checking web mail, posting regular content, or getting info for other applications and saving it to your computer. E-Card Generator - Make a site that allows people to generate their own little e-cards and send them to other people. Do not use Flash. Use a picture library and perhaps insightful mottos or quotes. Content Management System - Create a content management system (CMS) like Joomla, Drupal, PHP Nuke etc. Start small. Optional: Allow for the addition of modules/addons. Web Board (Forum) - Create a forum for you and your buddies to post, administer and share thoughts and ideas. CAPTCHA Maker - Ever see those images with letters a numbers when you signup for a service and then asks you to enter what you see? It keeps web bots from automatically signing up and spamming. Try creating one yourself for online forms. Files # Quiz Maker - Make an application which takes various questions from a file, picked randomly, and puts together a quiz for students. Each quiz can be different and then reads a key to grade the quizzes. Sort Excel/CSV File Utility - Reads a file of records, sorts them, and then writes them back to the file. Allow the user to choose various sort style and sorting based on a particular field. Create Zip File Maker - The user enters various files from different directories and the program zips them up into a zip file. Optional: Apply actual compression to the files. Start with Huffman Algorithm. PDF Generator - An application which can read in a text file, html file or some other file and generates a PDF file out of it. Great for a web based service where the user uploads the file and the program returns a PDF of the file. Optional: Deploy on GAE or Heroku if possible. Mp3 Tagger - Modify and add ID3v1 tags to MP3 files. See if you can also add in the album art into the MP3 file\u00e2\u0080\u0099s header as well as other ID3v2 tags. Code Snippet Manager - Another utility program that allows coders to put in functions, classes or other tidbits to save for use later. Organized by the type of snippet or language the coder can quickly look up code. Optional: For extra practice try adding syntax highlighting based on the language. Databases # SQL Query Analyzer - A utility application which a user can enter a query and have it run against a local database and look for ways to make it more efficient. Remote SQL Tool - A utility that can execute queries on remote servers from your local computer across the Internet. It should take in a remote host, user name and password, run the query and return the results. Report Generator - Create a utility that generates a report based on some tables in a database. Generates a sales reports based on the order/order details tables or sums up the days current database activity. Event Scheduler and Calendar - Make an application which allows the user to enter a date and time of an event, event notes and then schedule those events on a calendar. The user can then browse the calendar or search the calendar for specific events. Optional: Allow the application to create re-occurrence events that reoccur every day, week, month, year etc. Budget Tracker - Write an application that keeps track of a household\u00e2\u0080\u0099s budget. The user can add expenses, income, and recurring costs to find out how much they are saving or losing over a period of time. Optional: Allow the user to specify a date range and see the net flow of money in and out of the house budget for that time period. TV Show Tracker - Got a favorite show you don\u00e2\u0080\u0099t want to miss? Don\u00e2\u0080\u0099t have a PVR or want to be able to find the show to then PVR it later? Make an application which can search various online TV Guide sites, locate the shows/times/channels and add them to a database application. The database/website then can send you email reminders that a show is about to start and which channel it will be on. Travel Planner System - Make a system that allows users to put together their own little travel itinerary and keep track of the airline / hotel arrangements, points of interest, budget and schedule. Graphics and Multimedia # Slide Show - Make an application that shows various pictures in a slide show format. Optional: Try adding various effects like fade in/out, star wipe and window blinds transitions. Stream Video from Online - Try to create your own online streaming video player. Mp3 Player - A simple program for playing your favorite music files. Add features you think are missing from your favorite music player. Watermarking Application - Have some pictures you want copyright protected? Add your own logo or text lightly across the background so that no one can simply steal your graphics off your site. Make a program that will add this watermark to the picture. Optional: Use threading to process multiple images simultaneously. Turtle Graphics - This is a common project where you create a floor of 20 x 20 squares. Using various commands you tell a turtle to draw a line on the floor. You have move forward, left or right, lift or drop pen etc. Do a search online for \"Turtle Graphics\" for more information. Optional: Allow the program to read in the list of commands from a file. GIF Creator A program that puts together multiple images (PNGs, JPGs, TIFFs) to make a smooth GIF that can be exported. Optional: Make the program convert small video files to GIFs as well. Security # Caesar cipher - Implement a Caesar cipher, both encoding and decoding. The key is an integer from 1 to 25. This cipher rotates the letters of the alphabet (A to Z). The encoding replaces each letter with the 1st to 25th next letter in the alphabet (wrapping Z to A). So key 2 encrypts \"HI\" to \"JK\", but key 20 encrypts \"HI\" to \"BC\". This simple \"monoalphabetic substitution cipher\" provides almost no security, because an attacker who has the encoded message can either use frequency analysis to guess the key, or just try all 25 keys.","title":"Practice Problems"},{"location":"Engineering/Software-Engineering/practice_problems/#practice-problems","text":"Practice Problems Numbers Classic Algorithms Graph Data Structures Text Networking Classes Threading Web Files Databases Graphics and Multimedia Security","title":"Practice Problems"},{"location":"Engineering/Software-Engineering/practice_problems/#numbers","text":"Find PI to the Nth Digit - Enter a number and have the program generate PI up to that many decimal places. Keep a limit to how far the program will go. Find e to the Nth Digit - Just like the previous problem, but with e instead of PI. Enter a number and have the program generate e up to that many decimal places. Keep a limit to how far the program will go. Fibonacci Sequence - Enter a number and have the program generate the Fibonacci sequence to that number or to the Nth number. Prime Factorization - Have the user enter a number and find all Prime Factors (if there are any) and display them. Next Prime Number - Have the program find prime numbers until the user chooses to stop asking for the next one. Find Cost of Tile to Cover W x H Floor - Calculate the total cost of tile it would take to cover a floor plan of width and height, using a cost entered by the user. Mortgage Calculator - Calculate the monthly payments of a fixed term mortgage over given Nth terms at a given interest rate. Also figure out how long it will take the user to pay back the loan. For added complexity, add an option for users to select the compounding interval (Monthly, Weekly, Daily, Continually). Change Return Program - The user enters a cost and then the amount of money given. The program will figure out the change and the number of quarters, dimes, nickels, pennies needed for the change. Binary to Decimal and Back Converter - Develop a converter to convert a decimal number to binary or a binary number to its decimal equivalent. Calculator - A simple calculator to do basic operators. Make it a scientific calculator for added complexity. Unit Converter (temp, currency, volume, mass and more) - Converts various units between one another. The user enters the type of unit being entered, the type of unit they want to convert to and then the value. The program will then make the conversion. Alarm Clock - A simple clock where it plays a sound after X number of minutes/seconds or at a particular time. Distance Between Two Cities - Calculates the distance between two cities and allows the user to specify a unit of distance. This program may require finding coordinates for the cities like latitude and longitude. Credit Card Validator - Takes in a credit card number from a common credit card vendor (Visa, MasterCard, American Express, Discoverer) and validates it to make sure that it is a valid number (look into how credit cards use a checksum). Tax Calculator - Asks the user to enter a cost and either a country or state tax. It then returns the tax plus the total cost with tax. Factorial Finder - The Factorial of a positive integer, n, is defined as the product of the sequence n, n-1, n-2, ...1 and the factorial of zero, 0, is defined as being 1. Solve this using both loops and recursion. Complex Number Algebra - Show addition, multiplication, negation, and inversion of complex numbers in separate functions. (Subtraction and division operations can be made with pairs of these operations.) Print the results for each operation tested. Happy Numbers - A happy number is defined by the following process. Starting with any positive integer, replace the number by the sum of the squares of its digits, and repeat the process until the number equals 1 (where it will stay), or it loops endlessly in a cycle which does not include 1. Those numbers for which this process ends in 1 are happy numbers, while those that do not end in 1 are unhappy numbers. Display an example of your output here. Find first 8 happy numbers. Number Names - Show how to spell out a number in English. You can use a preexisting implementation or roll your own, but you should support inputs up to at least one million (or the maximum value of your language's default bounded integer type, if that's less). Optional: Support for inputs other than positive integers (like zero, negative integers, and floating-point numbers). Coin Flip Simulation - Write some code that simulates flipping a single coin however many times the user decides. The code should record the outcomes and count the number of tails and heads. Limit Calculator - Ask the user to enter f(x) and the limit value, then return the value of the limit statement Optional: Make the calculator capable of supporting infinite limits. Fast Exponentiation - Ask the user to enter 2 integers a and b and output a^b (i.e. pow(a,b)) in O(lg n) time complexity.","title":"Numbers"},{"location":"Engineering/Software-Engineering/practice_problems/#classic-algorithms","text":"Collatz Conjecture - Start with a number n > 1. Find the number of steps it takes to reach one using the following process: If n is even, divide it by 2. If n is odd, multiply it by 3 and add 1. Sorting - Implement two types of sorting algorithms: Merge sort and bubble sort. Closest pair problem - The closest pair of points problem or closest pair problem is a problem of computational geometry: given n points in metric space, find a pair of points with the smallest distance between them. Sieve of Eratosthenes - The sieve of Eratosthenes is one of the most efficient ways to find all of the smaller primes (below 10 million or so).","title":"Classic Algorithms"},{"location":"Engineering/Software-Engineering/practice_problems/#graph","text":"Graph from links - Create a program that will create a graph or network from a series of links. Eulerian Path - Create a program which will take as an input a graph and output either a Eulerian path or a Eulerian cycle, or state that it is not possible. A Eulerian Path starts at one node and traverses every edge of a graph through every node and finishes at another node. A Eulerian cycle is a eulerian Path that starts and finishes at the same node. Connected Graph - Create a program which takes a graph as an input and outputs whether every node is connected or not. Dijkstra\u00e2\u0080\u0099s Algorithm - Create a program that finds the shortest path through a graph using its edges. Minimum Spanning Tree - Create a program which takes a connected, undirected graph with weights and outputs the minimum spanning tree of the graph i.e., a subgraph that is a tree, contains all the vertices, and the sum of its weights is the least possible.","title":"Graph"},{"location":"Engineering/Software-Engineering/practice_problems/#data-structures","text":"Inverted index - An Inverted Index is a data structure used to create full text search. Given a set of text files, implement a program to create an inverted index. Also create a user interface to do a search using that inverted index which returns a list of files that contain the query term / terms. The search index can be in memory.","title":"Data Structures"},{"location":"Engineering/Software-Engineering/practice_problems/#text","text":"Fizz Buzz - Write a program that prints the numbers from 1 to 100. But for multiples of three print \u00e2\u0080\u009cFizz\u00e2\u0080\u009d instead of the number and for the multiples of five print \u00e2\u0080\u009cBuzz\u00e2\u0080\u009d. For numbers which are multiples of both three and five print \u00e2\u0080\u009cFizzBuzz\u00e2\u0080\u009d. Reverse a String - Enter a string and the program will reverse it and print it out. Pig Latin - Pig Latin is a game of alterations played on the English language game. To create the Pig Latin form of an English word the initial consonant sound is transposed to the end of the word and an ay is affixed (Ex.: \"banana\" would yield anana-bay). Read Wikipedia for more information on rules. Count Vowels - Enter a string and the program counts the number of vowels in the text. For added complexity have it report a sum of each vowel found. Check if Palindrome - Checks if the string entered by the user is a palindrome. That is that it reads the same forwards as backwards like \u00e2\u0080\u009cracecar\u00e2\u0080\u009d Count Words in a String - Counts the number of individual words in a string. For added complexity read these strings in from a text file and generate a summary. Text Editor - Notepad style application that can open, edit, and save text documents. Optional: Add syntax highlighting and other features. RSS Feed Creator - Given a link to RSS/Atom Feed, get all posts and display them. Quote Tracker (market symbols etc) - A program which can go out and check the current value of stocks for a list of symbols entered by the user. The user can set how often the stocks are checked. For CLI, show whether the stock has moved up or down. Optional: If GUI, the program can show green up and red down arrows to show which direction the stock value has moved. Guestbook / Journal - A simple application that allows people to add comments or write journal entries. It can allow comments or not and timestamps for all entries. Could also be made into a shout box. Optional: Deploy it on Google App Engine or Heroku or any other PaaS (if possible, of course). Vigenere / Vernam / Ceasar Ciphers - Functions for encrypting and decrypting data messages. Then send them to a friend. Regex Query Tool - A tool that allows the user to enter a text string and then in a separate control enter a regex pattern. It will run the regular expression against the source text and return any matches or flag errors in the regular expression.","title":"Text"},{"location":"Engineering/Software-Engineering/practice_problems/#networking","text":"FTP Program - A file transfer program which can transfer files back and forth from a remote web sever. Bandwidth Monitor - A small utility program that tracks how much data you have uploaded and downloaded from the net during the course of your current online session. See if you can find out what periods of the day you use more and less and generate a report or graph that shows it. Port Scanner - Enter an IP address and a port range where the program will then attempt to find open ports on the given computer by connecting to each of them. On any successful connections mark the port as open. Mail Checker (POP3 / IMAP) - The user enters various account information include web server and IP, protocol type (POP3 or IMAP) and the application will check for email at a given interval. Country from IP Lookup - Enter an IP address and find the country that IP is registered in. Optional: Find the Ip automatically. Whois Search Tool - Enter an IP or host address and have it look it up through whois and return the results to you. Site Checker with Time Scheduling - An application that attempts to connect to a website or server every so many minutes or a given time and check if it is up. If it is down, it will notify you by email or by posting a notice on screen.","title":"Networking"},{"location":"Engineering/Software-Engineering/practice_problems/#classes","text":"Product Inventory Project - Create an application which manages an inventory of products. Create a product class which has a price, id, and quantity on hand. Then create an inventory class which keeps track of various products and can sum up the inventory value. Airline / Hotel Reservation System - Create a reservation system which books airline seats or hotel rooms. It charges various rates for particular sections of the plane or hotel. Example, first class is going to cost more than coach. Hotel rooms have penthouse suites which cost more. Keep track of when rooms will be available and can be scheduled. Company Manager - Create an hierarchy of classes - abstract class Employee and subclasses HourlyEmployee, SalariedEmployee, Manager and Executive. Every one's pay is calculated differently, research a bit about it. After you've established an employee hierarchy, create a Company class that allows you to manage the employees. You should be able to hire, fire and raise employees. Bank Account Manager - Create a class called Account which will be an abstract class for three other classes called CheckingAccount, SavingsAccount and BusinessAccount. Manage credits and debits from these accounts through an ATM style program. Patient / Doctor Scheduler - Create a patient class and a doctor class. Have a doctor that can handle multiple patients and setup a scheduling program where a doctor can only handle 16 patients during an 8 hr work day. Recipe Creator and Manager - Create a recipe class with ingredients and a put them in a recipe manager program that organizes them into categories like deserts, main courses or by ingredients like chicken, beef, soups, pies etc. Image Gallery - Create an image abstract class and then a class that inherits from it for each image type. Put them in a program which displays them in a gallery style format for viewing. Shape Area and Perimeter Classes - Create an abstract class called Shape and then inherit from it other shapes like diamond, rectangle, circle, triangle etc. Then have each class override the area and perimeter functionality to handle each shape type. Flower Shop Ordering To Go - Create a flower shop application which deals in flower objects and use those flower objects in a bouquet object which can then be sold. Keep track of the number of objects and when you may need to order more. Family Tree Creator - Create a class called Person which will have a name, when they were born and when (and if) they died. Allow the user to create these Person classes and put them into a family tree structure. Print out the tree to the screen.","title":"Classes"},{"location":"Engineering/Software-Engineering/practice_problems/#threading","text":"Create A Progress Bar for Downloads - Create a progress bar for applications that can keep track of a download in progress. The progress bar will be on a separate thread and will communicate with the main thread using delegates. Bulk Thumbnail Creator - Picture processing can take a bit of time for some transformations. Especially if the image is large. Create an image program which can take hundreds of images and converts them to a specified size in the background thread while you do other things. For added complexity, have one thread handling re-sizing, have another bulk renaming of thumbnails etc.","title":"Threading"},{"location":"Engineering/Software-Engineering/practice_problems/#web","text":"Page Scraper - Create an application which connects to a site and pulls out all links, or images, and saves them to a list. Optional: Organize the indexed content and don\u00e2\u0080\u0099t allow duplicates. Have it put the results into an easily searchable index file. Online White Board - Create an application which allows you to draw pictures, write notes and use various colors to flesh out ideas for projects. Optional: Add feature to invite friends to collaborate on a white board online. Get Atomic Time from Internet Clock - This program will get the true atomic time from an atomic time clock on the Internet. Use any one of the atomic clocks returned by a simple Google search. Fetch Current Weather - Get the current weather for a given zip/postal code. Optional: Try locating the user automatically. Scheduled Auto Login and Action - Make an application which logs into a given site on a schedule and invokes a certain action and then logs out. This can be useful for checking web mail, posting regular content, or getting info for other applications and saving it to your computer. E-Card Generator - Make a site that allows people to generate their own little e-cards and send them to other people. Do not use Flash. Use a picture library and perhaps insightful mottos or quotes. Content Management System - Create a content management system (CMS) like Joomla, Drupal, PHP Nuke etc. Start small. Optional: Allow for the addition of modules/addons. Web Board (Forum) - Create a forum for you and your buddies to post, administer and share thoughts and ideas. CAPTCHA Maker - Ever see those images with letters a numbers when you signup for a service and then asks you to enter what you see? It keeps web bots from automatically signing up and spamming. Try creating one yourself for online forms.","title":"Web"},{"location":"Engineering/Software-Engineering/practice_problems/#files","text":"Quiz Maker - Make an application which takes various questions from a file, picked randomly, and puts together a quiz for students. Each quiz can be different and then reads a key to grade the quizzes. Sort Excel/CSV File Utility - Reads a file of records, sorts them, and then writes them back to the file. Allow the user to choose various sort style and sorting based on a particular field. Create Zip File Maker - The user enters various files from different directories and the program zips them up into a zip file. Optional: Apply actual compression to the files. Start with Huffman Algorithm. PDF Generator - An application which can read in a text file, html file or some other file and generates a PDF file out of it. Great for a web based service where the user uploads the file and the program returns a PDF of the file. Optional: Deploy on GAE or Heroku if possible. Mp3 Tagger - Modify and add ID3v1 tags to MP3 files. See if you can also add in the album art into the MP3 file\u00e2\u0080\u0099s header as well as other ID3v2 tags. Code Snippet Manager - Another utility program that allows coders to put in functions, classes or other tidbits to save for use later. Organized by the type of snippet or language the coder can quickly look up code. Optional: For extra practice try adding syntax highlighting based on the language.","title":"Files"},{"location":"Engineering/Software-Engineering/practice_problems/#databases","text":"SQL Query Analyzer - A utility application which a user can enter a query and have it run against a local database and look for ways to make it more efficient. Remote SQL Tool - A utility that can execute queries on remote servers from your local computer across the Internet. It should take in a remote host, user name and password, run the query and return the results. Report Generator - Create a utility that generates a report based on some tables in a database. Generates a sales reports based on the order/order details tables or sums up the days current database activity. Event Scheduler and Calendar - Make an application which allows the user to enter a date and time of an event, event notes and then schedule those events on a calendar. The user can then browse the calendar or search the calendar for specific events. Optional: Allow the application to create re-occurrence events that reoccur every day, week, month, year etc. Budget Tracker - Write an application that keeps track of a household\u00e2\u0080\u0099s budget. The user can add expenses, income, and recurring costs to find out how much they are saving or losing over a period of time. Optional: Allow the user to specify a date range and see the net flow of money in and out of the house budget for that time period. TV Show Tracker - Got a favorite show you don\u00e2\u0080\u0099t want to miss? Don\u00e2\u0080\u0099t have a PVR or want to be able to find the show to then PVR it later? Make an application which can search various online TV Guide sites, locate the shows/times/channels and add them to a database application. The database/website then can send you email reminders that a show is about to start and which channel it will be on. Travel Planner System - Make a system that allows users to put together their own little travel itinerary and keep track of the airline / hotel arrangements, points of interest, budget and schedule.","title":"Databases"},{"location":"Engineering/Software-Engineering/practice_problems/#graphics-and-multimedia","text":"Slide Show - Make an application that shows various pictures in a slide show format. Optional: Try adding various effects like fade in/out, star wipe and window blinds transitions. Stream Video from Online - Try to create your own online streaming video player. Mp3 Player - A simple program for playing your favorite music files. Add features you think are missing from your favorite music player. Watermarking Application - Have some pictures you want copyright protected? Add your own logo or text lightly across the background so that no one can simply steal your graphics off your site. Make a program that will add this watermark to the picture. Optional: Use threading to process multiple images simultaneously. Turtle Graphics - This is a common project where you create a floor of 20 x 20 squares. Using various commands you tell a turtle to draw a line on the floor. You have move forward, left or right, lift or drop pen etc. Do a search online for \"Turtle Graphics\" for more information. Optional: Allow the program to read in the list of commands from a file. GIF Creator A program that puts together multiple images (PNGs, JPGs, TIFFs) to make a smooth GIF that can be exported. Optional: Make the program convert small video files to GIFs as well.","title":"Graphics and Multimedia"},{"location":"Engineering/Software-Engineering/practice_problems/#security","text":"Caesar cipher - Implement a Caesar cipher, both encoding and decoding. The key is an integer from 1 to 25. This cipher rotates the letters of the alphabet (A to Z). The encoding replaces each letter with the 1st to 25th next letter in the alphabet (wrapping Z to A). So key 2 encrypts \"HI\" to \"JK\", but key 20 encrypts \"HI\" to \"BC\". This simple \"monoalphabetic substitution cipher\" provides almost no security, because an attacker who has the encoded message can either use frequency analysis to guess the key, or just try all 25 keys.","title":"Security"},{"location":"Engineering/Software-Engineering/Architecture/design_patterns/","text":"Design Patterns # Table of Contents Design Patterns What How we discovered? Why When Classification Builder Pattern What Why How When Factory Pattern What Why How When Thoughts Abstract Factory What Why How When Thoughts Singleton Pattern What Why How When Cons Prototype What Why How When Cons Object Pool What Why How When Cons What # a model solution to common design problems which we encouter often, occurs over-and-over describes the problem, and a genereal approach to solve them How we discovered? # Why # to ensure our work is consistent reliable understandable we dont want to re-invent the wheel, just check if for current problem there is design solution When # Classification # Creational (helps create objects in oops world) Builder Factory Abstract factory Singleton Prototype Object Pool Structural (helps design relationship between the objects) Adaptor Repository Facade Composite Behavioral (helps design object interaction and responsibilites; control the operation of some objects) Strategy / Policy Observer / Dependant / PubSub Command / Action / Transaction Concurrency No Pattern (python only?) Mono-State Pattern (python only?) Visitor Pattern Builder Pattern # What # helps separate the construction of a complex object e.g. a custom/assembled computer encapsulates the construction of the object ie. follows this: \"encapsulate what varies\" - hence single responsibility principle allows multistep construction process (main property) Why # separates the \"how\" from \"what\" assembly separates from components encapsulates what varies - the parts permits diff. representations can leverage Director concept, e.g. to manage order of building attributes builder adds parts to the products How # When # Factory Pattern # What # define interface for creating an object lets subclasses decide which object to create it achieves it through factory() method makes subclasses responsible for instantiation / defer instantiation to subclasses also known as Virtual Constructor Why # when we want to build/instantiate and return at runtime based on some params How # When # Thoughts # i think dependency-inversion could not be solved Abstract Factory # What # whereas Factory creates from one product class, AbstractFactory can produce from family of product class enforces dependencies between concrete class defer instantiation to subclasses also knows as Kit pattern Why # How # When # Thoughts # i think dependency-inversion could not be solved Singleton Pattern # What # ensures to create a single object/instance of a class .e.g. Web/DB connection pool, device access, buffer pools sometimes based on implementation - objects (its memory loc) could be diff. but there state would remain same (see monostate e.g.) responsible for creating that instance provides a global point of access can also facilitate \"lazy instantiation\", if instantiation is costly Why # read What disallowing creation on new instance reduces the global namespace, as it self it global subsclassible for extended purposes variable number of instances - as all of them are same only e.g. base class + meta class variants more flexible than a static class, since we can have multiple instances (in python, static class is one with no instances) monostate variant can share all/full state How # When # Cons # violates single-responsibilty principle how? it does it own task + creates and stores itw own instance as well non-standard class access / design (who knows there is .instance() static method ) harder to test ?? why?? carry a global state hard to sub-class ?? why - just mandatoryily override the .instance() ?? else don't follow .instance() design singletons are considered harmful by experts and are called antipattern Prototype # What # Why # How # When # Cons # Object Pool # What # Why # How # When # Cons # structural (helps design relationship between the objects) adaptor what why how when facade what why how when composite what why how when behavioral (helps design object interaction and responsibilites; control the operation of some objects) strategy / policy what why how when observer / dependant / pubsub what defines one-to-many relationship between a set of objects so that when the state of one changes, all its dependents are notified e.g. Lets say there is a News class to which many Reader can subscribe to here News is a Subject class and Reader is an Observer class many Observers can subscribe to the Subject the Subject should have facility to attach , detach , and notify the Observers we can implement a Push model using this pattern why separates the concerns subject, object, and main program acheives SOLID single-responsibilty individual classes for each interface-segregation by creating abstract classes for Subject and Object open-closed Subject and Observer are open for extension but closed for modification dependency-inversion Subject and Observer are programmed towards abstraction and not implementation note its a good idea to handle automatic detach of observers once they finish this will avoid memory leak and help automatic garbage collector by removing dangling references command / action / transaction usage CLI, GUI why encapsulate behaviors seperate command logic from client easiliy provide/provision additional capabilities validate undo how encapsulate a request as an object and facilitates to parameterize an object for diff. requests supports queues and logs queues for: updating a DB executing a list of commands can bake support for undoable operations and macros macros (sequences of commands) concurrency what why how when no pattern (python only?) what why in python helps avoid check for none how when mono-state pattern (python only?) what every instance of the class should have the same state (say if state is a dict, then all objects will have same dict) why one ans. is to achieve singleton how create a class dict state ; update that dict with instance inside __new__ ; add state to instance.__dict__ this way, first, second, .. last every instance will have same state when singleton many other use cases visitor pattern","title":"Design Patterns"},{"location":"Engineering/Software-Engineering/Architecture/design_patterns/#design-patterns","text":"","title":"Design Patterns"},{"location":"Engineering/Software-Engineering/Architecture/design_patterns/#what","text":"a model solution to common design problems which we encouter often, occurs over-and-over describes the problem, and a genereal approach to solve them","title":"What"},{"location":"Engineering/Software-Engineering/Architecture/design_patterns/#how-we-discovered","text":"","title":"How we discovered?"},{"location":"Engineering/Software-Engineering/Architecture/design_patterns/#why","text":"to ensure our work is consistent reliable understandable we dont want to re-invent the wheel, just check if for current problem there is design solution","title":"Why"},{"location":"Engineering/Software-Engineering/Architecture/design_patterns/#when","text":"","title":"When"},{"location":"Engineering/Software-Engineering/Architecture/design_patterns/#classification","text":"Creational (helps create objects in oops world) Builder Factory Abstract factory Singleton Prototype Object Pool Structural (helps design relationship between the objects) Adaptor Repository Facade Composite Behavioral (helps design object interaction and responsibilites; control the operation of some objects) Strategy / Policy Observer / Dependant / PubSub Command / Action / Transaction Concurrency No Pattern (python only?) Mono-State Pattern (python only?) Visitor Pattern","title":"Classification"},{"location":"Engineering/Software-Engineering/Architecture/design_patterns/#builder-pattern","text":"","title":"Builder Pattern"},{"location":"Engineering/Software-Engineering/Architecture/design_patterns/#what_1","text":"helps separate the construction of a complex object e.g. a custom/assembled computer encapsulates the construction of the object ie. follows this: \"encapsulate what varies\" - hence single responsibility principle allows multistep construction process (main property)","title":"What"},{"location":"Engineering/Software-Engineering/Architecture/design_patterns/#why_1","text":"separates the \"how\" from \"what\" assembly separates from components encapsulates what varies - the parts permits diff. representations can leverage Director concept, e.g. to manage order of building attributes builder adds parts to the products","title":"Why"},{"location":"Engineering/Software-Engineering/Architecture/design_patterns/#how","text":"","title":"How"},{"location":"Engineering/Software-Engineering/Architecture/design_patterns/#when_1","text":"","title":"When"},{"location":"Engineering/Software-Engineering/Architecture/design_patterns/#factory-pattern","text":"","title":"Factory Pattern"},{"location":"Engineering/Software-Engineering/Architecture/design_patterns/#what_2","text":"define interface for creating an object lets subclasses decide which object to create it achieves it through factory() method makes subclasses responsible for instantiation / defer instantiation to subclasses also known as Virtual Constructor","title":"What"},{"location":"Engineering/Software-Engineering/Architecture/design_patterns/#why_2","text":"when we want to build/instantiate and return at runtime based on some params","title":"Why"},{"location":"Engineering/Software-Engineering/Architecture/design_patterns/#how_1","text":"","title":"How"},{"location":"Engineering/Software-Engineering/Architecture/design_patterns/#when_2","text":"","title":"When"},{"location":"Engineering/Software-Engineering/Architecture/design_patterns/#thoughts","text":"i think dependency-inversion could not be solved","title":"Thoughts"},{"location":"Engineering/Software-Engineering/Architecture/design_patterns/#abstract-factory","text":"","title":"Abstract Factory"},{"location":"Engineering/Software-Engineering/Architecture/design_patterns/#what_3","text":"whereas Factory creates from one product class, AbstractFactory can produce from family of product class enforces dependencies between concrete class defer instantiation to subclasses also knows as Kit pattern","title":"What"},{"location":"Engineering/Software-Engineering/Architecture/design_patterns/#why_3","text":"","title":"Why"},{"location":"Engineering/Software-Engineering/Architecture/design_patterns/#how_2","text":"","title":"How"},{"location":"Engineering/Software-Engineering/Architecture/design_patterns/#when_3","text":"","title":"When"},{"location":"Engineering/Software-Engineering/Architecture/design_patterns/#thoughts_1","text":"i think dependency-inversion could not be solved","title":"Thoughts"},{"location":"Engineering/Software-Engineering/Architecture/design_patterns/#singleton-pattern","text":"","title":"Singleton Pattern"},{"location":"Engineering/Software-Engineering/Architecture/design_patterns/#what_4","text":"ensures to create a single object/instance of a class .e.g. Web/DB connection pool, device access, buffer pools sometimes based on implementation - objects (its memory loc) could be diff. but there state would remain same (see monostate e.g.) responsible for creating that instance provides a global point of access can also facilitate \"lazy instantiation\", if instantiation is costly","title":"What"},{"location":"Engineering/Software-Engineering/Architecture/design_patterns/#why_4","text":"read What disallowing creation on new instance reduces the global namespace, as it self it global subsclassible for extended purposes variable number of instances - as all of them are same only e.g. base class + meta class variants more flexible than a static class, since we can have multiple instances (in python, static class is one with no instances) monostate variant can share all/full state","title":"Why"},{"location":"Engineering/Software-Engineering/Architecture/design_patterns/#how_3","text":"","title":"How"},{"location":"Engineering/Software-Engineering/Architecture/design_patterns/#when_4","text":"","title":"When"},{"location":"Engineering/Software-Engineering/Architecture/design_patterns/#cons","text":"violates single-responsibilty principle how? it does it own task + creates and stores itw own instance as well non-standard class access / design (who knows there is .instance() static method ) harder to test ?? why?? carry a global state hard to sub-class ?? why - just mandatoryily override the .instance() ?? else don't follow .instance() design singletons are considered harmful by experts and are called antipattern","title":"Cons"},{"location":"Engineering/Software-Engineering/Architecture/design_patterns/#prototype","text":"","title":"Prototype"},{"location":"Engineering/Software-Engineering/Architecture/design_patterns/#what_5","text":"","title":"What"},{"location":"Engineering/Software-Engineering/Architecture/design_patterns/#why_5","text":"","title":"Why"},{"location":"Engineering/Software-Engineering/Architecture/design_patterns/#how_4","text":"","title":"How"},{"location":"Engineering/Software-Engineering/Architecture/design_patterns/#when_5","text":"","title":"When"},{"location":"Engineering/Software-Engineering/Architecture/design_patterns/#cons_1","text":"","title":"Cons"},{"location":"Engineering/Software-Engineering/Architecture/design_patterns/#object-pool","text":"","title":"Object Pool"},{"location":"Engineering/Software-Engineering/Architecture/design_patterns/#what_6","text":"","title":"What"},{"location":"Engineering/Software-Engineering/Architecture/design_patterns/#why_6","text":"","title":"Why"},{"location":"Engineering/Software-Engineering/Architecture/design_patterns/#how_5","text":"","title":"How"},{"location":"Engineering/Software-Engineering/Architecture/design_patterns/#when_6","text":"","title":"When"},{"location":"Engineering/Software-Engineering/Architecture/design_patterns/#cons_2","text":"structural (helps design relationship between the objects) adaptor what why how when facade what why how when composite what why how when behavioral (helps design object interaction and responsibilites; control the operation of some objects) strategy / policy what why how when observer / dependant / pubsub what defines one-to-many relationship between a set of objects so that when the state of one changes, all its dependents are notified e.g. Lets say there is a News class to which many Reader can subscribe to here News is a Subject class and Reader is an Observer class many Observers can subscribe to the Subject the Subject should have facility to attach , detach , and notify the Observers we can implement a Push model using this pattern why separates the concerns subject, object, and main program acheives SOLID single-responsibilty individual classes for each interface-segregation by creating abstract classes for Subject and Object open-closed Subject and Observer are open for extension but closed for modification dependency-inversion Subject and Observer are programmed towards abstraction and not implementation note its a good idea to handle automatic detach of observers once they finish this will avoid memory leak and help automatic garbage collector by removing dangling references command / action / transaction usage CLI, GUI why encapsulate behaviors seperate command logic from client easiliy provide/provision additional capabilities validate undo how encapsulate a request as an object and facilitates to parameterize an object for diff. requests supports queues and logs queues for: updating a DB executing a list of commands can bake support for undoable operations and macros macros (sequences of commands) concurrency what why how when no pattern (python only?) what why in python helps avoid check for none how when mono-state pattern (python only?) what every instance of the class should have the same state (say if state is a dict, then all objects will have same dict) why one ans. is to achieve singleton how create a class dict state ; update that dict with instance inside __new__ ; add state to instance.__dict__ this way, first, second, .. last every instance will have same state when singleton many other use cases visitor pattern","title":"Cons"},{"location":"Engineering/Software-Engineering/Architecture/design_principles/","text":"Design Principles # Table of Contents Design Principles Programming Zens / Rules / Principles Python's Zen Rob Pike's 5 Rules of Programming SOLID Single Responsibilty Principle What Issue Benefit Notes Open-Closed Principle Issue Benefit Liskov Substitution Interface Segregation What Issue Benefit How Dependency Inversion What Ref DRY WET KISS YAGNI Programming Zens / Rules / Principles # Python's Zen # Refer this . Rob Pike's 5 Rules of Programming # Rule 1. You can't tell where a program is going to spend its time. Bottlenecks occur in surprising places, so don't try to second guess and put in a speed hack until you've proven that's where the bottleneck is. Rule 2. Measure. Don't tune for speed until you've measured, and even then don't unless one part of the code overwhelms the rest. Rule 3. Fancy algorithms are slow when n is small, and n is usually small. Fancy algorithms have big constants. Until you know that n is frequently going to be big, don't get fancy. (Even if n does get big, use Rule 2 first.) Rule 4. Fancy algorithms are buggier than simple ones, and they're much harder to implement. Use simple algorithms as well as simple data structures. Rule 5. Data dominates. If you've chosen the right data structures and organized things well, the algorithms will almost always be self-evident. Data structures, not algorithms, are central to programming. Pike's rules 1 and 2 restate Tony Hoare's famous maxim \"Premature optimization is the root of all evil.\" Ken Thompson rephrased Pike's rules 3 and 4 as \"When in doubt, use brute force.\". Rules 3 and 4 are instances of the design philosophy KISS. Rule 5 was previously stated by Fred Brooks in The Mythical Man-Month. Rule 5 is often shortened to \"write stupid code that uses smart objects\". Ref: - https://users.ece.utexas.edu/~adnan/pike.html - https://github.com/robpike SOLID # Single Responsibilty Open/Closed Liskov Substitution Interface Segregation Dependency Inversion Most popular sets of design principle in OOPs software engineering. Promoter: Robert C. Martin (Uncle Bob), 2000 Single Responsibilty Principle # A class should have one, and only one, reason to change. - Robert C. Martin, 2000 What # a class should have a single responsibilty can be applied to other software components as well - function, method, module, package, microservice Issue # in todays world, requirements changes quite fast, thus the responsibilities of classes the more responsibilities a class have, more frequent it need to be refactored more refactoring --> more side effects in multiple components, at least in unit tests for that class Benefit # easy to maintain minimal side effect easy to explain a class/component improved testability makes developers/maintainers life easy Notes # don't overly simplify the class this would result in injecting a lot of dependencies to the dependent/consumer classes use commonsense Open-Closed Principle # A class is closed, since it may be compiled, stored in a library, baselined, and used by client classes. But it is also open, since any new class may use it as parent, adding new features. When a descendant class is defined, there is no need to change the original or to disturb its clients. - Bertrand Mayer, 1988 open: a class should open for extension usually by inheritence closed: but the class should be closed for modification \"As we learned over years that, Inheritance introduces tight coupling if the subclasses depend on implementation details of their parent class.\" Thus uncle Bob and others redefined the principle to Polymorphic Open-Closed principle: Software entities (classes, modules, functions, etc.) should be open for extension but closed for modification. - Robert C. Martin, 2000 it uses interfaces instead of supe/parent classes closed: the interface should be closed for modification open: a class should open for extension usually by implementing the interface define very much required properties/methods in the interface if two are more implementations have some code in common, then there could be a chance of inheritence or composition Issue # if interface/class kept open for modification, then more refactoring & more side-effect Benefit # no changes to existing code (class/interface), thus no side-effect easy to maintain, test, understand interface provides additional level of abstraction interface enables loose coupling between classes Liskov Substitution # This principle extends the Open/Closed principle by focusing on the behavior of a superclass and its subclasses. Let \u03a6(x) be a property provable about objects x of type T. Then \u03a6(y) should be true for objects y of type S where S is a subtype of T. - Barbara Liskov, 1987 objects of a superclass shall be replaceable with objects of its subclasses without breaking the application that requires the objects of its subclasses to behave in same way as the objects of the superclass i.e., sub-classes should stand for their parents without breaking anything don't implement any stricter validation rules on input parameters than implemented by the parent class apply at least the same rules to all the return/output parameters as applied by the parent class Interface Segregation # Clients should not be forced to depend upon interfaces they do not use. - Robert C. Martin, 2000 What # many specific interfaces are better than one do-it-all interfaces in python we use abstract base class with multiple inheritence to achieve this Issue # if multiple clients depends on a huge do-it-all interface, then change in requirements could lead to multiple modification, then more refactoring & more side-effect Benefit # no changes to existing code (class/interface), thus no side-effect easy to maintain, test, understand avoids bloated interface that define multiple responsibilities How # split the software into multiple, independent parts split a huge do-it-all interface into some heirchical parts, say IBase, IKindA extends IBase, IKindB extends IBase if a class really behaves as do-it-all, then it can implement all the small interfaces - like mixins Dependency Inversion # It is based on the Open/Closed principle and Liskov Substitution principle. High-level modules should not depend on low-level modules. Both should depend on abstractions. Abstractions should not depend on details. Details should depend on abstractions. - Robert C. Martin, 2000 What # we should program towards abstractions, not towards implementations implementations can vary, abstraction should not what does above point say? the higher class (container class) should not dependent on the lower classes (class being stored/used in higher class) instead depend upon the abstraction of the lower classes e.g. say Manager class should have a list of Empoyees class obj, instead of maintaing individual list of Developer, Designer, Tester class objs so text-book term a Context class uses-a Strategy Interface, whereas many ConcreteStrategy class implements the Strategy Interface Encapsulate algorithms and separate them as ConcreteStrategy in Python Strategy could be function, lambda as well applying the Open/Closed and the Liskov Substitution principles, makes the code also comply with the Dependency Inversion Principle Ref # books Design Patterns, Elements of Resusable Pbject-Oriented Software pluralsight - https://app.pluralsight.com/course-player?clipId=a880a2a0-4d79-4b5f-a9da-c40411ebb11f DRY # Don't Repeat Yourself WET # Write Everything Twice KISS # Keep It Simple, Stupid YAGNI # You Aren't Gonna Need It","title":"Design Principles"},{"location":"Engineering/Software-Engineering/Architecture/design_principles/#design-principles","text":"","title":"Design Principles"},{"location":"Engineering/Software-Engineering/Architecture/design_principles/#programming-zens-rules-principles","text":"","title":"Programming Zens / Rules / Principles"},{"location":"Engineering/Software-Engineering/Architecture/design_principles/#pythons-zen","text":"Refer this .","title":"Python's Zen"},{"location":"Engineering/Software-Engineering/Architecture/design_principles/#rob-pikes-5-rules-of-programming","text":"Rule 1. You can't tell where a program is going to spend its time. Bottlenecks occur in surprising places, so don't try to second guess and put in a speed hack until you've proven that's where the bottleneck is. Rule 2. Measure. Don't tune for speed until you've measured, and even then don't unless one part of the code overwhelms the rest. Rule 3. Fancy algorithms are slow when n is small, and n is usually small. Fancy algorithms have big constants. Until you know that n is frequently going to be big, don't get fancy. (Even if n does get big, use Rule 2 first.) Rule 4. Fancy algorithms are buggier than simple ones, and they're much harder to implement. Use simple algorithms as well as simple data structures. Rule 5. Data dominates. If you've chosen the right data structures and organized things well, the algorithms will almost always be self-evident. Data structures, not algorithms, are central to programming. Pike's rules 1 and 2 restate Tony Hoare's famous maxim \"Premature optimization is the root of all evil.\" Ken Thompson rephrased Pike's rules 3 and 4 as \"When in doubt, use brute force.\". Rules 3 and 4 are instances of the design philosophy KISS. Rule 5 was previously stated by Fred Brooks in The Mythical Man-Month. Rule 5 is often shortened to \"write stupid code that uses smart objects\". Ref: - https://users.ece.utexas.edu/~adnan/pike.html - https://github.com/robpike","title":"Rob Pike's 5 Rules of Programming"},{"location":"Engineering/Software-Engineering/Architecture/design_principles/#solid","text":"Single Responsibilty Open/Closed Liskov Substitution Interface Segregation Dependency Inversion Most popular sets of design principle in OOPs software engineering. Promoter: Robert C. Martin (Uncle Bob), 2000","title":"SOLID"},{"location":"Engineering/Software-Engineering/Architecture/design_principles/#single-responsibilty-principle","text":"A class should have one, and only one, reason to change. - Robert C. Martin, 2000","title":"Single Responsibilty Principle"},{"location":"Engineering/Software-Engineering/Architecture/design_principles/#what","text":"a class should have a single responsibilty can be applied to other software components as well - function, method, module, package, microservice","title":"What"},{"location":"Engineering/Software-Engineering/Architecture/design_principles/#issue","text":"in todays world, requirements changes quite fast, thus the responsibilities of classes the more responsibilities a class have, more frequent it need to be refactored more refactoring --> more side effects in multiple components, at least in unit tests for that class","title":"Issue"},{"location":"Engineering/Software-Engineering/Architecture/design_principles/#benefit","text":"easy to maintain minimal side effect easy to explain a class/component improved testability makes developers/maintainers life easy","title":"Benefit"},{"location":"Engineering/Software-Engineering/Architecture/design_principles/#notes","text":"don't overly simplify the class this would result in injecting a lot of dependencies to the dependent/consumer classes use commonsense","title":"Notes"},{"location":"Engineering/Software-Engineering/Architecture/design_principles/#open-closed-principle","text":"A class is closed, since it may be compiled, stored in a library, baselined, and used by client classes. But it is also open, since any new class may use it as parent, adding new features. When a descendant class is defined, there is no need to change the original or to disturb its clients. - Bertrand Mayer, 1988 open: a class should open for extension usually by inheritence closed: but the class should be closed for modification \"As we learned over years that, Inheritance introduces tight coupling if the subclasses depend on implementation details of their parent class.\" Thus uncle Bob and others redefined the principle to Polymorphic Open-Closed principle: Software entities (classes, modules, functions, etc.) should be open for extension but closed for modification. - Robert C. Martin, 2000 it uses interfaces instead of supe/parent classes closed: the interface should be closed for modification open: a class should open for extension usually by implementing the interface define very much required properties/methods in the interface if two are more implementations have some code in common, then there could be a chance of inheritence or composition","title":"Open-Closed Principle"},{"location":"Engineering/Software-Engineering/Architecture/design_principles/#issue_1","text":"if interface/class kept open for modification, then more refactoring & more side-effect","title":"Issue"},{"location":"Engineering/Software-Engineering/Architecture/design_principles/#benefit_1","text":"no changes to existing code (class/interface), thus no side-effect easy to maintain, test, understand interface provides additional level of abstraction interface enables loose coupling between classes","title":"Benefit"},{"location":"Engineering/Software-Engineering/Architecture/design_principles/#liskov-substitution","text":"This principle extends the Open/Closed principle by focusing on the behavior of a superclass and its subclasses. Let \u03a6(x) be a property provable about objects x of type T. Then \u03a6(y) should be true for objects y of type S where S is a subtype of T. - Barbara Liskov, 1987 objects of a superclass shall be replaceable with objects of its subclasses without breaking the application that requires the objects of its subclasses to behave in same way as the objects of the superclass i.e., sub-classes should stand for their parents without breaking anything don't implement any stricter validation rules on input parameters than implemented by the parent class apply at least the same rules to all the return/output parameters as applied by the parent class","title":"Liskov Substitution"},{"location":"Engineering/Software-Engineering/Architecture/design_principles/#interface-segregation","text":"Clients should not be forced to depend upon interfaces they do not use. - Robert C. Martin, 2000","title":"Interface Segregation"},{"location":"Engineering/Software-Engineering/Architecture/design_principles/#what_1","text":"many specific interfaces are better than one do-it-all interfaces in python we use abstract base class with multiple inheritence to achieve this","title":"What"},{"location":"Engineering/Software-Engineering/Architecture/design_principles/#issue_2","text":"if multiple clients depends on a huge do-it-all interface, then change in requirements could lead to multiple modification, then more refactoring & more side-effect","title":"Issue"},{"location":"Engineering/Software-Engineering/Architecture/design_principles/#benefit_2","text":"no changes to existing code (class/interface), thus no side-effect easy to maintain, test, understand avoids bloated interface that define multiple responsibilities","title":"Benefit"},{"location":"Engineering/Software-Engineering/Architecture/design_principles/#how","text":"split the software into multiple, independent parts split a huge do-it-all interface into some heirchical parts, say IBase, IKindA extends IBase, IKindB extends IBase if a class really behaves as do-it-all, then it can implement all the small interfaces - like mixins","title":"How"},{"location":"Engineering/Software-Engineering/Architecture/design_principles/#dependency-inversion","text":"It is based on the Open/Closed principle and Liskov Substitution principle. High-level modules should not depend on low-level modules. Both should depend on abstractions. Abstractions should not depend on details. Details should depend on abstractions. - Robert C. Martin, 2000","title":"Dependency Inversion"},{"location":"Engineering/Software-Engineering/Architecture/design_principles/#what_2","text":"we should program towards abstractions, not towards implementations implementations can vary, abstraction should not what does above point say? the higher class (container class) should not dependent on the lower classes (class being stored/used in higher class) instead depend upon the abstraction of the lower classes e.g. say Manager class should have a list of Empoyees class obj, instead of maintaing individual list of Developer, Designer, Tester class objs so text-book term a Context class uses-a Strategy Interface, whereas many ConcreteStrategy class implements the Strategy Interface Encapsulate algorithms and separate them as ConcreteStrategy in Python Strategy could be function, lambda as well applying the Open/Closed and the Liskov Substitution principles, makes the code also comply with the Dependency Inversion Principle","title":"What"},{"location":"Engineering/Software-Engineering/Architecture/design_principles/#ref","text":"books Design Patterns, Elements of Resusable Pbject-Oriented Software pluralsight - https://app.pluralsight.com/course-player?clipId=a880a2a0-4d79-4b5f-a9da-c40411ebb11f","title":"Ref"},{"location":"Engineering/Software-Engineering/Architecture/design_principles/#dry","text":"Don't Repeat Yourself","title":"DRY"},{"location":"Engineering/Software-Engineering/Architecture/design_principles/#wet","text":"Write Everything Twice","title":"WET"},{"location":"Engineering/Software-Engineering/Architecture/design_principles/#kiss","text":"Keep It Simple, Stupid","title":"KISS"},{"location":"Engineering/Software-Engineering/Architecture/design_principles/#yagni","text":"You Aren't Gonna Need It","title":"YAGNI"},{"location":"Engineering/Software-Engineering/Architecture/system_design/","text":"System Design # Table of Contents System Design Application Architecture Types Monolithic / N-Tier / Layered Cons Pros SOA (hybrid?) Cons Pros Microservice Frameworks Cons Pros Serverless Cons Pros Cloud Native Cons Pros Cloud Based Cloud Enabled Microservices Principles Patterns/Architecture (https://microservices.io/patterns/microservices.html) A few common properties of a service/software Trades-Off Scalability vs Performance Latency vs Throughput Availability vs Consistency CAP Theorem Partition Tolerance Scalability Availability Fail Over asynchronous calls dependency isolations set timeout use circuit breaker set current limit load balancing? Nines in availability Resiliency/Fault-tolerant/Reliability Techniques Redundancy Techniques Consistency Replication Techniques Maintainability Independent, autonomous Decentralized governance Auto-Provisioning Continuous delivery through DevOps Serviceability Failure isolation Failure detection Recovery Observability Log aggregation Application metrics Audit logging Distributed tracing Exception tracking Health check API Log deployments and changes Unclassified Scalability Patterns/Techniques Load Balancing / Horizonal Scaling Routing Clustering Stateless application Loose coupling Asynchrony/Concurrency Parallelism Lazy loading Caching Distributed Caching HTTP Caching Reverse proxy CDN Master/Slave Partitioning / Sharding Database Scalability Pattern/Techniques NoSQL RDBMS paritioning sharding replication Confusing terms Scale / Capacity Estimate Documentation Mind Map System Landscape diagram High-level architecture diagram (C4) Database schema diagram (ERD) Flowchart diagram Sequence diagram Tools Pratice Problems References Application Architecture Types # Monolithic / N-Tier / Layered # monolithic aka n-Tier app separates/segregates concerns and decompose code base into functional components/layers e.g. Data Access Layer, Presentation Layer, Business Logic Layer or, MVC (Model, View, Controller) Framework IDEA: to work with any component of the architecture independent of the other Cons # long term maintainability (cumbersome codebase) decoupling issue code change --> whole code base change deployment --> whole application deplyment difficulty adopting new technologies less agile (tight coupled features/code/team) -> takes more time scalling --> scale whole application --> more cost ... Pros # simple/fast development (in initial days) performant/fast (as no network calls) fewer intercommunication pains PS: Not outdated yet. SOA (hybrid?) # Service-based architectur was the evolution to the limitation of the Monolithic architecture decouples the application in smaller modules all the smaller modeules/services then work with an aggregation layer/bus/message queue etc. involves 2 main roles service provider service consumer idea/concept reusable modules easily integrateable modules Cons # complex management communication between each services scalling/mgnt of the communication bus/message queue e.g. Kafka? cost more human resource tech development decreased response time network calls more running codes more request/response validation Pros # better resuablity better maintainability better testability/debugability better reliability agile parallel development separates internal & external elements PS: better for complex systems, where decomposition is not quite feasible/suitable. Microservice # a kind of SOA are the evolution to some limitation of the SOA architecture idea/concept encouraging autonomus services/components to gain agility to priorities business to gain more capabilities in many terms reduce duplication increase cohesion reduce tight-coupling thus easier to understand, maintain, scale system need experience to get it right too granular or too broad can destroy the architecture could be defined by business case hierchy domain separation inter-service communication is through some light weight protocol REST over HTTP? really? an organized way of doing that is through API Proxy (by configuring service communication) or, by service mesh (like Istio) Frameworks # Sidecar Ambassador Adapter Cons # complex overall system overhead of inter-service communication automation to manage multiple small services careful planning, skilled & experience human resource harder to achieve data consistency & transaction management network calls latency can affect other service individual DBs more security concerns coz of more small public service/APIs multiple programing lang double-edged sword Pros # easy to develop, test, deploy autonomous/independent increased agility ability to scale hosrizontally a way towards cloud-native model/approach PS: Good but not required always. Serverless # application running on a server managed by a 3rd party/cloud vendor etc are event-driven idea deligate the server maintainance to 3rd party reduce devops cost server, db management/maintainance application scalability focus on feature dev/business logic only methods FaaS function as a service a piece of code / fuctions runs on the vendor-managed server BaaS backend as a service backend (db, storage, user authn service) managed by the vendor only need to consume those in some app/code MBaaS Mobile-Backend as a service functions for mobile applications vendors AWS Lambda Google Cloud Functions Azure Functions IBM OpenWhisk Oracle Fn Project Kubeless ... Cons # vendor lock-in all things are stored there only migration from one vendor to another could be challenging not for long-term tasks Pros # easy to deploy lower cost optimized infra cost by 3rd party enhanced scalability automatic & seamless scaling managed by 3rd party better for client-heavy apps better for apps which need exponential/limitless scaling really growing fast Cloud Native # for applications planning to deploy in cloud applies to SOA & Microservices (and to monolithic & serverless as well) an approach to deploy on public clouds (aws, gcp, azure, some one else's data center etc) make services ready to be containerized i.e. orchestration thus ready for CI/CD services are usually stateless services communicates through HTTP or messaging queue envolves multiple node of the service thus a centralized DB/session state/config server CI/CD API gateway container registry message-oriented middleware (MOM: publish/subscribe) service mesh (like Istio) orchestration (kubernetes/mesos) containerization brings container orchestration brings mesos, kubernetes Cons # complexity experience Pros # agility flexible resilient Cloud Based # somewhere in between Cloud Enabled & Cloud Native running on cloud, vendor is auto scaling server/db, managing backups/log etc Cloud Enabled # legacy monolithic apps, migrated to cloud depends on local services can't take advantage of shared services blah blah Microservices # aka Distributed System A distributed system is a model in which components located on networked computers communicate and coordinate their actions by passing messages A distributed system is one where different parts run on different physical or logical machines, communicating using non-local mechanisms. A detailed view on this hot/major topic. Principles # A few principle microservice architecture follows are ^8 : Scalability Availability Resiliency/Fault-tolerant/Reliability Consistency Maintainability Independent, autonomous Decentralized governance Auto-Provisioning Continuous delivery through DevOps Serviceability Failure isolation Failure detection Recovery Observability Patterns/Architecture (https://microservices.io/patterns/microservices.html) # Domain-driven-design CQRS (command-query responsibity segragation) Event-driven Database per service Decomposition patterns Decompose by business capability Decompose by subdomain The Database per Service pattern describes how each service has its own database in order to ensure loose coupling. The API Gateway pattern defines how clients access the services in a microservice architecture. The Client-side Discovery and Server-side Discovery patterns are used to route requests for a client to an available service instance in a microservice architecture. The Messaging and Remote Procedure Invocation patterns are two different ways that services can communicate. The Single Service per Host and Multiple Services per Host patterns are two different deployment strategies. Cross-cutting concerns patterns: Microservice chassis pattern and Externalized configuration Testing patterns: Service Component Test and Service Integration Contract Test Circuit Breaker Access Token as per ^8 Decomposition Patterns Decompose by Business Capability Decompose by Subdomain Strangler Pattern Integration Patterns API Gateway Pattern Aggregator Pattern Client-Side UI Composition Pattern Database Patterns Database per Service Shared Database per Service Command Query Responsibility Segregation (CQRS) Saga Pattern Observability Patterns Log Aggregation Performance Metrics Distributed Tracing Health Check Cross-Cutting Concern Patterns External Configuration Service Discovery Pattern Circuit Breaker Pattern Blue-Green Deployment Pattern A few common properties of a service/software # High Performance Low Latency High Throughput Trades-Off # Scalability vs Performance # A detailed view on this hot/major topic ^6 . Performance: about speed capability of a service w.r.t its response time (how fast) serving # of unit work over a period of time how fast the service is if there is only 1 user (1 traffic only) Scalability: about handling large load/traffic or doing large unit of work capability of a service to perform directly proportional to the hardwares/resources added increament in performance of the service when added more resources service is fast for 1 user, but how fast the service is under heavy load (high traffic) scale up aka vertical scaling increase server's power (cpu. ram, disk, network speed) scale out aka horizontal scaling increase # of servers/instances scalability gives - availability - resiliency/fault-tolerance/reliability - performance/responsiveness - low latency - high throughput Latency vs Throughput # strive for high throughput with acceptable latency In networks.. Latency: Time taken by a packet to travel from its source to destination. Throughput: The number of packets processed in a specific time duration. In softwares.. Latency: The time taken by a software to process a request and respond. Throughput: The number of requests processed in a specific time duration. Availability vs Consistency # CAP Theorem # asserts that in a distributed system (networked data-shared) can have only 2 of the 3 desireable properties at a time. ^5 consistency & partition tolerance or, availability & partition tolerance e.g. what NoSQL's BASE principle says: \"basically available, soft state, eventually consistent\" Note: The definition of consistency used in CAP theorem is different to the definition of consistency used in ACID. Partition Tolerance # ability to tolerate network partitions Scalability # Availability # ^12 Service is operational/available/running whenever needed % of time service is operational uptime divided by total time (uptime plus downtime) Or, each request to the service receives a response Techniques to improve availability: Fail Over # A fall back mechanism ^10 asynchronous calls # dependency isolations # set timeout # use circuit breaker # set current limit # load balancing? # Nines in availability # 90%, 99%, 99.9%, 99.99%, 99.999% Try calculate how much 99% is of 1 year (365 days). ^12 Resiliency/Fault-tolerant/Reliability # Resiliency: Service is functional with expected results for given input/environment (even in case of a fault/error) for the required time duration. ^1 Reliability: - involves availabilityi - if a system is reliable, you can say it is available. However, if it is available, it may not necessarily be reliable. - service is functional for a specific time interval without a failure - % of time the service works correctly - the ability of a distributed system to deliver its services even when one or several of its software or hardware components fail Say there are multiple available servers, but not responding.. then redirect the request to other server. It\u2019s impossible to eliminate failure in microservice applications \u2014 the cost of that would be infinite! Instead, your focus needs to be on designing microservices that are tolerant of dependency failures and able to gracefully recover from them or mitigate the impact of those failures on their own responsibilities ^13 . Techniques # step - 1 ensuring Availability Applying retries, rate limits, circuit breakers, health checks, and caching to mitigate interservice communication issues Applying safe communication standards across many services circuit breaker correlation ID (for better debugability) service mesh Redundancy # duplication of critical components (nodes, processes) with the aim to achieve reliability ^9 The (additional/copies/redundant) resources which are NOT strictly necessary for the distributed system to work correctly but good to have to avoid followings: failovers single point of failure Improves reliability Techniques # Consistency # All nodes see the same data (and behaviour? as well) at the same time. patterns to achieve consistency: Replication # includes redundancy but also involves synchronization of state between different/redundant nodes in a distributed system. yes. yes. sharing information to ensure consistency between redundant resources. ^9 improves availability improves reliability by implicit redundancy improves performance if geographycally distributed Techniques # active (push) passive (pull) Master Slave Master Master Tree Replication Buddy Replication Maintainability # Independent, autonomous # Decentralized governance # Auto-Provisioning # Continuous delivery through DevOps # Serviceability # Failure isolation # Failure detection # Recovery # Observability # Log aggregation # Application metrics # Audit logging # Distributed tracing # Exception tracking # Health check API # Log deployments and changes # Unclassified Scalability Patterns/Techniques # There are various techniques/methods/patterns to achieve so: Load Balancing / Horizonal Scaling # scalability availability Routing # performance (or may be just a business use case) Clustering # database cluster? nodes/servers cluster? - scalability - availability - reliability Stateless application # scalability consistency Loose coupling # maintainability serviceability Asynchrony/Concurrency # performance scalability availability Parallelism # performance Lazy loading # performance Caching # performance Distributed Caching # HTTP Caching # Reverse proxy # CDN # Master/Slave # scalability reliability whether its database or some service node.. slave could take over in case of master failure Partitioning / Sharding # performance scalability availability Database Scalability Pattern/Techniques # NoSQL # RDBMS # paritioning # sharding # replication # Confusing terms # availability vs reliability redundancy vs replication partitioning vs sharding partitioning a generic term; divide data types horizontal partitioning aka sharding is replicating [copying] the schema and then dividing the data based on some key (called as shard key) ^14 vertical partitioning is dividing up the schema it self scale up vs scale out Scale / Capacity Estimate # tbd Documentation # Mind Map # System Landscape diagram # High-level architecture diagram (C4) # Database schema diagram (ERD) # Flowchart diagram # Sequence diagram # Tools # MermaidJS PlantUML Lucidchart Draw.io excalidraw.com (For handwritten style) Ref: 1. https://tsh.io/blog/how-to-document-your-architecture/ 1. https://c4model.com/ 1. https://medium.com/swlh/what-should-a-product-manager-know-about-software-design-and-architecture-a5ec92dddfe7 Pratice Problems # Designing a URL Shortening service like TinyURL How do you design an Elevator of the Lift system? How do you design the Vending Machine in Java? How to design an ATM machine? Design a LRU Cache System https://www.geeksforgeeks.org/lru-cache-implementation/ Design & implement LFU cache Design phone book: search by name, phone number and print 10 most dialled numbers. Design a TODO list cloud application, multiple client support, millions of users, synchronize updates across multiple clients. Design an Authorization Service. How would you design a Parking Lot system? https://www.youtube.com/watch?v=NtMvNh0WFVM How do you design a traffic control system? How do you design a website like Pastebin? How do you design an API Rate Limiter? How do you design a Search Engine? What is required to design a garbage collection system? Design of Review Systems How do you design a recommendation system? How do you design a web crawler, and when should it be used? How do you design a chat application like WhatsApp or Facebook Messenger? How to design a global video streaming service like YouTube or NetFlix? How do you design global file sharing and storage apps like Google Drive or Dropbox? Design a translation service. Design Airport kiosk for these features check in , seat selection, boarding pass print how would you evaluate a software? must fulfill basic requirements cost vendor / license / open-source adoptability in team brainstorm within team maintainability ease of use / document / customer-community support modifiability as per special need scalability installation / setup system design type hld high level problem solve lld ludo class design no other things expectations not expected to complete uber or whatsapp hld: how many use cases you could think of lld: build fault tolerant, scalable by example whatsapp features chat call group chat group call try define requirements at your own/ come up with requirement I'll solve this things confirm requirements first then try start solving the problem uber come up with requirements Q: how much time to define the requirements? -> 5 to 10 minutes expected to code the same system design? -> No, very rare approach to solve uber? define req. user should be able to see drivers send request. asked to include toll in the prices SOLID principles? Most imp. Design Patterns? Most imp. how much to talk about lld vs hld? whatsapp? protocol? if you it--> it is a +ve point 2-3yrs exp -> good lld, no system design; >=4 yrs -> both (some time they skip lld) hld->partitioning, sharding, replication, load-balancing, rolling upgrade, rolling logs, high scalability, high intensive system, horizontal vs vertical scaling resources for preparation? hld start with core concept how can play safe? you can lead the interview start with req. if you know something, make interviewer ask those question always start with high level components same very wage, high level thing-> like say DB, don't say sql/nosql practice many problems prepare basic concepts 3 imp point of sys design partitioning availability sql/nosql; load balance top 3 design patterns interface segregation compositions SOLID freshers? only coding, os, dbms; no lld/hld- discuss tech stack? not imp, no brand name req. only basic concept like quue, cache (not redis vs mem; kafka vs rabitmq) how to handle opinion mis-match say sql vs no-sql back your decision 1 million data, need horizontal partitioning etc. use decision approach don't be rigid with your decision try to seek interviews feedback and try to mold your solution, try to incorporate req.? who tells? 75% we need to define and confirm other skills to deveope? leadership, mgmnt? imp for higher position how knowing big data helps sys-d interviews? helps how to setup queue, high vol. db etc when async, batch vs real-time? learn paritionning in nosql take examples of problems (to learn) CP & DP start writting recursive solution then memoize it, but first need to be confortable in recurr C++ STL, Java, Python For working profs: Facebook hacker, Google Code Jam, Codechef lo.. etc. 1 year, 2 prob each day is enough to be a good CP/DP DS, Algo, OS, N/W, DBMS Microsoft, Google, Amazon - CP, DS, Algo Explain from Zero Don't go blank in interview 1st & 2nd year of college - CP Ref: The course for LLD (Object Oriented Design & Design Patterns): https://practice.geeksforgeeks.org/co... The course for System Design: https://practice.geeksforgeeks.org/co... Our courses : https://practice.geeksforgeeks.org/co... LinkedIn Profiles of the people in the video: Shashi Bhushan Kumar https://www.linkedin.com/in/shashi-bh... Udit Agarwal https://www.linkedin.com/in/anomaly2104/ You can follow Udit's YouTube Channel for System Design videos: https://www.youtube.com/c/UditAgarwal21 References # FIXME: footnote link [^2]: How to design a system to scale to first 100 million users [^3]: System Design Primer","title":"System Design"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#system-design","text":"","title":"System Design"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#application-architecture-types","text":"","title":"Application Architecture Types"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#monolithic-n-tier-layered","text":"monolithic aka n-Tier app separates/segregates concerns and decompose code base into functional components/layers e.g. Data Access Layer, Presentation Layer, Business Logic Layer or, MVC (Model, View, Controller) Framework IDEA: to work with any component of the architecture independent of the other","title":"Monolithic / N-Tier / Layered"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#cons","text":"long term maintainability (cumbersome codebase) decoupling issue code change --> whole code base change deployment --> whole application deplyment difficulty adopting new technologies less agile (tight coupled features/code/team) -> takes more time scalling --> scale whole application --> more cost ...","title":"Cons"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#pros","text":"simple/fast development (in initial days) performant/fast (as no network calls) fewer intercommunication pains PS: Not outdated yet.","title":"Pros"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#soa-hybrid","text":"Service-based architectur was the evolution to the limitation of the Monolithic architecture decouples the application in smaller modules all the smaller modeules/services then work with an aggregation layer/bus/message queue etc. involves 2 main roles service provider service consumer idea/concept reusable modules easily integrateable modules","title":"SOA (hybrid?)"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#cons_1","text":"complex management communication between each services scalling/mgnt of the communication bus/message queue e.g. Kafka? cost more human resource tech development decreased response time network calls more running codes more request/response validation","title":"Cons"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#pros_1","text":"better resuablity better maintainability better testability/debugability better reliability agile parallel development separates internal & external elements PS: better for complex systems, where decomposition is not quite feasible/suitable.","title":"Pros"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#microservice","text":"a kind of SOA are the evolution to some limitation of the SOA architecture idea/concept encouraging autonomus services/components to gain agility to priorities business to gain more capabilities in many terms reduce duplication increase cohesion reduce tight-coupling thus easier to understand, maintain, scale system need experience to get it right too granular or too broad can destroy the architecture could be defined by business case hierchy domain separation inter-service communication is through some light weight protocol REST over HTTP? really? an organized way of doing that is through API Proxy (by configuring service communication) or, by service mesh (like Istio)","title":"Microservice"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#frameworks","text":"Sidecar Ambassador Adapter","title":"Frameworks"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#cons_2","text":"complex overall system overhead of inter-service communication automation to manage multiple small services careful planning, skilled & experience human resource harder to achieve data consistency & transaction management network calls latency can affect other service individual DBs more security concerns coz of more small public service/APIs multiple programing lang double-edged sword","title":"Cons"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#pros_2","text":"easy to develop, test, deploy autonomous/independent increased agility ability to scale hosrizontally a way towards cloud-native model/approach PS: Good but not required always.","title":"Pros"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#serverless","text":"application running on a server managed by a 3rd party/cloud vendor etc are event-driven idea deligate the server maintainance to 3rd party reduce devops cost server, db management/maintainance application scalability focus on feature dev/business logic only methods FaaS function as a service a piece of code / fuctions runs on the vendor-managed server BaaS backend as a service backend (db, storage, user authn service) managed by the vendor only need to consume those in some app/code MBaaS Mobile-Backend as a service functions for mobile applications vendors AWS Lambda Google Cloud Functions Azure Functions IBM OpenWhisk Oracle Fn Project Kubeless ...","title":"Serverless"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#cons_3","text":"vendor lock-in all things are stored there only migration from one vendor to another could be challenging not for long-term tasks","title":"Cons"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#pros_3","text":"easy to deploy lower cost optimized infra cost by 3rd party enhanced scalability automatic & seamless scaling managed by 3rd party better for client-heavy apps better for apps which need exponential/limitless scaling really growing fast","title":"Pros"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#cloud-native","text":"for applications planning to deploy in cloud applies to SOA & Microservices (and to monolithic & serverless as well) an approach to deploy on public clouds (aws, gcp, azure, some one else's data center etc) make services ready to be containerized i.e. orchestration thus ready for CI/CD services are usually stateless services communicates through HTTP or messaging queue envolves multiple node of the service thus a centralized DB/session state/config server CI/CD API gateway container registry message-oriented middleware (MOM: publish/subscribe) service mesh (like Istio) orchestration (kubernetes/mesos) containerization brings container orchestration brings mesos, kubernetes","title":"Cloud Native"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#cons_4","text":"complexity experience","title":"Cons"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#pros_4","text":"agility flexible resilient","title":"Pros"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#cloud-based","text":"somewhere in between Cloud Enabled & Cloud Native running on cloud, vendor is auto scaling server/db, managing backups/log etc","title":"Cloud Based"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#cloud-enabled","text":"legacy monolithic apps, migrated to cloud depends on local services can't take advantage of shared services blah blah","title":"Cloud Enabled"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#microservices","text":"aka Distributed System A distributed system is a model in which components located on networked computers communicate and coordinate their actions by passing messages A distributed system is one where different parts run on different physical or logical machines, communicating using non-local mechanisms. A detailed view on this hot/major topic.","title":"Microservices"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#principles","text":"A few principle microservice architecture follows are ^8 : Scalability Availability Resiliency/Fault-tolerant/Reliability Consistency Maintainability Independent, autonomous Decentralized governance Auto-Provisioning Continuous delivery through DevOps Serviceability Failure isolation Failure detection Recovery Observability","title":"Principles"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#patternsarchitecture-httpsmicroservicesiopatternsmicroserviceshtml","text":"Domain-driven-design CQRS (command-query responsibity segragation) Event-driven Database per service Decomposition patterns Decompose by business capability Decompose by subdomain The Database per Service pattern describes how each service has its own database in order to ensure loose coupling. The API Gateway pattern defines how clients access the services in a microservice architecture. The Client-side Discovery and Server-side Discovery patterns are used to route requests for a client to an available service instance in a microservice architecture. The Messaging and Remote Procedure Invocation patterns are two different ways that services can communicate. The Single Service per Host and Multiple Services per Host patterns are two different deployment strategies. Cross-cutting concerns patterns: Microservice chassis pattern and Externalized configuration Testing patterns: Service Component Test and Service Integration Contract Test Circuit Breaker Access Token as per ^8 Decomposition Patterns Decompose by Business Capability Decompose by Subdomain Strangler Pattern Integration Patterns API Gateway Pattern Aggregator Pattern Client-Side UI Composition Pattern Database Patterns Database per Service Shared Database per Service Command Query Responsibility Segregation (CQRS) Saga Pattern Observability Patterns Log Aggregation Performance Metrics Distributed Tracing Health Check Cross-Cutting Concern Patterns External Configuration Service Discovery Pattern Circuit Breaker Pattern Blue-Green Deployment Pattern","title":"Patterns/Architecture (https://microservices.io/patterns/microservices.html)"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#a-few-common-properties-of-a-servicesoftware","text":"High Performance Low Latency High Throughput","title":"A few common properties of a service/software"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#trades-off","text":"","title":"Trades-Off"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#scalability-vs-performance","text":"A detailed view on this hot/major topic ^6 . Performance: about speed capability of a service w.r.t its response time (how fast) serving # of unit work over a period of time how fast the service is if there is only 1 user (1 traffic only) Scalability: about handling large load/traffic or doing large unit of work capability of a service to perform directly proportional to the hardwares/resources added increament in performance of the service when added more resources service is fast for 1 user, but how fast the service is under heavy load (high traffic) scale up aka vertical scaling increase server's power (cpu. ram, disk, network speed) scale out aka horizontal scaling increase # of servers/instances scalability gives - availability - resiliency/fault-tolerance/reliability - performance/responsiveness - low latency - high throughput","title":"Scalability vs Performance"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#latency-vs-throughput","text":"strive for high throughput with acceptable latency In networks.. Latency: Time taken by a packet to travel from its source to destination. Throughput: The number of packets processed in a specific time duration. In softwares.. Latency: The time taken by a software to process a request and respond. Throughput: The number of requests processed in a specific time duration.","title":"Latency vs Throughput"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#availability-vs-consistency","text":"","title":"Availability vs Consistency"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#cap-theorem","text":"asserts that in a distributed system (networked data-shared) can have only 2 of the 3 desireable properties at a time. ^5 consistency & partition tolerance or, availability & partition tolerance e.g. what NoSQL's BASE principle says: \"basically available, soft state, eventually consistent\" Note: The definition of consistency used in CAP theorem is different to the definition of consistency used in ACID.","title":"CAP Theorem"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#partition-tolerance","text":"ability to tolerate network partitions","title":"Partition Tolerance"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#scalability","text":"","title":"Scalability"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#availability","text":"^12 Service is operational/available/running whenever needed % of time service is operational uptime divided by total time (uptime plus downtime) Or, each request to the service receives a response Techniques to improve availability:","title":"Availability"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#fail-over","text":"A fall back mechanism ^10","title":"Fail Over"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#asynchronous-calls","text":"","title":"asynchronous calls"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#dependency-isolations","text":"","title":"dependency isolations"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#set-timeout","text":"","title":"set timeout"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#use-circuit-breaker","text":"","title":"use circuit breaker"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#set-current-limit","text":"","title":"set current limit"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#load-balancing","text":"","title":"load balancing?"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#nines-in-availability","text":"90%, 99%, 99.9%, 99.99%, 99.999% Try calculate how much 99% is of 1 year (365 days). ^12","title":"Nines in availability"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#resiliencyfault-tolerantreliability","text":"Resiliency: Service is functional with expected results for given input/environment (even in case of a fault/error) for the required time duration. ^1 Reliability: - involves availabilityi - if a system is reliable, you can say it is available. However, if it is available, it may not necessarily be reliable. - service is functional for a specific time interval without a failure - % of time the service works correctly - the ability of a distributed system to deliver its services even when one or several of its software or hardware components fail Say there are multiple available servers, but not responding.. then redirect the request to other server. It\u2019s impossible to eliminate failure in microservice applications \u2014 the cost of that would be infinite! Instead, your focus needs to be on designing microservices that are tolerant of dependency failures and able to gracefully recover from them or mitigate the impact of those failures on their own responsibilities ^13 .","title":"Resiliency/Fault-tolerant/Reliability"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#techniques","text":"step - 1 ensuring Availability Applying retries, rate limits, circuit breakers, health checks, and caching to mitigate interservice communication issues Applying safe communication standards across many services circuit breaker correlation ID (for better debugability) service mesh","title":"Techniques"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#redundancy","text":"duplication of critical components (nodes, processes) with the aim to achieve reliability ^9 The (additional/copies/redundant) resources which are NOT strictly necessary for the distributed system to work correctly but good to have to avoid followings: failovers single point of failure Improves reliability","title":"Redundancy"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#techniques_1","text":"","title":"Techniques"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#consistency","text":"All nodes see the same data (and behaviour? as well) at the same time. patterns to achieve consistency:","title":"Consistency"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#replication","text":"includes redundancy but also involves synchronization of state between different/redundant nodes in a distributed system. yes. yes. sharing information to ensure consistency between redundant resources. ^9 improves availability improves reliability by implicit redundancy improves performance if geographycally distributed","title":"Replication"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#techniques_2","text":"active (push) passive (pull) Master Slave Master Master Tree Replication Buddy Replication","title":"Techniques"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#maintainability","text":"","title":"Maintainability"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#independent-autonomous","text":"","title":"Independent, autonomous"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#decentralized-governance","text":"","title":"Decentralized governance"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#auto-provisioning","text":"","title":"Auto-Provisioning"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#continuous-delivery-through-devops","text":"","title":"Continuous delivery through DevOps"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#serviceability","text":"","title":"Serviceability"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#failure-isolation","text":"","title":"Failure isolation"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#failure-detection","text":"","title":"Failure detection"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#recovery","text":"","title":"Recovery"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#observability","text":"","title":"Observability"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#log-aggregation","text":"","title":"Log aggregation"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#application-metrics","text":"","title":"Application metrics"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#audit-logging","text":"","title":"Audit logging"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#distributed-tracing","text":"","title":"Distributed tracing"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#exception-tracking","text":"","title":"Exception tracking"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#health-check-api","text":"","title":"Health check API"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#log-deployments-and-changes","text":"","title":"Log deployments and changes"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#unclassified-scalability-patternstechniques","text":"There are various techniques/methods/patterns to achieve so:","title":"Unclassified Scalability Patterns/Techniques"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#load-balancing-horizonal-scaling","text":"scalability availability","title":"Load Balancing / Horizonal Scaling"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#routing","text":"performance (or may be just a business use case)","title":"Routing"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#clustering","text":"database cluster? nodes/servers cluster? - scalability - availability - reliability","title":"Clustering"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#stateless-application","text":"scalability consistency","title":"Stateless application"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#loose-coupling","text":"maintainability serviceability","title":"Loose coupling"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#asynchronyconcurrency","text":"performance scalability availability","title":"Asynchrony/Concurrency"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#parallelism","text":"performance","title":"Parallelism"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#lazy-loading","text":"performance","title":"Lazy loading"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#caching","text":"performance","title":"Caching"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#distributed-caching","text":"","title":"Distributed Caching"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#http-caching","text":"","title":"HTTP Caching"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#reverse-proxy","text":"","title":"Reverse proxy"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#cdn","text":"","title":"CDN"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#masterslave","text":"scalability reliability whether its database or some service node.. slave could take over in case of master failure","title":"Master/Slave"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#partitioning-sharding","text":"performance scalability availability","title":"Partitioning / Sharding"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#database-scalability-patterntechniques","text":"","title":"Database Scalability Pattern/Techniques"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#nosql","text":"","title":"NoSQL"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#rdbms","text":"","title":"RDBMS"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#paritioning","text":"","title":"paritioning"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#sharding","text":"","title":"sharding"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#replication_1","text":"","title":"replication"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#confusing-terms","text":"availability vs reliability redundancy vs replication partitioning vs sharding partitioning a generic term; divide data types horizontal partitioning aka sharding is replicating [copying] the schema and then dividing the data based on some key (called as shard key) ^14 vertical partitioning is dividing up the schema it self scale up vs scale out","title":"Confusing terms"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#scale-capacity-estimate","text":"tbd","title":"Scale / Capacity Estimate"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#documentation","text":"","title":"Documentation"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#mind-map","text":"","title":"Mind Map"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#system-landscape-diagram","text":"","title":"System Landscape diagram"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#high-level-architecture-diagram-c4","text":"","title":"High-level architecture diagram (C4)"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#database-schema-diagram-erd","text":"","title":"Database schema diagram (ERD)"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#flowchart-diagram","text":"","title":"Flowchart diagram"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#sequence-diagram","text":"","title":"Sequence diagram"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#tools","text":"MermaidJS PlantUML Lucidchart Draw.io excalidraw.com (For handwritten style) Ref: 1. https://tsh.io/blog/how-to-document-your-architecture/ 1. https://c4model.com/ 1. https://medium.com/swlh/what-should-a-product-manager-know-about-software-design-and-architecture-a5ec92dddfe7","title":"Tools"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#pratice-problems","text":"Designing a URL Shortening service like TinyURL How do you design an Elevator of the Lift system? How do you design the Vending Machine in Java? How to design an ATM machine? Design a LRU Cache System https://www.geeksforgeeks.org/lru-cache-implementation/ Design & implement LFU cache Design phone book: search by name, phone number and print 10 most dialled numbers. Design a TODO list cloud application, multiple client support, millions of users, synchronize updates across multiple clients. Design an Authorization Service. How would you design a Parking Lot system? https://www.youtube.com/watch?v=NtMvNh0WFVM How do you design a traffic control system? How do you design a website like Pastebin? How do you design an API Rate Limiter? How do you design a Search Engine? What is required to design a garbage collection system? Design of Review Systems How do you design a recommendation system? How do you design a web crawler, and when should it be used? How do you design a chat application like WhatsApp or Facebook Messenger? How to design a global video streaming service like YouTube or NetFlix? How do you design global file sharing and storage apps like Google Drive or Dropbox? Design a translation service. Design Airport kiosk for these features check in , seat selection, boarding pass print how would you evaluate a software? must fulfill basic requirements cost vendor / license / open-source adoptability in team brainstorm within team maintainability ease of use / document / customer-community support modifiability as per special need scalability installation / setup system design type hld high level problem solve lld ludo class design no other things expectations not expected to complete uber or whatsapp hld: how many use cases you could think of lld: build fault tolerant, scalable by example whatsapp features chat call group chat group call try define requirements at your own/ come up with requirement I'll solve this things confirm requirements first then try start solving the problem uber come up with requirements Q: how much time to define the requirements? -> 5 to 10 minutes expected to code the same system design? -> No, very rare approach to solve uber? define req. user should be able to see drivers send request. asked to include toll in the prices SOLID principles? Most imp. Design Patterns? Most imp. how much to talk about lld vs hld? whatsapp? protocol? if you it--> it is a +ve point 2-3yrs exp -> good lld, no system design; >=4 yrs -> both (some time they skip lld) hld->partitioning, sharding, replication, load-balancing, rolling upgrade, rolling logs, high scalability, high intensive system, horizontal vs vertical scaling resources for preparation? hld start with core concept how can play safe? you can lead the interview start with req. if you know something, make interviewer ask those question always start with high level components same very wage, high level thing-> like say DB, don't say sql/nosql practice many problems prepare basic concepts 3 imp point of sys design partitioning availability sql/nosql; load balance top 3 design patterns interface segregation compositions SOLID freshers? only coding, os, dbms; no lld/hld- discuss tech stack? not imp, no brand name req. only basic concept like quue, cache (not redis vs mem; kafka vs rabitmq) how to handle opinion mis-match say sql vs no-sql back your decision 1 million data, need horizontal partitioning etc. use decision approach don't be rigid with your decision try to seek interviews feedback and try to mold your solution, try to incorporate req.? who tells? 75% we need to define and confirm other skills to deveope? leadership, mgmnt? imp for higher position how knowing big data helps sys-d interviews? helps how to setup queue, high vol. db etc when async, batch vs real-time? learn paritionning in nosql take examples of problems (to learn) CP & DP start writting recursive solution then memoize it, but first need to be confortable in recurr C++ STL, Java, Python For working profs: Facebook hacker, Google Code Jam, Codechef lo.. etc. 1 year, 2 prob each day is enough to be a good CP/DP DS, Algo, OS, N/W, DBMS Microsoft, Google, Amazon - CP, DS, Algo Explain from Zero Don't go blank in interview 1st & 2nd year of college - CP Ref: The course for LLD (Object Oriented Design & Design Patterns): https://practice.geeksforgeeks.org/co... The course for System Design: https://practice.geeksforgeeks.org/co... Our courses : https://practice.geeksforgeeks.org/co... LinkedIn Profiles of the people in the video: Shashi Bhushan Kumar https://www.linkedin.com/in/shashi-bh... Udit Agarwal https://www.linkedin.com/in/anomaly2104/ You can follow Udit's YouTube Channel for System Design videos: https://www.youtube.com/c/UditAgarwal21","title":"Pratice Problems"},{"location":"Engineering/Software-Engineering/Architecture/system_design/#references","text":"FIXME: footnote link [^2]: How to design a system to scale to first 100 million users [^3]: System Design Primer","title":"References"},{"location":"Engineering/Software-Engineering/Data-Engineering/apache_beam/","text":"Apache Beam # Table of Contents Apache Beam Introduction What Why How Trivia Read More Concept Pipeline PTransform WebSocket based PTransform ParDo DoFn Setup Prerequisites Create Run Run Locally Run on Dataflow Runner Python SKD Syntax FAQ Why does custom Python object cannot be used with ParDo Fn? TODO Explore Introduction # https://beam.apache.org/ What # https://beam.apache.org/get-started/beam-overview/ an advanced unified programming model to define a pipeline can define both batch and streaming data-parallel processing pipelines that pipeline can be run on any execution engine engine: Beam's supported distributed processing backends useful for embarrassingly parallel data processing tasks in which the problem can be decomposed into many smaller bundles of data that can be processed independently and in parallel can also use Beam for Extract, Transform, and Load (ETL) tasks and pure data integration such tasks are useful for moving data between different storage media and data sources, transforming data into a more desirable format, or loading data onto a new system Why # Unified Use a single programming model for both batch and streaming use cases Portable Execute pipelines on multiple execution environments Language & Engine Agnostic Java, Python, Go, Scala etc. Direct/Local, Apache Flink, Spark, Samza, Nemo, GCP Dataflow, Twister, Hazelcast Jet etc. Extensible Write and share new SDKs, IO connectors, and transformation libraries Open Source Community-based development How # Sources Beam reads your data from a diverse set of supported sources, no matter if it\u2019s on-prem or in the cloud Processing Beam executes your business logic for Batch and Streaming use cases Sinks Beam writes the results of your data processing logic to the most popular data destinations in the industry Trivia # was Google's internal project previously knows as dataflow Read More # Quickstart: https://beam.apache.org/get-started/quickstart-py/ Programming Guide: https://beam.apache.org/documentation/programming-guide/#transforms Concept # Pipeline --> DAG, Topology Input --> Spout, PCollection[PValue], Source Processor --> Bolt, PTransform[DoFn], Map, Lambda Output --> Sink, PCollection[PValue], Target %% HLD flow-diagram of a typical data pipeline framework flowchart TB %% nodes input1(\"Init\") input2(\"Data Extractor/Source/REST API Client\") processor1[\"Data Preprocessor\"] processor2[\"Data Transformer\"] processor3[\"Data Loader\"] output(\"Database\") %% relationship input1 --> input2 -- \"Data\" --> processor1 processor1 -- \"Preprocessed Data\" --> processor2 processor2 -- \"Transformed Data\" --> processor3 processor3 --> output Figure 1. Flow diagram of a typical data processing pipeline/topology Pipeline # PTransform # WebSocket based PTransform # https://rmannibucau.metawerx.net/apache-beam-websocket-output.html ParDo # DoFn # Setup # Prerequisites # Create # Run # Run Locally # Run on Dataflow Runner # Using service account: 1 2 3 4 5 6 7 8 9 $ python -m exploratory.wordcount \\ --region us-central1 \\ --input gs://dataflow-samples/shakespeare/kinglear.txt \\ --output gs://apache-beam-eg/results/outputs \\ --runner DataflowRunner \\ --project apache-beam-eg \\ --temp_location gs://apache-beam-eg/tmp/ \\ --service_account_email apache-beam-svc-ac@apache-beam-eg.iam.gserviceaccount.com \\ --subnetwork regions/us-central1/subnetworks/apache-beam-eg-usc1-temp # mention subnetwork other that `default` Python SKD # References: https://beam.apache.org/get-started/quickstart-py/ Syntax # Inspired from binary arithmetic operators. Those operators magic methods are overloaded to denote some different pipeline operations. e.g. | is a synonym for apply , which applies a PTransform to a PCollection to produce a new PCollection obj.__or__(self, other) obj.__ror__(self, other) >> allows you to name a step for easier display in various UIs obj.__rrshift__(self, other) <SomePCollection> |'SomeLabel' >> <SomePTransform> SomeLabel here is the string between the | and the >> is only used for these display purposes and identifying that particular application References: https://docs.python.org/3/reference/datamodel.html?highlight=ror#object. ror https://stackoverflow.com/questions/43796046/explain-apache-beam-python-syntax visualization https://stackoverflow.com/questions/54094081/how-to-render-a-pipeline-graph-in-beam FAQ # Why does custom Python object cannot be used with ParDo Fn? # https://stackoverflow.com/questions/55822881/why-does-custom-python-object-cannot-be-used-with-pardo-fn TODO # Explore # mode stream/batch policy fault-tolerance, atleast-once/exact-once? strategy processing ack partitioning/sharding based in input for parallelism state management windowing visualization debug on cloud tickers - to auto schedule","title":"Apache Beam"},{"location":"Engineering/Software-Engineering/Data-Engineering/apache_beam/#apache-beam","text":"","title":"Apache Beam"},{"location":"Engineering/Software-Engineering/Data-Engineering/apache_beam/#introduction","text":"https://beam.apache.org/","title":"Introduction"},{"location":"Engineering/Software-Engineering/Data-Engineering/apache_beam/#what","text":"https://beam.apache.org/get-started/beam-overview/ an advanced unified programming model to define a pipeline can define both batch and streaming data-parallel processing pipelines that pipeline can be run on any execution engine engine: Beam's supported distributed processing backends useful for embarrassingly parallel data processing tasks in which the problem can be decomposed into many smaller bundles of data that can be processed independently and in parallel can also use Beam for Extract, Transform, and Load (ETL) tasks and pure data integration such tasks are useful for moving data between different storage media and data sources, transforming data into a more desirable format, or loading data onto a new system","title":"What"},{"location":"Engineering/Software-Engineering/Data-Engineering/apache_beam/#why","text":"Unified Use a single programming model for both batch and streaming use cases Portable Execute pipelines on multiple execution environments Language & Engine Agnostic Java, Python, Go, Scala etc. Direct/Local, Apache Flink, Spark, Samza, Nemo, GCP Dataflow, Twister, Hazelcast Jet etc. Extensible Write and share new SDKs, IO connectors, and transformation libraries Open Source Community-based development","title":"Why"},{"location":"Engineering/Software-Engineering/Data-Engineering/apache_beam/#how","text":"Sources Beam reads your data from a diverse set of supported sources, no matter if it\u2019s on-prem or in the cloud Processing Beam executes your business logic for Batch and Streaming use cases Sinks Beam writes the results of your data processing logic to the most popular data destinations in the industry","title":"How"},{"location":"Engineering/Software-Engineering/Data-Engineering/apache_beam/#trivia","text":"was Google's internal project previously knows as dataflow","title":"Trivia"},{"location":"Engineering/Software-Engineering/Data-Engineering/apache_beam/#read-more","text":"Quickstart: https://beam.apache.org/get-started/quickstart-py/ Programming Guide: https://beam.apache.org/documentation/programming-guide/#transforms","title":"Read More"},{"location":"Engineering/Software-Engineering/Data-Engineering/apache_beam/#concept","text":"Pipeline --> DAG, Topology Input --> Spout, PCollection[PValue], Source Processor --> Bolt, PTransform[DoFn], Map, Lambda Output --> Sink, PCollection[PValue], Target %% HLD flow-diagram of a typical data pipeline framework flowchart TB %% nodes input1(\"Init\") input2(\"Data Extractor/Source/REST API Client\") processor1[\"Data Preprocessor\"] processor2[\"Data Transformer\"] processor3[\"Data Loader\"] output(\"Database\") %% relationship input1 --> input2 -- \"Data\" --> processor1 processor1 -- \"Preprocessed Data\" --> processor2 processor2 -- \"Transformed Data\" --> processor3 processor3 --> output Figure 1. Flow diagram of a typical data processing pipeline/topology","title":"Concept"},{"location":"Engineering/Software-Engineering/Data-Engineering/apache_beam/#pipeline","text":"","title":"Pipeline"},{"location":"Engineering/Software-Engineering/Data-Engineering/apache_beam/#ptransform","text":"","title":"PTransform"},{"location":"Engineering/Software-Engineering/Data-Engineering/apache_beam/#websocket-based-ptransform","text":"https://rmannibucau.metawerx.net/apache-beam-websocket-output.html","title":"WebSocket based PTransform"},{"location":"Engineering/Software-Engineering/Data-Engineering/apache_beam/#pardo","text":"","title":"ParDo"},{"location":"Engineering/Software-Engineering/Data-Engineering/apache_beam/#dofn","text":"","title":"DoFn"},{"location":"Engineering/Software-Engineering/Data-Engineering/apache_beam/#setup","text":"","title":"Setup"},{"location":"Engineering/Software-Engineering/Data-Engineering/apache_beam/#prerequisites","text":"","title":"Prerequisites"},{"location":"Engineering/Software-Engineering/Data-Engineering/apache_beam/#create","text":"","title":"Create"},{"location":"Engineering/Software-Engineering/Data-Engineering/apache_beam/#run","text":"","title":"Run"},{"location":"Engineering/Software-Engineering/Data-Engineering/apache_beam/#run-locally","text":"","title":"Run Locally"},{"location":"Engineering/Software-Engineering/Data-Engineering/apache_beam/#run-on-dataflow-runner","text":"Using service account: 1 2 3 4 5 6 7 8 9 $ python -m exploratory.wordcount \\ --region us-central1 \\ --input gs://dataflow-samples/shakespeare/kinglear.txt \\ --output gs://apache-beam-eg/results/outputs \\ --runner DataflowRunner \\ --project apache-beam-eg \\ --temp_location gs://apache-beam-eg/tmp/ \\ --service_account_email apache-beam-svc-ac@apache-beam-eg.iam.gserviceaccount.com \\ --subnetwork regions/us-central1/subnetworks/apache-beam-eg-usc1-temp # mention subnetwork other that `default`","title":"Run on Dataflow Runner"},{"location":"Engineering/Software-Engineering/Data-Engineering/apache_beam/#python-skd","text":"References: https://beam.apache.org/get-started/quickstart-py/","title":"Python SKD"},{"location":"Engineering/Software-Engineering/Data-Engineering/apache_beam/#syntax","text":"Inspired from binary arithmetic operators. Those operators magic methods are overloaded to denote some different pipeline operations. e.g. | is a synonym for apply , which applies a PTransform to a PCollection to produce a new PCollection obj.__or__(self, other) obj.__ror__(self, other) >> allows you to name a step for easier display in various UIs obj.__rrshift__(self, other) <SomePCollection> |'SomeLabel' >> <SomePTransform> SomeLabel here is the string between the | and the >> is only used for these display purposes and identifying that particular application References: https://docs.python.org/3/reference/datamodel.html?highlight=ror#object. ror https://stackoverflow.com/questions/43796046/explain-apache-beam-python-syntax visualization https://stackoverflow.com/questions/54094081/how-to-render-a-pipeline-graph-in-beam","title":"Syntax"},{"location":"Engineering/Software-Engineering/Data-Engineering/apache_beam/#faq","text":"","title":"FAQ"},{"location":"Engineering/Software-Engineering/Data-Engineering/apache_beam/#why-does-custom-python-object-cannot-be-used-with-pardo-fn","text":"https://stackoverflow.com/questions/55822881/why-does-custom-python-object-cannot-be-used-with-pardo-fn","title":"Why does custom Python object cannot be used with ParDo Fn?"},{"location":"Engineering/Software-Engineering/Data-Engineering/apache_beam/#todo","text":"","title":"TODO"},{"location":"Engineering/Software-Engineering/Data-Engineering/apache_beam/#explore","text":"mode stream/batch policy fault-tolerance, atleast-once/exact-once? strategy processing ack partitioning/sharding based in input for parallelism state management windowing visualization debug on cloud tickers - to auto schedule","title":"Explore"},{"location":"Engineering/Software-Engineering/Data-Engineering/apache_storm/","text":"Apache Storm # Table of Contents Apache Storm Setup Trident https://storm.apache.org/about/guarantees-data-processing.html Performance References Setup # https://storm.apache.org/releases/current/Setting-up-development-environment.html http://storm.apache.org/releases/current/Setting-up-a-Storm-cluster.html https://knowm.org/how-to-install-a-distributed-apache-storm-cluster/ Trident # https://www.alooma.com/blog/trident-exactly-once https://storm.apache.org/about/guarantees-data-processing.html # Performance # https://storm.apache.org/releases/current/Performance.html https://www.infoworld.com/article/2909898/review-storms-real-time-processing-comes-at-a-price.html https://stackoverflow.com/questions/35202367/storm-supervisors-workers-allocation-of-memory?answertab=active#tab-top https://www.inf.ed.ac.uk/teaching/courses/exc/slides/dataStreams03.pdf References # https://github.com/apache/storm","title":"Apache Storm"},{"location":"Engineering/Software-Engineering/Data-Engineering/apache_storm/#apache-storm","text":"","title":"Apache Storm"},{"location":"Engineering/Software-Engineering/Data-Engineering/apache_storm/#setup","text":"https://storm.apache.org/releases/current/Setting-up-development-environment.html http://storm.apache.org/releases/current/Setting-up-a-Storm-cluster.html https://knowm.org/how-to-install-a-distributed-apache-storm-cluster/","title":"Setup"},{"location":"Engineering/Software-Engineering/Data-Engineering/apache_storm/#trident","text":"https://www.alooma.com/blog/trident-exactly-once","title":"Trident"},{"location":"Engineering/Software-Engineering/Data-Engineering/apache_storm/#httpsstormapacheorgaboutguarantees-data-processinghtml","text":"","title":"https://storm.apache.org/about/guarantees-data-processing.html"},{"location":"Engineering/Software-Engineering/Data-Engineering/apache_storm/#performance","text":"https://storm.apache.org/releases/current/Performance.html https://www.infoworld.com/article/2909898/review-storms-real-time-processing-comes-at-a-price.html https://stackoverflow.com/questions/35202367/storm-supervisors-workers-allocation-of-memory?answertab=active#tab-top https://www.inf.ed.ac.uk/teaching/courses/exc/slides/dataStreams03.pdf","title":"Performance"},{"location":"Engineering/Software-Engineering/Data-Engineering/apache_storm/#references","text":"https://github.com/apache/storm","title":"References"},{"location":"Engineering/Software-Engineering/Data-Engineering/big_data/","text":"Big Data # Big Data File Formats CSV parquet OCR avro batch vs stream processing diff batch processing happens of blocks of data that have already been stored over a period of time works well in situations where you don\u2019t need real-time analytics results when it is more important to process large volumes of data to get more detailed insights than it is to get fast analytics results datasets typically looks-like: bounded: batch datasets represent a finite collection of data persistent: data is almost always backed by some type of permanent storage large: batch operations are often the only option for processing extremely large sets of data stream if we want analytics results in real time allows us to process data in real time as they arrive and quickly detect conditions within small time period from the point of receiving the data allows us to feed data into analytics tools as soon as they get generated and get instant analytics results datasets typically are: unbounded The total dataset is only defined as the amount of data that has entered the system so far The working dataset is perhaps more relevant, and is limited to a single item at a time Processing is event-based and does not \u201cend\u201d until explicitly stopped. Results are immediately available and will be continually updated as new data arrives Stream processing systems can handle a nearly unlimited amount of data, but they only process one (true stream processing) or very few (micro-batch processing) items at a time stream processing framework/engine category Compositional Engines developers define the Directed Acyclic Graph (DAG) in advance and then process the data e.g. Samza, Apex and Apache Storm Declarative Engines Developers use declarative engines to chain stream processing functions. The engine calculates the DAG as it ingests the data. Developers can specify the DAG explicitly in their code, and the engine optimizes it on the fly. e.g. Apache Spark and Flink Fully Managed Self-Service Engines A new category of stream processing engines is emerging, which not only manages the DAG but offers an end-to-end solution including ingestion of streaming data into storage infrastructure, organizing the data and facilitating streaming analytics. e.g. Upsolver examples batch common processing all the transaction at the end of day/week/month for various analysis https://miro.medium.com/max/622/1*DZFsgsjpZw3P0dNzUAYjgg.png google - amazon - project Access Point log mining for analytics stream common order status in online transaction/shopping google google doc collaborative editing google map location service amazon - project Access Point live stats Org/Site/Access Point/Client level events Anomaly detection frameworks Hadoop MapReduce Data Processing: batch only Category: data processing engine Fault-tolerance: uses replication for fault tolerance Scalability: scalable upto 1000 nodes in single Cluster Latency / Processing Speed: slow due to I/O disk (HDFS filesystem based) latency Cost: less due to disk usages Compatibility: compatible with all the data sources and file formats Security: more secure due to codebase maturity Scheduler: by external schedulers Ease of Use: low-level APIs -> more codes to write; complex debugging Duplicate Elimination: do not support License: Apache open-source software (OSS) Language Support: Java, C/C++, Ruby, Python, Perl, and Groovy Machine Learning: no inbuilt APIs; compatible with Apache Mahout - Apache Flink batch + stream - Apache Spark Data Processing: batch + stream (in-memory batch data processing framework and supports stream processing by micro-batching) Category: data analytics engine Fault-tolerance: uses RDD and other data storage models for fault tolerance Scalability: scalable upto 1000 nodes in single Cluster Latency / Processing Speed: very fast in memory; fast while running on disk Cost: more due to more RAM Compatibility: compatible with all the data sources and file formats Security: getting mature --> getting more secure Scheduler: have own schedulers Ease of Use: rich APIs -> easy to write code; easy to debug Duplicate Elimination: capable; process every records exactly once License: Apache open-source software (oss) Language Support: Java, Scala, Python, and R Machine Learning: inbuilt APIs cluster computing technology framework designed for fast computation on large-scale data processing features SQL queries, Graph Processing, and Machine Learning cluster manager options - Apache YARN, Mesos resource manager option - Hadoop Distributed File System (HDFS), Google cloud storage, Amazon S3, Microsoft Azure - Apache Storm \u2018Hadoop of real time\u2019 high throughput, low latency system guarantees at least once processing of messages best option for pure stream processing (need to tryout Flink) Data Processing: stream (BackType --> Twitter --> analyze impact of businesses on social media) Category: tbd Fault-tolerance: tbd Scalability: tbd Latency / Processing Speed: tbd Cost: tbd Compatibility: tbd Security: tbd Scheduler: htbd Ease of Use: tbd Duplicate Elimination: tbd License: Apache open-source software (oss) Language Support: tbd Machine Learning: tbd Apache Samza Apache Apex AWS Kinesis Azure Stream Analytics Microsoft TPL Dataflow Confluent KSQL frameworks comparision databases ES Cassandra message queues Kafka is a distributed messaging platform based on the publish/subscribe model that was developed by LinkedIn (while monolithic--> microservice to communicate stream data between services) Redis PubSub Google Cloud Pub/Sub File Formats # CSV # parquet # https://parquet.apache.org/docs/ https://github.com/apache/parquet-mr https://databricks.com/glossary/what-is-parquet https://db-blog.web.cern.ch/blog/zbigniew-baranowski/2017-01-performance-comparison-different-file-formats-and-storage-engines OCR # avro #","title":"Big Data"},{"location":"Engineering/Software-Engineering/Data-Engineering/big_data/#big-data","text":"Big Data File Formats CSV parquet OCR avro batch vs stream processing diff batch processing happens of blocks of data that have already been stored over a period of time works well in situations where you don\u2019t need real-time analytics results when it is more important to process large volumes of data to get more detailed insights than it is to get fast analytics results datasets typically looks-like: bounded: batch datasets represent a finite collection of data persistent: data is almost always backed by some type of permanent storage large: batch operations are often the only option for processing extremely large sets of data stream if we want analytics results in real time allows us to process data in real time as they arrive and quickly detect conditions within small time period from the point of receiving the data allows us to feed data into analytics tools as soon as they get generated and get instant analytics results datasets typically are: unbounded The total dataset is only defined as the amount of data that has entered the system so far The working dataset is perhaps more relevant, and is limited to a single item at a time Processing is event-based and does not \u201cend\u201d until explicitly stopped. Results are immediately available and will be continually updated as new data arrives Stream processing systems can handle a nearly unlimited amount of data, but they only process one (true stream processing) or very few (micro-batch processing) items at a time stream processing framework/engine category Compositional Engines developers define the Directed Acyclic Graph (DAG) in advance and then process the data e.g. Samza, Apex and Apache Storm Declarative Engines Developers use declarative engines to chain stream processing functions. The engine calculates the DAG as it ingests the data. Developers can specify the DAG explicitly in their code, and the engine optimizes it on the fly. e.g. Apache Spark and Flink Fully Managed Self-Service Engines A new category of stream processing engines is emerging, which not only manages the DAG but offers an end-to-end solution including ingestion of streaming data into storage infrastructure, organizing the data and facilitating streaming analytics. e.g. Upsolver examples batch common processing all the transaction at the end of day/week/month for various analysis https://miro.medium.com/max/622/1*DZFsgsjpZw3P0dNzUAYjgg.png google - amazon - project Access Point log mining for analytics stream common order status in online transaction/shopping google google doc collaborative editing google map location service amazon - project Access Point live stats Org/Site/Access Point/Client level events Anomaly detection frameworks Hadoop MapReduce Data Processing: batch only Category: data processing engine Fault-tolerance: uses replication for fault tolerance Scalability: scalable upto 1000 nodes in single Cluster Latency / Processing Speed: slow due to I/O disk (HDFS filesystem based) latency Cost: less due to disk usages Compatibility: compatible with all the data sources and file formats Security: more secure due to codebase maturity Scheduler: by external schedulers Ease of Use: low-level APIs -> more codes to write; complex debugging Duplicate Elimination: do not support License: Apache open-source software (OSS) Language Support: Java, C/C++, Ruby, Python, Perl, and Groovy Machine Learning: no inbuilt APIs; compatible with Apache Mahout - Apache Flink batch + stream - Apache Spark Data Processing: batch + stream (in-memory batch data processing framework and supports stream processing by micro-batching) Category: data analytics engine Fault-tolerance: uses RDD and other data storage models for fault tolerance Scalability: scalable upto 1000 nodes in single Cluster Latency / Processing Speed: very fast in memory; fast while running on disk Cost: more due to more RAM Compatibility: compatible with all the data sources and file formats Security: getting mature --> getting more secure Scheduler: have own schedulers Ease of Use: rich APIs -> easy to write code; easy to debug Duplicate Elimination: capable; process every records exactly once License: Apache open-source software (oss) Language Support: Java, Scala, Python, and R Machine Learning: inbuilt APIs cluster computing technology framework designed for fast computation on large-scale data processing features SQL queries, Graph Processing, and Machine Learning cluster manager options - Apache YARN, Mesos resource manager option - Hadoop Distributed File System (HDFS), Google cloud storage, Amazon S3, Microsoft Azure - Apache Storm \u2018Hadoop of real time\u2019 high throughput, low latency system guarantees at least once processing of messages best option for pure stream processing (need to tryout Flink) Data Processing: stream (BackType --> Twitter --> analyze impact of businesses on social media) Category: tbd Fault-tolerance: tbd Scalability: tbd Latency / Processing Speed: tbd Cost: tbd Compatibility: tbd Security: tbd Scheduler: htbd Ease of Use: tbd Duplicate Elimination: tbd License: Apache open-source software (oss) Language Support: tbd Machine Learning: tbd Apache Samza Apache Apex AWS Kinesis Azure Stream Analytics Microsoft TPL Dataflow Confluent KSQL frameworks comparision databases ES Cassandra message queues Kafka is a distributed messaging platform based on the publish/subscribe model that was developed by LinkedIn (while monolithic--> microservice to communicate stream data between services) Redis PubSub Google Cloud Pub/Sub","title":"Big Data"},{"location":"Engineering/Software-Engineering/Data-Engineering/big_data/#file-formats","text":"","title":"File Formats"},{"location":"Engineering/Software-Engineering/Data-Engineering/big_data/#csv","text":"","title":"CSV"},{"location":"Engineering/Software-Engineering/Data-Engineering/big_data/#parquet","text":"https://parquet.apache.org/docs/ https://github.com/apache/parquet-mr https://databricks.com/glossary/what-is-parquet https://db-blog.web.cern.ch/blog/zbigniew-baranowski/2017-01-performance-comparison-different-file-formats-and-storage-engines","title":"parquet"},{"location":"Engineering/Software-Engineering/Data-Engineering/big_data/#ocr","text":"","title":"OCR"},{"location":"Engineering/Software-Engineering/Data-Engineering/big_data/#avro","text":"","title":"avro"},{"location":"Engineering/Software-Engineering/Data-Engineering/gcp_dataflow/","text":"GCP Dataflow # Table of Contents GCP Dataflow Introduction What Why How Classic Template Why Template What How Create Stage Run Permission to run on Production env Run in Web Console Run as REST API Run as Python GCP Client Run using gcloud CLI Schedule Clean Clean up the Classic Template resources Clean up Google Cloud project resources how gcp dataflow stores things Python packaging to Flex Template Why Flex Template over Classic Template What How Create Stage Run Permission required Allowed ENV variables Build Allowed options/args/params Schedule Custom Worker Image Tech Concept Setup GCP Account gcloud CLI python 3.8 Apache Beam Job Authentication & Authorization Access Control with IAM Network Config CI/CD / Production Grade / Best Practice Observability Monitoring & Troubleshooting Execution Detail FAQ Deep Parallelism Autoscaling Autoscaling not working Reshuffling Optimization References Introduction # https://cloud.google.com/dataflow What # Why # How # https://www.ververica.com/blog/announcing-google-cloud-dataflow-on-flink-and-easy-flink-deployment-on-google-cloud https://research.google/pubs/pub41378/ https://storage.googleapis.com/pub-tools-public-publication-data/pdf/41378.pdf (paper:how apache beam works) https://pages.cs.wisc.edu/~akella/CS838/F12/838-CloudPapers/FlumeJava.pdf https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43864.pdf https://www.youtube.com/watch?v=3BrcmUqWNm0 https://www.youtube.com/watch?v=a7CymWiX3oM&t=399s https://cloud.google.com/dataflow/docs/concepts https://cloud.google.com/dataflow/docs/concepts/dataflow-templates https://cloud.google.com/dataflow/docs/guides/deploying-a-pipeline#parallelization-and-distribution https://cloud.google.com/dataflow/docs/guides/deploying-a-pipeline https://cloud.google.com/dataflow/docs/concepts/execution-details https://cloud.google.com/dataflow/docs/resources/faq https://cloud.google.com/dataflow/docs/resources/faq#how_many_instances_of_dofn_should_i_expect_dataflow_to_spin_up_ Classic Template # Why Template # Templates provide you with additional benefits compared to non-templated Dataflow deployment, such as: You can run your pipelines without the development environment and associated dependencies that are common with non-templated deployment. This is useful for scheduling recurring batch jobs. Templates separate the pipeline construction (performed by developers) from the running of the pipeline. Hence, there's no need to recompile the code every time the pipeline is run. Runtime parameters allow you to customize the running of the pipeline. Non-technical users can run templates with the Google Cloud Console, Google Cloud CLI, or the REST API. You can extend templates with user-defined functions. What # How # Create # Covert the Apache Beam pipeline into a Dataflow classic template. Ref: https://cloud.google.com/dataflow/docs/guides/templates/creating-templates Stage # Stage the classic template in cloud storage using the following command: 1 2 3 4 5 6 7 8 $ python -m exploratory.wordcount_with_option_and_value_provider_v2 \\ --region us-central1 \\ # cloud region --runner DataflowRunner \\ # beam runner engine --job_name wordcount-custom-job- $( date +%Y%m%d-%H%M%S ) \\ # dataflow job name --project $GCP_PROJECT \\ # cloud project --temp_location gs:// $GCP_PROJECT -dataflow-poc/tmp/ \\ # gcs path --staging_location gs:// $GCP_PROJECT -dataflow-poc/staging \\ # gcs path for staging files --template_location gs:// $GCP_PROJECT -dataflow-poc/templates/wordcount # gcs path to store template file Run # Permission to run on Production env # tbd Run in Web Console # Run as REST API # Run GCP Dataflow template job using REST API sample: Ref: https://cloud.google.com/dataflow/docs/guides/templates/provided-batch#running-the-bigquery-to-elasticsearch-template https://cloud.google.com/dataflow/docs/guides/templates/running-templates#example-1:-creating-a-custom-template-batch-job 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 $ curl \\ --url \"https://dataflow.googleapis.com/v1b3/projects/dataflow-eg/locations/us-central1/templates:launch?gcsPath=gs://dataflow-eg-dataflow-poc/templates/wordcount\" \\ --request POST \\ --header \"Authorization: Bearer \" $( gcloud auth print-access-token ) \\ --header 'Accept: application/json' \\ --header 'Content-Type: application/json' \\ --data '{ \"jobName\": \"wordcount-curl-job-' $( date +%Y%m%d-%H%M%S ) '\", \"environment\": { \"bypassTempDirValidation\": false, \"tempLocation\": \"gs://dataflow-eg-dataflow-poc/tmp/\", \"ipConfiguration\": \"WORKER_IP_UNSPECIFIED\", \"additionalExperiments\": [] }, \"parameters\": { \"input\": \"gs://dataflow-samples/shakespeare/kinglear.txt\", \"output\": \"gs://dataflow-eg-dataflow-poc/results/outputs-templated-curl\" } }' # sample output: { \"job\" : { \"id\" : \"2022-02-16_01_15_13-5260506404532792394\" , \"projectId\" : \"dataflow-eg\" , \"name\" : \"wordcount-curl-job-20220216-144511\" , \"type\" : \"JOB_TYPE_BATCH\" , \"currentStateTime\" : \"1970-01-01T00:00:00Z\" , \"createTime\" : \"2022-02-16T09:15:13.997352Z\" , \"location\" : \"us-central1\" , \"startTime\" : \"2022-02-16T09:15:13.997352Z\" } } Run as Python GCP Client # 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 # dataflow/run_template/main.py from googleapiclient.discovery import build project = 'your-gcp-project' job = 'unique-job-name' template = 'gs://dataflow-templates/latest/Word_Count' parameters = { 'inputFile' : 'gs://dataflow-samples/shakespeare/kinglear.txt' , 'output' : 'gs://<your-gcs-bucket>/wordcount/outputs' , } dataflow = build ( 'dataflow' , 'v1b3' ) request = dataflow . projects () . templates () . launch ( projectId = project , gcsPath = template , body = { 'jobName' : job , 'parameters' : parameters , } ) response = request . execute () Ref: https://github.com/GoogleCloudPlatform/python-docs-samples/tree/main/dataflow/run_template Run using gcloud CLI # 1 2 3 4 5 6 7 8 9 10 11 12 13 14 $ gcloud dataflow jobs run dataflow-templated-wordcount-custom-gcloud-job \\ --region us-central1 \\ --gcs-location gs://apache-beam-eg/templates/wordcount \\ --parameters input = gs://dataflow-samples/shakespeare/kinglear.txt,output = gs://apache-beam-eg/results/output_wordcount_gcloud # sample output: createTime: '2022-02-11T09:37:25.311851Z' currentStateTime: '1970-01-01T00:00:00Z' id: 2022 -02-11_01_37_24-16510436790575398178 location: us-central1 name: dataflow-templated-wordcount-custom-gcloud-job projectId: apache-beam-eg startTime: '2022-02-11T09:37:25.311851Z' type: JOB_TYPE_BATCH Schedule # For more refer to DevOps/GCP/Scheduler . Clean # Clean up the Classic Template resources # Stop the Dataflow pipeline. 1 2 3 4 5 gcloud dataflow jobs list \\ --filter 'NAME=<job name> AND STATE=Running' \\ --format 'value(JOB_ID)' \\ --region \"$REGION\" \\ | xargs gcloud dataflow jobs cancel --region \"$REGION\" Delete the template spec file from Cloud Storage. 1 gsutil rm <TEMPLATE_PATH> Clean up Google Cloud project resources # Delete the Cloud Scheduler jobs. 1 gcloud scheduler jobs delete <scheduler job name> how gcp dataflow stores things # 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 bucket staging/ job_name_uniq/ requirements.txt apache_beam-2.36.0-cp38-cp38-manylinux1_x86_64.whl dataflow_python_sdk.tar excel2json_3-0.1.6-py3-none-any.whl extra_packages.txt pickled_main_session # python pickled data of __main__ context pipeline.pb # python pickled data of whole python code workflow.tar.gz templates/ bigquery-poc-template # a json file tmp/ job_name_uniq/ # same as staged one, but here temporary requirements.txt apache_beam-2.36.0-cp38-cp38-manylinux1_x86_64.whl dataflow_python_sdk.tar excel2json_3-0.1.6-py3-none-any.whl extra_packages.txt pickled_main_session # python pickled data of __main__ context pipeline.pb # python pickled data of whole python code workflow.tar.gz Python packaging # use of MANIFEST.in with include_package_data=True vs package_data={} Ref: https://stackoverflow.com/questions/1612733/including-non-python-files-with-setup-py https://docs.python.org/3/distutils/sourcedist.html#the-manifest-in-template to # mention python dependencies https://beam.apache.org/documentation/sdks/python-pipeline-dependencies/ https://github.com/apache/beam/blob/master/sdks/python/apache_beam/examples/complete/juliaset/setup.py nameerror in main.py dataflow template https://cloud.google.com/dataflow/docs/resources/faq#how_do_i_handle_nameerrors use save_main_session=True pardo examples https://beam.apache.org/documentation/transforms/python/elementwise/pardo/ job failing due to dependenciesGIT_PAGER not working with requirements mentioned inside setup.py while combination of 3 works i.e. setup.py (required if local imports) extra_packages (required if local deps/.whl/tar.gz) requirements_text (required if pypi/git/remote deps) which contradicts https://issues.apache.org/jira/browse/BEAM-10115 setup.py thing not working in flex template symptom: module not found solution: use explicit --setup_file develop io connector https://beam.apache.org/documentation/io/developing-io-java/ https://www.youtube.com/watch?v=e5EdPNAw6N4 https://www.youtube.com/watch?v=eAN6rNc6EjE production ready arch https://cloud.google.com/architecture/building-production-ready-data-pipelines-using-dataflow-developing-and-testing Use dead letter queues for error handling https://cloud.google.com/architecture/building-production-ready-data-pipelines-using-dataflow-developing-and-testing#use_dead_letter_queues empty PCollection https://stackoverflow.com/questions/47624146/apache-beam-initialize-an-pcollection-to-empty DoFn vs PTransforms https://stackoverflow.com/questions/47706600/apache-beam-dofn-vs-ptransform catch: while running flex template, don't set runner explicitely to Dataflow otherwise another additional job gets triggered by default is uses Dataflow runner only observations work user a/c started experiencing some issue, started using my personal account wheel containing cloudvision-python as git dep pkg @ url works fine - except the kedro logging.yml part - on my personal account (see below cmd) multiple --extra_package is allowed - incase need to pass cloudvision.whl Flex Template # https://cloud.google.com/dataflow/docs/guides/templates/using-flex-templates#python_3 Why # Flex Template over Classic Template # Ref: https://cloud.google.com/dataflow/docs/concepts/dataflow-templates What # How # Note: For readability, we recommend that you set the entry point in your Dockerfile. When ENTRYPOINT is not set, Dataflow sets the entry point to the template launcher binary based on SDK language. https://cloud.google.com/dataflow/docs/guides/templates/configuring-flex-templates allowed parameters/flags https://cloud.google.com/dataflow/docs/reference/rest/v1b3/projects.locations.flexTemplates/launch#flextemplateruntimeenvironment Create # Stage # Run # Permission required # for Flex templates: https://cloud.google.com/dataflow/docs/guides/templates/configuring-flex-templates#understanding_flex_template_permissions Allowed ENV variables # https://cloud.google.com/dataflow/docs/reference/rest/v1b3/projects.locations.flexTemplates/launch#flextemplateruntimeenvironment Build # Allowed options/args/params # https://cloud.google.com/sdk/gcloud/reference/beta/dataflow/flex-template/build --env https://cloud.google.com/sdk/gcloud/reference/beta/dataflow/flex-template/build#--env Allowed ENV vars https://cloud.google.com/dataflow/docs/guides/templates/configuring-flex-templates#setting_required_dockerfile_environment_variables Schedule # 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 $ gcloud scheduler jobs create http apache-beam-eg-wordcount \\ --location = \"us-central1\" \\ --description = \"Run at every 00:00 UTC\" \\ --uri = \"https://dataflow.googleapis.com/v1b3/projects/apache-beam-eg/locations/us-central1/flexTemplates:launch\" \\ --http-method = \"post\" \\ --headers = \"Content-Type=application/octet-stream,User-Agent=Google-Cloud-Scheduler\" \\ --schedule = \"0 0 * * *\" \\ --time-zone = \"Etc/UTC\" \\ --message-body = '{ \"launch_parameter\": { \"jobName\": \"wordcount-v0-0-1\", \"parameters\": { \"setup_file\": \"/dataflow/wordcount/src/setup.py\" }, \"containerSpecGcsPath\": \"gs://apache-beam-eg/dataflow/wordcount/flex_template_launcher_v0.0.1.json\", \"environment\": { \"sdkContainerImage\": \"gcr.io/apache-beam-eg/dataflow/wordcount/flex_template_worker:v0.0.1\", \"additionalExperiments\": [ \"use_runner_v2\" ] } } }' \\ --oauth-service-account-email = \"apache-beam-eg@appspot.gserviceaccount.com\" \\ --oauth-token-scope = \"https://www.googleapis.com/auth/cloud-platform\" For more refer to DevOps/GCP/Scheduler . Custom Worker Image # Use mutistage Docker build https://cloud.google.com/dataflow/docs/guides/using-custom-containers?hl=en#use_a_custom_base_image_or_multi-stage_builds Tech Concept # https://cloud.google.com/dataflow/docs/concepts Setup # GCP Account # https://console.cloud.google.com/freetrial gcloud CLI # Refer to DevOps/GCP/Setup/gcloud CLI . python 3.8 # Use pyenve / conda to install appropriate python version. Apache Beam Job # https://cloud.google.com/dataflow/docs/quickstarts/quickstart-python Follow Apache Beam to write a sample Apache Beam pipeline, run them locally using Dataflow runner. Authentication & Authorization # To run the GCP dataflow template in cloud using REST. Authorization: OAuth2/OIDC scope: https://cloud.google.com/dataflow/docs/reference/rest/v1b3/projects.templates/launch Authentication: https://cloud.google.com/docs/authentication/ https://cloud.google.com/docs/authentication/getting-started#auth-cloud-implicit-python https://stackoverflow.com/questions/57433397/how-to-authorize-an-http-post-request-to-execute-dataflow-template-with-rest-api Access Control with IAM # For service accounts: https://cloud.google.com/dataflow/docs/concepts/access-control#roles Network Config # https://console.cloud.google.com/networking/networks/list?project=dataflow-eg CI/CD / Production Grade / Best Practice # https://cloud.google.com/architecture/cicd-pipeline-for-data-processing https://cloud.google.com/architecture/building-production-ready-data-pipelines-using-dataflow-deploying CI/CD example https://cloud.google.com/architecture/building-production-ready-data-pipelines-using-dataflow-deploying#example_cicd_pipeline https://medium.com/@zhongchen/dataflow-ci-cd-with-cloudbuild-1ad503c1c81 https://medium.com/everything-full-stack/dataflow-ci-cd-with-github-actions-65765f09713f https://medium.com/@emailchhavisharma/quick-steps-to-build-deploy-dataflow-flex-templates-python-java-728fc366f0d1 https://github.com/kwadie/dataflow-templates-cicd https://dataintegration.info/why-you-should-be-using-flex-templates-for-your-dataflow-deployments https://github.com/slilichenko/dataflow-jdbc-replication Observability # Monitoring & Troubleshooting # https://cloud.google.com/dataflow/pipelines/dataflow-monitoring-intf https://cloud.google.com/dataflow/pipelines/troubleshooting-your-pipeline Execution Detail # https://cloud.google.com/dataflow/docs/concepts/execution-details FAQ # https://cloud.google.com/dataflow/docs/resources/faq Deep # https://storage.googleapis.com/pub-tools-public-publication-data/pdf/41378.pdf https://medium.com/@raigonjolly/dataflow-for-google-cloud-professional-data-exam-9efd59377068 Parallelism # https://cloud.google.com/dataflow/docs/guides/deploying-a-pipeline#parallelization-and-distribution --num_workers=1-1000|3 --autoscaling_algorithm=NONE|THROUGHPUT_BASED max_num_workers Autoscaling # https://thegcpgurus.com/cloud-dataflow-how-to-implement-auto-scaling-data-pipelines/ https://cloud.google.com/dataflow/docs/guides/deploying-a-pipeline#batch-autoscaling https://www.youtube.com/watch?v=a7CymWiX3oM&t=399s Autoscaling not working # https://stackoverflow.com/questions/53885306/why-do-i-need-to-shuffle-my-pcollection-for-it-to-autoscale-on-cloud-dataflow Reshuffling # https://beam.apache.org/documentation/transforms/python/other/reshuffle/ https://stackoverflow.com/questions/54121642/apache-beam-dataflow-reshuffle https://www.programcreek.com/python/example/122924/apache_beam.Reshuffle https://beam.apache.org/releases/pydoc/current/apache_beam.transforms.util.html?highlight=reshuffle#apache_beam.transforms.util.Reshuffle https://stackoverflow.com/questions/53885306/why-do-i-need-to-shuffle-my-pcollection-for-it-to-autoscale-on-cloud-dataflow https://stackoverflow.com/questions/41212272/google-cloud-dataflow-consume-external-source https://stackoverflow.com/questions/46116443/dataflow-streaming-job-not-scaleing-past-1-worker https://stackoverflow.com/questions/46807450/scaling-of-pardo-transforms-having-blocking-network-calls https://stackoverflow.com/questions/41268877/dataflow-is-it-possible-to-run-parts-of-the-pipeline-synchronously-and-other-pa Optimization # https://www.carted.com/blog/improving-dataflow-pipelines-for-text-data-processing/ https://cloud.google.com/dataflow/docs/concepts/execution-details References # external https://medium.com/@zhongchen/schedule-your-dataflow-batch-jobs-with-cloud-scheduler-8390e0e958eb https://github.com/zhongchen/GCP-Demo/tree/master/demos/scheduler-dataflow-demo/terraform https://medium.com/@jamesmoore255/creating-a-template-for-the-python-cloud-dataflow-sdk-2fe36cc4167f google pricing calc","title":"GCP Dataflow"},{"location":"Engineering/Software-Engineering/Data-Engineering/gcp_dataflow/#gcp-dataflow","text":"","title":"GCP Dataflow"},{"location":"Engineering/Software-Engineering/Data-Engineering/gcp_dataflow/#introduction","text":"https://cloud.google.com/dataflow","title":"Introduction"},{"location":"Engineering/Software-Engineering/Data-Engineering/gcp_dataflow/#what","text":"","title":"What"},{"location":"Engineering/Software-Engineering/Data-Engineering/gcp_dataflow/#why","text":"","title":"Why"},{"location":"Engineering/Software-Engineering/Data-Engineering/gcp_dataflow/#how","text":"https://www.ververica.com/blog/announcing-google-cloud-dataflow-on-flink-and-easy-flink-deployment-on-google-cloud https://research.google/pubs/pub41378/ https://storage.googleapis.com/pub-tools-public-publication-data/pdf/41378.pdf (paper:how apache beam works) https://pages.cs.wisc.edu/~akella/CS838/F12/838-CloudPapers/FlumeJava.pdf https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43864.pdf https://www.youtube.com/watch?v=3BrcmUqWNm0 https://www.youtube.com/watch?v=a7CymWiX3oM&t=399s https://cloud.google.com/dataflow/docs/concepts https://cloud.google.com/dataflow/docs/concepts/dataflow-templates https://cloud.google.com/dataflow/docs/guides/deploying-a-pipeline#parallelization-and-distribution https://cloud.google.com/dataflow/docs/guides/deploying-a-pipeline https://cloud.google.com/dataflow/docs/concepts/execution-details https://cloud.google.com/dataflow/docs/resources/faq https://cloud.google.com/dataflow/docs/resources/faq#how_many_instances_of_dofn_should_i_expect_dataflow_to_spin_up_","title":"How"},{"location":"Engineering/Software-Engineering/Data-Engineering/gcp_dataflow/#classic-template","text":"","title":"Classic Template"},{"location":"Engineering/Software-Engineering/Data-Engineering/gcp_dataflow/#why-template","text":"Templates provide you with additional benefits compared to non-templated Dataflow deployment, such as: You can run your pipelines without the development environment and associated dependencies that are common with non-templated deployment. This is useful for scheduling recurring batch jobs. Templates separate the pipeline construction (performed by developers) from the running of the pipeline. Hence, there's no need to recompile the code every time the pipeline is run. Runtime parameters allow you to customize the running of the pipeline. Non-technical users can run templates with the Google Cloud Console, Google Cloud CLI, or the REST API. You can extend templates with user-defined functions.","title":"Why Template"},{"location":"Engineering/Software-Engineering/Data-Engineering/gcp_dataflow/#what_1","text":"","title":"What"},{"location":"Engineering/Software-Engineering/Data-Engineering/gcp_dataflow/#how_1","text":"","title":"How"},{"location":"Engineering/Software-Engineering/Data-Engineering/gcp_dataflow/#create","text":"Covert the Apache Beam pipeline into a Dataflow classic template. Ref: https://cloud.google.com/dataflow/docs/guides/templates/creating-templates","title":"Create"},{"location":"Engineering/Software-Engineering/Data-Engineering/gcp_dataflow/#stage","text":"Stage the classic template in cloud storage using the following command: 1 2 3 4 5 6 7 8 $ python -m exploratory.wordcount_with_option_and_value_provider_v2 \\ --region us-central1 \\ # cloud region --runner DataflowRunner \\ # beam runner engine --job_name wordcount-custom-job- $( date +%Y%m%d-%H%M%S ) \\ # dataflow job name --project $GCP_PROJECT \\ # cloud project --temp_location gs:// $GCP_PROJECT -dataflow-poc/tmp/ \\ # gcs path --staging_location gs:// $GCP_PROJECT -dataflow-poc/staging \\ # gcs path for staging files --template_location gs:// $GCP_PROJECT -dataflow-poc/templates/wordcount # gcs path to store template file","title":"Stage"},{"location":"Engineering/Software-Engineering/Data-Engineering/gcp_dataflow/#run","text":"","title":"Run"},{"location":"Engineering/Software-Engineering/Data-Engineering/gcp_dataflow/#permission-to-run-on-production-env","text":"tbd","title":"Permission to run on Production env"},{"location":"Engineering/Software-Engineering/Data-Engineering/gcp_dataflow/#run-in-web-console","text":"","title":"Run in Web Console"},{"location":"Engineering/Software-Engineering/Data-Engineering/gcp_dataflow/#run-as-rest-api","text":"Run GCP Dataflow template job using REST API sample: Ref: https://cloud.google.com/dataflow/docs/guides/templates/provided-batch#running-the-bigquery-to-elasticsearch-template https://cloud.google.com/dataflow/docs/guides/templates/running-templates#example-1:-creating-a-custom-template-batch-job 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 $ curl \\ --url \"https://dataflow.googleapis.com/v1b3/projects/dataflow-eg/locations/us-central1/templates:launch?gcsPath=gs://dataflow-eg-dataflow-poc/templates/wordcount\" \\ --request POST \\ --header \"Authorization: Bearer \" $( gcloud auth print-access-token ) \\ --header 'Accept: application/json' \\ --header 'Content-Type: application/json' \\ --data '{ \"jobName\": \"wordcount-curl-job-' $( date +%Y%m%d-%H%M%S ) '\", \"environment\": { \"bypassTempDirValidation\": false, \"tempLocation\": \"gs://dataflow-eg-dataflow-poc/tmp/\", \"ipConfiguration\": \"WORKER_IP_UNSPECIFIED\", \"additionalExperiments\": [] }, \"parameters\": { \"input\": \"gs://dataflow-samples/shakespeare/kinglear.txt\", \"output\": \"gs://dataflow-eg-dataflow-poc/results/outputs-templated-curl\" } }' # sample output: { \"job\" : { \"id\" : \"2022-02-16_01_15_13-5260506404532792394\" , \"projectId\" : \"dataflow-eg\" , \"name\" : \"wordcount-curl-job-20220216-144511\" , \"type\" : \"JOB_TYPE_BATCH\" , \"currentStateTime\" : \"1970-01-01T00:00:00Z\" , \"createTime\" : \"2022-02-16T09:15:13.997352Z\" , \"location\" : \"us-central1\" , \"startTime\" : \"2022-02-16T09:15:13.997352Z\" } }","title":"Run as REST API"},{"location":"Engineering/Software-Engineering/Data-Engineering/gcp_dataflow/#run-as-python-gcp-client","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 # dataflow/run_template/main.py from googleapiclient.discovery import build project = 'your-gcp-project' job = 'unique-job-name' template = 'gs://dataflow-templates/latest/Word_Count' parameters = { 'inputFile' : 'gs://dataflow-samples/shakespeare/kinglear.txt' , 'output' : 'gs://<your-gcs-bucket>/wordcount/outputs' , } dataflow = build ( 'dataflow' , 'v1b3' ) request = dataflow . projects () . templates () . launch ( projectId = project , gcsPath = template , body = { 'jobName' : job , 'parameters' : parameters , } ) response = request . execute () Ref: https://github.com/GoogleCloudPlatform/python-docs-samples/tree/main/dataflow/run_template","title":"Run as Python GCP Client"},{"location":"Engineering/Software-Engineering/Data-Engineering/gcp_dataflow/#run-using-gcloud-cli","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 $ gcloud dataflow jobs run dataflow-templated-wordcount-custom-gcloud-job \\ --region us-central1 \\ --gcs-location gs://apache-beam-eg/templates/wordcount \\ --parameters input = gs://dataflow-samples/shakespeare/kinglear.txt,output = gs://apache-beam-eg/results/output_wordcount_gcloud # sample output: createTime: '2022-02-11T09:37:25.311851Z' currentStateTime: '1970-01-01T00:00:00Z' id: 2022 -02-11_01_37_24-16510436790575398178 location: us-central1 name: dataflow-templated-wordcount-custom-gcloud-job projectId: apache-beam-eg startTime: '2022-02-11T09:37:25.311851Z' type: JOB_TYPE_BATCH","title":"Run using gcloud CLI"},{"location":"Engineering/Software-Engineering/Data-Engineering/gcp_dataflow/#schedule","text":"For more refer to DevOps/GCP/Scheduler .","title":"Schedule"},{"location":"Engineering/Software-Engineering/Data-Engineering/gcp_dataflow/#clean","text":"","title":"Clean"},{"location":"Engineering/Software-Engineering/Data-Engineering/gcp_dataflow/#clean-up-the-classic-template-resources","text":"Stop the Dataflow pipeline. 1 2 3 4 5 gcloud dataflow jobs list \\ --filter 'NAME=<job name> AND STATE=Running' \\ --format 'value(JOB_ID)' \\ --region \"$REGION\" \\ | xargs gcloud dataflow jobs cancel --region \"$REGION\" Delete the template spec file from Cloud Storage. 1 gsutil rm <TEMPLATE_PATH>","title":"Clean up the Classic Template resources"},{"location":"Engineering/Software-Engineering/Data-Engineering/gcp_dataflow/#clean-up-google-cloud-project-resources","text":"Delete the Cloud Scheduler jobs. 1 gcloud scheduler jobs delete <scheduler job name>","title":"Clean up Google Cloud project resources"},{"location":"Engineering/Software-Engineering/Data-Engineering/gcp_dataflow/#how-gcp-dataflow-stores-things","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 bucket staging/ job_name_uniq/ requirements.txt apache_beam-2.36.0-cp38-cp38-manylinux1_x86_64.whl dataflow_python_sdk.tar excel2json_3-0.1.6-py3-none-any.whl extra_packages.txt pickled_main_session # python pickled data of __main__ context pipeline.pb # python pickled data of whole python code workflow.tar.gz templates/ bigquery-poc-template # a json file tmp/ job_name_uniq/ # same as staged one, but here temporary requirements.txt apache_beam-2.36.0-cp38-cp38-manylinux1_x86_64.whl dataflow_python_sdk.tar excel2json_3-0.1.6-py3-none-any.whl extra_packages.txt pickled_main_session # python pickled data of __main__ context pipeline.pb # python pickled data of whole python code workflow.tar.gz","title":"how gcp dataflow stores things"},{"location":"Engineering/Software-Engineering/Data-Engineering/gcp_dataflow/#python-packaging","text":"use of MANIFEST.in with include_package_data=True vs package_data={} Ref: https://stackoverflow.com/questions/1612733/including-non-python-files-with-setup-py https://docs.python.org/3/distutils/sourcedist.html#the-manifest-in-template","title":"Python packaging"},{"location":"Engineering/Software-Engineering/Data-Engineering/gcp_dataflow/#to","text":"mention python dependencies https://beam.apache.org/documentation/sdks/python-pipeline-dependencies/ https://github.com/apache/beam/blob/master/sdks/python/apache_beam/examples/complete/juliaset/setup.py nameerror in main.py dataflow template https://cloud.google.com/dataflow/docs/resources/faq#how_do_i_handle_nameerrors use save_main_session=True pardo examples https://beam.apache.org/documentation/transforms/python/elementwise/pardo/ job failing due to dependenciesGIT_PAGER not working with requirements mentioned inside setup.py while combination of 3 works i.e. setup.py (required if local imports) extra_packages (required if local deps/.whl/tar.gz) requirements_text (required if pypi/git/remote deps) which contradicts https://issues.apache.org/jira/browse/BEAM-10115 setup.py thing not working in flex template symptom: module not found solution: use explicit --setup_file develop io connector https://beam.apache.org/documentation/io/developing-io-java/ https://www.youtube.com/watch?v=e5EdPNAw6N4 https://www.youtube.com/watch?v=eAN6rNc6EjE production ready arch https://cloud.google.com/architecture/building-production-ready-data-pipelines-using-dataflow-developing-and-testing Use dead letter queues for error handling https://cloud.google.com/architecture/building-production-ready-data-pipelines-using-dataflow-developing-and-testing#use_dead_letter_queues empty PCollection https://stackoverflow.com/questions/47624146/apache-beam-initialize-an-pcollection-to-empty DoFn vs PTransforms https://stackoverflow.com/questions/47706600/apache-beam-dofn-vs-ptransform catch: while running flex template, don't set runner explicitely to Dataflow otherwise another additional job gets triggered by default is uses Dataflow runner only observations work user a/c started experiencing some issue, started using my personal account wheel containing cloudvision-python as git dep pkg @ url works fine - except the kedro logging.yml part - on my personal account (see below cmd) multiple --extra_package is allowed - incase need to pass cloudvision.whl","title":"to"},{"location":"Engineering/Software-Engineering/Data-Engineering/gcp_dataflow/#flex-template","text":"https://cloud.google.com/dataflow/docs/guides/templates/using-flex-templates#python_3","title":"Flex Template"},{"location":"Engineering/Software-Engineering/Data-Engineering/gcp_dataflow/#why_1","text":"","title":"Why"},{"location":"Engineering/Software-Engineering/Data-Engineering/gcp_dataflow/#flex-template-over-classic-template","text":"Ref: https://cloud.google.com/dataflow/docs/concepts/dataflow-templates","title":"Flex Template over Classic Template"},{"location":"Engineering/Software-Engineering/Data-Engineering/gcp_dataflow/#what_2","text":"","title":"What"},{"location":"Engineering/Software-Engineering/Data-Engineering/gcp_dataflow/#how_2","text":"Note: For readability, we recommend that you set the entry point in your Dockerfile. When ENTRYPOINT is not set, Dataflow sets the entry point to the template launcher binary based on SDK language. https://cloud.google.com/dataflow/docs/guides/templates/configuring-flex-templates allowed parameters/flags https://cloud.google.com/dataflow/docs/reference/rest/v1b3/projects.locations.flexTemplates/launch#flextemplateruntimeenvironment","title":"How"},{"location":"Engineering/Software-Engineering/Data-Engineering/gcp_dataflow/#create_1","text":"","title":"Create"},{"location":"Engineering/Software-Engineering/Data-Engineering/gcp_dataflow/#stage_1","text":"","title":"Stage"},{"location":"Engineering/Software-Engineering/Data-Engineering/gcp_dataflow/#run_1","text":"","title":"Run"},{"location":"Engineering/Software-Engineering/Data-Engineering/gcp_dataflow/#permission-required","text":"for Flex templates: https://cloud.google.com/dataflow/docs/guides/templates/configuring-flex-templates#understanding_flex_template_permissions","title":"Permission required"},{"location":"Engineering/Software-Engineering/Data-Engineering/gcp_dataflow/#allowed-env-variables","text":"https://cloud.google.com/dataflow/docs/reference/rest/v1b3/projects.locations.flexTemplates/launch#flextemplateruntimeenvironment","title":"Allowed ENV variables"},{"location":"Engineering/Software-Engineering/Data-Engineering/gcp_dataflow/#build","text":"","title":"Build"},{"location":"Engineering/Software-Engineering/Data-Engineering/gcp_dataflow/#allowed-optionsargsparams","text":"https://cloud.google.com/sdk/gcloud/reference/beta/dataflow/flex-template/build --env https://cloud.google.com/sdk/gcloud/reference/beta/dataflow/flex-template/build#--env Allowed ENV vars https://cloud.google.com/dataflow/docs/guides/templates/configuring-flex-templates#setting_required_dockerfile_environment_variables","title":"Allowed options/args/params"},{"location":"Engineering/Software-Engineering/Data-Engineering/gcp_dataflow/#schedule_1","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 $ gcloud scheduler jobs create http apache-beam-eg-wordcount \\ --location = \"us-central1\" \\ --description = \"Run at every 00:00 UTC\" \\ --uri = \"https://dataflow.googleapis.com/v1b3/projects/apache-beam-eg/locations/us-central1/flexTemplates:launch\" \\ --http-method = \"post\" \\ --headers = \"Content-Type=application/octet-stream,User-Agent=Google-Cloud-Scheduler\" \\ --schedule = \"0 0 * * *\" \\ --time-zone = \"Etc/UTC\" \\ --message-body = '{ \"launch_parameter\": { \"jobName\": \"wordcount-v0-0-1\", \"parameters\": { \"setup_file\": \"/dataflow/wordcount/src/setup.py\" }, \"containerSpecGcsPath\": \"gs://apache-beam-eg/dataflow/wordcount/flex_template_launcher_v0.0.1.json\", \"environment\": { \"sdkContainerImage\": \"gcr.io/apache-beam-eg/dataflow/wordcount/flex_template_worker:v0.0.1\", \"additionalExperiments\": [ \"use_runner_v2\" ] } } }' \\ --oauth-service-account-email = \"apache-beam-eg@appspot.gserviceaccount.com\" \\ --oauth-token-scope = \"https://www.googleapis.com/auth/cloud-platform\" For more refer to DevOps/GCP/Scheduler .","title":"Schedule"},{"location":"Engineering/Software-Engineering/Data-Engineering/gcp_dataflow/#custom-worker-image","text":"Use mutistage Docker build https://cloud.google.com/dataflow/docs/guides/using-custom-containers?hl=en#use_a_custom_base_image_or_multi-stage_builds","title":"Custom Worker Image"},{"location":"Engineering/Software-Engineering/Data-Engineering/gcp_dataflow/#tech-concept","text":"https://cloud.google.com/dataflow/docs/concepts","title":"Tech Concept"},{"location":"Engineering/Software-Engineering/Data-Engineering/gcp_dataflow/#setup","text":"","title":"Setup"},{"location":"Engineering/Software-Engineering/Data-Engineering/gcp_dataflow/#gcp-account","text":"https://console.cloud.google.com/freetrial","title":"GCP Account"},{"location":"Engineering/Software-Engineering/Data-Engineering/gcp_dataflow/#gcloud-cli","text":"Refer to DevOps/GCP/Setup/gcloud CLI .","title":"gcloud CLI"},{"location":"Engineering/Software-Engineering/Data-Engineering/gcp_dataflow/#python-38","text":"Use pyenve / conda to install appropriate python version.","title":"python 3.8"},{"location":"Engineering/Software-Engineering/Data-Engineering/gcp_dataflow/#apache-beam-job","text":"https://cloud.google.com/dataflow/docs/quickstarts/quickstart-python Follow Apache Beam to write a sample Apache Beam pipeline, run them locally using Dataflow runner.","title":"Apache Beam Job"},{"location":"Engineering/Software-Engineering/Data-Engineering/gcp_dataflow/#authentication-authorization","text":"To run the GCP dataflow template in cloud using REST. Authorization: OAuth2/OIDC scope: https://cloud.google.com/dataflow/docs/reference/rest/v1b3/projects.templates/launch Authentication: https://cloud.google.com/docs/authentication/ https://cloud.google.com/docs/authentication/getting-started#auth-cloud-implicit-python https://stackoverflow.com/questions/57433397/how-to-authorize-an-http-post-request-to-execute-dataflow-template-with-rest-api","title":"Authentication &amp; Authorization"},{"location":"Engineering/Software-Engineering/Data-Engineering/gcp_dataflow/#access-control-with-iam","text":"For service accounts: https://cloud.google.com/dataflow/docs/concepts/access-control#roles","title":"Access Control with IAM"},{"location":"Engineering/Software-Engineering/Data-Engineering/gcp_dataflow/#network-config","text":"https://console.cloud.google.com/networking/networks/list?project=dataflow-eg","title":"Network Config"},{"location":"Engineering/Software-Engineering/Data-Engineering/gcp_dataflow/#cicd-production-grade-best-practice","text":"https://cloud.google.com/architecture/cicd-pipeline-for-data-processing https://cloud.google.com/architecture/building-production-ready-data-pipelines-using-dataflow-deploying CI/CD example https://cloud.google.com/architecture/building-production-ready-data-pipelines-using-dataflow-deploying#example_cicd_pipeline https://medium.com/@zhongchen/dataflow-ci-cd-with-cloudbuild-1ad503c1c81 https://medium.com/everything-full-stack/dataflow-ci-cd-with-github-actions-65765f09713f https://medium.com/@emailchhavisharma/quick-steps-to-build-deploy-dataflow-flex-templates-python-java-728fc366f0d1 https://github.com/kwadie/dataflow-templates-cicd https://dataintegration.info/why-you-should-be-using-flex-templates-for-your-dataflow-deployments https://github.com/slilichenko/dataflow-jdbc-replication","title":"CI/CD / Production Grade / Best Practice"},{"location":"Engineering/Software-Engineering/Data-Engineering/gcp_dataflow/#observability","text":"","title":"Observability"},{"location":"Engineering/Software-Engineering/Data-Engineering/gcp_dataflow/#monitoring-troubleshooting","text":"https://cloud.google.com/dataflow/pipelines/dataflow-monitoring-intf https://cloud.google.com/dataflow/pipelines/troubleshooting-your-pipeline","title":"Monitoring &amp; Troubleshooting"},{"location":"Engineering/Software-Engineering/Data-Engineering/gcp_dataflow/#execution-detail","text":"https://cloud.google.com/dataflow/docs/concepts/execution-details","title":"Execution Detail"},{"location":"Engineering/Software-Engineering/Data-Engineering/gcp_dataflow/#faq","text":"https://cloud.google.com/dataflow/docs/resources/faq","title":"FAQ"},{"location":"Engineering/Software-Engineering/Data-Engineering/gcp_dataflow/#deep","text":"https://storage.googleapis.com/pub-tools-public-publication-data/pdf/41378.pdf https://medium.com/@raigonjolly/dataflow-for-google-cloud-professional-data-exam-9efd59377068","title":"Deep"},{"location":"Engineering/Software-Engineering/Data-Engineering/gcp_dataflow/#parallelism","text":"https://cloud.google.com/dataflow/docs/guides/deploying-a-pipeline#parallelization-and-distribution --num_workers=1-1000|3 --autoscaling_algorithm=NONE|THROUGHPUT_BASED max_num_workers","title":"Parallelism"},{"location":"Engineering/Software-Engineering/Data-Engineering/gcp_dataflow/#autoscaling","text":"https://thegcpgurus.com/cloud-dataflow-how-to-implement-auto-scaling-data-pipelines/ https://cloud.google.com/dataflow/docs/guides/deploying-a-pipeline#batch-autoscaling https://www.youtube.com/watch?v=a7CymWiX3oM&t=399s","title":"Autoscaling"},{"location":"Engineering/Software-Engineering/Data-Engineering/gcp_dataflow/#autoscaling-not-working","text":"https://stackoverflow.com/questions/53885306/why-do-i-need-to-shuffle-my-pcollection-for-it-to-autoscale-on-cloud-dataflow","title":"Autoscaling not working"},{"location":"Engineering/Software-Engineering/Data-Engineering/gcp_dataflow/#reshuffling","text":"https://beam.apache.org/documentation/transforms/python/other/reshuffle/ https://stackoverflow.com/questions/54121642/apache-beam-dataflow-reshuffle https://www.programcreek.com/python/example/122924/apache_beam.Reshuffle https://beam.apache.org/releases/pydoc/current/apache_beam.transforms.util.html?highlight=reshuffle#apache_beam.transforms.util.Reshuffle https://stackoverflow.com/questions/53885306/why-do-i-need-to-shuffle-my-pcollection-for-it-to-autoscale-on-cloud-dataflow https://stackoverflow.com/questions/41212272/google-cloud-dataflow-consume-external-source https://stackoverflow.com/questions/46116443/dataflow-streaming-job-not-scaleing-past-1-worker https://stackoverflow.com/questions/46807450/scaling-of-pardo-transforms-having-blocking-network-calls https://stackoverflow.com/questions/41268877/dataflow-is-it-possible-to-run-parts-of-the-pipeline-synchronously-and-other-pa","title":"Reshuffling"},{"location":"Engineering/Software-Engineering/Data-Engineering/gcp_dataflow/#optimization","text":"https://www.carted.com/blog/improving-dataflow-pipelines-for-text-data-processing/ https://cloud.google.com/dataflow/docs/concepts/execution-details","title":"Optimization"},{"location":"Engineering/Software-Engineering/Data-Engineering/gcp_dataflow/#references","text":"external https://medium.com/@zhongchen/schedule-your-dataflow-batch-jobs-with-cloud-scheduler-8390e0e958eb https://github.com/zhongchen/GCP-Demo/tree/master/demos/scheduler-dataflow-demo/terraform https://medium.com/@jamesmoore255/creating-a-template-for-the-python-cloud-dataflow-sdk-2fe36cc4167f google pricing calc","title":"References"},{"location":"Engineering/Software-Engineering/Database/elasticsearch/","text":"ElasticSearch # - Shay Banon, 2010 Table of Contents ElasticSearch What How Components What # ElasticSearch (ES) is a search and analytic engine (more than a No-SQL database) based on Apache Lucene library provides distributed multi-tenant capable full-text search engine through HTTP web interface and (schema-free) JSON documents kind: index & document based How # How does ElasticSearch work? ES uses a concept/data structure called Inverted Index which is designed to perform very fast full-text search A document (says string/sentence) saved as well as is analyzed and tokenized for better search results the sentance, say \"this is a test string\" is split into words, n-grams and each part is mapped to relevant document id, position, and number of occurrences ES index is made up of one or more shards each shard can have zero or more replicas these are individual Lucene indices one ES index is made up of multiple Lucene indices either a single index's multiple shards could be searched, even multiple ES indices could be searched for a term any of this is just combination of multiple Lucene indices shard is the main scaling unit in ES Components # Nodes Master Controls the Elasticsearch cluster and is responsible for all cluster-wide operations like creating/deleting an index and adding/removing nodes. Data Stores data and executes data-related operations such as search and aggregation. Client Forwards cluster requests to the master node and data-related requests to data nodes. Indices Index Shard Document Replicas of shards Ref: - https://www.elastic.co/blog/found-elasticsearch-from-the-bottom-up - https://www.elastic.co/what-is/elasticsearch - https://www.knowi.com/blog/what-is-elastic-search/","title":"ElasticSearch"},{"location":"Engineering/Software-Engineering/Database/elasticsearch/#elasticsearch","text":"- Shay Banon, 2010","title":"ElasticSearch"},{"location":"Engineering/Software-Engineering/Database/elasticsearch/#what","text":"ElasticSearch (ES) is a search and analytic engine (more than a No-SQL database) based on Apache Lucene library provides distributed multi-tenant capable full-text search engine through HTTP web interface and (schema-free) JSON documents kind: index & document based","title":"What"},{"location":"Engineering/Software-Engineering/Database/elasticsearch/#how","text":"How does ElasticSearch work? ES uses a concept/data structure called Inverted Index which is designed to perform very fast full-text search A document (says string/sentence) saved as well as is analyzed and tokenized for better search results the sentance, say \"this is a test string\" is split into words, n-grams and each part is mapped to relevant document id, position, and number of occurrences ES index is made up of one or more shards each shard can have zero or more replicas these are individual Lucene indices one ES index is made up of multiple Lucene indices either a single index's multiple shards could be searched, even multiple ES indices could be searched for a term any of this is just combination of multiple Lucene indices shard is the main scaling unit in ES","title":"How"},{"location":"Engineering/Software-Engineering/Database/elasticsearch/#components","text":"Nodes Master Controls the Elasticsearch cluster and is responsible for all cluster-wide operations like creating/deleting an index and adding/removing nodes. Data Stores data and executes data-related operations such as search and aggregation. Client Forwards cluster requests to the master node and data-related requests to data nodes. Indices Index Shard Document Replicas of shards Ref: - https://www.elastic.co/blog/found-elasticsearch-from-the-bottom-up - https://www.elastic.co/what-is/elasticsearch - https://www.knowi.com/blog/what-is-elastic-search/","title":"Components"},{"location":"Engineering/Software-Engineering/Database/gcp_bigquery/","text":"GCP BigQuery # GCP BigQuery Introduction Dataset Multi-Region Dataset Quota & Limit Copying a Dataset Errors Table Table Schema Modify Errors Manually specifying table schema Vs Schema auto-detection Row-level Security Standard Table Quota & Limit Cost Partitioned Table Quota & Limit Cost Create Manage/Modify Query Clustered Table Quota & Limit Cost Partitioning vs Clustering Partitioning vs Sharding BigQuery Data Storage Cost Active Storage Cost Quota & Limit Long-term Storage Cost Quota & Limit Data Size Calculation BigQuery API Data Ingestion (load jobs) API Batch Loading Cost Quota & Limit Stream Loading (Legacy) Cost Quota & Limit BigQuery Storage Write API Cost Quota & Limit Data Fetch/Query/Extraction/Read/Export APIs Batch Export Cost Quota & Limit BigQuery Storage Read API Cost Quota & Limit SQL Query BigQuery SQL (Legacy SQL) Standard SQL (New) BigQuery SQL vs Standard SQL BigQuery Storage Read API Query Size Calculation Misc Quota & Limit Quota Error Diagnosis raise self._exception google.api_core.exceptions.Forbidden: 403 Exceeded rate limits: too many table update operations for this table. For more information, see https://cloud.google.com/bigquery/docs/troubleshoot-quotas Monitoring Quota Metrics BigQuery API Clients python-bigquery pandas-gbq Usecases Data Warehousing References Introduction # Provider managed, serverless, multi-cloud data warehouse. Dataset # Multi-Region Dataset # Quota & Limit # https://cloud.google.com/bigquery/quotas#dataset_limits Maximum number of datasets: unlimited (per project) Number of tables per dataset: unlimited When you use an API call, enumeration performance slows as you approach 50,000 tables in a dataset The Cloud console can display up to 50,000 tables for each dataset Number of authorized resources in a dataset's access control list: 2500 resources Copying a Dataset # https://cloud.google.com/bigquery/docs/copying-datasets#console Moving/Renaming of Dataset is currently not possible, thus there is \"Copying\" Errors # 1 2 raise exceptions.from_http_response(exc.response) google.api_core.exceptions.NotFound: 404 POST https://bigquery.googleapis.com/upload/bigquery/v2/projects/poc/jobs?uploadType=multipart: Not found: Dataset poc:ds_v2 Need to create Dataset before creating tables Table # https://cloud.google.com/bigquery/quotas#table_limits Table Schema # Modify # https://cloud.google.com/bigquery/docs/managing-table-schemas smooth modification: Adding columns to a schema definition https://cloud.google.com/bigquery/docs/managing-table-schemas#adding_columns_to_a_tables_schema_definition Deleting, or dropping, a column from a schema definition Relaxing a column's mode from REQUIRED to NULLABLE manual modification: https://cloud.google.com/bigquery/docs/manually-changing-schemas Changing a column's name Changing a column's data type Changing a column's mode (aside from relaxing REQUIRED columns to NULLABLE) Errors # 1 2 raise exceptions.from_http_response(exc.response) google.api_core.exceptions.BadRequest: 400 POST https://bigquery.googleapis.com/upload/bigquery/v2/projects/poc/jobs?uploadType=multipart: Provided Schema does not match Table poc:ds.table1. Cannot add fields (field: ColA) If you add new columns to an existing table schema, the columns must be NULLABLE or REPEATED. You cannot add a REQUIRED column to an existing table schema. REQUIRED columns can be added only when you create a table while loading data, or when you create an empty table with a schema definition. Manually specifying table schema Vs Schema auto-detection # Understand BigQuery Schema Auto-detection Using schema auto-detection Getting BigQuery to auto detect schema Possible \u201cBug\u201d in BigQuery Table Loading with Auto Detect Schema Load data from DataFrame Specify the type of columns whose type cannot be auto-detected. For example the \"title\" column uses pandas dtype \"object\", so its data type is ambiguous. Row-level Security # https://cloud.google.com/bigquery/quotas#row_level_security Standard Table # Quota & Limit # https://cloud.google.com/bigquery/quotas#standard_tables Table operations per day: 1500 Maximum number of columns per table: 10000 Cost # Partitioned Table # Quota & Limit # https://cloud.google.com/bigquery/quotas#partitioned_tables Number of partitions per partitioned table: 4000 Cost # Create # https://cloud.google.com/bigquery/docs/creating-partitioned-tables#python Manage/Modify # https://cloud.google.com/bigquery/docs/managing-partitioned-tables#api_2 Query # https://cloud.google.com/bigquery/docs/querying-partitioned-tables Clustered Table # Quota & Limit # Cost # Partitioning vs Clustering # https://cloud.google.com/bigquery/docs/partitioned-tables#partitioning_versus_clustering data is first partitioned and then clustered partitioning pros to set partition expiration time advance query cost estimation/pruning to define ranges (time, id etc) over partition cons clustering pros more granualarity than partitioning alone same column for both - partitioning as well as clustering to speed up queries having filters & aggregation clause when cardinality of values of a column/set of column is large suitable when partitioning results in small amount of data per partition suitable when partitioning results in large number of paritions (beyond limit & quota) suitable when mutation operations modifies most of the partitions in the table frequestly (e.g. every minutes/hours) cons query cost estimation/pruning is actually NOT possible only after query finishes, the cost comes up NO significant difference in query performance between a clustered and unclustered table if the table or partition is under 1 GB Partitioning vs Sharding # https://cloud.google.com/bigquery/docs/partitioned-tables#dt_partition_shard BigQuery Data Storage # https://cloud.google.com/bigquery/pricing#storage Cost # first 10 GB per month is free for more than 1PB, contact sales team for pricing pricing is prorated per MB, per second i.e. 1 GB of Long-term storage for half month is $0.005 If the table is edited, the price reverts back to the regular storage pricing, and the 90-day timer starts counting from zero Active Storage # if a table[-partition] is modified for <= 90 consecutive days Cost # first 10 GB per month is free $0.02 per GB Quota & Limit # Long-term Storage # if a table[-partition] is NOT modified for > 90 consecutive days Cost # first 10 GB per month is free pricing is ~50% half of Active Storage pricing $0.01 per GB Quota & Limit # Data Size Calculation # https://cloud.google.com/bigquery/pricing#data table size is calculated based on column data-type size when its in uncompressed form BigQuery API # https://cloud.google.com/bigquery/quotas#api_request_quotas Data Ingestion (load jobs) API # https://cloud.google.com/bigquery/docs/loading-data https://cloud.google.com/bigquery/pricing#data_ingestion_pricing Batch Loading # https://cloud.google.com/bigquery/docs/loading-data#batch_loading load bulk data into one or more table uses shared pool of slots no guarantee of availability/capacity Cost # free option to purchase dedicated slots to run load jobs flat-rate pricing Quota & Limit # https://cloud.google.com/bigquery/quotas#load_jobs Load jobs per day: 100000 jobs i.e. 1.15 job per second failed load jobs count toward this limit Maximum size per load job: 15 TB Load job execution-time limit: 6 hours A load job fails if it executes for longer than six hours CSV: Maximum file size compressed: 4 GB uncompressed: 5 TB NOTE: If regularly exceed the load job limits due to frequent updates, consider streaming data into BigQuery instead. Stream Loading (Legacy) # https://cloud.google.com/bigquery/streaming-data-into-bigquery load 1 row at a time (cost per successfull inserts) each row is considered atleast 1KB Cost # $0.01 per 200 MB Quota & Limit # https://cloud.google.com/bigquery/quotas#streaming_inserts Maximum bytes per second per project in the us and eu multi-regions: 1 GB per second in other region: 300 MB per second Maximum row size 10 MB HTTP request size limit 10 MB Maximum rows per request 50,000 rows BigQuery Storage Write API # Batch/Stream Loading using BigQuery Storage Write API https://cloud.google.com/bigquery/docs/write-api new, fast, cheaper, gRPC based API suitable for both batch & stream loading Cost # $0.025 per 1 GB first 2 TB per month are free Quota & Limit # https://cloud.google.com/bigquery/quotas#write-api-limits Concurrent connections 10,000 in multi-regions; 100 in regions Pending stream bytes 10 TB in multi-regions; 100 GB in regions Throughput 3 GB per second throughput in multi-regions; 300 MB per second in regions CreateWriteStream requests 30,000 streams every 4 hours, per project Batch commits 10,000 streams per table Request size 10 MB Data Fetch/Query/Extraction/Read/Export APIs # https://cloud.google.com/bigquery/pricing?hl=en#data_extraction_pricing Batch Export # Cost # Free using the shared slot pool Quota & Limit # export up to 50 terabytes per day for free using the shared slot pool BigQuery Storage Read API # Cost # $1.1 per TB read first 300 TB free per month Quota & Limit # SQL Query # BigQuery SQL (Legacy SQL) # https://cloud.google.com/bigquery/docs/reference/legacy-sql Standard SQL (New) # https://cloud.google.com/bigquery/docs/reference/standard-sql/introduction BigQuery SQL vs Standard SQL # https://cloud.google.com/bigquery/docs/reference/standard-sql/migrating-from-legacy-sql BigQuery Storage Read API # Query Size Calculation # https://cloud.google.com/bigquery/docs/estimate-costs#query_size_calculation Misc Quota & Limit # Quota Error # https://cloud.google.com/docs/quota#quota_errors rateLimitExceeded This value indicates a short-term limit To resolve these limit issues, retry the operation after few seconds Use exponential backoff between retry attempts That is, exponentially increase the delay between each retry quotaExceeded This value indicates a longer-term limit If you reach a longer-term quota limit, you should wait 10 minutes or longer before trying the operation again If you consistently reach one of these longer-term quota limits, you should analyze your workload for ways to mitigate the issue Mitigations can include optimizing your workload or requesting a quota increase Diagnosis # https://cloud.google.com/bigquery/docs/troubleshoot-quotas#diagnosis e.g. 1 2 3 4 5 6 7 SELECT job_id , creation_time , error_result FROM ` region - us ` . INFORMATION_SCHEMA . JOBS_BY_PROJECT WHERE creation_time > TIMESTAMP_SUB ( CURRENT_TIMESTAMP , INTERVAL 1 DAY ) AND error_result . reason IN ( 'rateLimitExceeded' , 'quotaExceeded' ) raise self._exception google.api_core.exceptions.Forbidden: 403 Exceeded rate limits: too many table update operations for this table. For more information, see https://cloud.google.com/bigquery/docs/troubleshoot-quotas # https://cloud.google.com/bigquery/docs/troubleshoot-quotas Monitoring Quota Metrics # https://cloud.google.com/docs/quota#monitoring_quota_metrics BigQuery API Clients # python-bigquery # https://github.com/googleapis/python-bigquery load_table_from_dataframe uses CSV format by default good performance for small batch pandas-gbq # https://github.com/googleapis/python-bigquery-pandas load_table_from_dataframe uses parquet format by default Usecases # Data Warehousing # https://cloud.google.com/architecture/confidential-data-warehouse-blueprint https://cloud.google.com/architecture/dw2bq/dw-bq-migration-overview References # Open Feature Requests","title":"GCP BigQuery"},{"location":"Engineering/Software-Engineering/Database/gcp_bigquery/#gcp-bigquery","text":"GCP BigQuery Introduction Dataset Multi-Region Dataset Quota & Limit Copying a Dataset Errors Table Table Schema Modify Errors Manually specifying table schema Vs Schema auto-detection Row-level Security Standard Table Quota & Limit Cost Partitioned Table Quota & Limit Cost Create Manage/Modify Query Clustered Table Quota & Limit Cost Partitioning vs Clustering Partitioning vs Sharding BigQuery Data Storage Cost Active Storage Cost Quota & Limit Long-term Storage Cost Quota & Limit Data Size Calculation BigQuery API Data Ingestion (load jobs) API Batch Loading Cost Quota & Limit Stream Loading (Legacy) Cost Quota & Limit BigQuery Storage Write API Cost Quota & Limit Data Fetch/Query/Extraction/Read/Export APIs Batch Export Cost Quota & Limit BigQuery Storage Read API Cost Quota & Limit SQL Query BigQuery SQL (Legacy SQL) Standard SQL (New) BigQuery SQL vs Standard SQL BigQuery Storage Read API Query Size Calculation Misc Quota & Limit Quota Error Diagnosis raise self._exception google.api_core.exceptions.Forbidden: 403 Exceeded rate limits: too many table update operations for this table. For more information, see https://cloud.google.com/bigquery/docs/troubleshoot-quotas Monitoring Quota Metrics BigQuery API Clients python-bigquery pandas-gbq Usecases Data Warehousing References","title":"GCP BigQuery"},{"location":"Engineering/Software-Engineering/Database/gcp_bigquery/#introduction","text":"Provider managed, serverless, multi-cloud data warehouse.","title":"Introduction"},{"location":"Engineering/Software-Engineering/Database/gcp_bigquery/#dataset","text":"","title":"Dataset"},{"location":"Engineering/Software-Engineering/Database/gcp_bigquery/#multi-region-dataset","text":"","title":"Multi-Region Dataset"},{"location":"Engineering/Software-Engineering/Database/gcp_bigquery/#quota-limit","text":"https://cloud.google.com/bigquery/quotas#dataset_limits Maximum number of datasets: unlimited (per project) Number of tables per dataset: unlimited When you use an API call, enumeration performance slows as you approach 50,000 tables in a dataset The Cloud console can display up to 50,000 tables for each dataset Number of authorized resources in a dataset's access control list: 2500 resources","title":"Quota &amp; Limit"},{"location":"Engineering/Software-Engineering/Database/gcp_bigquery/#copying-a-dataset","text":"https://cloud.google.com/bigquery/docs/copying-datasets#console Moving/Renaming of Dataset is currently not possible, thus there is \"Copying\"","title":"Copying a Dataset"},{"location":"Engineering/Software-Engineering/Database/gcp_bigquery/#errors","text":"1 2 raise exceptions.from_http_response(exc.response) google.api_core.exceptions.NotFound: 404 POST https://bigquery.googleapis.com/upload/bigquery/v2/projects/poc/jobs?uploadType=multipart: Not found: Dataset poc:ds_v2 Need to create Dataset before creating tables","title":"Errors"},{"location":"Engineering/Software-Engineering/Database/gcp_bigquery/#table","text":"https://cloud.google.com/bigquery/quotas#table_limits","title":"Table"},{"location":"Engineering/Software-Engineering/Database/gcp_bigquery/#table-schema","text":"","title":"Table Schema"},{"location":"Engineering/Software-Engineering/Database/gcp_bigquery/#modify","text":"https://cloud.google.com/bigquery/docs/managing-table-schemas smooth modification: Adding columns to a schema definition https://cloud.google.com/bigquery/docs/managing-table-schemas#adding_columns_to_a_tables_schema_definition Deleting, or dropping, a column from a schema definition Relaxing a column's mode from REQUIRED to NULLABLE manual modification: https://cloud.google.com/bigquery/docs/manually-changing-schemas Changing a column's name Changing a column's data type Changing a column's mode (aside from relaxing REQUIRED columns to NULLABLE)","title":"Modify"},{"location":"Engineering/Software-Engineering/Database/gcp_bigquery/#errors_1","text":"1 2 raise exceptions.from_http_response(exc.response) google.api_core.exceptions.BadRequest: 400 POST https://bigquery.googleapis.com/upload/bigquery/v2/projects/poc/jobs?uploadType=multipart: Provided Schema does not match Table poc:ds.table1. Cannot add fields (field: ColA) If you add new columns to an existing table schema, the columns must be NULLABLE or REPEATED. You cannot add a REQUIRED column to an existing table schema. REQUIRED columns can be added only when you create a table while loading data, or when you create an empty table with a schema definition.","title":"Errors"},{"location":"Engineering/Software-Engineering/Database/gcp_bigquery/#manually-specifying-table-schema-vs-schema-auto-detection","text":"Understand BigQuery Schema Auto-detection Using schema auto-detection Getting BigQuery to auto detect schema Possible \u201cBug\u201d in BigQuery Table Loading with Auto Detect Schema Load data from DataFrame Specify the type of columns whose type cannot be auto-detected. For example the \"title\" column uses pandas dtype \"object\", so its data type is ambiguous.","title":"Manually specifying table schema Vs Schema auto-detection"},{"location":"Engineering/Software-Engineering/Database/gcp_bigquery/#row-level-security","text":"https://cloud.google.com/bigquery/quotas#row_level_security","title":"Row-level Security"},{"location":"Engineering/Software-Engineering/Database/gcp_bigquery/#standard-table","text":"","title":"Standard Table"},{"location":"Engineering/Software-Engineering/Database/gcp_bigquery/#quota-limit_1","text":"https://cloud.google.com/bigquery/quotas#standard_tables Table operations per day: 1500 Maximum number of columns per table: 10000","title":"Quota &amp; Limit"},{"location":"Engineering/Software-Engineering/Database/gcp_bigquery/#cost","text":"","title":"Cost"},{"location":"Engineering/Software-Engineering/Database/gcp_bigquery/#partitioned-table","text":"","title":"Partitioned Table"},{"location":"Engineering/Software-Engineering/Database/gcp_bigquery/#quota-limit_2","text":"https://cloud.google.com/bigquery/quotas#partitioned_tables Number of partitions per partitioned table: 4000","title":"Quota &amp; Limit"},{"location":"Engineering/Software-Engineering/Database/gcp_bigquery/#cost_1","text":"","title":"Cost"},{"location":"Engineering/Software-Engineering/Database/gcp_bigquery/#create","text":"https://cloud.google.com/bigquery/docs/creating-partitioned-tables#python","title":"Create"},{"location":"Engineering/Software-Engineering/Database/gcp_bigquery/#managemodify","text":"https://cloud.google.com/bigquery/docs/managing-partitioned-tables#api_2","title":"Manage/Modify"},{"location":"Engineering/Software-Engineering/Database/gcp_bigquery/#query","text":"https://cloud.google.com/bigquery/docs/querying-partitioned-tables","title":"Query"},{"location":"Engineering/Software-Engineering/Database/gcp_bigquery/#clustered-table","text":"","title":"Clustered Table"},{"location":"Engineering/Software-Engineering/Database/gcp_bigquery/#quota-limit_3","text":"","title":"Quota &amp; Limit"},{"location":"Engineering/Software-Engineering/Database/gcp_bigquery/#cost_2","text":"","title":"Cost"},{"location":"Engineering/Software-Engineering/Database/gcp_bigquery/#partitioning-vs-clustering","text":"https://cloud.google.com/bigquery/docs/partitioned-tables#partitioning_versus_clustering data is first partitioned and then clustered partitioning pros to set partition expiration time advance query cost estimation/pruning to define ranges (time, id etc) over partition cons clustering pros more granualarity than partitioning alone same column for both - partitioning as well as clustering to speed up queries having filters & aggregation clause when cardinality of values of a column/set of column is large suitable when partitioning results in small amount of data per partition suitable when partitioning results in large number of paritions (beyond limit & quota) suitable when mutation operations modifies most of the partitions in the table frequestly (e.g. every minutes/hours) cons query cost estimation/pruning is actually NOT possible only after query finishes, the cost comes up NO significant difference in query performance between a clustered and unclustered table if the table or partition is under 1 GB","title":"Partitioning vs Clustering"},{"location":"Engineering/Software-Engineering/Database/gcp_bigquery/#partitioning-vs-sharding","text":"https://cloud.google.com/bigquery/docs/partitioned-tables#dt_partition_shard","title":"Partitioning vs Sharding"},{"location":"Engineering/Software-Engineering/Database/gcp_bigquery/#bigquery-data-storage","text":"https://cloud.google.com/bigquery/pricing#storage","title":"BigQuery Data Storage"},{"location":"Engineering/Software-Engineering/Database/gcp_bigquery/#cost_3","text":"first 10 GB per month is free for more than 1PB, contact sales team for pricing pricing is prorated per MB, per second i.e. 1 GB of Long-term storage for half month is $0.005 If the table is edited, the price reverts back to the regular storage pricing, and the 90-day timer starts counting from zero","title":"Cost"},{"location":"Engineering/Software-Engineering/Database/gcp_bigquery/#active-storage","text":"if a table[-partition] is modified for <= 90 consecutive days","title":"Active Storage"},{"location":"Engineering/Software-Engineering/Database/gcp_bigquery/#cost_4","text":"first 10 GB per month is free $0.02 per GB","title":"Cost"},{"location":"Engineering/Software-Engineering/Database/gcp_bigquery/#quota-limit_4","text":"","title":"Quota &amp; Limit"},{"location":"Engineering/Software-Engineering/Database/gcp_bigquery/#long-term-storage","text":"if a table[-partition] is NOT modified for > 90 consecutive days","title":"Long-term Storage"},{"location":"Engineering/Software-Engineering/Database/gcp_bigquery/#cost_5","text":"first 10 GB per month is free pricing is ~50% half of Active Storage pricing $0.01 per GB","title":"Cost"},{"location":"Engineering/Software-Engineering/Database/gcp_bigquery/#quota-limit_5","text":"","title":"Quota &amp; Limit"},{"location":"Engineering/Software-Engineering/Database/gcp_bigquery/#data-size-calculation","text":"https://cloud.google.com/bigquery/pricing#data table size is calculated based on column data-type size when its in uncompressed form","title":"Data Size Calculation"},{"location":"Engineering/Software-Engineering/Database/gcp_bigquery/#bigquery-api","text":"https://cloud.google.com/bigquery/quotas#api_request_quotas","title":"BigQuery API"},{"location":"Engineering/Software-Engineering/Database/gcp_bigquery/#data-ingestion-load-jobs-api","text":"https://cloud.google.com/bigquery/docs/loading-data https://cloud.google.com/bigquery/pricing#data_ingestion_pricing","title":"Data Ingestion (load jobs) API"},{"location":"Engineering/Software-Engineering/Database/gcp_bigquery/#batch-loading","text":"https://cloud.google.com/bigquery/docs/loading-data#batch_loading load bulk data into one or more table uses shared pool of slots no guarantee of availability/capacity","title":"Batch Loading"},{"location":"Engineering/Software-Engineering/Database/gcp_bigquery/#cost_6","text":"free option to purchase dedicated slots to run load jobs flat-rate pricing","title":"Cost"},{"location":"Engineering/Software-Engineering/Database/gcp_bigquery/#quota-limit_6","text":"https://cloud.google.com/bigquery/quotas#load_jobs Load jobs per day: 100000 jobs i.e. 1.15 job per second failed load jobs count toward this limit Maximum size per load job: 15 TB Load job execution-time limit: 6 hours A load job fails if it executes for longer than six hours CSV: Maximum file size compressed: 4 GB uncompressed: 5 TB NOTE: If regularly exceed the load job limits due to frequent updates, consider streaming data into BigQuery instead.","title":"Quota &amp; Limit"},{"location":"Engineering/Software-Engineering/Database/gcp_bigquery/#stream-loading-legacy","text":"https://cloud.google.com/bigquery/streaming-data-into-bigquery load 1 row at a time (cost per successfull inserts) each row is considered atleast 1KB","title":"Stream Loading (Legacy)"},{"location":"Engineering/Software-Engineering/Database/gcp_bigquery/#cost_7","text":"$0.01 per 200 MB","title":"Cost"},{"location":"Engineering/Software-Engineering/Database/gcp_bigquery/#quota-limit_7","text":"https://cloud.google.com/bigquery/quotas#streaming_inserts Maximum bytes per second per project in the us and eu multi-regions: 1 GB per second in other region: 300 MB per second Maximum row size 10 MB HTTP request size limit 10 MB Maximum rows per request 50,000 rows","title":"Quota &amp; Limit"},{"location":"Engineering/Software-Engineering/Database/gcp_bigquery/#bigquery-storage-write-api","text":"Batch/Stream Loading using BigQuery Storage Write API https://cloud.google.com/bigquery/docs/write-api new, fast, cheaper, gRPC based API suitable for both batch & stream loading","title":"BigQuery Storage Write API"},{"location":"Engineering/Software-Engineering/Database/gcp_bigquery/#cost_8","text":"$0.025 per 1 GB first 2 TB per month are free","title":"Cost"},{"location":"Engineering/Software-Engineering/Database/gcp_bigquery/#quota-limit_8","text":"https://cloud.google.com/bigquery/quotas#write-api-limits Concurrent connections 10,000 in multi-regions; 100 in regions Pending stream bytes 10 TB in multi-regions; 100 GB in regions Throughput 3 GB per second throughput in multi-regions; 300 MB per second in regions CreateWriteStream requests 30,000 streams every 4 hours, per project Batch commits 10,000 streams per table Request size 10 MB","title":"Quota &amp; Limit"},{"location":"Engineering/Software-Engineering/Database/gcp_bigquery/#data-fetchqueryextractionreadexport-apis","text":"https://cloud.google.com/bigquery/pricing?hl=en#data_extraction_pricing","title":"Data Fetch/Query/Extraction/Read/Export APIs"},{"location":"Engineering/Software-Engineering/Database/gcp_bigquery/#batch-export","text":"","title":"Batch Export"},{"location":"Engineering/Software-Engineering/Database/gcp_bigquery/#cost_9","text":"Free using the shared slot pool","title":"Cost"},{"location":"Engineering/Software-Engineering/Database/gcp_bigquery/#quota-limit_9","text":"export up to 50 terabytes per day for free using the shared slot pool","title":"Quota &amp; Limit"},{"location":"Engineering/Software-Engineering/Database/gcp_bigquery/#bigquery-storage-read-api","text":"","title":"BigQuery Storage Read API"},{"location":"Engineering/Software-Engineering/Database/gcp_bigquery/#cost_10","text":"$1.1 per TB read first 300 TB free per month","title":"Cost"},{"location":"Engineering/Software-Engineering/Database/gcp_bigquery/#quota-limit_10","text":"","title":"Quota &amp; Limit"},{"location":"Engineering/Software-Engineering/Database/gcp_bigquery/#sql-query","text":"","title":"SQL Query"},{"location":"Engineering/Software-Engineering/Database/gcp_bigquery/#bigquery-sql-legacy-sql","text":"https://cloud.google.com/bigquery/docs/reference/legacy-sql","title":"BigQuery SQL (Legacy SQL)"},{"location":"Engineering/Software-Engineering/Database/gcp_bigquery/#standard-sql-new","text":"https://cloud.google.com/bigquery/docs/reference/standard-sql/introduction","title":"Standard SQL (New)"},{"location":"Engineering/Software-Engineering/Database/gcp_bigquery/#bigquery-sql-vs-standard-sql","text":"https://cloud.google.com/bigquery/docs/reference/standard-sql/migrating-from-legacy-sql","title":"BigQuery SQL vs Standard SQL"},{"location":"Engineering/Software-Engineering/Database/gcp_bigquery/#bigquery-storage-read-api_1","text":"","title":"BigQuery Storage Read API"},{"location":"Engineering/Software-Engineering/Database/gcp_bigquery/#query-size-calculation","text":"https://cloud.google.com/bigquery/docs/estimate-costs#query_size_calculation","title":"Query Size Calculation"},{"location":"Engineering/Software-Engineering/Database/gcp_bigquery/#misc-quota-limit","text":"","title":"Misc Quota &amp; Limit"},{"location":"Engineering/Software-Engineering/Database/gcp_bigquery/#quota-error","text":"https://cloud.google.com/docs/quota#quota_errors rateLimitExceeded This value indicates a short-term limit To resolve these limit issues, retry the operation after few seconds Use exponential backoff between retry attempts That is, exponentially increase the delay between each retry quotaExceeded This value indicates a longer-term limit If you reach a longer-term quota limit, you should wait 10 minutes or longer before trying the operation again If you consistently reach one of these longer-term quota limits, you should analyze your workload for ways to mitigate the issue Mitigations can include optimizing your workload or requesting a quota increase","title":"Quota Error"},{"location":"Engineering/Software-Engineering/Database/gcp_bigquery/#diagnosis","text":"https://cloud.google.com/bigquery/docs/troubleshoot-quotas#diagnosis e.g. 1 2 3 4 5 6 7 SELECT job_id , creation_time , error_result FROM ` region - us ` . INFORMATION_SCHEMA . JOBS_BY_PROJECT WHERE creation_time > TIMESTAMP_SUB ( CURRENT_TIMESTAMP , INTERVAL 1 DAY ) AND error_result . reason IN ( 'rateLimitExceeded' , 'quotaExceeded' )","title":"Diagnosis"},{"location":"Engineering/Software-Engineering/Database/gcp_bigquery/#raise-self_exception-googleapi_coreexceptionsforbidden-403-exceeded-rate-limits-too-many-table-update-operations-for-this-table-for-more-information-see-httpscloudgooglecombigquerydocstroubleshoot-quotas","text":"https://cloud.google.com/bigquery/docs/troubleshoot-quotas","title":"raise self._exception google.api_core.exceptions.Forbidden: 403 Exceeded rate limits: too many table update operations for this table. For more information, see https://cloud.google.com/bigquery/docs/troubleshoot-quotas"},{"location":"Engineering/Software-Engineering/Database/gcp_bigquery/#monitoring-quota-metrics","text":"https://cloud.google.com/docs/quota#monitoring_quota_metrics","title":"Monitoring Quota Metrics"},{"location":"Engineering/Software-Engineering/Database/gcp_bigquery/#bigquery-api-clients","text":"","title":"BigQuery API Clients"},{"location":"Engineering/Software-Engineering/Database/gcp_bigquery/#python-bigquery","text":"https://github.com/googleapis/python-bigquery load_table_from_dataframe uses CSV format by default good performance for small batch","title":"python-bigquery"},{"location":"Engineering/Software-Engineering/Database/gcp_bigquery/#pandas-gbq","text":"https://github.com/googleapis/python-bigquery-pandas load_table_from_dataframe uses parquet format by default","title":"pandas-gbq"},{"location":"Engineering/Software-Engineering/Database/gcp_bigquery/#usecases","text":"","title":"Usecases"},{"location":"Engineering/Software-Engineering/Database/gcp_bigquery/#data-warehousing","text":"https://cloud.google.com/architecture/confidential-data-warehouse-blueprint https://cloud.google.com/architecture/dw2bq/dw-bq-migration-overview","title":"Data Warehousing"},{"location":"Engineering/Software-Engineering/Database/gcp_bigquery/#references","text":"Open Feature Requests","title":"References"},{"location":"Engineering/Software-Engineering/Database/mysql/","text":"MySQL # Table of Contents MySQL Install Run Help chage root password Database Install test_db Install Sakila db prepared by mysql GRANTS To see databases To create a database To use a database Tables To see tables Subprograms Aggregation Functions Analytical Functions Window Function Install # 1 sudo apt install mysql-server Run # 1 sudo mysql -u root -p #default password for root is \"password\" if you get : 1 ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/var/run/mysqld/mysqld.sock' (2) then check 1 service mysql start Default password for root is: password Help # 1 2 3 4 5 mysql> help; or \\h chage root password # 1 sudo mysqladmin password <new password> Database # Install test_db # sakila employees source: https://github.com/toransahu/test_db Install Sakila db prepared by mysql # 1 2 3 4 5 wget http://downloads.mysql.com/docs/sakila-db.tar.gz tar -xzf sakila-db.tar.gz cd sakila-db mysql -u root -p < sakila-schema.sql mysql -u root -p < sakila-data.sql GRANTS # 1 GRANT ALL PRIVILEGES ON * . * TO 'username' @ 'localhost' IDENTIFIED BY 'password' ; To see databases # 1 mysql> show databases; To create a database # 1 2 3 mysql> create database <db_name>; Query OK, 1 row affected (0.00 sec) To use a database # 1 mysql> use <db_name>; Tables # To see tables # 1 show tables; Subprograms # Aggregation Functions # Analytical Functions # MySQL doesn't have Analytical Functions while it is in MSSQL Oracle PostgreSQL MariaDB Window Function # over()","title":"MySQL"},{"location":"Engineering/Software-Engineering/Database/mysql/#mysql","text":"","title":"MySQL"},{"location":"Engineering/Software-Engineering/Database/mysql/#install","text":"1 sudo apt install mysql-server","title":"Install"},{"location":"Engineering/Software-Engineering/Database/mysql/#run","text":"1 sudo mysql -u root -p #default password for root is \"password\" if you get : 1 ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/var/run/mysqld/mysqld.sock' (2) then check 1 service mysql start Default password for root is: password","title":"Run"},{"location":"Engineering/Software-Engineering/Database/mysql/#help","text":"1 2 3 4 5 mysql> help; or \\h","title":"Help"},{"location":"Engineering/Software-Engineering/Database/mysql/#chage-root-password","text":"1 sudo mysqladmin password <new password>","title":"chage root password"},{"location":"Engineering/Software-Engineering/Database/mysql/#database","text":"","title":"Database"},{"location":"Engineering/Software-Engineering/Database/mysql/#install-test_db","text":"sakila employees source: https://github.com/toransahu/test_db","title":"Install test_db"},{"location":"Engineering/Software-Engineering/Database/mysql/#install-sakila-db-prepared-by-mysql","text":"1 2 3 4 5 wget http://downloads.mysql.com/docs/sakila-db.tar.gz tar -xzf sakila-db.tar.gz cd sakila-db mysql -u root -p < sakila-schema.sql mysql -u root -p < sakila-data.sql","title":"Install Sakila db prepared by mysql"},{"location":"Engineering/Software-Engineering/Database/mysql/#grants","text":"1 GRANT ALL PRIVILEGES ON * . * TO 'username' @ 'localhost' IDENTIFIED BY 'password' ;","title":"GRANTS"},{"location":"Engineering/Software-Engineering/Database/mysql/#to-see-databases","text":"1 mysql> show databases;","title":"To see databases"},{"location":"Engineering/Software-Engineering/Database/mysql/#to-create-a-database","text":"1 2 3 mysql> create database <db_name>; Query OK, 1 row affected (0.00 sec)","title":"To create a database"},{"location":"Engineering/Software-Engineering/Database/mysql/#to-use-a-database","text":"1 mysql> use <db_name>;","title":"To use a database"},{"location":"Engineering/Software-Engineering/Database/mysql/#tables","text":"","title":"Tables"},{"location":"Engineering/Software-Engineering/Database/mysql/#to-see-tables","text":"1 show tables;","title":"To see tables"},{"location":"Engineering/Software-Engineering/Database/mysql/#subprograms","text":"","title":"Subprograms"},{"location":"Engineering/Software-Engineering/Database/mysql/#aggregation-functions","text":"","title":"Aggregation Functions"},{"location":"Engineering/Software-Engineering/Database/mysql/#analytical-functions","text":"MySQL doesn't have Analytical Functions while it is in MSSQL Oracle PostgreSQL MariaDB","title":"Analytical Functions"},{"location":"Engineering/Software-Engineering/Database/mysql/#window-function","text":"over()","title":"Window Function"},{"location":"Engineering/Software-Engineering/Database/oracle_plsql/","text":"Oracle DB (PL/SQL) # Table of Contents Oracle DB (PL/SQL) Introduction DB Increase DB size? Tablespace & Datafiles?? DB instance? Schema? Table, Temporary Tables, & View Clauses Joins Subprograms Package Stored Procedure & Function In-Built Functions Aggregation Functions Analytical Functions Window Function LAG/LEAD Functions Introduction # DB # Consist of tablespace and tablespaces consists datafiles Increase DB size? # Increase size of data file Add new data file to existing table space Add new table space with atleast one datafile in it Datafile with dynamic extension 1 ALTER DATABASE DATAFILE < DATAFILE1 . ORA > AUTOEXTEND ON NEXT 20 M MAZSIZE 1000 M ; Tablespace & Datafiles?? # Oracle Database stores data logically in tablespaces and physically in datafiles associated with the corresponding tablespace. Oracle DB consist of at least two logical storage unit SYSTEM (Default created, either locally or dictionary managed) SYSAUX ( Auxiliary to SYSTEM) TEMP (optional, required when SYSTEM is locally managed) src: https://docs.oracle.com/cd/B28359_01/server.111/b28318/physical.htm#CNCPT401 DB instance? # db is collection datafiles on server. DB instance is the allocated memory & collection of precesses running on the server when db server starts. db instances manages datafiles. db instance serves the data in datafiles to db users. Schema? # organization of data as a blueprint of how the database is constructed (divided into database tables, packages, functions, views etc) i.e. schema objects. An Oracle database associates a separate schema with each database user schema objects includes: tables, views, sequences, synonyms, indexes, clusters, database links, snapshots, procedures, functions, packages non-schema objects: users, roles, contexts, directory objects Table, Temporary Tables, & View # Table: a preliminary storage for storing data and information in RDBMS a collection of related data entries and it consists of columns and rows Syntax: 1 CREATE TABLE table_1 AS ( Col1 NUMBER ); View: It is a saved SELECT query. It is a virtual table, which does not exist as stored data values in db (unless its indexed view) Advantages: It can join tables to create some complex statical query to use frequently Does not takes extra space to store values can be used as security mechanism: i.e. only read access, no edit access Types: View Indexed View: Used to create index on view Only useful when view is created by joining various tables, otherwise no diff in a indexed view * table Takes space same as tables Syntax: 1 CREATE VIEW view_1 as SELECT statement ; Temporary Table: Oracle can create temp tables to store session specific or transaction specific data data does not persists after session/transaction definition persits? Yes Syntax: 1 CREATE GLOBAL TEMPORARY TABLE table_1 ( col1 VARCHAR2 ); Clauses: ON COMMIT PRESERVE ROWS: Used when: need to hold intermediate data If the amount of data to be processed or utilized from your PL/SQL procedure is too large to fit comfortably in a PL/SQL table Note: A TRUNCATE command issued in transaction/session specific temporary table truncates data in its own session/tranxn. Does not affect other session/trnxn. What can be created on temporary tables: Index View Triggers Clauses # FROM Where Optional part of SELECT, DELETE, ALTER, UPDATE Where clause restricts/filters result of a SELECT, DELETE, ALTER, UPDATE queries Having Having clause restricts/filters result of a \"select query with GROUP BY\" [1] Applied to each groups of grouped table If there is no GROUP BY clause then HAVING applies to whole table (table is treated as a single group) The SELECT query cannot refer directly to any COLUMN not mentioned in GROUP BY clause, It can however refer to constants, aggregates. [2] Aggregate in HAVING do not need to appear in SELECT Subquery can be used in HAVING [3] e.g. 1 2 3 [ 1 ] SELECT emp_no , max ( salary ) m , min ( salary ) min_salary FROM salaries group by emp_no having m > 70000 ; [ 2 ] SELECT emp_no , salary m FROM salaries having m > 70000 ; [ 3 ] SELECT emp_no , max ( salary ) m FROM salaries group by emp_no having m > ( SELECT avg ( salary ) from salaries ); ORDER BY To specify the order in which row should appear Optional to use with SELECT, INSERT, CREATE VIEW Meaningless in sub-queries Order: DESC, ASC Uses: Using Correlation Name: 1 SELECT first_col AS f from tab_1 ORDER BY f ; Using Column number 1 SELECT emp_no , salary , addr from tab_1 ORDER BY 1 , 2 , 3 ; Using function 1 SELECT i , len from measure ORDER BY sin ( i ); Using NULL order 1 SELECT i , len from measure ORDER BY NULLS LAST ; GROUP BY Optional part of SELECT The SELECT query cannot refer directly to any COLUMN not mentioned in GROUP BY clause, It can however refer to constants, aggregates. Typically used with AGGREGATE functions FOR UPDATE Optional part of SELECT query Syntax: SELECT A, B, C FROM T_1 FOR UPDATE Use: In cursors to make them updatable. WITH Materialization technique same as View & Temporary tables known as subquery factoring Useful when subqueries are used multiple times e.g. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 WITH sum_sales AS ( select sum ( quantity ) all_sales from stores ), number_stores AS ( select count ( * ) nbr_stores from stores ), sales_by_store AS ( select store_name , sum ( quantity ) store_sales from store natural join sales ) SELECT store_name FROM store , sum_sales , number_stores , sales_by_store where store_sales > ( all_sales / nbr_stores ); USING Used in JOIN inplace of ON e.g. 1 SELECT * FROM COUNTRIES JOIN CITIES USING ( COUNTRY ); where COUNTRY column should exist in both the tables. CONSTRAINT Optional part of CREATE/ ALTER table. Level: Column level: Column-level constraints (except for check constraints) refer to only one column Table level: Table constraints allow you to specify more than one column in a PRIMARY KEY, UNIQUE, CHECK, or FOREIGN KEY constraint definition Types: NOT NULL: Column level, PRIMARY KEY: Both level, nameable FOREIGN KEY: Both level, nameable CHECK: Both level, nameable, [DISABLE] keyword to disable UNIQUE: Both level, nameable e.g.: 1 2 3 4 5 6 7 8 CREATE TABLE suppliers ( supplier_id numeric ( 4 ) NOT NULL CONSTRAINT unq_supplier_id UNIQUE , supplier_name varchar2 ( 50 ) NOT NULL , CONSTRAINT check_supplier_name CHECK ( supplier_name = upper ( supplier_name )), CONSTRAINT pk_supplier_id PRIMARY KEY ); Joins # Used to combine & then retrieve data from multiple tables or views Types: Equijoin: Joins 2 tables on the basis of equality of 2 columns 1 SELECT A . col1 , A . col2 , B . col1 , B . col2 FROM A , B where A . col3 = B . col4 ; Self Join Join a table with itself with the help of alias 1 SELECT A . col1 , A . col2 , B . col1 , B . col2 FROM Salary AS A JOIN Salary AS B ON A . col3 = B . col4 ; Cartesian Product Joins 2 tables without any condition 1 SELECT A . col1 , A . col2 , B . col1 , B . col2 FROM A , B ; Inner Join (Join) Simple join Returns all rows that satisfy the join condition 1 SELECT A . col1 , A . col2 , B . col1 , B . col2 FROM A JOIN B on ON A . col3 = B . col4 ; Left Outer Join (Left Join) Returns all rows that satisfy the join condition and also returns some or all of those rows from left table for which join condition didn't worked 1 2 SELECT A . col1 , A . col2 , B . col1 , B . col2 FROM A LEFT [ OUTER ] JOIN B ON A . col3 = B . col4 ; SELECT A . col1 , A . col2 , B . col1 , B . col2 FROM A LEFT [ OUTER ] JOIN B WHERE A . col3 ( + ) = B . col4 ; Right Outer Join (Right Join) Returns all rows that satisfy the join condition and also returns some or all of those rows from right table for which join condition didn't worked 1 2 SELECT A . col1 , A . col2 , B . col1 , B . col2 FROM A RIGHT [ OUTER ] JOIN B ON A . col3 = B . col4 ; SELECT A . col1 , A . col2 , B . col1 , B . col2 FROM A , B WHERE A . col3 = B . col4 ( + ); Full Outer Join (Full Join) Returns all rows that satisfy the join condition and also returns some or all of those rows from both the table for which join condition didn't worked 1 SELECT A . col1 , A . col2 , B . col1 , B . col2 FROM A FULL JOIN B on ON A . col3 = B . col4 ; Anti Join Returns rows from the table with does not exists in other table Using NOT IN clause 1 2 3 4 5 SELECT * FROM employees WHERE department_id NOT IN ( SELECT department_id FROM departments WHERE location_id = 1700 ) ORDER BY last_name ; Semi Join Returns rows from table that satisfies EXISTS subquery condition 1 2 3 4 5 6 SELECT * FROM departments WHERE EXISTS ( SELECT * FROM employees WHERE departments . department_id = employees . department_id AND employees . salary > 2500 ) ORDER BY department_name ; Subprograms # Subprograms are the building blocks of modular, maintainable applications. e.g. : Package, Procedure, Function Package # A schema object that groups logically related PL/SQL types, variables, and subprograms Packages usually have two parts, a specification (spec): The specification is the interface to the package. It declares the types/collection types, variables, constants, exceptions, cursors, subprograms, and overloaded subprograms that can be referenced from outside the package. a body: The body defines the queries for the cursors and the code for the subprograms. Advantages: Modularity Easy application Design Information Hiding: Public, Private Better performance: The whole package gets loaded into memory after first call Syntax: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 CREATE PACKAGE emp_bonus AS TYPE EmpRecTyp IS RECORD ( employees % ROWTYPE ); PROCEDURE calc_bonus ( date_hired employees . hire_date % TYPE ); CURSOR desc_salary RETURN EmpRecTyp ; FUNCTION hire_employee ( last_name VARCHAR2 , first_name VARCHAR2 ) RETURN NUMBER ; END emp_bonus ; / CREATE PACKAGE BODY emp_bonus AS PROCEDURE calc_bonus ( date_hired employees . hire_date % TYPE , salary NUMBER ) IS BEGIN DBMS_OUTPUT . PUT_LINE ( 'Employees hired on ' || date_hired || ' get bonus.' ); IF salary <= 0 THEN RAISE invalid_salary EXCEPTION ; END IF ; EXCEPTION WHEN invalid_salary THEN DBMS_OUTPUT . PUT_LINE ( 'Invalid salary input!' ); WHEN Others THEN RAISE ; END ; CURSOR desc_salary RETURN EmpRecTyp IS SELECT employee_id , salary FROM employees ORDER BY salary DESC ; FUNCTION hire_employee ( last_name VARCHAR2 , first_name VARCHAR2 ) RETURN NUMBER IS new_emp_id NUMBER ; BEGIN NULL ; RETURN new_emp_id ; END hire_employee ; END emp_bonus ; / Stored Procedure & Function # Schema level subprograms/program unit/ commonly used codes stored in database Procedure Function Stored Procedures can call functions. Functions cannot call stored Procedures. Can have select statements as well as DML statements Cannot use DML statements Can use both table variables as well as temporary table in it. Cannot use temp tables Procedures cannot be utilized in a select statement Function can be embedded in a select statement. Procedure can return multiple OUT values(max. 1024) Function returns 1 value only however it can be collection datatype Syntax Procedure 1 2 3 4 5 6 7 8 9 10 11 12 13 CREATE OR REPLACE PROCEDURE ADD_EVALUATION ( evaluation_id IN NUMBER , employee_id IN NUMBER , evaluation_date IN DATE , job_id IN VARCHAR2 , manager_id OUT NUMBER , department_id OUT NUMBER ) AS BEGIN NULL ; END ADD_EVALUATION ; Function 1 2 3 4 5 6 7 8 9 CREATE OR REPLACE FUNCTION calculate_score ( cat IN VARCHAR2 , score IN NUMBER , weight IN NUMBER ) RETURN NUMBER AS BEGIN RETURN NULL ; END calculate_score ; In-Built Functions # Aggregation Functions # Max, Min, Count, Sum Analytical Functions # Top? Last? Rank? Window Function # over() LAG/LEAD Functions #","title":"Oracle DB (PL/SQL)"},{"location":"Engineering/Software-Engineering/Database/oracle_plsql/#oracle-db-plsql","text":"","title":"Oracle DB (PL/SQL)"},{"location":"Engineering/Software-Engineering/Database/oracle_plsql/#introduction","text":"","title":"Introduction"},{"location":"Engineering/Software-Engineering/Database/oracle_plsql/#db","text":"Consist of tablespace and tablespaces consists datafiles","title":"DB"},{"location":"Engineering/Software-Engineering/Database/oracle_plsql/#increase-db-size","text":"Increase size of data file Add new data file to existing table space Add new table space with atleast one datafile in it Datafile with dynamic extension 1 ALTER DATABASE DATAFILE < DATAFILE1 . ORA > AUTOEXTEND ON NEXT 20 M MAZSIZE 1000 M ;","title":"Increase DB size?"},{"location":"Engineering/Software-Engineering/Database/oracle_plsql/#tablespace-datafiles","text":"Oracle Database stores data logically in tablespaces and physically in datafiles associated with the corresponding tablespace. Oracle DB consist of at least two logical storage unit SYSTEM (Default created, either locally or dictionary managed) SYSAUX ( Auxiliary to SYSTEM) TEMP (optional, required when SYSTEM is locally managed) src: https://docs.oracle.com/cd/B28359_01/server.111/b28318/physical.htm#CNCPT401","title":"Tablespace &amp; Datafiles??"},{"location":"Engineering/Software-Engineering/Database/oracle_plsql/#db-instance","text":"db is collection datafiles on server. DB instance is the allocated memory & collection of precesses running on the server when db server starts. db instances manages datafiles. db instance serves the data in datafiles to db users.","title":"DB instance?"},{"location":"Engineering/Software-Engineering/Database/oracle_plsql/#schema","text":"organization of data as a blueprint of how the database is constructed (divided into database tables, packages, functions, views etc) i.e. schema objects. An Oracle database associates a separate schema with each database user schema objects includes: tables, views, sequences, synonyms, indexes, clusters, database links, snapshots, procedures, functions, packages non-schema objects: users, roles, contexts, directory objects","title":"Schema?"},{"location":"Engineering/Software-Engineering/Database/oracle_plsql/#table-temporary-tables-view","text":"Table: a preliminary storage for storing data and information in RDBMS a collection of related data entries and it consists of columns and rows Syntax: 1 CREATE TABLE table_1 AS ( Col1 NUMBER ); View: It is a saved SELECT query. It is a virtual table, which does not exist as stored data values in db (unless its indexed view) Advantages: It can join tables to create some complex statical query to use frequently Does not takes extra space to store values can be used as security mechanism: i.e. only read access, no edit access Types: View Indexed View: Used to create index on view Only useful when view is created by joining various tables, otherwise no diff in a indexed view * table Takes space same as tables Syntax: 1 CREATE VIEW view_1 as SELECT statement ; Temporary Table: Oracle can create temp tables to store session specific or transaction specific data data does not persists after session/transaction definition persits? Yes Syntax: 1 CREATE GLOBAL TEMPORARY TABLE table_1 ( col1 VARCHAR2 ); Clauses: ON COMMIT PRESERVE ROWS: Used when: need to hold intermediate data If the amount of data to be processed or utilized from your PL/SQL procedure is too large to fit comfortably in a PL/SQL table Note: A TRUNCATE command issued in transaction/session specific temporary table truncates data in its own session/tranxn. Does not affect other session/trnxn. What can be created on temporary tables: Index View Triggers","title":"Table, Temporary Tables, &amp; View"},{"location":"Engineering/Software-Engineering/Database/oracle_plsql/#clauses","text":"FROM Where Optional part of SELECT, DELETE, ALTER, UPDATE Where clause restricts/filters result of a SELECT, DELETE, ALTER, UPDATE queries Having Having clause restricts/filters result of a \"select query with GROUP BY\" [1] Applied to each groups of grouped table If there is no GROUP BY clause then HAVING applies to whole table (table is treated as a single group) The SELECT query cannot refer directly to any COLUMN not mentioned in GROUP BY clause, It can however refer to constants, aggregates. [2] Aggregate in HAVING do not need to appear in SELECT Subquery can be used in HAVING [3] e.g. 1 2 3 [ 1 ] SELECT emp_no , max ( salary ) m , min ( salary ) min_salary FROM salaries group by emp_no having m > 70000 ; [ 2 ] SELECT emp_no , salary m FROM salaries having m > 70000 ; [ 3 ] SELECT emp_no , max ( salary ) m FROM salaries group by emp_no having m > ( SELECT avg ( salary ) from salaries ); ORDER BY To specify the order in which row should appear Optional to use with SELECT, INSERT, CREATE VIEW Meaningless in sub-queries Order: DESC, ASC Uses: Using Correlation Name: 1 SELECT first_col AS f from tab_1 ORDER BY f ; Using Column number 1 SELECT emp_no , salary , addr from tab_1 ORDER BY 1 , 2 , 3 ; Using function 1 SELECT i , len from measure ORDER BY sin ( i ); Using NULL order 1 SELECT i , len from measure ORDER BY NULLS LAST ; GROUP BY Optional part of SELECT The SELECT query cannot refer directly to any COLUMN not mentioned in GROUP BY clause, It can however refer to constants, aggregates. Typically used with AGGREGATE functions FOR UPDATE Optional part of SELECT query Syntax: SELECT A, B, C FROM T_1 FOR UPDATE Use: In cursors to make them updatable. WITH Materialization technique same as View & Temporary tables known as subquery factoring Useful when subqueries are used multiple times e.g. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 WITH sum_sales AS ( select sum ( quantity ) all_sales from stores ), number_stores AS ( select count ( * ) nbr_stores from stores ), sales_by_store AS ( select store_name , sum ( quantity ) store_sales from store natural join sales ) SELECT store_name FROM store , sum_sales , number_stores , sales_by_store where store_sales > ( all_sales / nbr_stores ); USING Used in JOIN inplace of ON e.g. 1 SELECT * FROM COUNTRIES JOIN CITIES USING ( COUNTRY ); where COUNTRY column should exist in both the tables. CONSTRAINT Optional part of CREATE/ ALTER table. Level: Column level: Column-level constraints (except for check constraints) refer to only one column Table level: Table constraints allow you to specify more than one column in a PRIMARY KEY, UNIQUE, CHECK, or FOREIGN KEY constraint definition Types: NOT NULL: Column level, PRIMARY KEY: Both level, nameable FOREIGN KEY: Both level, nameable CHECK: Both level, nameable, [DISABLE] keyword to disable UNIQUE: Both level, nameable e.g.: 1 2 3 4 5 6 7 8 CREATE TABLE suppliers ( supplier_id numeric ( 4 ) NOT NULL CONSTRAINT unq_supplier_id UNIQUE , supplier_name varchar2 ( 50 ) NOT NULL , CONSTRAINT check_supplier_name CHECK ( supplier_name = upper ( supplier_name )), CONSTRAINT pk_supplier_id PRIMARY KEY );","title":"Clauses"},{"location":"Engineering/Software-Engineering/Database/oracle_plsql/#joins","text":"Used to combine & then retrieve data from multiple tables or views Types: Equijoin: Joins 2 tables on the basis of equality of 2 columns 1 SELECT A . col1 , A . col2 , B . col1 , B . col2 FROM A , B where A . col3 = B . col4 ; Self Join Join a table with itself with the help of alias 1 SELECT A . col1 , A . col2 , B . col1 , B . col2 FROM Salary AS A JOIN Salary AS B ON A . col3 = B . col4 ; Cartesian Product Joins 2 tables without any condition 1 SELECT A . col1 , A . col2 , B . col1 , B . col2 FROM A , B ; Inner Join (Join) Simple join Returns all rows that satisfy the join condition 1 SELECT A . col1 , A . col2 , B . col1 , B . col2 FROM A JOIN B on ON A . col3 = B . col4 ; Left Outer Join (Left Join) Returns all rows that satisfy the join condition and also returns some or all of those rows from left table for which join condition didn't worked 1 2 SELECT A . col1 , A . col2 , B . col1 , B . col2 FROM A LEFT [ OUTER ] JOIN B ON A . col3 = B . col4 ; SELECT A . col1 , A . col2 , B . col1 , B . col2 FROM A LEFT [ OUTER ] JOIN B WHERE A . col3 ( + ) = B . col4 ; Right Outer Join (Right Join) Returns all rows that satisfy the join condition and also returns some or all of those rows from right table for which join condition didn't worked 1 2 SELECT A . col1 , A . col2 , B . col1 , B . col2 FROM A RIGHT [ OUTER ] JOIN B ON A . col3 = B . col4 ; SELECT A . col1 , A . col2 , B . col1 , B . col2 FROM A , B WHERE A . col3 = B . col4 ( + ); Full Outer Join (Full Join) Returns all rows that satisfy the join condition and also returns some or all of those rows from both the table for which join condition didn't worked 1 SELECT A . col1 , A . col2 , B . col1 , B . col2 FROM A FULL JOIN B on ON A . col3 = B . col4 ; Anti Join Returns rows from the table with does not exists in other table Using NOT IN clause 1 2 3 4 5 SELECT * FROM employees WHERE department_id NOT IN ( SELECT department_id FROM departments WHERE location_id = 1700 ) ORDER BY last_name ; Semi Join Returns rows from table that satisfies EXISTS subquery condition 1 2 3 4 5 6 SELECT * FROM departments WHERE EXISTS ( SELECT * FROM employees WHERE departments . department_id = employees . department_id AND employees . salary > 2500 ) ORDER BY department_name ;","title":"Joins"},{"location":"Engineering/Software-Engineering/Database/oracle_plsql/#subprograms","text":"Subprograms are the building blocks of modular, maintainable applications. e.g. : Package, Procedure, Function","title":"Subprograms"},{"location":"Engineering/Software-Engineering/Database/oracle_plsql/#package","text":"A schema object that groups logically related PL/SQL types, variables, and subprograms Packages usually have two parts, a specification (spec): The specification is the interface to the package. It declares the types/collection types, variables, constants, exceptions, cursors, subprograms, and overloaded subprograms that can be referenced from outside the package. a body: The body defines the queries for the cursors and the code for the subprograms. Advantages: Modularity Easy application Design Information Hiding: Public, Private Better performance: The whole package gets loaded into memory after first call Syntax: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 CREATE PACKAGE emp_bonus AS TYPE EmpRecTyp IS RECORD ( employees % ROWTYPE ); PROCEDURE calc_bonus ( date_hired employees . hire_date % TYPE ); CURSOR desc_salary RETURN EmpRecTyp ; FUNCTION hire_employee ( last_name VARCHAR2 , first_name VARCHAR2 ) RETURN NUMBER ; END emp_bonus ; / CREATE PACKAGE BODY emp_bonus AS PROCEDURE calc_bonus ( date_hired employees . hire_date % TYPE , salary NUMBER ) IS BEGIN DBMS_OUTPUT . PUT_LINE ( 'Employees hired on ' || date_hired || ' get bonus.' ); IF salary <= 0 THEN RAISE invalid_salary EXCEPTION ; END IF ; EXCEPTION WHEN invalid_salary THEN DBMS_OUTPUT . PUT_LINE ( 'Invalid salary input!' ); WHEN Others THEN RAISE ; END ; CURSOR desc_salary RETURN EmpRecTyp IS SELECT employee_id , salary FROM employees ORDER BY salary DESC ; FUNCTION hire_employee ( last_name VARCHAR2 , first_name VARCHAR2 ) RETURN NUMBER IS new_emp_id NUMBER ; BEGIN NULL ; RETURN new_emp_id ; END hire_employee ; END emp_bonus ; /","title":"Package"},{"location":"Engineering/Software-Engineering/Database/oracle_plsql/#stored-procedure-function","text":"Schema level subprograms/program unit/ commonly used codes stored in database Procedure Function Stored Procedures can call functions. Functions cannot call stored Procedures. Can have select statements as well as DML statements Cannot use DML statements Can use both table variables as well as temporary table in it. Cannot use temp tables Procedures cannot be utilized in a select statement Function can be embedded in a select statement. Procedure can return multiple OUT values(max. 1024) Function returns 1 value only however it can be collection datatype Syntax Procedure 1 2 3 4 5 6 7 8 9 10 11 12 13 CREATE OR REPLACE PROCEDURE ADD_EVALUATION ( evaluation_id IN NUMBER , employee_id IN NUMBER , evaluation_date IN DATE , job_id IN VARCHAR2 , manager_id OUT NUMBER , department_id OUT NUMBER ) AS BEGIN NULL ; END ADD_EVALUATION ; Function 1 2 3 4 5 6 7 8 9 CREATE OR REPLACE FUNCTION calculate_score ( cat IN VARCHAR2 , score IN NUMBER , weight IN NUMBER ) RETURN NUMBER AS BEGIN RETURN NULL ; END calculate_score ;","title":"Stored Procedure &amp; Function"},{"location":"Engineering/Software-Engineering/Database/oracle_plsql/#in-built-functions","text":"","title":"In-Built Functions"},{"location":"Engineering/Software-Engineering/Database/oracle_plsql/#aggregation-functions","text":"Max, Min, Count, Sum","title":"Aggregation Functions"},{"location":"Engineering/Software-Engineering/Database/oracle_plsql/#analytical-functions","text":"Top? Last? Rank?","title":"Analytical Functions"},{"location":"Engineering/Software-Engineering/Database/oracle_plsql/#window-function","text":"over()","title":"Window Function"},{"location":"Engineering/Software-Engineering/Database/oracle_plsql/#laglead-functions","text":"","title":"LAG/LEAD Functions"},{"location":"Engineering/Software-Engineering/Database/postgresql/","text":"PostgreSQL # Table of Contents PostgreSQL Introduction Installation Using PostgreSQL Roles and Databases Switching Over to the postgres Account Accessing a Postgres Prompt Without Switching Accounts Create a New Role Create a New Database Create and Delete Tables Simple Doc: https://medium.com/coding-blocks/creating-user-database-and-adding-access-on-postgresql-8bfcd2f4a91e Introduction # Relational database management systems are a key component of many web sites and applications. They provide a structured way to store, organize, and access information. PostgreSQL, or Postgres, is a relational database management system that provides an implementation of the SQL querying language. It is a popular choice for many small and large projects and has the advantage of being standards-compliant and having many advanced features like reliable transactions and concurrency without read locks. In this guide, we will demonstrate how to install Postgres on an Ubuntu 16.04 VPS instance and go over some basic ways to use it. Installation # Ubuntu's default repositories contain Postgres packages, so we can install these easily using the apt packaging system. Since this is our first time using apt in this session, we need to refresh our local package index. We can then install the Postgres package and a -contrib package that adds some additional utilities and functionality: 1 2 $ sudo apt-get update $ sudo apt-get install postgresql postgresql-contrib Now that our software is installed, we can go over how it works and how it may be different from similar database management systems you may have used. Using PostgreSQL Roles and Databases # By default, Postgres uses a concept called \"roles\" to handle in authentication and authorization. These are, in some ways, similar to regular Unix-style accounts, but Postgres does not distinguish between users and groups and instead prefers the more flexible term \"role\". Upon installation Postgres is set up to use ident authentication, which means that it associates Postgres roles with a matching Unix/Linux system account. If a role exists within Postgres, a Unix/Linux username with the same name will be able to sign in as that role. There are a few ways to utilize this account to access Postgres. Switching Over to the postgres Account # The installation procedure created a user account called postgres that is associated with the default Postgres role. In order to use Postgres, we can log into that account. Switch over to the postgres linux account on your server by typing: 1 $ sudo -i -u postgres You can now access a Postgres prompt immediately by typing: 1 $ psql You will be logged in and able to interact with the database management system right away. Exit out of the PostgreSQL prompt by typing: 1 \\q You should now be back in the postgres Linux command prompt. Accessing a Postgres Prompt Without Switching Accounts # You can also run the command you'd like with the postgres account directly with sudo. For instance, in the last example, we just wanted to get to a Postgres prompt. We could do this in one step by running the single command psql as the postgres user with sudo like this: 1 $ sudo -u postgres psql This will log you directly into Postgres without the intermediary bash shell in between. Again, you can exit the interactive Postgres session by typing: 1 postgres-# \\q Create a New Role # Currently, we just have the postgres role configured within the database. We can create new roles from the command line with the createrole command. The --interactive flag will prompt you for the necessary values. If you are logged in as the postgres linux account, you can create a new user by typing: 1 2 3 $ createuser --interactive # createuser is a linux command added by postgres while installation # see `man createuser` If, instead, you prefer to use sudo for each command without switching from your normal account, you can type: 1 $ sudo -u postgres createuser --interactive The script will prompt you with some choices and, based on your responses, execute the correct Postgres commands to create a user to your specifications. Output 1 2 Enter name of role to add: sammy Shall the new role be a superuser? ( y/n ) y You can get more control by passing some additional flags. Check out the options by looking at the man page: 1 $ man createuser Create a New Database # By default, another assumption that the Postgres authentication system makes is that there will be an database with the same name as the role being used to login, which the role has access to . So if in the last section, we created a user called sammy, that role will attempt to connect to a database which is also called sammy by default. You can create the appropriate database with the createdb command. If you are logged in as the postgres linux account, you would type something like: 1 2 $ createdb sammy # createdb is also a linux command added by postgres If, instead, you prefer to use sudo for each command without switching from your normal account, you would type: 1 $ sudo -u postgres createdb sammy Open a Postgres Prompt with the New Role To log in with ident based authentication, you'll need a Linux user with the same name as your Postgres role and database. If you don't have a matching Linux user available, you can create one with the adduser command. You will have to do this from an account with sudo privileges (not logged in as the postgres user): 1 $ sudo adduser sammy Once you have the appropriate account available, you can either switch over and connect to the database by typing: 1 2 $ sudo -i -u sammy $ psql Or, you can do this inline: 1 $ sudo -u sammy psql You will be logged in automatically assuming that all of the components have been properly configured. If you want your user to connect to a different database, you can do so by specifying the database like this: 1 $ psql -d postgres Once logged in, you can get check your current connection information by typing: 1 2 3 user =# \\ conninfo You are connected to database \"sammy\" as user \"sammy\" via socket in \"/var/run/postgresql\" at port \"5432\" . This can be useful if you are connecting to non-default databases or with non-default users. Create and Delete Tables # Now that you know how to connect to the PostgreSQL database system, we can to go over how to complete some basic tasks. First, we can create a table to store some data. Let's create a table that describes playground equipment. The basic syntax for this command is something like this: 1 2 3 4 5 CREATE TABLE table_name ( column_name1 col_type ( field_length ) column_constraints , column_name2 col_type ( field_length ), column_name3 col_type ( field_length ) ); As you can see, we give the table a name, and then define the columns that we want, as well as the column type and the max length of the field data. We can also optionally add table constraints for each column. You can learn more about how to create and manage tables in Postgres here. For our purposes, we're going to create a simple table like this: 1 2 3 4 5 6 7 CREATE TABLE playground ( equip_id serial PRIMARY KEY , type varchar ( 50 ) NOT NULL , color varchar ( 25 ) NOT NULL , location varchar ( 25 ) check ( location in ( 'north' , 'south' , 'west' , 'east' , 'northeast' , 'southeast' , 'southwest' , 'northwest' )), install_date date ); We have made a playground table that inventories the equipment that we have. This starts with an equipment ID, which is of the serial type. This data type is an auto-incrementing integer. We have given this column the constraint of primary key which means that the values must be unique and not null. For two of our columns (equip_id and install_date), we have not given a field length. This is because some column types don't require a set length because the length is implied by the type. We then give columns for the equipment type and color, each of which cannot be empty. We create a location column and create a constraint that requires the value to be one of eight possible values. The last column is a date column that records the date that we installed the equipment. We can see our new table by typing: 1 2 3 4 5 6 7 user=# \\d List of relations Schema | Name | Type | Owner --------+-------------------------+----------+------- public | playground | table | sammy public | playground_equip_id_seq | sequence | sammy (2 rows) Our playground table is here, but we also have something called playground_equip_id_seq that is of the type sequence. This is a representation of the serial type we gave our equip_id column. This keeps track of the next number in the sequence and is created automatically for columns of this type. If you want to see just the table without the sequence, you can type: 1 2 3 4 5 6 7 \\dt Output List of relations Schema | Name | Type | Owner --------+------------+-------+------- public | playground | table | sammy (1 row) Add, Query, and Delete Data in a Table Now that we have a table, we can insert some data into it. Let's add a slide and a swing. We do this by calling the table we're wanting to add to, naming the columns and then providing data for each column. Our slide and swing could be added like this: 1 2 INSERT INTO playground ( type , color , location , install_date ) VALUES ( 'slide' , 'blue' , 'south' , '2014-04-28' ); INSERT INTO playground ( type , color , location , install_date ) VALUES ( 'swing' , 'yellow' , 'northwest' , '2010-08-16' ); You should take care when entering the data to avoid a few common hangups. First, keep in mind that the column names should not be quoted, but the column values that you're entering do need quotes. Another thing to keep in mind is that we do not enter a value for the equip_id column. This is because this is auto-generated whenever a new row in the table is created. We can then get back the information we've added by typing: 1 SELECT * FROM playground ; Output 1 2 3 4 5 equip_id | type | color | location | install_date ----------+-------+--------+-----------+-------------- 1 | slide | blue | south | 2014-04-28 2 | swing | yellow | northwest | 2010-08-16 (2 rows) Here, you can see that our equip_id has been filled in successfully and that all of our other data has been organized correctly. If the slide on the playground breaks and we have to remove it, we can also remove the row from our table by typing: 1 DELETE FROM playground WHERE type = 'slide' ; If we query our table again, we will see our slide is no longer a part of the table: 1 SELECT * FROM playground ; Output 1 2 3 4 equip_id | type | color | location | install_date ----------+-------+--------+-----------+-------------- 2 | swing | yellow | northwest | 2010-08-16 (1 row) How To Add and Delete Columns from a Table If we want to modify a table after it has been created to add an additional column, we can do that easily. We can add a column to show the last maintenance visit for each piece of equipment by typing: 1 ALTER TABLE playground ADD last_maint date ; If you view your table information again, you will see the new column has been added (but no data has been entered): 1 SELECT * FROM playground ; Output 1 2 3 4 equip_id | type | color | location | install_date | last_maint ----------+-------+--------+-----------+--------------+------------ 2 | swing | yellow | northwest | 2010-08-16 | (1 row) We can delete a column just as easily. If we find that our work crew uses a separate tool to keep track of maintenance history, we can get rid of the column here by typing: 1 ALTER TABLE playground DROP last_maint ; How To Update Data in a Table We know how to add records to a table and how to delete them, but we haven't covered how to modify existing entries yet. You can update the values of an existing entry by querying for the record you want and setting the column to the value you wish to use. We can query for the \"swing\" record (this will match every swing in our table) and change its color to \"red\". This could be useful if we gave the swing set a paint job: 1 UPDATE playground SET color = 'red' WHERE type = 'swing' ; We can verify that the operation was successful by querying our data again: 1 SELECT * FROM playground ; Output 1 2 3 4 equip_id | type | color | location | install_date ----------+-------+-------+-----------+-------------- 2 | swing | red | northwest | 2010-08-16 (1 row) As you can see, our slide is now registered as being red. Source","title":"PostgreSQL"},{"location":"Engineering/Software-Engineering/Database/postgresql/#postgresql","text":"","title":"PostgreSQL"},{"location":"Engineering/Software-Engineering/Database/postgresql/#introduction","text":"Relational database management systems are a key component of many web sites and applications. They provide a structured way to store, organize, and access information. PostgreSQL, or Postgres, is a relational database management system that provides an implementation of the SQL querying language. It is a popular choice for many small and large projects and has the advantage of being standards-compliant and having many advanced features like reliable transactions and concurrency without read locks. In this guide, we will demonstrate how to install Postgres on an Ubuntu 16.04 VPS instance and go over some basic ways to use it.","title":"Introduction"},{"location":"Engineering/Software-Engineering/Database/postgresql/#installation","text":"Ubuntu's default repositories contain Postgres packages, so we can install these easily using the apt packaging system. Since this is our first time using apt in this session, we need to refresh our local package index. We can then install the Postgres package and a -contrib package that adds some additional utilities and functionality: 1 2 $ sudo apt-get update $ sudo apt-get install postgresql postgresql-contrib Now that our software is installed, we can go over how it works and how it may be different from similar database management systems you may have used.","title":"Installation"},{"location":"Engineering/Software-Engineering/Database/postgresql/#using-postgresql-roles-and-databases","text":"By default, Postgres uses a concept called \"roles\" to handle in authentication and authorization. These are, in some ways, similar to regular Unix-style accounts, but Postgres does not distinguish between users and groups and instead prefers the more flexible term \"role\". Upon installation Postgres is set up to use ident authentication, which means that it associates Postgres roles with a matching Unix/Linux system account. If a role exists within Postgres, a Unix/Linux username with the same name will be able to sign in as that role. There are a few ways to utilize this account to access Postgres.","title":"Using PostgreSQL Roles and Databases"},{"location":"Engineering/Software-Engineering/Database/postgresql/#switching-over-to-the-postgres-account","text":"The installation procedure created a user account called postgres that is associated with the default Postgres role. In order to use Postgres, we can log into that account. Switch over to the postgres linux account on your server by typing: 1 $ sudo -i -u postgres You can now access a Postgres prompt immediately by typing: 1 $ psql You will be logged in and able to interact with the database management system right away. Exit out of the PostgreSQL prompt by typing: 1 \\q You should now be back in the postgres Linux command prompt.","title":"Switching Over to the postgres Account"},{"location":"Engineering/Software-Engineering/Database/postgresql/#accessing-a-postgres-prompt-without-switching-accounts","text":"You can also run the command you'd like with the postgres account directly with sudo. For instance, in the last example, we just wanted to get to a Postgres prompt. We could do this in one step by running the single command psql as the postgres user with sudo like this: 1 $ sudo -u postgres psql This will log you directly into Postgres without the intermediary bash shell in between. Again, you can exit the interactive Postgres session by typing: 1 postgres-# \\q","title":"Accessing a Postgres Prompt Without Switching Accounts"},{"location":"Engineering/Software-Engineering/Database/postgresql/#create-a-new-role","text":"Currently, we just have the postgres role configured within the database. We can create new roles from the command line with the createrole command. The --interactive flag will prompt you for the necessary values. If you are logged in as the postgres linux account, you can create a new user by typing: 1 2 3 $ createuser --interactive # createuser is a linux command added by postgres while installation # see `man createuser` If, instead, you prefer to use sudo for each command without switching from your normal account, you can type: 1 $ sudo -u postgres createuser --interactive The script will prompt you with some choices and, based on your responses, execute the correct Postgres commands to create a user to your specifications. Output 1 2 Enter name of role to add: sammy Shall the new role be a superuser? ( y/n ) y You can get more control by passing some additional flags. Check out the options by looking at the man page: 1 $ man createuser","title":"Create a New Role"},{"location":"Engineering/Software-Engineering/Database/postgresql/#create-a-new-database","text":"By default, another assumption that the Postgres authentication system makes is that there will be an database with the same name as the role being used to login, which the role has access to . So if in the last section, we created a user called sammy, that role will attempt to connect to a database which is also called sammy by default. You can create the appropriate database with the createdb command. If you are logged in as the postgres linux account, you would type something like: 1 2 $ createdb sammy # createdb is also a linux command added by postgres If, instead, you prefer to use sudo for each command without switching from your normal account, you would type: 1 $ sudo -u postgres createdb sammy Open a Postgres Prompt with the New Role To log in with ident based authentication, you'll need a Linux user with the same name as your Postgres role and database. If you don't have a matching Linux user available, you can create one with the adduser command. You will have to do this from an account with sudo privileges (not logged in as the postgres user): 1 $ sudo adduser sammy Once you have the appropriate account available, you can either switch over and connect to the database by typing: 1 2 $ sudo -i -u sammy $ psql Or, you can do this inline: 1 $ sudo -u sammy psql You will be logged in automatically assuming that all of the components have been properly configured. If you want your user to connect to a different database, you can do so by specifying the database like this: 1 $ psql -d postgres Once logged in, you can get check your current connection information by typing: 1 2 3 user =# \\ conninfo You are connected to database \"sammy\" as user \"sammy\" via socket in \"/var/run/postgresql\" at port \"5432\" . This can be useful if you are connecting to non-default databases or with non-default users.","title":"Create a New Database"},{"location":"Engineering/Software-Engineering/Database/postgresql/#create-and-delete-tables","text":"Now that you know how to connect to the PostgreSQL database system, we can to go over how to complete some basic tasks. First, we can create a table to store some data. Let's create a table that describes playground equipment. The basic syntax for this command is something like this: 1 2 3 4 5 CREATE TABLE table_name ( column_name1 col_type ( field_length ) column_constraints , column_name2 col_type ( field_length ), column_name3 col_type ( field_length ) ); As you can see, we give the table a name, and then define the columns that we want, as well as the column type and the max length of the field data. We can also optionally add table constraints for each column. You can learn more about how to create and manage tables in Postgres here. For our purposes, we're going to create a simple table like this: 1 2 3 4 5 6 7 CREATE TABLE playground ( equip_id serial PRIMARY KEY , type varchar ( 50 ) NOT NULL , color varchar ( 25 ) NOT NULL , location varchar ( 25 ) check ( location in ( 'north' , 'south' , 'west' , 'east' , 'northeast' , 'southeast' , 'southwest' , 'northwest' )), install_date date ); We have made a playground table that inventories the equipment that we have. This starts with an equipment ID, which is of the serial type. This data type is an auto-incrementing integer. We have given this column the constraint of primary key which means that the values must be unique and not null. For two of our columns (equip_id and install_date), we have not given a field length. This is because some column types don't require a set length because the length is implied by the type. We then give columns for the equipment type and color, each of which cannot be empty. We create a location column and create a constraint that requires the value to be one of eight possible values. The last column is a date column that records the date that we installed the equipment. We can see our new table by typing: 1 2 3 4 5 6 7 user=# \\d List of relations Schema | Name | Type | Owner --------+-------------------------+----------+------- public | playground | table | sammy public | playground_equip_id_seq | sequence | sammy (2 rows) Our playground table is here, but we also have something called playground_equip_id_seq that is of the type sequence. This is a representation of the serial type we gave our equip_id column. This keeps track of the next number in the sequence and is created automatically for columns of this type. If you want to see just the table without the sequence, you can type: 1 2 3 4 5 6 7 \\dt Output List of relations Schema | Name | Type | Owner --------+------------+-------+------- public | playground | table | sammy (1 row) Add, Query, and Delete Data in a Table Now that we have a table, we can insert some data into it. Let's add a slide and a swing. We do this by calling the table we're wanting to add to, naming the columns and then providing data for each column. Our slide and swing could be added like this: 1 2 INSERT INTO playground ( type , color , location , install_date ) VALUES ( 'slide' , 'blue' , 'south' , '2014-04-28' ); INSERT INTO playground ( type , color , location , install_date ) VALUES ( 'swing' , 'yellow' , 'northwest' , '2010-08-16' ); You should take care when entering the data to avoid a few common hangups. First, keep in mind that the column names should not be quoted, but the column values that you're entering do need quotes. Another thing to keep in mind is that we do not enter a value for the equip_id column. This is because this is auto-generated whenever a new row in the table is created. We can then get back the information we've added by typing: 1 SELECT * FROM playground ; Output 1 2 3 4 5 equip_id | type | color | location | install_date ----------+-------+--------+-----------+-------------- 1 | slide | blue | south | 2014-04-28 2 | swing | yellow | northwest | 2010-08-16 (2 rows) Here, you can see that our equip_id has been filled in successfully and that all of our other data has been organized correctly. If the slide on the playground breaks and we have to remove it, we can also remove the row from our table by typing: 1 DELETE FROM playground WHERE type = 'slide' ; If we query our table again, we will see our slide is no longer a part of the table: 1 SELECT * FROM playground ; Output 1 2 3 4 equip_id | type | color | location | install_date ----------+-------+--------+-----------+-------------- 2 | swing | yellow | northwest | 2010-08-16 (1 row) How To Add and Delete Columns from a Table If we want to modify a table after it has been created to add an additional column, we can do that easily. We can add a column to show the last maintenance visit for each piece of equipment by typing: 1 ALTER TABLE playground ADD last_maint date ; If you view your table information again, you will see the new column has been added (but no data has been entered): 1 SELECT * FROM playground ; Output 1 2 3 4 equip_id | type | color | location | install_date | last_maint ----------+-------+--------+-----------+--------------+------------ 2 | swing | yellow | northwest | 2010-08-16 | (1 row) We can delete a column just as easily. If we find that our work crew uses a separate tool to keep track of maintenance history, we can get rid of the column here by typing: 1 ALTER TABLE playground DROP last_maint ; How To Update Data in a Table We know how to add records to a table and how to delete them, but we haven't covered how to modify existing entries yet. You can update the values of an existing entry by querying for the record you want and setting the column to the value you wish to use. We can query for the \"swing\" record (this will match every swing in our table) and change its color to \"red\". This could be useful if we gave the swing set a paint job: 1 UPDATE playground SET color = 'red' WHERE type = 'swing' ; We can verify that the operation was successful by querying our data again: 1 SELECT * FROM playground ; Output 1 2 3 4 equip_id | type | color | location | install_date ----------+-------+-------+-----------+-------------- 2 | swing | red | northwest | 2010-08-16 (1 row) As you can see, our slide is now registered as being red. Source","title":"Create and Delete Tables"},{"location":"Engineering/Software-Engineering/DevOps/basics/","text":"Basics # Basics CI/CD CircleCI Travis CI Github Actions Ansible Jenkins Chef Puppet Containers & VM Docker Vagrant Servers nginx gunicorn ngrok Apache Tomcat CloudFlare Proxy Server Reverse Proxy Server Load Balancer Algorithms Cluster Management & Orchestration Minikube (Run Local) Prerequisites Installation Start Cluster Explore Cluster Deploy Applications LoadBalancer deployment Manager cluster Access application More Kubernetes (k8s) Pod Deployment Service ReplicaSet Secrets ConfigMaps Ingress Namespace Job Start Cluster Addons SealedSecret k8s Load Balancer k8s Long Lived Conn Docker Swarm Docker Compose Apache Mesos Infrastructure as Code Terraform Queues RabbitMQ Celery Redis Apache Kafka ELK Stack Elasticsearch Logstash Kibana Others Documentation Tool Sphinx ReadTheDoc MkDocs GitBook MoinMoin Ref CI/CD # CircleCI # Travis CI # Github Actions # Ansible # Jenkins # Chef # Puppet # Containers & VM # Docker # Image, Repo, Tag naming convention 1 2 3 4 5 6 7 8 9 10 11 12 13 # BUILD an image & tag it - from Dockerfile in \".\" (current dir) # semantics: # docker build -t <tag> # docker build -t <namespace>/<image|repo name>:<tag> . $ docker build -t <target name>: [ <tag> ] . # where target name is called image or repo name # TAG an image # semantics: # docker tag <source>[:<tag>] <target>[:<tag>] # docker tag <img ID|existing tag> <reposotiry>:<tag> # docker tag <image> <namespace>/<repo name>:<tag> $ docker tag <source> [ :<tag> ] <target> [ :<tag> ] tag: is a pointer to an image are alias (e.g. my_image:latest, my_image:v1) to the image IDs (f1477ec11d12) think of how diff. git tags can refer to a commit SHA image: is identified by an ID (hash/msg digest) of configs/layers (check if for same 2 config/Dockerfile if the ID is same) each image can have 0 or more tags repo: is an remote location under a namespace (i.e. account/username) where image(s?) are stored :latest is not always LATEST tag https://blog.container-solutions.com/docker-latest-confusion https://stackoverflow.com/questions/44500367/when-would-a-docker-image-and-its-repository-have-different-names Vagrant # Servers # nginx # gunicorn # ngrok # Apache Tomcat # CloudFlare # Proxy Server # Reverse Proxy Server # Load Balancer # https://www.ateam-oracle.com/long-lived-tcp-connections-and-load-balancers what happens when one or more servers comes up behind a load balancer? what in case of long lived TCP conn? Algorithms # https://blog.twitter.com/engineering/en_us/topics/infrastructure/2019/daperture-load-balancer.html Cluster Management & Orchestration # (Data center, cluster manager, container-orchestration system) Minikube (Run Local) # https://github.com/kubernetes/minikube https://minikube.sigs.k8s.io/docs/start/ Prerequisites # Installation # 1 2 $ curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 $ sudo install minikube-linux-amd64 /usr/local/bin/minikube Start Cluster # 1 $ minikube start Explore Cluster # Get all the pods 1 2 3 4 5 6 $ minikube kubectl -- get pods -A # or $ alias kubectl = 'minikube kubectl --' $ kubectl get pods -A Deploy Applications # 1 2 3 4 5 6 7 8 9 10 11 # create deployment $ minikube kubectl -- create deployment hello-minikube --image = k8s.gcr.io/echoserver:1.4 # expose the port 8080 of the container $ minikube kubectl -- expose deployment hello-minikube --type = NodePort --port = 8080 # show hello-minikube service $ minikube kubectl -- get services hello-minikube # open the service in browser # via option a) $ minikube service hello-minikube # or, via option b) $ minikube kubectl -- port-forward service/hello-minikube 7080 :8080 # open localhost:7080 LoadBalancer deployment # the standard way to expose application to internet each service gets its own IP 1 2 3 4 5 6 7 8 # create a load-balanced deployment $ minikube kubectl -- create deployment balanced --image = k8s.gcr.io/echoserver:1.4 # expose the server over port 8080 $ minikube kubectl -- expose deployment balanced --type = LoadBalancer --port = 8080 # now create a tunnel in diff. window $ minikube tunnel # now show info of the service & open the <EXTERNAL-IP>:8080 to access $ minikube kubectl -- get services balanced Manager cluster # 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 # Upgrade your cluster: $ minikube start --kubernetes-version = latest # Pause Kubernetes without impacting deployed applications: $ minikube pause # Unpause a paused instance: $ minikube unpause # Halt the cluster: $ minikube stop # Increase the default memory limit (requires a restart): $ minikube config set memory 16384 # Restart $ minikube stop && minikube start # Browse the catalog of easily installed Kubernetes services: $ minikube addons list # Enable an addon $ minikube addons enable <name> # Enable addon at startup $ minikube start --addons <name1> --addons <name2> # Open addon server in browser (iff) $ minikube addons open <name> # Disable an addon $ minikube addons disable <name> # Start a second local cluster $ minikube start -p cluster2 # Create a second cluster running an older Kubernetes release: $ minikube start -p aged --kubernetes-version = v1.16.1 # Delete the local cluster $ minikube delete # Delete all of the minikube clusters: $ minikube delete --all Access application # More ways: https://minikube.sigs.k8s.io/docs/handbook/accessing/ More # https://minikube.sigs.k8s.io/docs/handbook/ Kubernetes (k8s) # Pod # A Kubernetes Pod is a group of one or more Containers, tied together for the purposes of administration and networking. Deployment # A Kubernetes Deployment checks on the health of your Pod and restarts the Pod's Container if it terminates Deployments are the recommended way to manage the creation and scaling of Pods Service # By default, the Pod is only accessible by its internal IP address within the Kubernetes cluster. To make the hello-node Container accessible from outside the Kubernetes virtual network, you have to expose the Pod as a Kubernetes Service. An abstract way to expose an application running on a set of Pods as a network service. With Kubernetes you don't need to modify your application to use an unfamiliar service discovery mechanism. Kubernetes gives Pods their own IP addresses and a single DNS name for a set of Pods, and can load-balance across them. ReplicaSet # A ReplicaSet's purpose is to maintain a stable set of replica Pods running at any given time. As such, it is often used to guarantee the availability of a specified number of identical Pods. Secrets # A Secret is an object that contains a small amount of sensitive data such as a password, a token, or a key. Such information might otherwise be put in a Pod specification or in a container image. Using a Secret means that you don't need to include confidential data in your application code. ConfigMaps # A ConfigMap is an API object used to store non-confidential data in key-value pairs. Pods can consume ConfigMaps as environment variables, command-line arguments, or as configuration files in a volume. A ConfigMap allows you to decouple environment-specific configuration from your container images, so that your applications are easily portable. Ingress # Namespace # Job # Start Cluster # tbd Addons # SealedSecret # https://github.com/bitnami-labs/sealed-secrets k8s Load Balancer # https://aws.amazon.com/blogs/opensource/network-load-balancer-nginx-ingress-controller-eks/ https://cloud.google.com/kubernetes-engine/docs/tutorials/http-balancer k8s Long Lived Conn # Issue https://learnk8s.io/kubernetes-long-lived-connections https://tech.xing.com/a-reason-for-unexplained-connection-timeouts-on-kubernetes-docker-abd041cf7e02 https://kubernetes.io/blog/2019/03/29/kube-proxy-subtleties-debugging-an-intermittent-connection-reset/ https://www.edureka.co/community/57404/set-up-a-websocket-in-google-kubernetes-engine Sol https://medium.com/johnjjung/how-to-use-gcp-loadbalancer-with-websockets-on-kubernetes-using-services-ingresses-and-backend-16a5565e4702 Docker Swarm # Docker Compose # Apache Mesos # Infrastructure as Code # Terraform # https://www.terraform.io/ Queues # RabbitMQ # Celery # Redis # Source: https://logz.io/blog/kafka-vs-redis/ Apache Kafka # a distributed streaming platform/framework Source: https://logz.io/blog/kafka-vs-redis/ Use Case building realtime data pipelines and streaming applications messaging application ELK Stack # Elasticsearch # a no-sql db Logstash # a log pipeline tool Kibana # a vizualization tool Others # Documentation Tool # Sphinx # ReadTheDoc # MkDocs # GitBook # MoinMoin # Ref # https://www.znetlive.com/blog/compare-top-devops-tools-docker-kubernetes-puppet-chef-ansible/","title":"Basics"},{"location":"Engineering/Software-Engineering/DevOps/basics/#basics","text":"Basics CI/CD CircleCI Travis CI Github Actions Ansible Jenkins Chef Puppet Containers & VM Docker Vagrant Servers nginx gunicorn ngrok Apache Tomcat CloudFlare Proxy Server Reverse Proxy Server Load Balancer Algorithms Cluster Management & Orchestration Minikube (Run Local) Prerequisites Installation Start Cluster Explore Cluster Deploy Applications LoadBalancer deployment Manager cluster Access application More Kubernetes (k8s) Pod Deployment Service ReplicaSet Secrets ConfigMaps Ingress Namespace Job Start Cluster Addons SealedSecret k8s Load Balancer k8s Long Lived Conn Docker Swarm Docker Compose Apache Mesos Infrastructure as Code Terraform Queues RabbitMQ Celery Redis Apache Kafka ELK Stack Elasticsearch Logstash Kibana Others Documentation Tool Sphinx ReadTheDoc MkDocs GitBook MoinMoin Ref","title":"Basics"},{"location":"Engineering/Software-Engineering/DevOps/basics/#cicd","text":"","title":"CI/CD"},{"location":"Engineering/Software-Engineering/DevOps/basics/#circleci","text":"","title":"CircleCI"},{"location":"Engineering/Software-Engineering/DevOps/basics/#travis-ci","text":"","title":"Travis CI"},{"location":"Engineering/Software-Engineering/DevOps/basics/#github-actions","text":"","title":"Github Actions"},{"location":"Engineering/Software-Engineering/DevOps/basics/#ansible","text":"","title":"Ansible"},{"location":"Engineering/Software-Engineering/DevOps/basics/#jenkins","text":"","title":"Jenkins"},{"location":"Engineering/Software-Engineering/DevOps/basics/#chef","text":"","title":"Chef"},{"location":"Engineering/Software-Engineering/DevOps/basics/#puppet","text":"","title":"Puppet"},{"location":"Engineering/Software-Engineering/DevOps/basics/#containers-vm","text":"","title":"Containers &amp; VM"},{"location":"Engineering/Software-Engineering/DevOps/basics/#docker","text":"Image, Repo, Tag naming convention 1 2 3 4 5 6 7 8 9 10 11 12 13 # BUILD an image & tag it - from Dockerfile in \".\" (current dir) # semantics: # docker build -t <tag> # docker build -t <namespace>/<image|repo name>:<tag> . $ docker build -t <target name>: [ <tag> ] . # where target name is called image or repo name # TAG an image # semantics: # docker tag <source>[:<tag>] <target>[:<tag>] # docker tag <img ID|existing tag> <reposotiry>:<tag> # docker tag <image> <namespace>/<repo name>:<tag> $ docker tag <source> [ :<tag> ] <target> [ :<tag> ] tag: is a pointer to an image are alias (e.g. my_image:latest, my_image:v1) to the image IDs (f1477ec11d12) think of how diff. git tags can refer to a commit SHA image: is identified by an ID (hash/msg digest) of configs/layers (check if for same 2 config/Dockerfile if the ID is same) each image can have 0 or more tags repo: is an remote location under a namespace (i.e. account/username) where image(s?) are stored :latest is not always LATEST tag https://blog.container-solutions.com/docker-latest-confusion https://stackoverflow.com/questions/44500367/when-would-a-docker-image-and-its-repository-have-different-names","title":"Docker"},{"location":"Engineering/Software-Engineering/DevOps/basics/#vagrant","text":"","title":"Vagrant"},{"location":"Engineering/Software-Engineering/DevOps/basics/#servers","text":"","title":"Servers"},{"location":"Engineering/Software-Engineering/DevOps/basics/#nginx","text":"","title":"nginx"},{"location":"Engineering/Software-Engineering/DevOps/basics/#gunicorn","text":"","title":"gunicorn"},{"location":"Engineering/Software-Engineering/DevOps/basics/#ngrok","text":"","title":"ngrok"},{"location":"Engineering/Software-Engineering/DevOps/basics/#apache-tomcat","text":"","title":"Apache Tomcat"},{"location":"Engineering/Software-Engineering/DevOps/basics/#cloudflare","text":"","title":"CloudFlare"},{"location":"Engineering/Software-Engineering/DevOps/basics/#proxy-server","text":"","title":"Proxy Server"},{"location":"Engineering/Software-Engineering/DevOps/basics/#reverse-proxy-server","text":"","title":"Reverse Proxy Server"},{"location":"Engineering/Software-Engineering/DevOps/basics/#load-balancer","text":"https://www.ateam-oracle.com/long-lived-tcp-connections-and-load-balancers what happens when one or more servers comes up behind a load balancer? what in case of long lived TCP conn?","title":"Load Balancer"},{"location":"Engineering/Software-Engineering/DevOps/basics/#algorithms","text":"https://blog.twitter.com/engineering/en_us/topics/infrastructure/2019/daperture-load-balancer.html","title":"Algorithms"},{"location":"Engineering/Software-Engineering/DevOps/basics/#cluster-management-orchestration","text":"(Data center, cluster manager, container-orchestration system)","title":"Cluster Management &amp; Orchestration"},{"location":"Engineering/Software-Engineering/DevOps/basics/#minikube-run-local","text":"https://github.com/kubernetes/minikube https://minikube.sigs.k8s.io/docs/start/","title":"Minikube (Run Local)"},{"location":"Engineering/Software-Engineering/DevOps/basics/#prerequisites","text":"","title":"Prerequisites"},{"location":"Engineering/Software-Engineering/DevOps/basics/#installation","text":"1 2 $ curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 $ sudo install minikube-linux-amd64 /usr/local/bin/minikube","title":"Installation"},{"location":"Engineering/Software-Engineering/DevOps/basics/#start-cluster","text":"1 $ minikube start","title":"Start Cluster"},{"location":"Engineering/Software-Engineering/DevOps/basics/#explore-cluster","text":"Get all the pods 1 2 3 4 5 6 $ minikube kubectl -- get pods -A # or $ alias kubectl = 'minikube kubectl --' $ kubectl get pods -A","title":"Explore Cluster"},{"location":"Engineering/Software-Engineering/DevOps/basics/#deploy-applications","text":"1 2 3 4 5 6 7 8 9 10 11 # create deployment $ minikube kubectl -- create deployment hello-minikube --image = k8s.gcr.io/echoserver:1.4 # expose the port 8080 of the container $ minikube kubectl -- expose deployment hello-minikube --type = NodePort --port = 8080 # show hello-minikube service $ minikube kubectl -- get services hello-minikube # open the service in browser # via option a) $ minikube service hello-minikube # or, via option b) $ minikube kubectl -- port-forward service/hello-minikube 7080 :8080 # open localhost:7080","title":"Deploy Applications"},{"location":"Engineering/Software-Engineering/DevOps/basics/#loadbalancer-deployment","text":"the standard way to expose application to internet each service gets its own IP 1 2 3 4 5 6 7 8 # create a load-balanced deployment $ minikube kubectl -- create deployment balanced --image = k8s.gcr.io/echoserver:1.4 # expose the server over port 8080 $ minikube kubectl -- expose deployment balanced --type = LoadBalancer --port = 8080 # now create a tunnel in diff. window $ minikube tunnel # now show info of the service & open the <EXTERNAL-IP>:8080 to access $ minikube kubectl -- get services balanced","title":"LoadBalancer deployment"},{"location":"Engineering/Software-Engineering/DevOps/basics/#manager-cluster","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 # Upgrade your cluster: $ minikube start --kubernetes-version = latest # Pause Kubernetes without impacting deployed applications: $ minikube pause # Unpause a paused instance: $ minikube unpause # Halt the cluster: $ minikube stop # Increase the default memory limit (requires a restart): $ minikube config set memory 16384 # Restart $ minikube stop && minikube start # Browse the catalog of easily installed Kubernetes services: $ minikube addons list # Enable an addon $ minikube addons enable <name> # Enable addon at startup $ minikube start --addons <name1> --addons <name2> # Open addon server in browser (iff) $ minikube addons open <name> # Disable an addon $ minikube addons disable <name> # Start a second local cluster $ minikube start -p cluster2 # Create a second cluster running an older Kubernetes release: $ minikube start -p aged --kubernetes-version = v1.16.1 # Delete the local cluster $ minikube delete # Delete all of the minikube clusters: $ minikube delete --all","title":"Manager cluster"},{"location":"Engineering/Software-Engineering/DevOps/basics/#access-application","text":"More ways: https://minikube.sigs.k8s.io/docs/handbook/accessing/","title":"Access application"},{"location":"Engineering/Software-Engineering/DevOps/basics/#more","text":"https://minikube.sigs.k8s.io/docs/handbook/","title":"More"},{"location":"Engineering/Software-Engineering/DevOps/basics/#kubernetes-k8s","text":"","title":"Kubernetes (k8s)"},{"location":"Engineering/Software-Engineering/DevOps/basics/#pod","text":"A Kubernetes Pod is a group of one or more Containers, tied together for the purposes of administration and networking.","title":"Pod"},{"location":"Engineering/Software-Engineering/DevOps/basics/#deployment","text":"A Kubernetes Deployment checks on the health of your Pod and restarts the Pod's Container if it terminates Deployments are the recommended way to manage the creation and scaling of Pods","title":"Deployment"},{"location":"Engineering/Software-Engineering/DevOps/basics/#service","text":"By default, the Pod is only accessible by its internal IP address within the Kubernetes cluster. To make the hello-node Container accessible from outside the Kubernetes virtual network, you have to expose the Pod as a Kubernetes Service. An abstract way to expose an application running on a set of Pods as a network service. With Kubernetes you don't need to modify your application to use an unfamiliar service discovery mechanism. Kubernetes gives Pods their own IP addresses and a single DNS name for a set of Pods, and can load-balance across them.","title":"Service"},{"location":"Engineering/Software-Engineering/DevOps/basics/#replicaset","text":"A ReplicaSet's purpose is to maintain a stable set of replica Pods running at any given time. As such, it is often used to guarantee the availability of a specified number of identical Pods.","title":"ReplicaSet"},{"location":"Engineering/Software-Engineering/DevOps/basics/#secrets","text":"A Secret is an object that contains a small amount of sensitive data such as a password, a token, or a key. Such information might otherwise be put in a Pod specification or in a container image. Using a Secret means that you don't need to include confidential data in your application code.","title":"Secrets"},{"location":"Engineering/Software-Engineering/DevOps/basics/#configmaps","text":"A ConfigMap is an API object used to store non-confidential data in key-value pairs. Pods can consume ConfigMaps as environment variables, command-line arguments, or as configuration files in a volume. A ConfigMap allows you to decouple environment-specific configuration from your container images, so that your applications are easily portable.","title":"ConfigMaps"},{"location":"Engineering/Software-Engineering/DevOps/basics/#ingress","text":"","title":"Ingress"},{"location":"Engineering/Software-Engineering/DevOps/basics/#namespace","text":"","title":"Namespace"},{"location":"Engineering/Software-Engineering/DevOps/basics/#job","text":"","title":"Job"},{"location":"Engineering/Software-Engineering/DevOps/basics/#start-cluster_1","text":"tbd","title":"Start Cluster"},{"location":"Engineering/Software-Engineering/DevOps/basics/#addons","text":"","title":"Addons"},{"location":"Engineering/Software-Engineering/DevOps/basics/#sealedsecret","text":"https://github.com/bitnami-labs/sealed-secrets","title":"SealedSecret"},{"location":"Engineering/Software-Engineering/DevOps/basics/#k8s-load-balancer","text":"https://aws.amazon.com/blogs/opensource/network-load-balancer-nginx-ingress-controller-eks/ https://cloud.google.com/kubernetes-engine/docs/tutorials/http-balancer","title":"k8s Load Balancer"},{"location":"Engineering/Software-Engineering/DevOps/basics/#k8s-long-lived-conn","text":"Issue https://learnk8s.io/kubernetes-long-lived-connections https://tech.xing.com/a-reason-for-unexplained-connection-timeouts-on-kubernetes-docker-abd041cf7e02 https://kubernetes.io/blog/2019/03/29/kube-proxy-subtleties-debugging-an-intermittent-connection-reset/ https://www.edureka.co/community/57404/set-up-a-websocket-in-google-kubernetes-engine Sol https://medium.com/johnjjung/how-to-use-gcp-loadbalancer-with-websockets-on-kubernetes-using-services-ingresses-and-backend-16a5565e4702","title":"k8s Long Lived Conn"},{"location":"Engineering/Software-Engineering/DevOps/basics/#docker-swarm","text":"","title":"Docker Swarm"},{"location":"Engineering/Software-Engineering/DevOps/basics/#docker-compose","text":"","title":"Docker Compose"},{"location":"Engineering/Software-Engineering/DevOps/basics/#apache-mesos","text":"","title":"Apache Mesos"},{"location":"Engineering/Software-Engineering/DevOps/basics/#infrastructure-as-code","text":"","title":"Infrastructure as Code"},{"location":"Engineering/Software-Engineering/DevOps/basics/#terraform","text":"https://www.terraform.io/","title":"Terraform"},{"location":"Engineering/Software-Engineering/DevOps/basics/#queues","text":"","title":"Queues"},{"location":"Engineering/Software-Engineering/DevOps/basics/#rabbitmq","text":"","title":"RabbitMQ"},{"location":"Engineering/Software-Engineering/DevOps/basics/#celery","text":"","title":"Celery"},{"location":"Engineering/Software-Engineering/DevOps/basics/#redis","text":"Source: https://logz.io/blog/kafka-vs-redis/","title":"Redis"},{"location":"Engineering/Software-Engineering/DevOps/basics/#apache-kafka","text":"a distributed streaming platform/framework Source: https://logz.io/blog/kafka-vs-redis/ Use Case building realtime data pipelines and streaming applications messaging application","title":"Apache Kafka"},{"location":"Engineering/Software-Engineering/DevOps/basics/#elk-stack","text":"","title":"ELK Stack"},{"location":"Engineering/Software-Engineering/DevOps/basics/#elasticsearch","text":"a no-sql db","title":"Elasticsearch"},{"location":"Engineering/Software-Engineering/DevOps/basics/#logstash","text":"a log pipeline tool","title":"Logstash"},{"location":"Engineering/Software-Engineering/DevOps/basics/#kibana","text":"a vizualization tool","title":"Kibana"},{"location":"Engineering/Software-Engineering/DevOps/basics/#others","text":"","title":"Others"},{"location":"Engineering/Software-Engineering/DevOps/basics/#documentation-tool","text":"","title":"Documentation Tool"},{"location":"Engineering/Software-Engineering/DevOps/basics/#sphinx","text":"","title":"Sphinx"},{"location":"Engineering/Software-Engineering/DevOps/basics/#readthedoc","text":"","title":"ReadTheDoc"},{"location":"Engineering/Software-Engineering/DevOps/basics/#mkdocs","text":"","title":"MkDocs"},{"location":"Engineering/Software-Engineering/DevOps/basics/#gitbook","text":"","title":"GitBook"},{"location":"Engineering/Software-Engineering/DevOps/basics/#moinmoin","text":"","title":"MoinMoin"},{"location":"Engineering/Software-Engineering/DevOps/basics/#ref","text":"https://www.znetlive.com/blog/compare-top-devops-tools-docker-kubernetes-puppet-chef-ansible/","title":"Ref"},{"location":"Engineering/Software-Engineering/DevOps/Clouds/aws/","text":"AWS # AWS AWS Cost Estimate Optimization IAM - Identity and Access Management Computing AMI - Amazon Machine Image Instance Storage Bucket Database CI/CD AWS Elastic Beanstalk AWS CodeDeploy AWS Cost # Estimate # https://calculator.aws/#/createCalculator Optimization # https://docs.aws.amazon.com/wellarchitected/latest/cost-optimization-pillar/wellarchitected-cost-optimization-pillar.pdf https://www.cloudhealthtech.com/blog/10-aws-cost-optimization-best-practices IAM - Identity and Access Management # Computing # AMI - Amazon Machine Image # Instance # Storage # Bucket # Database # CI/CD # AWS Elastic Beanstalk # Source: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/Welcome.html AWS CodeDeploy # AWS CodeDeploy is a deployment service that automates application deployments to Amazon EC2 instances, on-premises instances, or serverless Lambda functions. Source: https://docs.aws.amazon.com/codedeploy/latest/userguide/welcome.html","title":"AWS"},{"location":"Engineering/Software-Engineering/DevOps/Clouds/aws/#aws","text":"AWS AWS Cost Estimate Optimization IAM - Identity and Access Management Computing AMI - Amazon Machine Image Instance Storage Bucket Database CI/CD AWS Elastic Beanstalk AWS CodeDeploy","title":"AWS"},{"location":"Engineering/Software-Engineering/DevOps/Clouds/aws/#aws-cost","text":"","title":"AWS Cost"},{"location":"Engineering/Software-Engineering/DevOps/Clouds/aws/#estimate","text":"https://calculator.aws/#/createCalculator","title":"Estimate"},{"location":"Engineering/Software-Engineering/DevOps/Clouds/aws/#optimization","text":"https://docs.aws.amazon.com/wellarchitected/latest/cost-optimization-pillar/wellarchitected-cost-optimization-pillar.pdf https://www.cloudhealthtech.com/blog/10-aws-cost-optimization-best-practices","title":"Optimization"},{"location":"Engineering/Software-Engineering/DevOps/Clouds/aws/#iam-identity-and-access-management","text":"","title":"IAM - Identity and Access Management"},{"location":"Engineering/Software-Engineering/DevOps/Clouds/aws/#computing","text":"","title":"Computing"},{"location":"Engineering/Software-Engineering/DevOps/Clouds/aws/#ami-amazon-machine-image","text":"","title":"AMI - Amazon Machine Image"},{"location":"Engineering/Software-Engineering/DevOps/Clouds/aws/#instance","text":"","title":"Instance"},{"location":"Engineering/Software-Engineering/DevOps/Clouds/aws/#storage","text":"","title":"Storage"},{"location":"Engineering/Software-Engineering/DevOps/Clouds/aws/#bucket","text":"","title":"Bucket"},{"location":"Engineering/Software-Engineering/DevOps/Clouds/aws/#database","text":"","title":"Database"},{"location":"Engineering/Software-Engineering/DevOps/Clouds/aws/#cicd","text":"","title":"CI/CD"},{"location":"Engineering/Software-Engineering/DevOps/Clouds/aws/#aws-elastic-beanstalk","text":"Source: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/Welcome.html","title":"AWS Elastic Beanstalk"},{"location":"Engineering/Software-Engineering/DevOps/Clouds/aws/#aws-codedeploy","text":"AWS CodeDeploy is a deployment service that automates application deployments to Amazon EC2 instances, on-premises instances, or serverless Lambda functions. Source: https://docs.aws.amazon.com/codedeploy/latest/userguide/welcome.html","title":"AWS CodeDeploy"},{"location":"Engineering/Software-Engineering/DevOps/Clouds/gcp/","text":"GCP # GCP Getting Started Setup gcloud CLI Logging Structured Logging BigQuery Dataflow Storage Cost GCP Cost Estimate Scheduler Cost Quota & Limit Ways Using HTTP Using PubSub Using AppEngine HTTP Terraform script for the same: Vertex AI Getting Started # Ref: https://cloud.google.com/dataflow/docs/quickstarts/quickstart-python In the Google Cloud Console, on the project selector page, select or create a Google Cloud project. Note: If you don't plan to keep the resources that you create in this procedure, create a project instead of selecting an existing project. After you finish these steps, you can delete the project, removing all resources associated with the project. Go to project selector Make sure that billing is enabled for your Cloud project. Learn how to check if billing is enabled on a project. Enable the Dataflow, Compute Engine, Cloud Logging, Cloud Storage, Google Cloud Storage JSON, BigQuery, Cloud Pub/Sub, Cloud Datastore, and Cloud Resource Manager APIs. Enable the APIs Create a service account: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 In the Cloud Console, go to the Create service account page. Go to Create service account Select your project. In the Service account name field, enter a name. The Cloud Console fills in the Service account ID field based on this name. In the Service account description field, enter a description. For example, Service account for quickstart. Click Create and continue. To provide access to your project, grant the following role(s) to your service account: Project > Owner . Click the Select a role field, then select the first (or only) role. For additional roles, click Add another role and add each additional role. Note: The Role field affects which resources your service account can access in your project. You can revoke these roles or grant additional roles later. In production environments, do not grant the Owner, Editor, or Viewer roles. Instead, grant a predefined role or custom role that meets your needs. Click Continue. Click Done to finish creating the service account. Do not close your browser window. You will use it in the next step. Create a service account key: 1 2 3 4 5 In the Cloud Console, click the email address for the service account that you created. Click Keys. Click Add key, then click Create new key. Click Create. A JSON key file is downloaded to your computer. Click Close. Set the environment variable GOOGLE_APPLICATION_CREDENTIALS to the path of the JSON file that contains your service account key. This variable only applies to your current shell session, so if you open a new session, set the variable again. Example: Linux or macOS export GOOGLE_APPLICATION_CREDENTIALS=\"KEY_PATH \" Replace KEY_PATH with the path of the JSON file that contains your service account key. For example: export GOOGLE_APPLICATION_CREDENTIALS=\"/home/user/Downloads/service-account-file.json\" Example: Windows For PowerShell: $env:GOOGLE_APPLICATION_CREDENTIALS=\"KEY_PATH \" Replace KEY_PATH with the path of the JSON file that contains your service account key. For example: $env:GOOGLE_APPLICATION_CREDENTIALS=\"C:\\Users\\username\\Downloads\\service-account-file.json\" For command prompt: set GOOGLE_APPLICATION_CREDENTIALS=KEY_PATH Replace KEY_PATH with the path of the JSON file that contains your service account key. Create a Cloud Storage bucket: 1 2 3 4 5 6 7 8 9 10 11 12 13 In the Cloud Console, go to the Cloud Storage Browser page. Go to Browser Click Create bucket. On the Create a bucket page, enter your bucket information. To go to the next step, click Continue. For Name your bucket, enter a unique bucket name. Don't include sensitive information in the bucket name, because the bucket namespace is global and publicly visible. For Choose where to store your data, do the following: Select a Location type option. Select a Location option. For Choose a default storage class for your data, select the following: Standard. For Choose how to control access to objects, select an Access control option. For Advanced settings (optional), specify an encryption method, a retention policy, or bucket labels. Click Create. Copy the Google Cloud project ID and the Cloud Storage bucket name. You need these values later in this document. Setup # gcloud CLI # 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 $ gcloud auth login $ gcloud config configurations create <dataflow-config> Created [ demo-config ] . Activated [ demo-config ] . $ gcloud config set project <dataflow-eg> Updated property [ core/project ] . $ gcloud config set account <toran@email.com> Updated property [ core/account ] . $ gcloud config set compute/region <us-central1> Updated property [ compute/region ] . $ gcloud config set compute/zone <us-central1-f> Updated property [ compute/zone ] . Logging # Structured Logging # https://cloud.google.com/logging/docs/structured-logging BigQuery # Dataflow # Storage # Cost # https://cloud.google.com/storage/pricing#storage-pricing GCP Cost # Estimate # https://cloud.google.com/products/calculator Scheduler # To schedule the dataflow batch job. Requires OAuth authetication from service account Requires https://www.googleapis.com/auth/cloud-platform authorization scope Ref: https://www.thecodebuzz.com/schedule-dataflow-job-google-cloud-scheduler/ Cost # pricing is based exclusively on the job execution of job is not billed; in fact existence of a job is billed $0.10 for per job per 31 days (i.e. $0.003/job/day) least time unit is a day Note: A paused job is counted as a job. Quota & Limit # Ways # Using HTTP # https://cloud.google.com/sdk/gcloud/reference/scheduler/jobs/update/http Use this option along with Dataflow REST api t o launch the dataflow template. Select the service account for OAuth credentials. 1 $ curl Using PubSub # tbd Using AppEngine HTTP # tbd Terraform script for the same: # 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 resource \"google_cloud_scheduler_job\" \"scheduler\" { name = \"scheduler-demo\" schedule = \"0 0 * * *\" # This needs to be us-central1 even if App Engine is in us-central. # You will get a resource not found error if just using us-central. region = \"us-central1\" http_target { http_method = \"POST\" uri = \"https://dataflow.googleapis.com/v1b3/projects/${var.project_id}/locations/${var.region}/templates:launch?gcsPath=gs://${var.bucket}/templates/dataflow-demo-template\" oauth_token { service_account_email = google_service_account.cloud-scheduler-demo.email } # need to encode the string body = base64encode ( <<- EOT { \"jobName\": \"test-cloud-scheduler\", \"parameters\": { \"region\": \"${var.region}\", \"autoscalingAlgorithm\": \"THROUGHPUT_BASED\", }, \"environment\": { \"maxWorkers\": \"10\", \"tempLocation\": \"gs://${var.bucket}/temp\", \"zone\": \"${var.region}-a\" } } EOT ) } } Ref: https://cloud.google.com/community/tutorials/schedule-dataflow-jobs-with-cloud-scheduler Vertex AI # https://cloud.google.com/vertex-ai/pricing","title":"GCP"},{"location":"Engineering/Software-Engineering/DevOps/Clouds/gcp/#gcp","text":"GCP Getting Started Setup gcloud CLI Logging Structured Logging BigQuery Dataflow Storage Cost GCP Cost Estimate Scheduler Cost Quota & Limit Ways Using HTTP Using PubSub Using AppEngine HTTP Terraform script for the same: Vertex AI","title":"GCP"},{"location":"Engineering/Software-Engineering/DevOps/Clouds/gcp/#getting-started","text":"Ref: https://cloud.google.com/dataflow/docs/quickstarts/quickstart-python In the Google Cloud Console, on the project selector page, select or create a Google Cloud project. Note: If you don't plan to keep the resources that you create in this procedure, create a project instead of selecting an existing project. After you finish these steps, you can delete the project, removing all resources associated with the project. Go to project selector Make sure that billing is enabled for your Cloud project. Learn how to check if billing is enabled on a project. Enable the Dataflow, Compute Engine, Cloud Logging, Cloud Storage, Google Cloud Storage JSON, BigQuery, Cloud Pub/Sub, Cloud Datastore, and Cloud Resource Manager APIs. Enable the APIs Create a service account: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 In the Cloud Console, go to the Create service account page. Go to Create service account Select your project. In the Service account name field, enter a name. The Cloud Console fills in the Service account ID field based on this name. In the Service account description field, enter a description. For example, Service account for quickstart. Click Create and continue. To provide access to your project, grant the following role(s) to your service account: Project > Owner . Click the Select a role field, then select the first (or only) role. For additional roles, click Add another role and add each additional role. Note: The Role field affects which resources your service account can access in your project. You can revoke these roles or grant additional roles later. In production environments, do not grant the Owner, Editor, or Viewer roles. Instead, grant a predefined role or custom role that meets your needs. Click Continue. Click Done to finish creating the service account. Do not close your browser window. You will use it in the next step. Create a service account key: 1 2 3 4 5 In the Cloud Console, click the email address for the service account that you created. Click Keys. Click Add key, then click Create new key. Click Create. A JSON key file is downloaded to your computer. Click Close. Set the environment variable GOOGLE_APPLICATION_CREDENTIALS to the path of the JSON file that contains your service account key. This variable only applies to your current shell session, so if you open a new session, set the variable again. Example: Linux or macOS export GOOGLE_APPLICATION_CREDENTIALS=\"KEY_PATH \" Replace KEY_PATH with the path of the JSON file that contains your service account key. For example: export GOOGLE_APPLICATION_CREDENTIALS=\"/home/user/Downloads/service-account-file.json\" Example: Windows For PowerShell: $env:GOOGLE_APPLICATION_CREDENTIALS=\"KEY_PATH \" Replace KEY_PATH with the path of the JSON file that contains your service account key. For example: $env:GOOGLE_APPLICATION_CREDENTIALS=\"C:\\Users\\username\\Downloads\\service-account-file.json\" For command prompt: set GOOGLE_APPLICATION_CREDENTIALS=KEY_PATH Replace KEY_PATH with the path of the JSON file that contains your service account key. Create a Cloud Storage bucket: 1 2 3 4 5 6 7 8 9 10 11 12 13 In the Cloud Console, go to the Cloud Storage Browser page. Go to Browser Click Create bucket. On the Create a bucket page, enter your bucket information. To go to the next step, click Continue. For Name your bucket, enter a unique bucket name. Don't include sensitive information in the bucket name, because the bucket namespace is global and publicly visible. For Choose where to store your data, do the following: Select a Location type option. Select a Location option. For Choose a default storage class for your data, select the following: Standard. For Choose how to control access to objects, select an Access control option. For Advanced settings (optional), specify an encryption method, a retention policy, or bucket labels. Click Create. Copy the Google Cloud project ID and the Cloud Storage bucket name. You need these values later in this document.","title":"Getting Started"},{"location":"Engineering/Software-Engineering/DevOps/Clouds/gcp/#setup","text":"","title":"Setup"},{"location":"Engineering/Software-Engineering/DevOps/Clouds/gcp/#gcloud-cli","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 $ gcloud auth login $ gcloud config configurations create <dataflow-config> Created [ demo-config ] . Activated [ demo-config ] . $ gcloud config set project <dataflow-eg> Updated property [ core/project ] . $ gcloud config set account <toran@email.com> Updated property [ core/account ] . $ gcloud config set compute/region <us-central1> Updated property [ compute/region ] . $ gcloud config set compute/zone <us-central1-f> Updated property [ compute/zone ] .","title":"gcloud CLI"},{"location":"Engineering/Software-Engineering/DevOps/Clouds/gcp/#logging","text":"","title":"Logging"},{"location":"Engineering/Software-Engineering/DevOps/Clouds/gcp/#structured-logging","text":"https://cloud.google.com/logging/docs/structured-logging","title":"Structured Logging"},{"location":"Engineering/Software-Engineering/DevOps/Clouds/gcp/#bigquery","text":"","title":"BigQuery"},{"location":"Engineering/Software-Engineering/DevOps/Clouds/gcp/#dataflow","text":"","title":"Dataflow"},{"location":"Engineering/Software-Engineering/DevOps/Clouds/gcp/#storage","text":"","title":"Storage"},{"location":"Engineering/Software-Engineering/DevOps/Clouds/gcp/#cost","text":"https://cloud.google.com/storage/pricing#storage-pricing","title":"Cost"},{"location":"Engineering/Software-Engineering/DevOps/Clouds/gcp/#gcp-cost","text":"","title":"GCP Cost"},{"location":"Engineering/Software-Engineering/DevOps/Clouds/gcp/#estimate","text":"https://cloud.google.com/products/calculator","title":"Estimate"},{"location":"Engineering/Software-Engineering/DevOps/Clouds/gcp/#scheduler","text":"To schedule the dataflow batch job. Requires OAuth authetication from service account Requires https://www.googleapis.com/auth/cloud-platform authorization scope Ref: https://www.thecodebuzz.com/schedule-dataflow-job-google-cloud-scheduler/","title":"Scheduler"},{"location":"Engineering/Software-Engineering/DevOps/Clouds/gcp/#cost_1","text":"pricing is based exclusively on the job execution of job is not billed; in fact existence of a job is billed $0.10 for per job per 31 days (i.e. $0.003/job/day) least time unit is a day Note: A paused job is counted as a job.","title":"Cost"},{"location":"Engineering/Software-Engineering/DevOps/Clouds/gcp/#quota-limit","text":"","title":"Quota &amp; Limit"},{"location":"Engineering/Software-Engineering/DevOps/Clouds/gcp/#ways","text":"","title":"Ways"},{"location":"Engineering/Software-Engineering/DevOps/Clouds/gcp/#using-http","text":"https://cloud.google.com/sdk/gcloud/reference/scheduler/jobs/update/http Use this option along with Dataflow REST api t o launch the dataflow template. Select the service account for OAuth credentials. 1 $ curl","title":"Using HTTP"},{"location":"Engineering/Software-Engineering/DevOps/Clouds/gcp/#using-pubsub","text":"tbd","title":"Using PubSub"},{"location":"Engineering/Software-Engineering/DevOps/Clouds/gcp/#using-appengine-http","text":"tbd","title":"Using AppEngine HTTP"},{"location":"Engineering/Software-Engineering/DevOps/Clouds/gcp/#terraform-script-for-the-same","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 resource \"google_cloud_scheduler_job\" \"scheduler\" { name = \"scheduler-demo\" schedule = \"0 0 * * *\" # This needs to be us-central1 even if App Engine is in us-central. # You will get a resource not found error if just using us-central. region = \"us-central1\" http_target { http_method = \"POST\" uri = \"https://dataflow.googleapis.com/v1b3/projects/${var.project_id}/locations/${var.region}/templates:launch?gcsPath=gs://${var.bucket}/templates/dataflow-demo-template\" oauth_token { service_account_email = google_service_account.cloud-scheduler-demo.email } # need to encode the string body = base64encode ( <<- EOT { \"jobName\": \"test-cloud-scheduler\", \"parameters\": { \"region\": \"${var.region}\", \"autoscalingAlgorithm\": \"THROUGHPUT_BASED\", }, \"environment\": { \"maxWorkers\": \"10\", \"tempLocation\": \"gs://${var.bucket}/temp\", \"zone\": \"${var.region}-a\" } } EOT ) } } Ref: https://cloud.google.com/community/tutorials/schedule-dataflow-jobs-with-cloud-scheduler","title":"Terraform script for the same:"},{"location":"Engineering/Software-Engineering/DevOps/Clouds/gcp/#vertex-ai","text":"https://cloud.google.com/vertex-ai/pricing","title":"Vertex AI"},{"location":"Engineering/Software-Engineering/Product-Development/product_mindset/","text":"Product Mindset # Product Mindset Intro Key Features / competancy Technical Expertise Problem Solving Scalable Architecture Knowldege of New Technologies DS/ML, Blockchain Modular Monolith Experience with Startups Product Mindset Startup Ecosystem Agile Manifesto Why What What NOT How Ref Product Mindset Principles Manage Scope Screep Techniques Misc Hard Problem vs Technical challenges Intro # Key Features / competancy # Technical Expertise # Problem Solving # Scalable Architecture # Knowldege of New Technologies # DS/ML, Blockchain # Modular Monolith # Experience with Startups # Product Mindset # Startup Ecosystem # Agile Manifesto # Why # What # What NOT # How # scrum, retro, vemo Ref # https://agilemanifesto.org/ Product Mindset Principles # Must Should(level up) First Time Right Manage Scope Creep Commit & Deliver Improve Feature Adoption Solve Technical Problems Innovate Design w/ short term agility Ensure Architectural Longevity Manage Scope Screep # expansion of scope, rework vague requirements (b/c of that \ud83d\udc47) internal client detour weak project management gold platting Techniques # Context Map Uncover edge/complex/conflicting scenarios upfront TBD - Meaning - Techniques Misc # Hard Problem vs Technical challenges # Technical Challenge: Figuring out problem is tough, solution is straight-forward/simple Hard Problem: Figureing out problem is tough/easy, but solcing that is not simple as well","title":"Product Mindset"},{"location":"Engineering/Software-Engineering/Product-Development/product_mindset/#product-mindset","text":"Product Mindset Intro Key Features / competancy Technical Expertise Problem Solving Scalable Architecture Knowldege of New Technologies DS/ML, Blockchain Modular Monolith Experience with Startups Product Mindset Startup Ecosystem Agile Manifesto Why What What NOT How Ref Product Mindset Principles Manage Scope Screep Techniques Misc Hard Problem vs Technical challenges","title":"Product Mindset"},{"location":"Engineering/Software-Engineering/Product-Development/product_mindset/#intro","text":"","title":"Intro"},{"location":"Engineering/Software-Engineering/Product-Development/product_mindset/#key-features-competancy","text":"","title":"Key Features / competancy"},{"location":"Engineering/Software-Engineering/Product-Development/product_mindset/#technical-expertise","text":"","title":"Technical Expertise"},{"location":"Engineering/Software-Engineering/Product-Development/product_mindset/#problem-solving","text":"","title":"Problem Solving"},{"location":"Engineering/Software-Engineering/Product-Development/product_mindset/#scalable-architecture","text":"","title":"Scalable Architecture"},{"location":"Engineering/Software-Engineering/Product-Development/product_mindset/#knowldege-of-new-technologies","text":"","title":"Knowldege of New Technologies"},{"location":"Engineering/Software-Engineering/Product-Development/product_mindset/#dsml-blockchain","text":"","title":"DS/ML, Blockchain"},{"location":"Engineering/Software-Engineering/Product-Development/product_mindset/#modular-monolith","text":"","title":"Modular Monolith"},{"location":"Engineering/Software-Engineering/Product-Development/product_mindset/#experience-with-startups","text":"","title":"Experience with Startups"},{"location":"Engineering/Software-Engineering/Product-Development/product_mindset/#product-mindset_1","text":"","title":"Product Mindset"},{"location":"Engineering/Software-Engineering/Product-Development/product_mindset/#startup-ecosystem","text":"","title":"Startup Ecosystem"},{"location":"Engineering/Software-Engineering/Product-Development/product_mindset/#agile-manifesto","text":"","title":"Agile Manifesto"},{"location":"Engineering/Software-Engineering/Product-Development/product_mindset/#why","text":"","title":"Why"},{"location":"Engineering/Software-Engineering/Product-Development/product_mindset/#what","text":"","title":"What"},{"location":"Engineering/Software-Engineering/Product-Development/product_mindset/#what-not","text":"","title":"What NOT"},{"location":"Engineering/Software-Engineering/Product-Development/product_mindset/#how","text":"scrum, retro, vemo","title":"How"},{"location":"Engineering/Software-Engineering/Product-Development/product_mindset/#ref","text":"https://agilemanifesto.org/","title":"Ref"},{"location":"Engineering/Software-Engineering/Product-Development/product_mindset/#product-mindset-principles","text":"Must Should(level up) First Time Right Manage Scope Creep Commit & Deliver Improve Feature Adoption Solve Technical Problems Innovate Design w/ short term agility Ensure Architectural Longevity","title":"Product Mindset Principles"},{"location":"Engineering/Software-Engineering/Product-Development/product_mindset/#manage-scope-screep","text":"expansion of scope, rework vague requirements (b/c of that \ud83d\udc47) internal client detour weak project management gold platting","title":"Manage Scope Screep"},{"location":"Engineering/Software-Engineering/Product-Development/product_mindset/#techniques","text":"Context Map Uncover edge/complex/conflicting scenarios upfront TBD - Meaning - Techniques","title":"Techniques"},{"location":"Engineering/Software-Engineering/Product-Development/product_mindset/#misc","text":"","title":"Misc"},{"location":"Engineering/Software-Engineering/Product-Development/product_mindset/#hard-problem-vs-technical-challenges","text":"Technical Challenge: Figuring out problem is tough, solution is straight-forward/simple Hard Problem: Figureing out problem is tough/easy, but solcing that is not simple as well","title":"Hard Problem vs Technical challenges"},{"location":"Engineering/Software-Engineering/Product-Development/regulations_and_compliances/","text":"Regulations & Compliances # Table of Contents Regulations & Compliances GDPR CCPA GDPR vs CCPA SOC2 Type of Data Personal Data (PII) Pseudonymous Data Anonymous Data PII Anonymization/Deidentification Techniques Data Masking GDPR # Global Data Protection Regulation https://www.gdpreu.org/the-regulation/ PII anonymization applicable to Companies that do business in the European Union (EU) or European Economic Area (EEA) Ref: https://www.gdpreu.org/the-regulation/key-concepts/personal-data/ https://linfordco.com/blog/gdpr-soc-2/ CCPA # California Consumer Protection Act https://oag.ca.gov/privacy/ccpa PII de-identification applicable to Companies that do business in the United States of America Ref: - https://www.termsfeed.com/blog/ccpa/ GDPR vs CCPA # https://www.termsfeed.com/blog/gdpr-anonymization-versus-ccpa-de-identification/ SOC2 # System & Organization Control https://soc2.co.uk/ https://en.wikipedia.org/wiki/System_and_Organization_Controls https://cloud.google.com/security/compliance/soc-2 https://www.onelogin.com/compliance/soc-2-type-2 https://linfordco.com/blog/gdpr-soc-2/ Type of Data # Personal Data (PII) # Pseudonymous Data # Anonymous Data # PII Anonymization/Deidentification Techniques # https://help.branch.io/using-branch/docs/best-practices-to-avoid-sending-pii-to-branch Hashing of personally identifiable information is not sufficient https://www.johndcook.com/blog/2019/07/20/hashing-pii-does-not-protect-privacy/ https://dl.gi.de/handle/20.500.12116/16294 https://dl.gi.de/bitstream/handle/20.500.12116/16294/sicherheit2018-04.pdf?sequence=1&isAllowed=y quantum computing https://medium.com/meeco/why-hashed-personally-identifiable-information-pii-on-the-blockchain-can-be-safe-b842357b9663 https://blog.littledata.io/2016/08/03/personally-identifiable-information-pii-hashing-and-google-analytics/ https://www.imperva.com/learn/data-security/data-obfuscation/ https://arstechnica.com/information-technology/2013/05/how-crackers-make-minced-meat-out-of-your-passwords/ sha256 hashing is ideal ways to increase the effort of an attacker choose slow algo use salt/padding Data Masking # replace the PII with general values, which are still unique #","title":"Regulations & Compliances"},{"location":"Engineering/Software-Engineering/Product-Development/regulations_and_compliances/#regulations-compliances","text":"","title":"Regulations &amp; Compliances"},{"location":"Engineering/Software-Engineering/Product-Development/regulations_and_compliances/#gdpr","text":"Global Data Protection Regulation https://www.gdpreu.org/the-regulation/ PII anonymization applicable to Companies that do business in the European Union (EU) or European Economic Area (EEA) Ref: https://www.gdpreu.org/the-regulation/key-concepts/personal-data/ https://linfordco.com/blog/gdpr-soc-2/","title":"GDPR"},{"location":"Engineering/Software-Engineering/Product-Development/regulations_and_compliances/#ccpa","text":"California Consumer Protection Act https://oag.ca.gov/privacy/ccpa PII de-identification applicable to Companies that do business in the United States of America Ref: - https://www.termsfeed.com/blog/ccpa/","title":"CCPA"},{"location":"Engineering/Software-Engineering/Product-Development/regulations_and_compliances/#gdpr-vs-ccpa","text":"https://www.termsfeed.com/blog/gdpr-anonymization-versus-ccpa-de-identification/","title":"GDPR vs CCPA"},{"location":"Engineering/Software-Engineering/Product-Development/regulations_and_compliances/#soc2","text":"System & Organization Control https://soc2.co.uk/ https://en.wikipedia.org/wiki/System_and_Organization_Controls https://cloud.google.com/security/compliance/soc-2 https://www.onelogin.com/compliance/soc-2-type-2 https://linfordco.com/blog/gdpr-soc-2/","title":"SOC2"},{"location":"Engineering/Software-Engineering/Product-Development/regulations_and_compliances/#type-of-data","text":"","title":"Type of Data"},{"location":"Engineering/Software-Engineering/Product-Development/regulations_and_compliances/#personal-data-pii","text":"","title":"Personal Data (PII)"},{"location":"Engineering/Software-Engineering/Product-Development/regulations_and_compliances/#pseudonymous-data","text":"","title":"Pseudonymous Data"},{"location":"Engineering/Software-Engineering/Product-Development/regulations_and_compliances/#anonymous-data","text":"","title":"Anonymous Data"},{"location":"Engineering/Software-Engineering/Product-Development/regulations_and_compliances/#pii-anonymizationdeidentification-techniques","text":"https://help.branch.io/using-branch/docs/best-practices-to-avoid-sending-pii-to-branch Hashing of personally identifiable information is not sufficient https://www.johndcook.com/blog/2019/07/20/hashing-pii-does-not-protect-privacy/ https://dl.gi.de/handle/20.500.12116/16294 https://dl.gi.de/bitstream/handle/20.500.12116/16294/sicherheit2018-04.pdf?sequence=1&isAllowed=y quantum computing https://medium.com/meeco/why-hashed-personally-identifiable-information-pii-on-the-blockchain-can-be-safe-b842357b9663 https://blog.littledata.io/2016/08/03/personally-identifiable-information-pii-hashing-and-google-analytics/ https://www.imperva.com/learn/data-security/data-obfuscation/ https://arstechnica.com/information-technology/2013/05/how-crackers-make-minced-meat-out-of-your-passwords/ sha256 hashing is ideal ways to increase the effort of an attacker choose slow algo use salt/padding","title":"PII Anonymization/Deidentification Techniques"},{"location":"Engineering/Software-Engineering/Product-Development/regulations_and_compliances/#data-masking","text":"replace the PII with general values, which are still unique","title":"Data Masking"},{"location":"Engineering/Software-Engineering/Product-Development/regulations_and_compliances/#_1","text":"","title":""},{"location":"Engineering/Software-Engineering/Product-Development/Practices/code_review/","text":"Code Review # Table of Contents Code Review The Code Reviewer\u2019s Guide The Standard of Code Review Best Pratices What to look for in a code review? Design Functionality Complexity Tests Naming Comments Style Consistency Documentation Every Line Context Good Things Navigating a CL in Review Speed of Code Reviews How to Write Code Review Comments Handling Pushback in Code Reviews The Change Author\u2019s Guide Writing Good CL Descriptions Small CLs How to Handle Reviewer Comments Terminology References The Code Reviewer\u2019s Guide # Primary purpose of code review is - make sure that the overall code health of the code base is improving over time. The Standard of Code Review # reviewers should favor approving a CL once it is in a state where it definitely improves the overall code health of the system being worked on, even if the CL isn\u2019t perfect if a CL adds a feature that the reviewer doesn\u2019t want in their system, then the reviewer can certainly deny approval even if the code is well-designed there is no such thing as \u201cperfect\u201d code\u2014there is only better code Reviewers should not require the author to polish every tiny piece of a CL before granting approval. Rather, the reviewer should balance out the need to make forward progress compared to the importance of the changes they are suggesting Instead of seeking perfection, what a reviewer should seek is continuous improvement A CL that, as a whole, improves the maintainability, readability, and understandability of the system shouldn\u2019t be delayed for days or weeks because it isn\u2019t \u201cperfect. Reviewers should always feel free to leave comments expressing that something could be better, but if it\u2019s not very important, prefix it with something like \u201cNit: \u201c to let the author know that it\u2019s just a point of polish that they could choose to ignore. mentoring share knowledge to imrove code health over time resolving conflicts Don\u2019t let a CL sit around because the author and the reviewer can\u2019t come to an agreement. discuss, meet escalate But, Don\u2019t let a CL sit around because the author and the reviewer can\u2019t come to an agreement. Best Pratices # Know What to Look for in a Code Review Build and Test \u2014 Before Review Don't Review Code for Longer Than 60 Minutes Check No More Than 400 Lines at a Time Give Feedback That Helps (Not Hurts) Communicate Goals and Expectations Include Everyone in the Code Review Process Foster a Positive Culture Automate to Save Time What to look for in a code review? # Design # -The code is well-designed. Functionality # The functionality is good for the users of the code. Any UI changes are sensible and look good. Any parallel programming is done safely. Complexity # The code isn\u2019t more complex than it needs to be. Neither in re-use not for read. The developer isn\u2019t implementing things they might need in the future but don\u2019t know they need now. Tests # Code has appropriate unit tests. Tests are well-designed. Naming # The developer used clear names for everything. Comments # Comments are clear and useful, and mostly explain why instead of what. Style # Code is appropriately documented. The code conforms to our style guides. Consistency # Style wise Documentation # If a CL changes how users build, test, interact with, or release code, check to see that it also updates associated documentation, including READMEs. Every Line # look at every line of code that you have been assigned to review at least be sure that you understand what all the code is doing. Context # look at the CL in a broad context Good Things # If you see something nice in the CL, tell the developer, especially when they addressed one of your comments in a great way. Code reviews often just focus on mistakes, but they should offer encouragement and appreciation for good practices, as well Navigating a CL in Review # Speed of Code Reviews # How to Write Code Review Comments # Handling Pushback in Code Reviews # The Change Author\u2019s Guide # Writing Good CL Descriptions # Small CLs # How to Handle Reviewer Comments # Terminology # CL: Stands for \u201cchangelist\u201d, which means one self-contained change that has been submitted to version control or which is undergoing code review. Organizations often call this a \u201cchange\u201d, \u201cpatch\u201d, or \u201cpull-request\u201d. LGTM: Means \u201cLooks Good to Me\u201d. It is what a code reviewer says when approving a CL. References # https://google.github.io/eng-practices/ https://stackoverflow.blog/2019/09/30/how-to-make-good-code-reviews-better/ https://www.perforce.com/blog/qac/9-best-practices-for-code-review","title":"Code Review"},{"location":"Engineering/Software-Engineering/Product-Development/Practices/code_review/#code-review","text":"","title":"Code Review"},{"location":"Engineering/Software-Engineering/Product-Development/Practices/code_review/#the-code-reviewers-guide","text":"Primary purpose of code review is - make sure that the overall code health of the code base is improving over time.","title":"The Code Reviewer\u2019s Guide"},{"location":"Engineering/Software-Engineering/Product-Development/Practices/code_review/#the-standard-of-code-review","text":"reviewers should favor approving a CL once it is in a state where it definitely improves the overall code health of the system being worked on, even if the CL isn\u2019t perfect if a CL adds a feature that the reviewer doesn\u2019t want in their system, then the reviewer can certainly deny approval even if the code is well-designed there is no such thing as \u201cperfect\u201d code\u2014there is only better code Reviewers should not require the author to polish every tiny piece of a CL before granting approval. Rather, the reviewer should balance out the need to make forward progress compared to the importance of the changes they are suggesting Instead of seeking perfection, what a reviewer should seek is continuous improvement A CL that, as a whole, improves the maintainability, readability, and understandability of the system shouldn\u2019t be delayed for days or weeks because it isn\u2019t \u201cperfect. Reviewers should always feel free to leave comments expressing that something could be better, but if it\u2019s not very important, prefix it with something like \u201cNit: \u201c to let the author know that it\u2019s just a point of polish that they could choose to ignore. mentoring share knowledge to imrove code health over time resolving conflicts Don\u2019t let a CL sit around because the author and the reviewer can\u2019t come to an agreement. discuss, meet escalate But, Don\u2019t let a CL sit around because the author and the reviewer can\u2019t come to an agreement.","title":"The Standard of Code Review"},{"location":"Engineering/Software-Engineering/Product-Development/Practices/code_review/#best-pratices","text":"Know What to Look for in a Code Review Build and Test \u2014 Before Review Don't Review Code for Longer Than 60 Minutes Check No More Than 400 Lines at a Time Give Feedback That Helps (Not Hurts) Communicate Goals and Expectations Include Everyone in the Code Review Process Foster a Positive Culture Automate to Save Time","title":"Best Pratices"},{"location":"Engineering/Software-Engineering/Product-Development/Practices/code_review/#what-to-look-for-in-a-code-review","text":"","title":"What to look for in a code review?"},{"location":"Engineering/Software-Engineering/Product-Development/Practices/code_review/#design","text":"-The code is well-designed.","title":"Design"},{"location":"Engineering/Software-Engineering/Product-Development/Practices/code_review/#functionality","text":"The functionality is good for the users of the code. Any UI changes are sensible and look good. Any parallel programming is done safely.","title":"Functionality"},{"location":"Engineering/Software-Engineering/Product-Development/Practices/code_review/#complexity","text":"The code isn\u2019t more complex than it needs to be. Neither in re-use not for read. The developer isn\u2019t implementing things they might need in the future but don\u2019t know they need now.","title":"Complexity"},{"location":"Engineering/Software-Engineering/Product-Development/Practices/code_review/#tests","text":"Code has appropriate unit tests. Tests are well-designed.","title":"Tests"},{"location":"Engineering/Software-Engineering/Product-Development/Practices/code_review/#naming","text":"The developer used clear names for everything.","title":"Naming"},{"location":"Engineering/Software-Engineering/Product-Development/Practices/code_review/#comments","text":"Comments are clear and useful, and mostly explain why instead of what.","title":"Comments"},{"location":"Engineering/Software-Engineering/Product-Development/Practices/code_review/#style","text":"Code is appropriately documented. The code conforms to our style guides.","title":"Style"},{"location":"Engineering/Software-Engineering/Product-Development/Practices/code_review/#consistency","text":"Style wise","title":"Consistency"},{"location":"Engineering/Software-Engineering/Product-Development/Practices/code_review/#documentation","text":"If a CL changes how users build, test, interact with, or release code, check to see that it also updates associated documentation, including READMEs.","title":"Documentation"},{"location":"Engineering/Software-Engineering/Product-Development/Practices/code_review/#every-line","text":"look at every line of code that you have been assigned to review at least be sure that you understand what all the code is doing.","title":"Every Line"},{"location":"Engineering/Software-Engineering/Product-Development/Practices/code_review/#context","text":"look at the CL in a broad context","title":"Context"},{"location":"Engineering/Software-Engineering/Product-Development/Practices/code_review/#good-things","text":"If you see something nice in the CL, tell the developer, especially when they addressed one of your comments in a great way. Code reviews often just focus on mistakes, but they should offer encouragement and appreciation for good practices, as well","title":"Good Things"},{"location":"Engineering/Software-Engineering/Product-Development/Practices/code_review/#navigating-a-cl-in-review","text":"","title":"Navigating a CL in Review"},{"location":"Engineering/Software-Engineering/Product-Development/Practices/code_review/#speed-of-code-reviews","text":"","title":"Speed of Code Reviews"},{"location":"Engineering/Software-Engineering/Product-Development/Practices/code_review/#how-to-write-code-review-comments","text":"","title":"How to Write Code Review Comments"},{"location":"Engineering/Software-Engineering/Product-Development/Practices/code_review/#handling-pushback-in-code-reviews","text":"","title":"Handling Pushback in Code Reviews"},{"location":"Engineering/Software-Engineering/Product-Development/Practices/code_review/#the-change-authors-guide","text":"","title":"The Change Author\u2019s Guide"},{"location":"Engineering/Software-Engineering/Product-Development/Practices/code_review/#writing-good-cl-descriptions","text":"","title":"Writing Good CL Descriptions"},{"location":"Engineering/Software-Engineering/Product-Development/Practices/code_review/#small-cls","text":"","title":"Small CLs"},{"location":"Engineering/Software-Engineering/Product-Development/Practices/code_review/#how-to-handle-reviewer-comments","text":"","title":"How to Handle Reviewer Comments"},{"location":"Engineering/Software-Engineering/Product-Development/Practices/code_review/#terminology","text":"CL: Stands for \u201cchangelist\u201d, which means one self-contained change that has been submitted to version control or which is undergoing code review. Organizations often call this a \u201cchange\u201d, \u201cpatch\u201d, or \u201cpull-request\u201d. LGTM: Means \u201cLooks Good to Me\u201d. It is what a code reviewer says when approving a CL.","title":"Terminology"},{"location":"Engineering/Software-Engineering/Product-Development/Practices/code_review/#references","text":"https://google.github.io/eng-practices/ https://stackoverflow.blog/2019/09/30/how-to-make-good-code-reviews-better/ https://www.perforce.com/blog/qac/9-best-practices-for-code-review","title":"References"},{"location":"Engineering/Software-Engineering/Web/caching/","text":"Caching # Caching Redis Redis Cluster Ref: external in-memory based internal in-memory based browser-based cdn-based - https://aws.amazon.com/cloudfront/ proxy-server based Redis # https://www.datadoghq.com/pdf/Understanding-the-Top-5-Redis-Performance-Metrics.pdf https://tech.trivago.com/2017/01/25/learn-redis-the-hard-way-in-production/ https://yunpengn.github.io/blog/2019/05/04/consistent-redis-sql/ Redis Cluster # https://redis.io/topics/cluster-tutorial https://redis.io/topics/cluster-spec https://scalegrid.io/blog/intro-to-redis-cluster-sharding-advantages-limitations-deploying-and-client-connections/ Ref: # https://www.bookofspeed.com/ https://www.mnot.net/cache_docs/ https://stackoverflow.com/questions/7651458/how-to-get-started-with-web-caching-cdns-and-proxy-servers","title":"Caching"},{"location":"Engineering/Software-Engineering/Web/caching/#caching","text":"Caching Redis Redis Cluster Ref: external in-memory based internal in-memory based browser-based cdn-based - https://aws.amazon.com/cloudfront/ proxy-server based","title":"Caching"},{"location":"Engineering/Software-Engineering/Web/caching/#redis","text":"https://www.datadoghq.com/pdf/Understanding-the-Top-5-Redis-Performance-Metrics.pdf https://tech.trivago.com/2017/01/25/learn-redis-the-hard-way-in-production/ https://yunpengn.github.io/blog/2019/05/04/consistent-redis-sql/","title":"Redis"},{"location":"Engineering/Software-Engineering/Web/caching/#redis-cluster","text":"https://redis.io/topics/cluster-tutorial https://redis.io/topics/cluster-spec https://scalegrid.io/blog/intro-to-redis-cluster-sharding-advantages-limitations-deploying-and-client-connections/","title":"Redis Cluster"},{"location":"Engineering/Software-Engineering/Web/caching/#ref","text":"https://www.bookofspeed.com/ https://www.mnot.net/cache_docs/ https://stackoverflow.com/questions/7651458/how-to-get-started-with-web-caching-cdns-and-proxy-servers","title":"Ref:"},{"location":"Engineering/Software-Engineering/Web/security/","text":"Security # Table of Contents Security Encryption Hashing MD5 SHA1 SHA256 PBKDF2 SCrypt BCrypt Argon2 Argon2i Argon2d Argon2id Ref Deidentifying PII JWT Base 64 Encoding Base 64 URL Encoding Parts of JWT JOSE Header Secret Payload/Body/Claim Claim JWS JWA JWE Unsecured JWT Ref Token-Based Authentication OpenID 1.0 OpenID 2.0 OpenID Attribute Exchange 1.0 OAuth 1.0 OAuth 2.0 Authorization Grant Types (Flow) Authorization Code Flow Participants Prerequisites Setup of an Authorization Server Register an Resource Provider Server as Client in Authorization Server Flow 1. End-user opens Way2SMS App 2. Writes a SMS and clicks on Send to My Google Contacts 3. As End-user has not authorized the Way2SMS server (as well as app) to access End-user's resource stored at Google server 4. Way2SMS server provides the Way2SMS app a link to redirect the End-user there to complete the authorization with Google OAuth2 server Parameters 5. Way2SMS app opens this URL in app/browser, if not already logged-in, then it shows a Login Form 6. End-user had to enter their Google credentials 7. Once the credentials are validated, it shows that the service provide (Way2SMS server) wants to access this. this and this, do you consent? 8. If end-user accepts and agrees on the consent then in return Google authorization server redirects the end-user to the registered redirect_url by appending an \"authorization code\" to it Parameters Response 9. Now the service provider server (Way2SMS server) has the authorization code (also, if required, then Way2SMS App can provide the same through some helper API of Way2SMS server) 10. Service provider server can now call authorization server to get an access code in exchange of authorization code, by supplying a few important details about the registered service provider server Parameter Response Parameter 11. Now, the service provider (Way2SMS server) can access the Google contacts of the End-user by supplying the access token Response Extra Possible Flows 1. Service Provider can Introspect an Access Token Parameters Response 2. Service Provider can Revoke an Access Token Parameters Response 3. Service Provider can Refresh an Access Token Parameters Response Ref PKCE Why What How Ref Implicit Flow Ref OpenID Connect OAuth, OpenID, OpenID Connect SAML Encryption # Refer to /Computer-Science/se#encryption Hashing # aka Obfuscation puctuation matters while hashing Strings Algo Date Size (bits) Size (bytes) Length (hex digits) MD5 TODO 128 16 32 SHA-1 TODO 160 20 40 SHA-256 TODO 256 32 64 MD5 # SHA1 # SHA256 # PBKDF2 # SCrypt # BCrypt # Argon2 # Argon2i # Argon2d # Argon2id # Ref # https://medium.com/analytics-vidhya/password-hashing-pbkdf2-scrypt-bcrypt-and-argon2-e25aaf41598e Deidentifying PII # PII Anonymization/Deidentification Techniques JWT # JSON Web Token a compact, URL-safe way/means of representing claims securely between two parties URL-safe 3 parts separated by dots open & industry standard RFC 7519 status: concensus of IETF format intended for space constrained environments such as: HTTP Authorization headers HTTP URI query parameters a string representing a set of claims as a JSON object that is encoded in a JWS or JWE, enabling the claims to be digitally signed or MACed and/or encrypted can have zero or more claims a signed JWT is an abstraction over either JWS or JWE 1 2 3 4 5 6 7 8 9 10 11 12 13 14 // Header: Algorithm and Token type { \"alg\": \"HS256\", \"typ\": \"JWT\" } // Payload: Data { \"name\": \"John\" \"iat\": 1615455676 } // Signature some secret string // optionally base64/base64url encoded Base 64 Encoding # binary to text encoding represents binary data (8-bit bytes) to ASCII string format using radix-64 representation designed to carry data (binary data) across channels that only reliably supports text content Base 64 URL Encoding # First encode the string to base64, this may result in encoded string having some signs like + , / , = which have different meaning in filesystem/urls. Thus need to encoded them further to make them URL-safe. Index Base64 Base64Url 0 A A 1 B B 2 C C 3 D D 4 E E 5 F F 6 G G 7 H H 8 I I 9 J J 10 K K 11 L L 12 M M 13 N N 14 O O 15 P P 16 Q Q 17 R R 18 S S 19 T T 20 U U 21 V V 22 W W 23 X X 24 Y Y 25 Z Z 26 a a 27 b b 28 c c 29 d d 30 e e 31 f f 32 g g 33 h h 34 i i 35 j j 36 k k 37 l l 38 m m 39 n n 40 o o 41 p p 42 q q 43 r r 44 s s 45 t t 46 u u 47 v v 48 w w 49 x x 50 y y 51 z z 52 0 0 53 1 1 54 2 2 55 3 3 56 4 4 57 5 5 58 6 6 59 7 7 60 8 8 61 9 9 62 + - 63 / _ = (optional) Parts of JWT # JWT is combination of 3 base64 url encoded strings, parted by 2 dots JOSE Header # typ defines the media type of the complete JWT optional; recommended value: JWT defined by JWT specifications cty defines the content type; defines structural information recommended to use in case of nested JWTs only defined by JWT specifications alg defined by JWS specifications kid defined by JWS specifications Secret # Payload/Body/Claim # Claim # a piece of information asserted about a subject asserted: say something clearly and firmly in determined and confident way key val pair key == always string val could be any JSON data type JWS # JSON Web Signature JWA # JSON Web Algorithm RFC 7518 all applicable algorithms for signing are defined under this JWE # JSON Web Encryption Unsecured JWT # is a JWS object without any algorithm ( alg: none ) in its JOSE header is a JWS w/o signature Ref # https://medium.facilelogin.com/jwt-jws-and-jwe-for-not-so-dummies-b63310d201a3 Token-Based Authentication # OpenID 1.0 # (2006) lets an app ask an authority for proof that an end user owns an identity (a URL). an Authentication protocol deprecated example End user to app: I am Steve A. Smith. App to authority: Is this Steve A. Smith? The end user and authority speak for a moment. Authority to app: Yes, that is Steve A. Smith. OpenID 2.0 # (2007) does the same as OpenID 1.0, but adds a second identity format (XRI) and adds flexibility to how the end user specifies the identity and authority. an Authentication protocol deprecated OpenID Attribute Exchange 1.0 # (2007) extends OpenID 2.0 by letting an app fetch & store end user profile information with the authority - in addition to verifying the end user's identity. an Authentication protocol deprecated example: End user to app: I am Steve A. Smith. App to authority: Is this Steve A. Smith? Oh, and if it is, also fetch me his email address and phone number. The end user and authority speak for a moment. Authority to app: Yes, that is Steve A. Smith. His email is steve@domain.com and phone number is 123-456-7890. OAuth 1.0 # (2010) lets an end user grant an app limited access to resources on a third-party server that an authority owns. example: App to end user: We'd like to access your pictures on some other server. The end user and authority speak for a moment. Authority to app: Here is an access token. App to third-party server: Here is the access token that proves I am allowed to access pictures for an end user. an Authorization protocol deprecated OAuth 2.0 # (2012) does the same thing as OAuth 1.0 but with a completely new protocol an Authorization framework Authorization Grant Types (Flow) # There are multiple ways in OAuth2.0 itself to achieve authorization, which are already specified in the RFC based on different client type/scenarios. Authorization Code Flow PKCE Client Credential Device Code Refresh Token Legacy: Implicit Flow Legacy: Password Grant Authorization Code Flow # This is a suitable flow for following kind of case. Participants # End User (2nd party) Client (Mobile/SPA App - 3rd party) Service Provider (Resource server - 3rd party, Authorization server knows this as Registered Client as well) Authorization server (1st party) Example: Toran wants to use Way2SMS App (and internally their server) to send SMS to all the contacts stored in his Google account. Here, End user = Toran Client = Way2SMS App Service Provider = Way2SMS server Authorization Server = Google Where, the service provider is a 3rd party server. If there is no 3rd party server, and only Mobile/SPA clients, then PKCE is a better/secure flow. Even, now its recommended to use PKCE for 3rd party resource provider server case as well. Prerequisites # Setup of an Authorization Server # Authorization Base 1 https://authorization-server/oauth2/authorize Token URL 1 https://authorization-server/oauth2/token/ Redirect URL 1 https://authorization-server/oauth2/callback Authorization URL 1 https://authorization-server/oauth2/authorize/?response_type=code&state=random_state_string&client_id=:client_id Register an Resource Provider Server as Client in Authorization Server # Input: 1 2 3 client_type: confidential/public (public for Mobile App, Web App; confidential for backends/servers) Authorization Grant Type: authorization-code Redirect URL: https://resource-provider/oauth2/callback Output: 1 2 client_id: BOqLOb2uaTn4ppi5nlj2ErTWpr9fKt5YQrwpDvfs client_secret: bamZTX1gknB99XZvKWYucUPVdY5Aum5k8LDoZzUWz0VhwaSZpFhCyUrrxshaAJKtaZ5bTJXimBCLgjs0yPZxvow9NBrqEXG6qeK0VFy2incKLzZdw97Q4jNxAafBq3c8 Keep note of this. Flow # 1. End-user opens Way2SMS App # 2. Writes a SMS and clicks on Send to My Google Contacts # 3. As End-user has not authorized the Way2SMS server (as well as app) to access End-user's resource stored at Google server # 4. Way2SMS server provides the Way2SMS app a link to redirect the End-user there to complete the authorization with Google OAuth2 server # The shared URL (called Authorization API) by Way2SMS server has a few query params appended by the Way2SMS server 1 GET https://authorization-server/oauth2/authorize?response_type=code&state=random_state_string&client_id=:client_id&redirect_url=<https://service-provider/oauth2/callback>&scope=profile%20email Parameters # Name Type In Description client_id string query ID of the service provider server (Way2SMS server), that is registered with Google OAuth Serverredirect_url response_type string query indicates the expectation in return i.e. an authorization code state string query any state encoded as JWT if maintained by service provider which should be send back to it by authorization server after authorization scope string query (optional)space separated scope to request additional level of access - depends on particular service Note: Here state may serve multiple purposes: Serves as a CSRF protection mechanism if it contains a random value per request. When the user is redirected back to your app, double check that the state value matches what you set it to originally. This may be used to indicate what action in the app to perform after authorization is complete, for example, indicating which of your app\u2019s pages to redirect to after authorization. This gives your app a chance to persist data between the user being directed to the authorization server and back again, such as using the state parameter as a session key. Some services support registering multiple redirect URLs, and some require the redirect URL to be specified on each request. Check the service\u2019s documentation for the specifics. 5. Way2SMS app opens this URL in app/browser, if not already logged-in, then it shows a Login Form # 6. End-user had to enter their Google credentials # 7. Once the credentials are validated, it shows that the service provide (Way2SMS server) wants to access this. this and this, do you consent? # 8. If end-user accepts and agrees on the consent then in return Google authorization server redirects the end-user to the registered redirect_url by appending an \"authorization code\" to it # 1 GET https://service-provider/oauth2/callback?code=:code&state=random_state_string\" Parameters # Name Type In Description code string query Authorization code, an intermediary step of getting access token from OAuth Server state string query any state encoded as JWT if maintained by service provider which is sent back to it by authorization server after authorization Note: Authorization Code is single-use (temporary) code As the Authorization Code is being sent by the OAuth server through GET query param - its visible to everyone (End-user, Way2SMS client, Way2SMS server) Response # This depends how resource server has coded its GET /oauth2/callback API I would say, just show an JSON response with Authorization Code. 1 2 3 { \"code\" : \"<authorization code>\" } Note: The service provider (Way2SMS server) has already received the authorization code & state . Its just doing an adhoc work of showing the code as response - if it does not, then also its fine. 9. Now the service provider server (Way2SMS server) has the authorization code (also, if required, then Way2SMS App can provide the same through some helper API of Way2SMS server) # 10. Service provider server can now call authorization server to get an access code in exchange of authorization code, by supplying a few important details about the registered service provider server # 1 POST https://authorization-server/oauth2/token/ Parameter # Name Type In Description Content-Type string header Content-Type is responsible for telling the HTTP client or server what type of data is being sent grant_type string body The type of grant flow we are using in this case authorization_code client_id string body ID of the application that is registered with OAuth Server client_secret string body This is generated along with client id when registering the user with OAuth Server code string body The code which we got back from when we hit authorize url of our OAuth Server Note: The request along with code will be authorized by client_secret . It may check a) if the client_id is correct & registered or not; b) if so, then the code is generated by that client_id or not etc. client_secret adds an extra security layer. so that even if the code gets intercepted by someone - it could not be utilized without a valid client_secret The client_id & client_secret could be passed as HTTP Basic Authentication, in a few authorization servers, check respective docs. Response # 1 2 3 4 5 6 7 { \"access_token\" : \"rzKuDDYGSiNIJWuz6hJJc0n5NatPM5\" , \"expires_in\" : 10800 , \"token_type\" : \"Bearer\" , \"scope\" : \"read write groups introspection\" , \"refresh_token\" : \"1PnkGf9PQfG7urgXx1JCXYAtBpB3mf\" } Parameter # Name Type In Description access_token string body the access token itself, which we will use to query resource in resource server expires_in int body It is an integer representing the TTL of the access token (i.e. when the token will expire) token_type string body This will usually be the word \u201cBearer\u201d (to indicate a bearer token). Possession of the bearer token is considered authentication. scope string body the scope is a way to restrict access to specified areas e.g. profile, email, group etc refresh_token string body a refresh token that can be used to acquire a new access token when the original expires Note: As the service provider (Way2SMS server/resource provider) is calling this API in backend, and receiving access token as response - its totally hidden from anyone else (i.e. Way2SMS App, End-user, any Middle-man) 11. Now, the service provider (Way2SMS server) can access the Google contacts of the End-user by supplying the access token # 1 2 3 4 5 $ curl \\ --header \"Authorization: Bearer <access_token>\" \\ --header \"Content-Type: application/json\" \\ --request GET \\ --url http://authorization-server/v1/contacts \\ Response # 1 2 3 { \"contacts\": [...] } Extra Possible Flows # 1. Service Provider can Introspect an Access Token # 1 2 3 4 5 6 7 8 9 $ curl \\ --header \"Content-Type: application/x-www-form-urlencoded\" \\ --request POST \\ --url https://authorization-server/oauth2/introspect/ \\ --data '{ \"token\": \"<access_token>\", \"client_id\": \"<client_id>\", \"client_secret\": \"<client_secret>\" }' Parameters # Name Type In Description access_token string body the access token itself, which we will use to query resource in resource server client_id string body ID of the application that is registered with OAuth Server client_secret string body This is generated along with client id when registering the user with OAuth Server Response # 1 2 3 { \"active\": false } 2. Service Provider can Revoke an Access Token # 1 2 3 4 5 6 7 8 9 10 $ curl \\ --header \"Content-Type: application/x-www-form-urlencoded\" \\ --request POST \\ --url http://authorization-server/oauth2/revoke_token/ \\ --data '{ \"token\": \"<access_token>\", \"token_type_hint\": \"access_token\", \"client_id\": \"<client_id>\", \"client_secret\": \"<client_secret>\" }' Parameters # Name Type In Description access_token string body the access token itself, which we will use to query resource in resource server token_type_hint string body (optional) designating either \u2018access_token\u2019 or \u2018refresh_token\u2019 client_id string body ID of the application that is registered with OAuth Server client_secret string body This is generated along with client id when registering the user with OAuth Server Response # 1 2 3 HTTP 200 {} 3. Service Provider can Refresh an Access Token # 1 2 3 4 5 6 7 8 9 10 $ curl \\ --header \"Content-Type: application/x-www-form-urlencoded\" \\ --request POST \\ --url https://authorization-server/oauth2/token/ \\ --data '{ \"grant_type\": \"refresh_token\", \"refresh_token\": \"<refresh_token>\", \"client_id\": \"<client_id>\", \"client_secret\": \"<client_secret>\" }' Parameters # Name Type In Description refresh_token string body the access token itself, which we will use to query resource in resource server grant_type string body this mentions that get me new access_token in exchange of refresh_token - by calling the same token API client_id string body ID of the application that is registered with OAuth Server client_secret string body This is generated along with client id when registering the user with OAuth Server Response # 1 2 3 4 5 6 7 { \"access_token\" : \"<your_new_access_token>\" , \"token_type\" : \"Bearer\" , \"expires_in\" : 36000 , \"refresh_token\" : \"<your_new_refresh_token>\" , \"scope\" : \"read write\" } Ref # https://www.oauth.com/oauth2-servers/server-side-apps/authorization-code/ https://oauth.net/2/ PKCE # (Proof Key for Code Exchange, pronounced pixie) Why # Single-page apps (or browser-based apps) run entirely in the browser after loading the Javascript and HTML source code from a web page. Since the entire source is available to the browser, they cannot maintain the confidentiality of a client secret, so the secret is not used for these apps. The flow is exactly the same as the authorization code flow, but at the last step, the authorization code is exchanged for an access token without using the client secret. Because of this, Single-page apps (or browser based apps) must use an OAuth flow that does not require a client secret. Or, say does not require a predefined client secret. What # PKCE (RFC 7636) is an extension to the Authorization Code flow to prevent several attacks and to be able to securely perform the OAuth exchange from public clients. How # This extension describes a technique for public clients to mitigate the threat of having the authorization code intercepted. The technique involves the client first creating a secret, and then using that secret again when exchanging the authorization code for an access token. This way if the code is intercepted, it will not be useful since the token request relies on the initial secret. Note: Post 2019, its recommended for confidentical clients (backend/servers) using Authorization Code Flow to follow PKCE. Ref # https://www.oauth.com/oauth2-servers/pkce/ https://www.oauth.com/oauth2-servers/single-page-apps/ https://www.oauth.com/oauth2-servers/mobile-and-native-apps/ https://www.oauth.com/oauth2-servers/server-side-apps/authorization-code/ https://oauth.net/2/ Implicit Flow # Some services use the alternative Implicit Flow for single-page apps, rather than allow the app to use the Authorization Code flow with no secret. The Implicit Flow bypasses the code exchange step, and instead the access token is returned in the query string fragment to the client immediately. In order for a single-page app to use the Authorization Code flow, it must be able to make a POST request to the authorization server. This means if the authorization server is on a different domain, the server will need to support the appropriate CORS headers. If supporting CORS headers is not an option, then the service may use the Implicit Flow instead. Ref # https://www.oauth.com/oauth2-servers/single-page-apps/ OpenID Connect # (2014) combines the features of OpenID 2.0, OpenID Attribute Exchange 1.0, and OAuth 2.0 in a single protocol. It allows an application to use an authority... to verify the end user's identity, to fetch the end user's profile info, and to gain limited access to the end user's stuff. an Authorization as well as Authentication protocol OAuth, OpenID, OpenID Connect # OpenID is about verifying a person's identity (authentication). OAuth is about accessing a person's stuff (authorization). OpenID Connect does both. All three let a person give their username/password (or other credential) to a trusted authority instead of to a less trusted app. SAML # https://gravitational.com/blog/how-saml-authentication-works/ https://en.wikipedia.org/wiki/SAML_2.0#SP_redirect_request;_IdP_POST_response https://docs.oasis-open.org/security/saml/v2.0/saml-bindings-2.0-os.pdf http://docs.oasis-open.org/security/saml/Post2.0/sstc-saml-tech-overview-2.0-cd-02.html#5.1.2.SP-Initiated%20SSO:%20%20Redirect/POST%20Bindings|outline http://docs.oasis-open.org/security/saml/Post2.0/sstc-saml-tech-overview-2.0-cd-02.html#5.1.3.SP-Initiated%20SSO:%20%20POST/Artifact%20Bindings|outline http://saml.xml.org/saml-specifications http://saml.xml.org/wiki/sp-initiated-single-sign-on-postartifact-bindings http://saml.xml.org/wiki/idp-initiated-single-sign-on-post-binding http://docs.oasis-open.org/security/saml/v2.0/saml-core-2.0-os.pdf (3.4.1 Element ) for schema details https://blogs.oracle.com/dcarru/sp-vs-idp-initiated-sso Azure: AuthnRequest: https://docs.microsoft.com/en-us/azure/active-directory/develop/single-sign-on-saml-protocol#authnrequest Claims: https://docs.microsoft.com/en-us/azure/active-directory/develop/active-directory-saml-claims-customization SSO: https://docs.microsoft.com/en-us/azure/active-directory/manage-apps/what-is-single-sign-on#saml-sso Setup: https://docs.microsoft.com/en-us/azure/active-directory/manage-apps/configure-single-sign-on-non-gallery-applications Error AADSTS750054: https://docs.microsoft.com/en-us/azure/active-directory/manage-apps/application-sign-in-problem-federated-sso-gallery#saml-request-not-present-in-the-request Test SAML: https://docs.microsoft.com/en-us/azure/active-directory/azuread-dev/howto-v1-debug-saml-sso-issues Okta: SAML: https://developer.okta.com/docs/concepts/saml/ OneLogin https://www.samltool.com/generic_sso_req.php https://developers.onelogin.com/saml/python https://github.com/onelogin/python3-saml AuthnRequest: https://developers.onelogin.com/saml/examples/authnrequest SAMLRequest https://github.com/onelogin/python3-saml/blob/a11c34103e0aa75ee1930471c492b5f8f69962c0/src/onelogin/saml2/auth.py#L378-L379 Setup https://support.templafy.com/hc/en-us/articles/115005026225-How-to-setup-SSO-with-OneLogin-SAML- Test https://onelogin.service-now.com/support?id=kb_article&sys_id=83f71bc3db1e9f0024c780c74b961970 StackOverflow https://stackoverflow.com/questions/30388926/http-redirect-binding-saml-request Tools deflate + b64emcode: https://www.samltool.com/encode.php urlencode: https://www.urlencoder.org/","title":"Security"},{"location":"Engineering/Software-Engineering/Web/security/#security","text":"","title":"Security"},{"location":"Engineering/Software-Engineering/Web/security/#encryption","text":"Refer to /Computer-Science/se#encryption","title":"Encryption"},{"location":"Engineering/Software-Engineering/Web/security/#hashing","text":"aka Obfuscation puctuation matters while hashing Strings Algo Date Size (bits) Size (bytes) Length (hex digits) MD5 TODO 128 16 32 SHA-1 TODO 160 20 40 SHA-256 TODO 256 32 64","title":"Hashing"},{"location":"Engineering/Software-Engineering/Web/security/#md5","text":"","title":"MD5"},{"location":"Engineering/Software-Engineering/Web/security/#sha1","text":"","title":"SHA1"},{"location":"Engineering/Software-Engineering/Web/security/#sha256","text":"","title":"SHA256"},{"location":"Engineering/Software-Engineering/Web/security/#pbkdf2","text":"","title":"PBKDF2"},{"location":"Engineering/Software-Engineering/Web/security/#scrypt","text":"","title":"SCrypt"},{"location":"Engineering/Software-Engineering/Web/security/#bcrypt","text":"","title":"BCrypt"},{"location":"Engineering/Software-Engineering/Web/security/#argon2","text":"","title":"Argon2"},{"location":"Engineering/Software-Engineering/Web/security/#argon2i","text":"","title":"Argon2i"},{"location":"Engineering/Software-Engineering/Web/security/#argon2d","text":"","title":"Argon2d"},{"location":"Engineering/Software-Engineering/Web/security/#argon2id","text":"","title":"Argon2id"},{"location":"Engineering/Software-Engineering/Web/security/#ref","text":"https://medium.com/analytics-vidhya/password-hashing-pbkdf2-scrypt-bcrypt-and-argon2-e25aaf41598e","title":"Ref"},{"location":"Engineering/Software-Engineering/Web/security/#deidentifying-pii","text":"PII Anonymization/Deidentification Techniques","title":"Deidentifying PII"},{"location":"Engineering/Software-Engineering/Web/security/#jwt","text":"JSON Web Token a compact, URL-safe way/means of representing claims securely between two parties URL-safe 3 parts separated by dots open & industry standard RFC 7519 status: concensus of IETF format intended for space constrained environments such as: HTTP Authorization headers HTTP URI query parameters a string representing a set of claims as a JSON object that is encoded in a JWS or JWE, enabling the claims to be digitally signed or MACed and/or encrypted can have zero or more claims a signed JWT is an abstraction over either JWS or JWE 1 2 3 4 5 6 7 8 9 10 11 12 13 14 // Header: Algorithm and Token type { \"alg\": \"HS256\", \"typ\": \"JWT\" } // Payload: Data { \"name\": \"John\" \"iat\": 1615455676 } // Signature some secret string // optionally base64/base64url encoded","title":"JWT"},{"location":"Engineering/Software-Engineering/Web/security/#base-64-encoding","text":"binary to text encoding represents binary data (8-bit bytes) to ASCII string format using radix-64 representation designed to carry data (binary data) across channels that only reliably supports text content","title":"Base 64 Encoding"},{"location":"Engineering/Software-Engineering/Web/security/#base-64-url-encoding","text":"First encode the string to base64, this may result in encoded string having some signs like + , / , = which have different meaning in filesystem/urls. Thus need to encoded them further to make them URL-safe. Index Base64 Base64Url 0 A A 1 B B 2 C C 3 D D 4 E E 5 F F 6 G G 7 H H 8 I I 9 J J 10 K K 11 L L 12 M M 13 N N 14 O O 15 P P 16 Q Q 17 R R 18 S S 19 T T 20 U U 21 V V 22 W W 23 X X 24 Y Y 25 Z Z 26 a a 27 b b 28 c c 29 d d 30 e e 31 f f 32 g g 33 h h 34 i i 35 j j 36 k k 37 l l 38 m m 39 n n 40 o o 41 p p 42 q q 43 r r 44 s s 45 t t 46 u u 47 v v 48 w w 49 x x 50 y y 51 z z 52 0 0 53 1 1 54 2 2 55 3 3 56 4 4 57 5 5 58 6 6 59 7 7 60 8 8 61 9 9 62 + - 63 / _ = (optional)","title":"Base 64 URL Encoding"},{"location":"Engineering/Software-Engineering/Web/security/#parts-of-jwt","text":"JWT is combination of 3 base64 url encoded strings, parted by 2 dots","title":"Parts of JWT"},{"location":"Engineering/Software-Engineering/Web/security/#jose-header","text":"typ defines the media type of the complete JWT optional; recommended value: JWT defined by JWT specifications cty defines the content type; defines structural information recommended to use in case of nested JWTs only defined by JWT specifications alg defined by JWS specifications kid defined by JWS specifications","title":"JOSE Header"},{"location":"Engineering/Software-Engineering/Web/security/#secret","text":"","title":"Secret"},{"location":"Engineering/Software-Engineering/Web/security/#payloadbodyclaim","text":"","title":"Payload/Body/Claim"},{"location":"Engineering/Software-Engineering/Web/security/#claim","text":"a piece of information asserted about a subject asserted: say something clearly and firmly in determined and confident way key val pair key == always string val could be any JSON data type","title":"Claim"},{"location":"Engineering/Software-Engineering/Web/security/#jws","text":"JSON Web Signature","title":"JWS"},{"location":"Engineering/Software-Engineering/Web/security/#jwa","text":"JSON Web Algorithm RFC 7518 all applicable algorithms for signing are defined under this","title":"JWA"},{"location":"Engineering/Software-Engineering/Web/security/#jwe","text":"JSON Web Encryption","title":"JWE"},{"location":"Engineering/Software-Engineering/Web/security/#unsecured-jwt","text":"is a JWS object without any algorithm ( alg: none ) in its JOSE header is a JWS w/o signature","title":"Unsecured JWT"},{"location":"Engineering/Software-Engineering/Web/security/#ref_1","text":"https://medium.facilelogin.com/jwt-jws-and-jwe-for-not-so-dummies-b63310d201a3","title":"Ref"},{"location":"Engineering/Software-Engineering/Web/security/#token-based-authentication","text":"","title":"Token-Based Authentication"},{"location":"Engineering/Software-Engineering/Web/security/#openid-10","text":"(2006) lets an app ask an authority for proof that an end user owns an identity (a URL). an Authentication protocol deprecated example End user to app: I am Steve A. Smith. App to authority: Is this Steve A. Smith? The end user and authority speak for a moment. Authority to app: Yes, that is Steve A. Smith.","title":"OpenID 1.0"},{"location":"Engineering/Software-Engineering/Web/security/#openid-20","text":"(2007) does the same as OpenID 1.0, but adds a second identity format (XRI) and adds flexibility to how the end user specifies the identity and authority. an Authentication protocol deprecated","title":"OpenID 2.0"},{"location":"Engineering/Software-Engineering/Web/security/#openid-attribute-exchange-10","text":"(2007) extends OpenID 2.0 by letting an app fetch & store end user profile information with the authority - in addition to verifying the end user's identity. an Authentication protocol deprecated example: End user to app: I am Steve A. Smith. App to authority: Is this Steve A. Smith? Oh, and if it is, also fetch me his email address and phone number. The end user and authority speak for a moment. Authority to app: Yes, that is Steve A. Smith. His email is steve@domain.com and phone number is 123-456-7890.","title":"OpenID Attribute Exchange 1.0"},{"location":"Engineering/Software-Engineering/Web/security/#oauth-10","text":"(2010) lets an end user grant an app limited access to resources on a third-party server that an authority owns. example: App to end user: We'd like to access your pictures on some other server. The end user and authority speak for a moment. Authority to app: Here is an access token. App to third-party server: Here is the access token that proves I am allowed to access pictures for an end user. an Authorization protocol deprecated","title":"OAuth 1.0"},{"location":"Engineering/Software-Engineering/Web/security/#oauth-20","text":"(2012) does the same thing as OAuth 1.0 but with a completely new protocol an Authorization framework","title":"OAuth 2.0"},{"location":"Engineering/Software-Engineering/Web/security/#authorization-grant-types-flow","text":"There are multiple ways in OAuth2.0 itself to achieve authorization, which are already specified in the RFC based on different client type/scenarios. Authorization Code Flow PKCE Client Credential Device Code Refresh Token Legacy: Implicit Flow Legacy: Password Grant","title":"Authorization Grant Types (Flow)"},{"location":"Engineering/Software-Engineering/Web/security/#authorization-code-flow","text":"This is a suitable flow for following kind of case.","title":"Authorization Code Flow"},{"location":"Engineering/Software-Engineering/Web/security/#participants","text":"End User (2nd party) Client (Mobile/SPA App - 3rd party) Service Provider (Resource server - 3rd party, Authorization server knows this as Registered Client as well) Authorization server (1st party) Example: Toran wants to use Way2SMS App (and internally their server) to send SMS to all the contacts stored in his Google account. Here, End user = Toran Client = Way2SMS App Service Provider = Way2SMS server Authorization Server = Google Where, the service provider is a 3rd party server. If there is no 3rd party server, and only Mobile/SPA clients, then PKCE is a better/secure flow. Even, now its recommended to use PKCE for 3rd party resource provider server case as well.","title":"Participants"},{"location":"Engineering/Software-Engineering/Web/security/#prerequisites","text":"","title":"Prerequisites"},{"location":"Engineering/Software-Engineering/Web/security/#setup-of-an-authorization-server","text":"Authorization Base 1 https://authorization-server/oauth2/authorize Token URL 1 https://authorization-server/oauth2/token/ Redirect URL 1 https://authorization-server/oauth2/callback Authorization URL 1 https://authorization-server/oauth2/authorize/?response_type=code&state=random_state_string&client_id=:client_id","title":"Setup of an Authorization Server"},{"location":"Engineering/Software-Engineering/Web/security/#register-an-resource-provider-server-as-client-in-authorization-server","text":"Input: 1 2 3 client_type: confidential/public (public for Mobile App, Web App; confidential for backends/servers) Authorization Grant Type: authorization-code Redirect URL: https://resource-provider/oauth2/callback Output: 1 2 client_id: BOqLOb2uaTn4ppi5nlj2ErTWpr9fKt5YQrwpDvfs client_secret: bamZTX1gknB99XZvKWYucUPVdY5Aum5k8LDoZzUWz0VhwaSZpFhCyUrrxshaAJKtaZ5bTJXimBCLgjs0yPZxvow9NBrqEXG6qeK0VFy2incKLzZdw97Q4jNxAafBq3c8 Keep note of this.","title":"Register an Resource Provider Server as Client in Authorization Server"},{"location":"Engineering/Software-Engineering/Web/security/#flow","text":"","title":"Flow"},{"location":"Engineering/Software-Engineering/Web/security/#1-end-user-opens-way2sms-app","text":"","title":"1. End-user opens Way2SMS App"},{"location":"Engineering/Software-Engineering/Web/security/#2-writes-a-sms-and-clicks-on-send-to-my-google-contacts","text":"","title":"2. Writes a SMS and clicks on Send to My Google Contacts"},{"location":"Engineering/Software-Engineering/Web/security/#3-as-end-user-has-not-authorized-the-way2sms-server-as-well-as-app-to-access-end-users-resource-stored-at-google-server","text":"","title":"3. As End-user has not authorized the Way2SMS server (as well as app) to access End-user's resource stored at Google server"},{"location":"Engineering/Software-Engineering/Web/security/#4-way2sms-server-provides-the-way2sms-app-a-link-to-redirect-the-end-user-there-to-complete-the-authorization-with-google-oauth2-server","text":"The shared URL (called Authorization API) by Way2SMS server has a few query params appended by the Way2SMS server 1 GET https://authorization-server/oauth2/authorize?response_type=code&state=random_state_string&client_id=:client_id&redirect_url=<https://service-provider/oauth2/callback>&scope=profile%20email","title":"4. Way2SMS server provides the Way2SMS app a link to redirect the End-user there to complete the authorization with Google OAuth2 server"},{"location":"Engineering/Software-Engineering/Web/security/#parameters","text":"Name Type In Description client_id string query ID of the service provider server (Way2SMS server), that is registered with Google OAuth Serverredirect_url response_type string query indicates the expectation in return i.e. an authorization code state string query any state encoded as JWT if maintained by service provider which should be send back to it by authorization server after authorization scope string query (optional)space separated scope to request additional level of access - depends on particular service Note: Here state may serve multiple purposes: Serves as a CSRF protection mechanism if it contains a random value per request. When the user is redirected back to your app, double check that the state value matches what you set it to originally. This may be used to indicate what action in the app to perform after authorization is complete, for example, indicating which of your app\u2019s pages to redirect to after authorization. This gives your app a chance to persist data between the user being directed to the authorization server and back again, such as using the state parameter as a session key. Some services support registering multiple redirect URLs, and some require the redirect URL to be specified on each request. Check the service\u2019s documentation for the specifics.","title":"Parameters"},{"location":"Engineering/Software-Engineering/Web/security/#5-way2sms-app-opens-this-url-in-appbrowser-if-not-already-logged-in-then-it-shows-a-login-form","text":"","title":"5. Way2SMS app opens this URL in app/browser, if not already logged-in, then it shows a Login Form"},{"location":"Engineering/Software-Engineering/Web/security/#6-end-user-had-to-enter-their-google-credentials","text":"","title":"6. End-user had to enter their Google credentials"},{"location":"Engineering/Software-Engineering/Web/security/#7-once-the-credentials-are-validated-it-shows-that-the-service-provide-way2sms-server-wants-to-access-this-this-and-this-do-you-consent","text":"","title":"7. Once the credentials are validated, it shows that the service provide (Way2SMS server) wants to access this. this and this, do you consent?"},{"location":"Engineering/Software-Engineering/Web/security/#8-if-end-user-accepts-and-agrees-on-the-consent-then-in-return-google-authorization-server-redirects-the-end-user-to-the-registered-redirect_url-by-appending-an-authorization-code-to-it","text":"1 GET https://service-provider/oauth2/callback?code=:code&state=random_state_string\"","title":"8. If end-user accepts and agrees on the consent then in return Google authorization server redirects the end-user to the registered redirect_url by appending an \"authorization code\" to it"},{"location":"Engineering/Software-Engineering/Web/security/#parameters_1","text":"Name Type In Description code string query Authorization code, an intermediary step of getting access token from OAuth Server state string query any state encoded as JWT if maintained by service provider which is sent back to it by authorization server after authorization Note: Authorization Code is single-use (temporary) code As the Authorization Code is being sent by the OAuth server through GET query param - its visible to everyone (End-user, Way2SMS client, Way2SMS server)","title":"Parameters"},{"location":"Engineering/Software-Engineering/Web/security/#response","text":"This depends how resource server has coded its GET /oauth2/callback API I would say, just show an JSON response with Authorization Code. 1 2 3 { \"code\" : \"<authorization code>\" } Note: The service provider (Way2SMS server) has already received the authorization code & state . Its just doing an adhoc work of showing the code as response - if it does not, then also its fine.","title":"Response"},{"location":"Engineering/Software-Engineering/Web/security/#9-now-the-service-provider-server-way2sms-server-has-the-authorization-code-also-if-required-then-way2sms-app-can-provide-the-same-through-some-helper-api-of-way2sms-server","text":"","title":"9. Now the service provider server (Way2SMS server) has the authorization code (also, if required, then Way2SMS App can provide the same through some helper API of Way2SMS server)"},{"location":"Engineering/Software-Engineering/Web/security/#10-service-provider-server-can-now-call-authorization-server-to-get-an-access-code-in-exchange-of-authorization-code-by-supplying-a-few-important-details-about-the-registered-service-provider-server","text":"1 POST https://authorization-server/oauth2/token/","title":"10. Service provider server can now call authorization server to get an access code in exchange of authorization code, by supplying a few important details about the registered service provider server"},{"location":"Engineering/Software-Engineering/Web/security/#parameter","text":"Name Type In Description Content-Type string header Content-Type is responsible for telling the HTTP client or server what type of data is being sent grant_type string body The type of grant flow we are using in this case authorization_code client_id string body ID of the application that is registered with OAuth Server client_secret string body This is generated along with client id when registering the user with OAuth Server code string body The code which we got back from when we hit authorize url of our OAuth Server Note: The request along with code will be authorized by client_secret . It may check a) if the client_id is correct & registered or not; b) if so, then the code is generated by that client_id or not etc. client_secret adds an extra security layer. so that even if the code gets intercepted by someone - it could not be utilized without a valid client_secret The client_id & client_secret could be passed as HTTP Basic Authentication, in a few authorization servers, check respective docs.","title":"Parameter"},{"location":"Engineering/Software-Engineering/Web/security/#response_1","text":"1 2 3 4 5 6 7 { \"access_token\" : \"rzKuDDYGSiNIJWuz6hJJc0n5NatPM5\" , \"expires_in\" : 10800 , \"token_type\" : \"Bearer\" , \"scope\" : \"read write groups introspection\" , \"refresh_token\" : \"1PnkGf9PQfG7urgXx1JCXYAtBpB3mf\" }","title":"Response"},{"location":"Engineering/Software-Engineering/Web/security/#parameter_1","text":"Name Type In Description access_token string body the access token itself, which we will use to query resource in resource server expires_in int body It is an integer representing the TTL of the access token (i.e. when the token will expire) token_type string body This will usually be the word \u201cBearer\u201d (to indicate a bearer token). Possession of the bearer token is considered authentication. scope string body the scope is a way to restrict access to specified areas e.g. profile, email, group etc refresh_token string body a refresh token that can be used to acquire a new access token when the original expires Note: As the service provider (Way2SMS server/resource provider) is calling this API in backend, and receiving access token as response - its totally hidden from anyone else (i.e. Way2SMS App, End-user, any Middle-man)","title":"Parameter"},{"location":"Engineering/Software-Engineering/Web/security/#11-now-the-service-provider-way2sms-server-can-access-the-google-contacts-of-the-end-user-by-supplying-the-access-token","text":"1 2 3 4 5 $ curl \\ --header \"Authorization: Bearer <access_token>\" \\ --header \"Content-Type: application/json\" \\ --request GET \\ --url http://authorization-server/v1/contacts \\","title":"11. Now, the service provider (Way2SMS server) can access the Google contacts of the End-user by supplying the access token"},{"location":"Engineering/Software-Engineering/Web/security/#response_2","text":"1 2 3 { \"contacts\": [...] }","title":"Response"},{"location":"Engineering/Software-Engineering/Web/security/#extra-possible-flows","text":"","title":"Extra Possible Flows"},{"location":"Engineering/Software-Engineering/Web/security/#1-service-provider-can-introspect-an-access-token","text":"1 2 3 4 5 6 7 8 9 $ curl \\ --header \"Content-Type: application/x-www-form-urlencoded\" \\ --request POST \\ --url https://authorization-server/oauth2/introspect/ \\ --data '{ \"token\": \"<access_token>\", \"client_id\": \"<client_id>\", \"client_secret\": \"<client_secret>\" }'","title":"1. Service Provider can Introspect an Access Token"},{"location":"Engineering/Software-Engineering/Web/security/#parameters_2","text":"Name Type In Description access_token string body the access token itself, which we will use to query resource in resource server client_id string body ID of the application that is registered with OAuth Server client_secret string body This is generated along with client id when registering the user with OAuth Server","title":"Parameters"},{"location":"Engineering/Software-Engineering/Web/security/#response_3","text":"1 2 3 { \"active\": false }","title":"Response"},{"location":"Engineering/Software-Engineering/Web/security/#2-service-provider-can-revoke-an-access-token","text":"1 2 3 4 5 6 7 8 9 10 $ curl \\ --header \"Content-Type: application/x-www-form-urlencoded\" \\ --request POST \\ --url http://authorization-server/oauth2/revoke_token/ \\ --data '{ \"token\": \"<access_token>\", \"token_type_hint\": \"access_token\", \"client_id\": \"<client_id>\", \"client_secret\": \"<client_secret>\" }'","title":"2. Service Provider can Revoke an Access Token"},{"location":"Engineering/Software-Engineering/Web/security/#parameters_3","text":"Name Type In Description access_token string body the access token itself, which we will use to query resource in resource server token_type_hint string body (optional) designating either \u2018access_token\u2019 or \u2018refresh_token\u2019 client_id string body ID of the application that is registered with OAuth Server client_secret string body This is generated along with client id when registering the user with OAuth Server","title":"Parameters"},{"location":"Engineering/Software-Engineering/Web/security/#response_4","text":"1 2 3 HTTP 200 {}","title":"Response"},{"location":"Engineering/Software-Engineering/Web/security/#3-service-provider-can-refresh-an-access-token","text":"1 2 3 4 5 6 7 8 9 10 $ curl \\ --header \"Content-Type: application/x-www-form-urlencoded\" \\ --request POST \\ --url https://authorization-server/oauth2/token/ \\ --data '{ \"grant_type\": \"refresh_token\", \"refresh_token\": \"<refresh_token>\", \"client_id\": \"<client_id>\", \"client_secret\": \"<client_secret>\" }'","title":"3. Service Provider can Refresh an Access Token"},{"location":"Engineering/Software-Engineering/Web/security/#parameters_4","text":"Name Type In Description refresh_token string body the access token itself, which we will use to query resource in resource server grant_type string body this mentions that get me new access_token in exchange of refresh_token - by calling the same token API client_id string body ID of the application that is registered with OAuth Server client_secret string body This is generated along with client id when registering the user with OAuth Server","title":"Parameters"},{"location":"Engineering/Software-Engineering/Web/security/#response_5","text":"1 2 3 4 5 6 7 { \"access_token\" : \"<your_new_access_token>\" , \"token_type\" : \"Bearer\" , \"expires_in\" : 36000 , \"refresh_token\" : \"<your_new_refresh_token>\" , \"scope\" : \"read write\" }","title":"Response"},{"location":"Engineering/Software-Engineering/Web/security/#ref_2","text":"https://www.oauth.com/oauth2-servers/server-side-apps/authorization-code/ https://oauth.net/2/","title":"Ref"},{"location":"Engineering/Software-Engineering/Web/security/#pkce","text":"(Proof Key for Code Exchange, pronounced pixie)","title":"PKCE"},{"location":"Engineering/Software-Engineering/Web/security/#why","text":"Single-page apps (or browser-based apps) run entirely in the browser after loading the Javascript and HTML source code from a web page. Since the entire source is available to the browser, they cannot maintain the confidentiality of a client secret, so the secret is not used for these apps. The flow is exactly the same as the authorization code flow, but at the last step, the authorization code is exchanged for an access token without using the client secret. Because of this, Single-page apps (or browser based apps) must use an OAuth flow that does not require a client secret. Or, say does not require a predefined client secret.","title":"Why"},{"location":"Engineering/Software-Engineering/Web/security/#what","text":"PKCE (RFC 7636) is an extension to the Authorization Code flow to prevent several attacks and to be able to securely perform the OAuth exchange from public clients.","title":"What"},{"location":"Engineering/Software-Engineering/Web/security/#how","text":"This extension describes a technique for public clients to mitigate the threat of having the authorization code intercepted. The technique involves the client first creating a secret, and then using that secret again when exchanging the authorization code for an access token. This way if the code is intercepted, it will not be useful since the token request relies on the initial secret. Note: Post 2019, its recommended for confidentical clients (backend/servers) using Authorization Code Flow to follow PKCE.","title":"How"},{"location":"Engineering/Software-Engineering/Web/security/#ref_3","text":"https://www.oauth.com/oauth2-servers/pkce/ https://www.oauth.com/oauth2-servers/single-page-apps/ https://www.oauth.com/oauth2-servers/mobile-and-native-apps/ https://www.oauth.com/oauth2-servers/server-side-apps/authorization-code/ https://oauth.net/2/","title":"Ref"},{"location":"Engineering/Software-Engineering/Web/security/#implicit-flow","text":"Some services use the alternative Implicit Flow for single-page apps, rather than allow the app to use the Authorization Code flow with no secret. The Implicit Flow bypasses the code exchange step, and instead the access token is returned in the query string fragment to the client immediately. In order for a single-page app to use the Authorization Code flow, it must be able to make a POST request to the authorization server. This means if the authorization server is on a different domain, the server will need to support the appropriate CORS headers. If supporting CORS headers is not an option, then the service may use the Implicit Flow instead.","title":"Implicit Flow"},{"location":"Engineering/Software-Engineering/Web/security/#ref_4","text":"https://www.oauth.com/oauth2-servers/single-page-apps/","title":"Ref"},{"location":"Engineering/Software-Engineering/Web/security/#openid-connect","text":"(2014) combines the features of OpenID 2.0, OpenID Attribute Exchange 1.0, and OAuth 2.0 in a single protocol. It allows an application to use an authority... to verify the end user's identity, to fetch the end user's profile info, and to gain limited access to the end user's stuff. an Authorization as well as Authentication protocol","title":"OpenID Connect"},{"location":"Engineering/Software-Engineering/Web/security/#oauth-openid-openid-connect","text":"OpenID is about verifying a person's identity (authentication). OAuth is about accessing a person's stuff (authorization). OpenID Connect does both. All three let a person give their username/password (or other credential) to a trusted authority instead of to a less trusted app.","title":"OAuth, OpenID, OpenID Connect"},{"location":"Engineering/Software-Engineering/Web/security/#saml","text":"https://gravitational.com/blog/how-saml-authentication-works/ https://en.wikipedia.org/wiki/SAML_2.0#SP_redirect_request;_IdP_POST_response https://docs.oasis-open.org/security/saml/v2.0/saml-bindings-2.0-os.pdf http://docs.oasis-open.org/security/saml/Post2.0/sstc-saml-tech-overview-2.0-cd-02.html#5.1.2.SP-Initiated%20SSO:%20%20Redirect/POST%20Bindings|outline http://docs.oasis-open.org/security/saml/Post2.0/sstc-saml-tech-overview-2.0-cd-02.html#5.1.3.SP-Initiated%20SSO:%20%20POST/Artifact%20Bindings|outline http://saml.xml.org/saml-specifications http://saml.xml.org/wiki/sp-initiated-single-sign-on-postartifact-bindings http://saml.xml.org/wiki/idp-initiated-single-sign-on-post-binding http://docs.oasis-open.org/security/saml/v2.0/saml-core-2.0-os.pdf (3.4.1 Element ) for schema details https://blogs.oracle.com/dcarru/sp-vs-idp-initiated-sso Azure: AuthnRequest: https://docs.microsoft.com/en-us/azure/active-directory/develop/single-sign-on-saml-protocol#authnrequest Claims: https://docs.microsoft.com/en-us/azure/active-directory/develop/active-directory-saml-claims-customization SSO: https://docs.microsoft.com/en-us/azure/active-directory/manage-apps/what-is-single-sign-on#saml-sso Setup: https://docs.microsoft.com/en-us/azure/active-directory/manage-apps/configure-single-sign-on-non-gallery-applications Error AADSTS750054: https://docs.microsoft.com/en-us/azure/active-directory/manage-apps/application-sign-in-problem-federated-sso-gallery#saml-request-not-present-in-the-request Test SAML: https://docs.microsoft.com/en-us/azure/active-directory/azuread-dev/howto-v1-debug-saml-sso-issues Okta: SAML: https://developer.okta.com/docs/concepts/saml/ OneLogin https://www.samltool.com/generic_sso_req.php https://developers.onelogin.com/saml/python https://github.com/onelogin/python3-saml AuthnRequest: https://developers.onelogin.com/saml/examples/authnrequest SAMLRequest https://github.com/onelogin/python3-saml/blob/a11c34103e0aa75ee1930471c492b5f8f69962c0/src/onelogin/saml2/auth.py#L378-L379 Setup https://support.templafy.com/hc/en-us/articles/115005026225-How-to-setup-SSO-with-OneLogin-SAML- Test https://onelogin.service-now.com/support?id=kb_article&sys_id=83f71bc3db1e9f0024c780c74b961970 StackOverflow https://stackoverflow.com/questions/30388926/http-redirect-binding-saml-request Tools deflate + b64emcode: https://www.samltool.com/encode.php urlencode: https://www.urlencoder.org/","title":"SAML"},{"location":"Engineering/Software-Engineering/Web/API/graphql/","text":"GraphQL # GraphQL Intro REST vs GraphQL Terminologies Schema Field Explore Forming Calls With GraphQL Authenticating with GraphQL The GraphQL endpoint Communicating with GraphQL About query and mutation operations Working with variables Query Mutation Intro # An architectural and conceptual improvement over REST. GraphQL is a data query language developed internally by Facebook in 2012 before being publicly released in 2015. It provides an alternative to REST and ad-hoc webservice architectures. REST vs GraphQL # Lets say, we are consuming Facebook API A typical nested field data distribution: 1 2 3 4 5 6 7 8 9 10 FACEBOOK | |------ USERS | |------- TORAN | |----- INFO | | | | | | | |----- ASSETS | | - GraphQL can retrieve various fields from different endpoint routes in fewer or single query - whereas REST requires multiple calls for each fields - e.g. - REST 1 Terminologies # Schema # Field # # Explore # Forming Calls With GraphQL # Authenticating with GraphQL # The GraphQL endpoint # Communicating with GraphQL # About query and mutation operations # Working with variables # Query # Mutation #","title":"GraphQL"},{"location":"Engineering/Software-Engineering/Web/API/graphql/#graphql","text":"GraphQL Intro REST vs GraphQL Terminologies Schema Field Explore Forming Calls With GraphQL Authenticating with GraphQL The GraphQL endpoint Communicating with GraphQL About query and mutation operations Working with variables Query Mutation","title":"GraphQL"},{"location":"Engineering/Software-Engineering/Web/API/graphql/#intro","text":"An architectural and conceptual improvement over REST. GraphQL is a data query language developed internally by Facebook in 2012 before being publicly released in 2015. It provides an alternative to REST and ad-hoc webservice architectures.","title":"Intro"},{"location":"Engineering/Software-Engineering/Web/API/graphql/#rest-vs-graphql","text":"Lets say, we are consuming Facebook API A typical nested field data distribution: 1 2 3 4 5 6 7 8 9 10 FACEBOOK | |------ USERS | |------- TORAN | |----- INFO | | | | | | | |----- ASSETS | | - GraphQL can retrieve various fields from different endpoint routes in fewer or single query - whereas REST requires multiple calls for each fields - e.g. - REST 1","title":"REST vs GraphQL"},{"location":"Engineering/Software-Engineering/Web/API/graphql/#terminologies","text":"","title":"Terminologies"},{"location":"Engineering/Software-Engineering/Web/API/graphql/#schema","text":"","title":"Schema"},{"location":"Engineering/Software-Engineering/Web/API/graphql/#field","text":"","title":"Field"},{"location":"Engineering/Software-Engineering/Web/API/graphql/#_1","text":"","title":""},{"location":"Engineering/Software-Engineering/Web/API/graphql/#explore","text":"","title":"Explore"},{"location":"Engineering/Software-Engineering/Web/API/graphql/#forming-calls-with-graphql","text":"","title":"Forming Calls With GraphQL"},{"location":"Engineering/Software-Engineering/Web/API/graphql/#authenticating-with-graphql","text":"","title":"Authenticating with GraphQL"},{"location":"Engineering/Software-Engineering/Web/API/graphql/#the-graphql-endpoint","text":"","title":"The GraphQL endpoint"},{"location":"Engineering/Software-Engineering/Web/API/graphql/#communicating-with-graphql","text":"","title":"Communicating with GraphQL"},{"location":"Engineering/Software-Engineering/Web/API/graphql/#about-query-and-mutation-operations","text":"","title":"About query and mutation operations"},{"location":"Engineering/Software-Engineering/Web/API/graphql/#working-with-variables","text":"","title":"Working with variables"},{"location":"Engineering/Software-Engineering/Web/API/graphql/#query","text":"","title":"Query"},{"location":"Engineering/Software-Engineering/Web/API/graphql/#mutation","text":"","title":"Mutation"},{"location":"Engineering/Software-Engineering/Web/API/grpc/","text":"gRPC # gRPC Doc Environment Variables Channel Arguments Auth TLS Language-wise Python Channel Arguments FAQs Doc # https://github.com/grpc/grpc/tree/master/doc Environment Variables # https://github.com/grpc/grpc/blob/master/doc/environment_variables.md Channel Arguments # https://github.com/grpc/grpc/blob/v1.46.x/include/grpc/impl/codegen/grpc_types.h https://github.com/grpc/grpc/blob/master/doc/keepalive.md Auth # https://grpc.io/docs/guides/auth/ TLS # https://itnext.io/practical-guide-to-securing-grpc-connections-with-go-and-tls-part-1-f63058e9d6d1 https://dev.to/techschoolguru/how-to-secure-grpc-connection-with-ssl-tls-in-go-4ph?signin=true Language-wise # Python # https://grpc.github.io/grpc/python/grpc.html Channel Arguments # https://grpc.github.io/grpc/python/glossary.html#term-channel_arguments FAQs # https://blog.jeffli.me/blog/2017/08/02/keep-python-grpc-client-connection-truly-alive/ https://stackoverflow.com/questions/23238319/websockets-ping-pong-why-not-tcp-keepalive","title":"gRPC"},{"location":"Engineering/Software-Engineering/Web/API/grpc/#grpc","text":"gRPC Doc Environment Variables Channel Arguments Auth TLS Language-wise Python Channel Arguments FAQs","title":"gRPC"},{"location":"Engineering/Software-Engineering/Web/API/grpc/#doc","text":"https://github.com/grpc/grpc/tree/master/doc","title":"Doc"},{"location":"Engineering/Software-Engineering/Web/API/grpc/#environment-variables","text":"https://github.com/grpc/grpc/blob/master/doc/environment_variables.md","title":"Environment Variables"},{"location":"Engineering/Software-Engineering/Web/API/grpc/#channel-arguments","text":"https://github.com/grpc/grpc/blob/v1.46.x/include/grpc/impl/codegen/grpc_types.h https://github.com/grpc/grpc/blob/master/doc/keepalive.md","title":"Channel Arguments"},{"location":"Engineering/Software-Engineering/Web/API/grpc/#auth","text":"https://grpc.io/docs/guides/auth/","title":"Auth"},{"location":"Engineering/Software-Engineering/Web/API/grpc/#tls","text":"https://itnext.io/practical-guide-to-securing-grpc-connections-with-go-and-tls-part-1-f63058e9d6d1 https://dev.to/techschoolguru/how-to-secure-grpc-connection-with-ssl-tls-in-go-4ph?signin=true","title":"TLS"},{"location":"Engineering/Software-Engineering/Web/API/grpc/#language-wise","text":"","title":"Language-wise"},{"location":"Engineering/Software-Engineering/Web/API/grpc/#python","text":"https://grpc.github.io/grpc/python/grpc.html","title":"Python"},{"location":"Engineering/Software-Engineering/Web/API/grpc/#channel-arguments_1","text":"https://grpc.github.io/grpc/python/glossary.html#term-channel_arguments","title":"Channel Arguments"},{"location":"Engineering/Software-Engineering/Web/API/grpc/#faqs","text":"https://blog.jeffli.me/blog/2017/08/02/keep-python-grpc-client-connection-truly-alive/ https://stackoverflow.com/questions/23238319/websockets-ping-pong-why-not-tcp-keepalive","title":"FAQs"},{"location":"Engineering/Software-Engineering/Web/API/rest/","text":"REST # REST REST API HTTP Request Methods (HTTP verbs) GET POST PUT DELETE PATCH HEAD OPTIONS CONNECT TRACE HTTP Response Codes REST API # Representation State Transfer An architectural pattern for creating an API that uses HTTP as its communication method for designing network based application Resource: If we have endpoint/URI(URL or URN), lets say https://127.0.0.1/coders then we have resource. Here coders is a resource. Its nothing new, we already used to make URLs that way Representation: When a client makes a GET request to coders/toran/ client gets following JSON response 1 2 3 4 { \"nickname\" : \"toran\" , \"powerLevel\" : 5 } so, this is representation in the form of JSON having metadata representation also could be XML, HTML the same applies when a client sends a request which contains a 'coder' data, its sends representation Representational State: like browsing the web a HTML page is a representation of a resource at current state (or current data) of that resource. When we submit a form, we just send a representation back to the server Representational State Transfer a client and server exchange representations of a resource, which reflects its current state or desired state. So, REST is a way for two machines to transfer the state of a resource via representations. Note: There is a very common confusion in terminology. \"Resource\" - In URI \"app/questions/1\" here \"1\" is also know as resource and at the same time \"questions\" is also know as resource. - Some people around the globe also use \"1\" as record/entity & \"questions\" as resource. - We could generalize \"1\" as \"resource object\" and \"questions\" as \"resource class\" HTTP Request Methods (HTTP verbs) # HTTP defines a set of request methods to indicate the desired action to be performed for a given resource GET # 1 2 GET /questions/:question_id HTTP/1.1 Host: www.example.com The GET method requests a representation of a specified resource. GET request should only used for data retrieval. (But can also be used for submit/posting/sending data) a resource is mentioned in URL. POST # 1 2 POST /questions/<existing_question> HTTP/1.1 Host: www.example.com/ Used to modify/update an existing entity of a specified resource. Do mention the object/entity of the resource in the URL to update we should use PUT and PATCH for update PUT would create a new unwanted resource when the resouce doesn't exists while updating that particular resource PATCH is perfect for partial updation But there is no restrictions using POST for updates 1 2 POST /questions/ HTTP/1.1 Host: www.example.com Used to submit a new entity/data to a specified resource. Do not mention the object/entity of the resource in the URL while creating it. Otherwise it will give \"Resource Not Found\" error. Use this if you want server to let the name, the URL object while creating a new one Causes change in state of the resource Two simultaneous POST requests works fine (lets say, 1st request is updating some part & 2nd request is updating other part of an object) The UPDATE performed by the POST method might not result in a resource that can be identified by a URI. PUT # Used to create a new resource entity or overrite (Completely replaces) the existing one. For a new resource entity: 1 2 PUT /questions/:new_question_id HTTP/1.1 Host: www.example.com To overrite an existing one: 1 2 PUT /questions/:existing_question_id HTTP/1.1 Host: www.example.com Usefull when you know the name of the entity/object Use this if you want to name the URL object while creating a new one PUT is idempotent, so if you PUT the same object twice, it has no effect (will overrite) Can create or update an object with the same URL DELETE # 1 2 DELETE /api/resource/:id HTTP/1.1 Host: www.example.com/ Deletes/destroyes a specified resource object/entity/record PATCH # 1 2 3 4 5 6 7 PATCH /users/1 HTTP/1.1 Host: www.example.com Content-Type: application/example If-Match: \"c0b42b66e\" Content-Length: 120 {changes} where [changes] could be JSON/XML like {email:'toran.sahu@yahoo.com'} Used to make partial modification to an existing resource object/entity. Works differently than POST & PUT for update Need to have URL object known HEAD # 1 2 3 4 5 6 7 $ curl http://0.0.0.0:8010/ --head HTTP/1.0 200 OK Server: SimpleHTTP/0.6 Python/3.7.3 Date: Sat, 23 Aug 2020 17:14:52 GMT Content-type: application/json; charset=utf-8 Content-Length: 13306 Ask for the response same as GET but without response/message body. i.e. returns Header & Status Code, except Body Usages? to fetch headers, or to verify the API by status code to get the content size of a media file - without fetcching it actually request has body? No can send query params only successful response has body? No safe? Yes idempotent? Yes cacheable? Yes allowed in HTML forms? No OPTIONS # 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 $ curl /api/resource -X OPTIONS -i # responds with header; and empty body HTTP/2 200 OK date: Sat, 07 Aug 2021 16:48:46 GMT content-length: 0 vary: Origin access-control-allow-origin: https://api.toran.com access-control-allow-credentials: true access-control-allow-headers: Accept,Accept-Version,Content-Length,Content-MD5,Content-Type,Date,X-Auth-Token,x-requested-with,x-csrftoken,x-page-total,Accept-Language,Connection,x-page-limit,Accept-Encoding,x-page-page,cache-control access-control-allow-methods: GET,HEAD,POST,PATCH,PUT,DELETE access-control-max-age: 3600 content-security-policy: default-src 'self'; object-src 'none'; style-src 'self' 'unsafe-inline'; script-src 'self' 'unsafe-inline'; strict-transport-security: max-age=31536000; includeSubDomains; preload referrer-policy: strict-origin-when-cross-origin permissions-policy: geolocation=() server: kong/2.1.4 # (optional) Body, btw DRF sends body by default {\"name\":\"Api Resource\",\"description\":\"\",\"renders\":[\"application/json\",\"text/html\"],\"parses\":[\"application/json\",\"application/x-www-form-urlencoded\",\"multipart/form-data\"]} Used to describe the communication option for the target resource tell the capabilities of the API server, like list of supported: HTTP methods by the server header keys Content-Type Accepts request has body? No can send query params only successful response has body? Yes (optional) safe? Yes idempotent? Yes cacheable? No allowed in HTML forms? No usage? to communicate with server during CORS case (preflight) tell the server that I will do this, do you support those? 1 2 3 4 OPTIONS /resources/post-here/ HTTP/1.1 Host: bar.example Access-Control-Request-Method: POST Access-Control-Request-Headers: X-PINGOTHER, Content-Type meaning, I'll do POST and with headers X-PINGOTHER and Content-Type . do you supposet those? Response: 1 2 3 HTTP/1.1 204 No Content Access-Control-Allow-Methods: POST, GET, OPTIONS Access-Control-Allow-Headers: X-PINGOTHER, Content-Type CONNECT # starts two-way communications with the requested resource it can be used to open a tunnel ref https://developer.mozilla.org/en-US/docs/Web/HTTP/Methods/connect https://reqbin.com/Article/HttpConnect TRACE # Performs message loop-back test along the path to the target resource HTTP Response Codes # 1XX informational 101 switching protocol 2XX Success 200 (Ok) 201 (Created) 204 (No Content) 3XX Redirection 304 4XX Client Error 404 5XX Server Error 501 (Not Implemented)","title":"REST"},{"location":"Engineering/Software-Engineering/Web/API/rest/#rest","text":"REST REST API HTTP Request Methods (HTTP verbs) GET POST PUT DELETE PATCH HEAD OPTIONS CONNECT TRACE HTTP Response Codes","title":"REST"},{"location":"Engineering/Software-Engineering/Web/API/rest/#rest-api","text":"Representation State Transfer An architectural pattern for creating an API that uses HTTP as its communication method for designing network based application Resource: If we have endpoint/URI(URL or URN), lets say https://127.0.0.1/coders then we have resource. Here coders is a resource. Its nothing new, we already used to make URLs that way Representation: When a client makes a GET request to coders/toran/ client gets following JSON response 1 2 3 4 { \"nickname\" : \"toran\" , \"powerLevel\" : 5 } so, this is representation in the form of JSON having metadata representation also could be XML, HTML the same applies when a client sends a request which contains a 'coder' data, its sends representation Representational State: like browsing the web a HTML page is a representation of a resource at current state (or current data) of that resource. When we submit a form, we just send a representation back to the server Representational State Transfer a client and server exchange representations of a resource, which reflects its current state or desired state. So, REST is a way for two machines to transfer the state of a resource via representations. Note: There is a very common confusion in terminology. \"Resource\" - In URI \"app/questions/1\" here \"1\" is also know as resource and at the same time \"questions\" is also know as resource. - Some people around the globe also use \"1\" as record/entity & \"questions\" as resource. - We could generalize \"1\" as \"resource object\" and \"questions\" as \"resource class\"","title":"REST API"},{"location":"Engineering/Software-Engineering/Web/API/rest/#http-request-methods-http-verbs","text":"HTTP defines a set of request methods to indicate the desired action to be performed for a given resource","title":"HTTP Request Methods (HTTP verbs)"},{"location":"Engineering/Software-Engineering/Web/API/rest/#get","text":"1 2 GET /questions/:question_id HTTP/1.1 Host: www.example.com The GET method requests a representation of a specified resource. GET request should only used for data retrieval. (But can also be used for submit/posting/sending data) a resource is mentioned in URL.","title":"GET"},{"location":"Engineering/Software-Engineering/Web/API/rest/#post","text":"1 2 POST /questions/<existing_question> HTTP/1.1 Host: www.example.com/ Used to modify/update an existing entity of a specified resource. Do mention the object/entity of the resource in the URL to update we should use PUT and PATCH for update PUT would create a new unwanted resource when the resouce doesn't exists while updating that particular resource PATCH is perfect for partial updation But there is no restrictions using POST for updates 1 2 POST /questions/ HTTP/1.1 Host: www.example.com Used to submit a new entity/data to a specified resource. Do not mention the object/entity of the resource in the URL while creating it. Otherwise it will give \"Resource Not Found\" error. Use this if you want server to let the name, the URL object while creating a new one Causes change in state of the resource Two simultaneous POST requests works fine (lets say, 1st request is updating some part & 2nd request is updating other part of an object) The UPDATE performed by the POST method might not result in a resource that can be identified by a URI.","title":"POST"},{"location":"Engineering/Software-Engineering/Web/API/rest/#put","text":"Used to create a new resource entity or overrite (Completely replaces) the existing one. For a new resource entity: 1 2 PUT /questions/:new_question_id HTTP/1.1 Host: www.example.com To overrite an existing one: 1 2 PUT /questions/:existing_question_id HTTP/1.1 Host: www.example.com Usefull when you know the name of the entity/object Use this if you want to name the URL object while creating a new one PUT is idempotent, so if you PUT the same object twice, it has no effect (will overrite) Can create or update an object with the same URL","title":"PUT"},{"location":"Engineering/Software-Engineering/Web/API/rest/#delete","text":"1 2 DELETE /api/resource/:id HTTP/1.1 Host: www.example.com/ Deletes/destroyes a specified resource object/entity/record","title":"DELETE"},{"location":"Engineering/Software-Engineering/Web/API/rest/#patch","text":"1 2 3 4 5 6 7 PATCH /users/1 HTTP/1.1 Host: www.example.com Content-Type: application/example If-Match: \"c0b42b66e\" Content-Length: 120 {changes} where [changes] could be JSON/XML like {email:'toran.sahu@yahoo.com'} Used to make partial modification to an existing resource object/entity. Works differently than POST & PUT for update Need to have URL object known","title":"PATCH"},{"location":"Engineering/Software-Engineering/Web/API/rest/#head","text":"1 2 3 4 5 6 7 $ curl http://0.0.0.0:8010/ --head HTTP/1.0 200 OK Server: SimpleHTTP/0.6 Python/3.7.3 Date: Sat, 23 Aug 2020 17:14:52 GMT Content-type: application/json; charset=utf-8 Content-Length: 13306 Ask for the response same as GET but without response/message body. i.e. returns Header & Status Code, except Body Usages? to fetch headers, or to verify the API by status code to get the content size of a media file - without fetcching it actually request has body? No can send query params only successful response has body? No safe? Yes idempotent? Yes cacheable? Yes allowed in HTML forms? No","title":"HEAD"},{"location":"Engineering/Software-Engineering/Web/API/rest/#options","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 $ curl /api/resource -X OPTIONS -i # responds with header; and empty body HTTP/2 200 OK date: Sat, 07 Aug 2021 16:48:46 GMT content-length: 0 vary: Origin access-control-allow-origin: https://api.toran.com access-control-allow-credentials: true access-control-allow-headers: Accept,Accept-Version,Content-Length,Content-MD5,Content-Type,Date,X-Auth-Token,x-requested-with,x-csrftoken,x-page-total,Accept-Language,Connection,x-page-limit,Accept-Encoding,x-page-page,cache-control access-control-allow-methods: GET,HEAD,POST,PATCH,PUT,DELETE access-control-max-age: 3600 content-security-policy: default-src 'self'; object-src 'none'; style-src 'self' 'unsafe-inline'; script-src 'self' 'unsafe-inline'; strict-transport-security: max-age=31536000; includeSubDomains; preload referrer-policy: strict-origin-when-cross-origin permissions-policy: geolocation=() server: kong/2.1.4 # (optional) Body, btw DRF sends body by default {\"name\":\"Api Resource\",\"description\":\"\",\"renders\":[\"application/json\",\"text/html\"],\"parses\":[\"application/json\",\"application/x-www-form-urlencoded\",\"multipart/form-data\"]} Used to describe the communication option for the target resource tell the capabilities of the API server, like list of supported: HTTP methods by the server header keys Content-Type Accepts request has body? No can send query params only successful response has body? Yes (optional) safe? Yes idempotent? Yes cacheable? No allowed in HTML forms? No usage? to communicate with server during CORS case (preflight) tell the server that I will do this, do you support those? 1 2 3 4 OPTIONS /resources/post-here/ HTTP/1.1 Host: bar.example Access-Control-Request-Method: POST Access-Control-Request-Headers: X-PINGOTHER, Content-Type meaning, I'll do POST and with headers X-PINGOTHER and Content-Type . do you supposet those? Response: 1 2 3 HTTP/1.1 204 No Content Access-Control-Allow-Methods: POST, GET, OPTIONS Access-Control-Allow-Headers: X-PINGOTHER, Content-Type","title":"OPTIONS"},{"location":"Engineering/Software-Engineering/Web/API/rest/#connect","text":"starts two-way communications with the requested resource it can be used to open a tunnel ref https://developer.mozilla.org/en-US/docs/Web/HTTP/Methods/connect https://reqbin.com/Article/HttpConnect","title":"CONNECT"},{"location":"Engineering/Software-Engineering/Web/API/rest/#trace","text":"Performs message loop-back test along the path to the target resource","title":"TRACE"},{"location":"Engineering/Software-Engineering/Web/API/rest/#http-response-codes","text":"1XX informational 101 switching protocol 2XX Success 200 (Ok) 201 (Created) 204 (No Content) 3XX Redirection 304 4XX Client Error 404 5XX Server Error 501 (Not Implemented)","title":"HTTP Response Codes"},{"location":"Engineering/Software-Engineering/Web/API/websocket/","text":"Websocket # Websocket Background Forms of I/O Non-preemptive multitasking Event loops Subroutines Coroutines Implementation possible using: select / poll gevent-websocket gunicorn Pre-fork worker model production-ready WebSocket Issue: Probable Solutions TCP KEEPALIVE vs WebSocket Ping/Pong Ref TODO Background # Forms of I/O # Blocking write, read Non-blocking write, read + poll / select Asynchronous aio_write, aio_read mutual exclusion, semaphores Non-preemptive multitasking # Event loops # Subroutines # Any callable unit - method, procedure, func etc. Coroutines # what Subroutines for non-preemptive multitasking, by allowing execution to be suspended and resumed why Coroutines ensure that the developer uses a blocking style of programming that is similar to threading, but provide the benefits of non-blocking I/O how to understand it better http://web.archive.org/web/20071001004937/http://members.verizon.net/olsongt/stackless/why_stackless.html#the-real-world-is-concurrent Implementation possible using: # async/await Eventlet stackless python Tasklet Greenlet gevent select / poll # Available almost anything else with a TCP/IP protocol stack that either utilizes or is modeled after the BSD implementation A variation on the theme of polling, a select loop uses the select system call to sleep until a condition occurs on a file descriptor (e.g., when data is available for reading), a timeout occurs, or a signal is received (e.g., when a child process dies) gevent-websocket # dependencies graph gevent-websocket gevent greenlet gunicorn # Unicorn port for Python. Pre-fork worker model. Pre-fork worker model # Pre-forking basically means a master creates forks which handle each request. A fork is a completely separate *nix process. The pre in pre-fork means that these processes are forked before a request comes in. They can however usually be increased or decreased as the load goes up and down. Pre-forking can be used when you have libraries that are NOT thread safe. It also means issues within a request causing problems will only affect the process which they are processed by and not the entire server. Ref: https://stackoverflow.com/questions/25834333/what-exactly-is-a-pre-fork-web-server-model production-ready # https://docs.gunicorn.org/en/stable/design.html https://medium.com/building-the-system/gunicorn-3-means-of-concurrency-efbb547674b7 WebSocket Issue: # fd leakage https://stackoverflow.com/questions/561988/detect-file-handle-leaks-in-python close it garbage collector will eventually close it, but after a long time.. you may run out of it till then a socket with select when its fileno is greater than FD_SETSIZE will raise the exception ValueError: filedescriptor out of range in select() https://codeghar.com/blog/use-select-in-python-socket-programming-at-your-own-risk.html Changed in version 3.5: The selector is now retried with a recomputed timeout when interrupted by a signal if the signal handler did not raise an exception (see PEP 475 for the rationale), instead of returning an empty list of events before the timeout. https://docs.python.org/3.5/library/selectors.html#module-selectors Connection already closed: https://stackoverflow.com/questions/10585355/sending-websocket-ping-pong-frame-from-browser implement server-initiated ping https://tools.ietf.org/html/rfc6455#section-5.5.2 DEBUG: Load balancers are the issue, as conn. kept alive for more time (55 hrs) [Errno 104] Connection reset by peer may be core python sock issue https://github.com/python/cpython/blob/3466922320d54a922cfe6d6d44e89e1cea4023ef/Modules/errnomodule.c#L558-L561 1 minute idle conn termination issue ping/pong vs keep-alive https://stackoverflow.com/questions/23238319/websockets-ping-pong-why-not-tcp-keepalive both are not same TCP endpoints and WebSocket endpoints are diff. FIX: server-initiated ping was a solution to keep alive idle connections possible wss close reason if \"utf-8 encoded bytestring to unicode\" conversion fails; close the conn how long a wss or tcp conn can last? https://stackoverflow.com/questions/50197453/how-long-can-a-websocket-connection-last https://stackoverflow.com/questions/45693430/how-long-can-a-tcp-connection-stay-open papi-ws questions and why config with single worker? 1 2 3 4 5 6 exec gunicorn repo.wsgi:ws \\ --worker-class geventwebsocket.gunicorn.workers.GeventWebSocketWorker \\ --worker-connections 4000 \\ --bind 0.0.0.0:8000 \\ --threads 8 \\ --pythonpath . --logger-class repo.glogger.gunicorn_logger why not --workers=(2*CPU)+1 and why --threads=8 ? [x] why event loop in-combination with gevent is not detecting epoll system call (for a scalable I/O event notification mechanism) my linux does have it 1 2 3 4 >>> import select >>> select.epoll() <select.epoll object at 0x7f30c5ac4228> >>> hasattr(select, 'epoll') FIX: checked again and found gevent.selectors.GeventSelector Probable Solutions # increase --keep-alive (default:2 sec) When Gunicorn is deployed behind a load balancer, it often makes sense to set this to a higher value. increase --timeout (default:30 sec) Workers silent for more than this many seconds are killed and restarted. Generally set to thirty seconds. Only set this noticeably higher if you\u2019re sure of the repercussions for sync workers. For the non sync workers it just means that the worker process is still communicating and is not tied to the length of time required to handle a single request. Long-lived TCP connections: Network Load Balancer supports long-running TCP connections that can be open for months or years, making it ideal for WebSocket-type applications, IoT, gaming, and messaging applications. TCP KEEPALIVE vs WebSocket Ping/Pong # https://stackoverflow.com/questions/23238319/websockets-ping-pong-why-not-tcp-keepalive Ref # https://tools.ietf.org/html/rfc6455 https://developer.mozilla.org/en-US/docs/Web/API/WebSockets_API/Writing_WebSocket_servers https://www.slideshare.net/jaquayle/best-practices-in-building-websockets-apis-elad-wertzberger-liveperson https://hpbn.co/websocket/ https://www.quora.com/What-are-the-best-practices-for-managing-connections-to-Websockets-on-the-server-side TODO # https://stackoverflow.com/questions/14925413/gevent-websocket-detecting-closed-connection https://stackoverflow.com/questions/31188874/why-is-gevent-websocket-synchronous https://stackoverflow.com/questions/23314964/is-there-a-server-cost-to-using-websockets","title":"Websocket"},{"location":"Engineering/Software-Engineering/Web/API/websocket/#websocket","text":"Websocket Background Forms of I/O Non-preemptive multitasking Event loops Subroutines Coroutines Implementation possible using: select / poll gevent-websocket gunicorn Pre-fork worker model production-ready WebSocket Issue: Probable Solutions TCP KEEPALIVE vs WebSocket Ping/Pong Ref TODO","title":"Websocket"},{"location":"Engineering/Software-Engineering/Web/API/websocket/#background","text":"","title":"Background"},{"location":"Engineering/Software-Engineering/Web/API/websocket/#forms-of-io","text":"Blocking write, read Non-blocking write, read + poll / select Asynchronous aio_write, aio_read mutual exclusion, semaphores","title":"Forms of I/O"},{"location":"Engineering/Software-Engineering/Web/API/websocket/#non-preemptive-multitasking","text":"","title":"Non-preemptive multitasking"},{"location":"Engineering/Software-Engineering/Web/API/websocket/#event-loops","text":"","title":"Event loops"},{"location":"Engineering/Software-Engineering/Web/API/websocket/#subroutines","text":"Any callable unit - method, procedure, func etc.","title":"Subroutines"},{"location":"Engineering/Software-Engineering/Web/API/websocket/#coroutines","text":"what Subroutines for non-preemptive multitasking, by allowing execution to be suspended and resumed why Coroutines ensure that the developer uses a blocking style of programming that is similar to threading, but provide the benefits of non-blocking I/O how to understand it better http://web.archive.org/web/20071001004937/http://members.verizon.net/olsongt/stackless/why_stackless.html#the-real-world-is-concurrent","title":"Coroutines"},{"location":"Engineering/Software-Engineering/Web/API/websocket/#implementation-possible-using","text":"async/await Eventlet stackless python Tasklet Greenlet gevent","title":"Implementation possible using:"},{"location":"Engineering/Software-Engineering/Web/API/websocket/#select-poll","text":"Available almost anything else with a TCP/IP protocol stack that either utilizes or is modeled after the BSD implementation A variation on the theme of polling, a select loop uses the select system call to sleep until a condition occurs on a file descriptor (e.g., when data is available for reading), a timeout occurs, or a signal is received (e.g., when a child process dies)","title":"select / poll"},{"location":"Engineering/Software-Engineering/Web/API/websocket/#gevent-websocket","text":"dependencies graph gevent-websocket gevent greenlet","title":"gevent-websocket"},{"location":"Engineering/Software-Engineering/Web/API/websocket/#gunicorn","text":"Unicorn port for Python. Pre-fork worker model.","title":"gunicorn"},{"location":"Engineering/Software-Engineering/Web/API/websocket/#pre-fork-worker-model","text":"Pre-forking basically means a master creates forks which handle each request. A fork is a completely separate *nix process. The pre in pre-fork means that these processes are forked before a request comes in. They can however usually be increased or decreased as the load goes up and down. Pre-forking can be used when you have libraries that are NOT thread safe. It also means issues within a request causing problems will only affect the process which they are processed by and not the entire server. Ref: https://stackoverflow.com/questions/25834333/what-exactly-is-a-pre-fork-web-server-model","title":"Pre-fork worker model"},{"location":"Engineering/Software-Engineering/Web/API/websocket/#production-ready","text":"https://docs.gunicorn.org/en/stable/design.html https://medium.com/building-the-system/gunicorn-3-means-of-concurrency-efbb547674b7","title":"production-ready"},{"location":"Engineering/Software-Engineering/Web/API/websocket/#websocket-issue","text":"fd leakage https://stackoverflow.com/questions/561988/detect-file-handle-leaks-in-python close it garbage collector will eventually close it, but after a long time.. you may run out of it till then a socket with select when its fileno is greater than FD_SETSIZE will raise the exception ValueError: filedescriptor out of range in select() https://codeghar.com/blog/use-select-in-python-socket-programming-at-your-own-risk.html Changed in version 3.5: The selector is now retried with a recomputed timeout when interrupted by a signal if the signal handler did not raise an exception (see PEP 475 for the rationale), instead of returning an empty list of events before the timeout. https://docs.python.org/3.5/library/selectors.html#module-selectors Connection already closed: https://stackoverflow.com/questions/10585355/sending-websocket-ping-pong-frame-from-browser implement server-initiated ping https://tools.ietf.org/html/rfc6455#section-5.5.2 DEBUG: Load balancers are the issue, as conn. kept alive for more time (55 hrs) [Errno 104] Connection reset by peer may be core python sock issue https://github.com/python/cpython/blob/3466922320d54a922cfe6d6d44e89e1cea4023ef/Modules/errnomodule.c#L558-L561 1 minute idle conn termination issue ping/pong vs keep-alive https://stackoverflow.com/questions/23238319/websockets-ping-pong-why-not-tcp-keepalive both are not same TCP endpoints and WebSocket endpoints are diff. FIX: server-initiated ping was a solution to keep alive idle connections possible wss close reason if \"utf-8 encoded bytestring to unicode\" conversion fails; close the conn how long a wss or tcp conn can last? https://stackoverflow.com/questions/50197453/how-long-can-a-websocket-connection-last https://stackoverflow.com/questions/45693430/how-long-can-a-tcp-connection-stay-open papi-ws questions and why config with single worker? 1 2 3 4 5 6 exec gunicorn repo.wsgi:ws \\ --worker-class geventwebsocket.gunicorn.workers.GeventWebSocketWorker \\ --worker-connections 4000 \\ --bind 0.0.0.0:8000 \\ --threads 8 \\ --pythonpath . --logger-class repo.glogger.gunicorn_logger why not --workers=(2*CPU)+1 and why --threads=8 ? [x] why event loop in-combination with gevent is not detecting epoll system call (for a scalable I/O event notification mechanism) my linux does have it 1 2 3 4 >>> import select >>> select.epoll() <select.epoll object at 0x7f30c5ac4228> >>> hasattr(select, 'epoll') FIX: checked again and found gevent.selectors.GeventSelector","title":"WebSocket Issue:"},{"location":"Engineering/Software-Engineering/Web/API/websocket/#probable-solutions","text":"increase --keep-alive (default:2 sec) When Gunicorn is deployed behind a load balancer, it often makes sense to set this to a higher value. increase --timeout (default:30 sec) Workers silent for more than this many seconds are killed and restarted. Generally set to thirty seconds. Only set this noticeably higher if you\u2019re sure of the repercussions for sync workers. For the non sync workers it just means that the worker process is still communicating and is not tied to the length of time required to handle a single request. Long-lived TCP connections: Network Load Balancer supports long-running TCP connections that can be open for months or years, making it ideal for WebSocket-type applications, IoT, gaming, and messaging applications.","title":"Probable Solutions"},{"location":"Engineering/Software-Engineering/Web/API/websocket/#tcp-keepalive-vs-websocket-pingpong","text":"https://stackoverflow.com/questions/23238319/websockets-ping-pong-why-not-tcp-keepalive","title":"TCP KEEPALIVE vs WebSocket Ping/Pong"},{"location":"Engineering/Software-Engineering/Web/API/websocket/#ref","text":"https://tools.ietf.org/html/rfc6455 https://developer.mozilla.org/en-US/docs/Web/API/WebSockets_API/Writing_WebSocket_servers https://www.slideshare.net/jaquayle/best-practices-in-building-websockets-apis-elad-wertzberger-liveperson https://hpbn.co/websocket/ https://www.quora.com/What-are-the-best-practices-for-managing-connections-to-Websockets-on-the-server-side","title":"Ref"},{"location":"Engineering/Software-Engineering/Web/API/websocket/#todo","text":"https://stackoverflow.com/questions/14925413/gevent-websocket-detecting-closed-connection https://stackoverflow.com/questions/31188874/why-is-gevent-websocket-synchronous https://stackoverflow.com/questions/23314964/is-there-a-server-cost-to-using-websockets","title":"TODO"},{"location":"Human-Science/faq/","text":"FAQs # FAQs About Yourself? Note Why we? Realities Note Disclosable realities Why we choose you? Qualities - tbd Strength I.Q. E.Q. Notes Weakness Notes About Yourself? # \u2713 \u2715 Name Parents name, Siblings, DoB District Village (unless not unique) Highest edu School Past work experience Hobbies Married Note # stress / expose the area you want to intiate/direct the discussion on Why we? # Realities # \u2713 \u2715 Power/Money diverse work/multiple hats, learning to contribute in project/product/pusporse X financial stability/security social reputation relocation Note # should be simple Disclosable realities # \u2713 \u2715 Offer service Why we choose you? # due to the qualities Qualities # apptitude dicision making analytical skill logical/reasoning ability scientific temperament values integrity objectivity/neutrality compassion hard work perseverance patience technical - tbd # Strength # I.Q. # objectivity / neutral / natural justice dicision making ability quick correct hardworking patience perseverance tolerance planning & execution E.Q. # empathy / compassion / sensitivity balance friends vs family work vs family life thoughts quick leaner curious focus / target creative expressions health consious discipline Notes # Avoid being super human In limit / range Approach civil service values creativity Weakness # workaholic over-sensitive self discipline control / carelessness sleep cycle diet control routine / time management gaming introvert [social smartness] shy submissive artistic expressions painting calligraphy music [E.Q.] anger one-sided decision not able to mugup things / not a man of facts / man of concepts Notes # Avoid illegalities Avoid characteristical ones Avoid insensitive statements Which are rectifiable in-progess","title":"FAQs"},{"location":"Human-Science/faq/#faqs","text":"FAQs About Yourself? Note Why we? Realities Note Disclosable realities Why we choose you? Qualities - tbd Strength I.Q. E.Q. Notes Weakness Notes","title":"FAQs"},{"location":"Human-Science/faq/#about-yourself","text":"\u2713 \u2715 Name Parents name, Siblings, DoB District Village (unless not unique) Highest edu School Past work experience Hobbies Married","title":"About Yourself?"},{"location":"Human-Science/faq/#note","text":"stress / expose the area you want to intiate/direct the discussion on","title":"Note"},{"location":"Human-Science/faq/#why-we","text":"","title":"Why we?"},{"location":"Human-Science/faq/#realities","text":"\u2713 \u2715 Power/Money diverse work/multiple hats, learning to contribute in project/product/pusporse X financial stability/security social reputation relocation","title":"Realities"},{"location":"Human-Science/faq/#note_1","text":"should be simple","title":"Note"},{"location":"Human-Science/faq/#disclosable-realities","text":"\u2713 \u2715 Offer service","title":"Disclosable realities"},{"location":"Human-Science/faq/#why-we-choose-you","text":"due to the qualities","title":"Why we choose you?"},{"location":"Human-Science/faq/#qualities","text":"apptitude dicision making analytical skill logical/reasoning ability scientific temperament values integrity objectivity/neutrality compassion hard work perseverance patience technical","title":"Qualities"},{"location":"Human-Science/faq/#-tbd","text":"","title":"- tbd"},{"location":"Human-Science/faq/#strength","text":"","title":"Strength"},{"location":"Human-Science/faq/#iq","text":"objectivity / neutral / natural justice dicision making ability quick correct hardworking patience perseverance tolerance planning & execution","title":"I.Q."},{"location":"Human-Science/faq/#eq","text":"empathy / compassion / sensitivity balance friends vs family work vs family life thoughts quick leaner curious focus / target creative expressions health consious discipline","title":"E.Q."},{"location":"Human-Science/faq/#notes","text":"Avoid being super human In limit / range Approach civil service values creativity","title":"Notes"},{"location":"Human-Science/faq/#weakness","text":"workaholic over-sensitive self discipline control / carelessness sleep cycle diet control routine / time management gaming introvert [social smartness] shy submissive artistic expressions painting calligraphy music [E.Q.] anger one-sided decision not able to mugup things / not a man of facts / man of concepts","title":"Weakness"},{"location":"Human-Science/faq/#notes_1","text":"Avoid illegalities Avoid characteristical ones Avoid insensitive statements Which are rectifiable in-progess","title":"Notes"},{"location":"Human-Science/human_science/","text":"Behavioral, Moral, and Social # Table of contents Behavioral, Moral, and Social Nature Emotional Intelligence don't assume Accusing and Blaming Acceptance & Arguement Trust Verbal Abuse Anger Management Tactful Honesty Niceness/Empathic over Truth/Honesty Observation Vs Judgement Over confidence, Confidence, and Under confidence Lack of confidence Self Validation Confidence Measurement Forgiveness Parenting Habits Team Inter-personal qualities Way to boost Nature # ambitious analytical angry annoying anxious arrogant artistic blammer calm caring clever compassionate controlling creative curious disciplined dominating emotional empathic frank friendly greedy hard-worker humble idealistic impatient integral intelligent irritable jealous judgemental logical manipulative natural neutral nossy objectional observer over XXX over clever over smart patient perseverant poker quick XXX quick decision maker quick thinker realistic sensible sensitive serious short-tempered skeptic smart sympathic technical toxic workaholic worrier Emotional Intelligence # https://getpocket.com/explore/item/7-powerful-habits-of-people-with-high-emotional-intelligence?utm_source=pocket-newtab-intl-en talk/communicate to make them feel good know strength & weakness, and seek help don't assume # Accusing and Blaming # Acceptance & Arguement # Related Topics Trust Over confidence Trust # Verbal Abuse # Verbally abusing or attacking another is a maladaptive behavior that can be occasionally displayed by anyone, especially during stressful times or times when one is experiencing significant physical discomfort. It can also be used as a defensive mechanism when an individual feels like he or she is being attacked by another individual, or as a method of achieving vengeance on an individual. However, verbal abusive behaviors can also be used to intentionally manipulate others. When an individual starts verbally abusing another, the intensity and the frequency of abuse tend to increase over time. It can be as harmful as physical abuse, and can negatively impact the victim\u2019s health.[14] The victims of verbal abuse may suffer from emotional pain and mental distress, which over time could lead to succumbing due to stress-related illness Victims of verbal abuse are three times more likely to develop personality disorders. Parents who verbally abuse their children may cause their children to develop personality disorders during adolescence and young adulthood. Related Topics Parenting Anger Management Ref https://en.wikipedia.org/wiki/Verbal_abuse https://www.joinonelove.org/learn/11-common-patterns-verbal-abuse/ https://www.bustle.com/wellness/how-to-send-good-morning-text https://dayoneservices.org/healthy-relationships/ Anger Management # Tactful Honesty # Niceness/Empathic over Truth/Honesty # Observation Vs Judgement # Over confidence, Confidence, and Under confidence # Over confidence arrogant denying the lack/gap Under confident covering the lack/gap Confidence knowing & accepting the lack/gap Lack of confidence # Self Validation # accepting your limitation, flaws, and mistakes Confidence Measurement # Self evaluation Judgement of accuracy (w.r.t self evaluation report) Forgiveness # A good read Parenting # Habits # Make it Obvious Intention & command should be clear Stacking After [CURRENT HABIT], I will [NEW HABIT] Environment Make it Attractive Reward/temptation After [HABIT INEED], I will [HABIT I WANT] Culture where the habit is a norm Make it Easy Make unwanted things unliking Start small Make it Satisfying Team # Inter-personal qualities # Trust Conflict Commitment Accountability Team Result Way to boost # be Humble be Hungry be Smart (inter-personally)","title":"Behavioral, Moral, and Social"},{"location":"Human-Science/human_science/#behavioral-moral-and-social","text":"","title":"Behavioral, Moral, and Social"},{"location":"Human-Science/human_science/#nature","text":"ambitious analytical angry annoying anxious arrogant artistic blammer calm caring clever compassionate controlling creative curious disciplined dominating emotional empathic frank friendly greedy hard-worker humble idealistic impatient integral intelligent irritable jealous judgemental logical manipulative natural neutral nossy objectional observer over XXX over clever over smart patient perseverant poker quick XXX quick decision maker quick thinker realistic sensible sensitive serious short-tempered skeptic smart sympathic technical toxic workaholic worrier","title":"Nature"},{"location":"Human-Science/human_science/#emotional-intelligence","text":"https://getpocket.com/explore/item/7-powerful-habits-of-people-with-high-emotional-intelligence?utm_source=pocket-newtab-intl-en talk/communicate to make them feel good know strength & weakness, and seek help","title":"Emotional Intelligence"},{"location":"Human-Science/human_science/#dont-assume","text":"","title":"don't assume"},{"location":"Human-Science/human_science/#accusing-and-blaming","text":"","title":"Accusing and Blaming"},{"location":"Human-Science/human_science/#acceptance-arguement","text":"Related Topics Trust Over confidence","title":"Acceptance &amp; Arguement"},{"location":"Human-Science/human_science/#trust","text":"","title":"Trust"},{"location":"Human-Science/human_science/#verbal-abuse","text":"Verbally abusing or attacking another is a maladaptive behavior that can be occasionally displayed by anyone, especially during stressful times or times when one is experiencing significant physical discomfort. It can also be used as a defensive mechanism when an individual feels like he or she is being attacked by another individual, or as a method of achieving vengeance on an individual. However, verbal abusive behaviors can also be used to intentionally manipulate others. When an individual starts verbally abusing another, the intensity and the frequency of abuse tend to increase over time. It can be as harmful as physical abuse, and can negatively impact the victim\u2019s health.[14] The victims of verbal abuse may suffer from emotional pain and mental distress, which over time could lead to succumbing due to stress-related illness Victims of verbal abuse are three times more likely to develop personality disorders. Parents who verbally abuse their children may cause their children to develop personality disorders during adolescence and young adulthood. Related Topics Parenting Anger Management Ref https://en.wikipedia.org/wiki/Verbal_abuse https://www.joinonelove.org/learn/11-common-patterns-verbal-abuse/ https://www.bustle.com/wellness/how-to-send-good-morning-text https://dayoneservices.org/healthy-relationships/","title":"Verbal Abuse"},{"location":"Human-Science/human_science/#anger-management","text":"","title":"Anger Management"},{"location":"Human-Science/human_science/#tactful-honesty","text":"","title":"Tactful Honesty"},{"location":"Human-Science/human_science/#nicenessempathic-over-truthhonesty","text":"","title":"Niceness/Empathic over Truth/Honesty"},{"location":"Human-Science/human_science/#observation-vs-judgement","text":"","title":"Observation Vs Judgement"},{"location":"Human-Science/human_science/#over-confidence-confidence-and-under-confidence","text":"Over confidence arrogant denying the lack/gap Under confident covering the lack/gap Confidence knowing & accepting the lack/gap","title":"Over confidence, Confidence, and Under confidence"},{"location":"Human-Science/human_science/#lack-of-confidence","text":"","title":"Lack of confidence"},{"location":"Human-Science/human_science/#self-validation","text":"accepting your limitation, flaws, and mistakes","title":"Self Validation"},{"location":"Human-Science/human_science/#confidence-measurement","text":"Self evaluation Judgement of accuracy (w.r.t self evaluation report)","title":"Confidence Measurement"},{"location":"Human-Science/human_science/#forgiveness","text":"A good read","title":"Forgiveness"},{"location":"Human-Science/human_science/#parenting","text":"","title":"Parenting"},{"location":"Human-Science/human_science/#habits","text":"Make it Obvious Intention & command should be clear Stacking After [CURRENT HABIT], I will [NEW HABIT] Environment Make it Attractive Reward/temptation After [HABIT INEED], I will [HABIT I WANT] Culture where the habit is a norm Make it Easy Make unwanted things unliking Start small Make it Satisfying","title":"Habits"},{"location":"Human-Science/human_science/#team","text":"","title":"Team"},{"location":"Human-Science/human_science/#inter-personal-qualities","text":"Trust Conflict Commitment Accountability Team Result","title":"Inter-personal qualities"},{"location":"Human-Science/human_science/#way-to-boost","text":"be Humble be Hungry be Smart (inter-personally)","title":"Way to boost"},{"location":"Human-Science/misc/","text":"Finance # Do's # Keep 6-12x of monthly salary as emergency (crises, opportunity etc.) fund in FD Invest savings in equity/MF (paper investment) and real state/gold (non-paper investment) Invest in health and skill-up Index fund for long-term Optimize taxes (business?) Don't # Put all money in the bank, money looses value over time Take loan for anything, loan only for value generating (productive) thing Blindly trust govt. scheme, analyse that first","title":"Finance"},{"location":"Human-Science/misc/#finance","text":"","title":"Finance"},{"location":"Human-Science/misc/#dos","text":"Keep 6-12x of monthly salary as emergency (crises, opportunity etc.) fund in FD Invest savings in equity/MF (paper investment) and real state/gold (non-paper investment) Invest in health and skill-up Index fund for long-term Optimize taxes (business?)","title":"Do's"},{"location":"Human-Science/misc/#dont","text":"Put all money in the bank, money looses value over time Take loan for anything, loan only for value generating (productive) thing Blindly trust govt. scheme, analyse that first","title":"Don't"},{"location":"Languages/c_cpp/","text":"C/C++ # C/C++ Introduction The Features of C++ as a Language Basic concepts Compiler Machine Code Assembly Code Linker Different kinds of files Source Code (.c/.cpp) Header Files (.h) Object Files (.o/.obj) Binary Executable Library Files (.a) Shared Library Files (.so) Preprocessor Preprocessor Directives define include if, #elif, #else, #ifdef, #ifndef, #endif GNU Compile Collection (GCC) Compile using gcc Background Flow gcc Misc is there any need to compile the .h files? How to organize things between .h files and .c/.cpp file? Should is include/import .c/.cpp file instead .h because it just works fine? C++ Keywords Expressions Operators Declaration Function definition Template declaration Explicit template instantiation Explicit template specialization Namespace definition Linkage specification Attribute declaration (attr ;) (since C++11) Empty declaration (;) (since C++11) A function declaration without a decl-specifier-seq: Specifiers Declaration Specifiers Type Specifiers Declarators Initialization Functions In-built Lambda Pass by Value Pass by Reference Statements Expression Statements Compound Statements Selection Statements Label Statements Iteration Statements Jump Statements Declaration Statements Try Blocks Atomic and Synchronized Blocks Classes Templates STL (Standard Template Library) Exceptions Misc namespace vs include REST API Microservice Based on REST Introduction # Source-0 Source-1 Source-2 Source-3 The Features of C++ as a Language # is an open ISO-standardized language. For a time, C++ had no official standard and was maintained by a de-facto standard, however since 1998, C++ is standardized by a committee of the ISO. Their page may be accessed here . is a compiled language. C++ compiles directly to a machine's native code, allowing it to be one of the fastest languages in the world, if optimized. is a strongly-typed unsafe language. C++ is a language that expects the programmer to know what he or she is doing, but allows for incredible amounts of control as a result. supports both manifest and inferred typing. As of the latest C++ standard, C++ supports both manifest and inferred typing, allowing flexibility and a means of avoiding verbosity where desired. supports both static and dynamic type checking. C++ allows type conversions to be checked either at compile-time or at run-time, again offering another degree of flexibility. Most C++ type checking is, however, static. offers many paradigm choices. C++ offers remarkable support for procedural, generic, and object-oriented programming paradigms, with many other paradigms being possible as well. is portable. As one of the most frequently used languages in the world and as an open language, C++ has a wide range of compilers that run on many different platforms that support it. Code that exclusively uses C++'s standard library will run on many platforms with few to no changes. is upwards compatible with C C++, being a language that directly builds off C, is compatible with almost all C code. C++ can use C libraries with few to no modifications of the libraries' code. has incredible library support. A search for \"library\" on the popular project-management website SourceForge will yield over 3000 results for C++ libraries. A link to the results of the search may be found here . Basic concepts # Source-GCC Source-Compile-Cycle Compiler # Compiler compiles/translates all the source code into machine readable (raw binary/machine) code. - creates a .o file (relocatable machine code for module func.c) from the output of the preprocessor - can see the assembly code in func.o using either objdump or gdb Machine Code # aka Raw binary code e.g. 1 8B 0E 34 12 Assembly Code # Assembly is one level higher that is semi readable without having to memorize a bunch of hex or binary codes allows you to use symbolic names for addresses and do some simple math in creating the code. e.g. 1 MOV CX, 1234H Linker # Linker links together a number of object files to produce a binary file which can be directly executed. - creates an executable file (a.out file) from one or more .o files and .a or .so files (static or dynamic libraries) - can use objdump (or in gdb the disass command) to disassemble the code in the executable Different kinds of files # Source Code (.c/.cpp) # contain function definitions Header Files (.h) # contain function declarations (also known as function prototypes) and various preprocessor statements used to allow source code files to access externally-defined functions Object Files (.o/.obj) # (output of compiler & input to the linker) These files are produced as the output of the compiler consist of function definitions in binary form are not executable by themselves Object files end in \".o\" by convention can see the assembly code in func.o using either objdump or gdb Binary Executable # (output of linker & executable with resolved reference) produced as the output of a program called a \"linker\" linker links together a number of object files to produce a binary file which can be directly executed have no special suffix on Unix operating systems (or .out default suffix) have .exe suffix on Windows operating systems generates assembler output (a.out, default name) Library Files (.a) # files are archives are groups of objects or static libraries input to the linker similar to the .jar or .dll files Shared Library Files (.so) # Preprocessor # Before the C compiler starts compiling a source code file, the file is processed by a preprocessor automatically invoked by compiler before compilation expands the source code file by incorporating the pre-processor files (#include <.h/.c/.cpp>) included in the source code either it creates a real file or creates modified source code in memory for short time before being sent to the compiler Preprocessor Directives # Declarative define # mainly used to define constants 1 #define BIGNUM 1000000 include # used to access function definitions defined outside of a source code file 1 2 #include <stdio.h> #include \"somelocalcode.h\" or 1 #include \"somelocalcode.c\" Conditional if, #elif, #else, #ifdef, #ifndef, #endif # GNU Compile Collection (GCC) # aka GNU C Compiler Compile using gcc # Background # Lets say func.h have a function declarations : hello() and we are defining it in func.c file (func.c does not have main() function) now want to use the hello() function in source.c, so we can include func.h file, or func.c file (BAD IDEA) Flow # func.c & source.c both need #include \"func.h\" , because func.c is defining the code which backs the hello() function source.c is using/calling the hello() function and invoking its behavior, so it has to know the behavior & memory size need to allocate for the same but it does not need the actual definition/implementation of hello() yet the compiler will generate .o (object file, compiled but not executable) file func.o from func.c, and source.o from source.c which includes main method and unresolved reference of hello() function 1 2 gcc -c func.c gcc -c source.c here, func.o & source.o are not self executable because func.o doesn't have main() , and source.o have one un-resolved reference now, comes the linker linker will combine the two object files func.o & source.o into an executable file now it connects the dot between both the object files & resolves the un-resolved reference now, at run time, program can jump to the correct location 1 gcc func.o source.o -o program gcc Misc # is there any need to compile the .h files? # No. How to organize things between .h files and .c/.cpp file? # Put as much as you can in the .c and as little as possible in the .h. The includes in the .c are only included when that one file is compiled, but the includes for the .h have to be included by every file that uses it. Should is include/import .c/.cpp file instead .h because it just works fine? # No. Lets say .c file has definition of class Foo & this .c/.cpp file are used/referenced by multiple source code files (compilation units), then those all source code files will have definition of class Foo multiple times and which may cause a problem because linker will get confused and throw error. C++ Keywords # Expressions # Operators # Arrow --> : Used to access classes, structure, or union member using pointer. e.g. 1 Dot . : Used to access classes, structure, or union member. e.g. 1 Scope Resolution :: : Qualifies the abstracted/hidden member/names e.g. 1 2 3 4 5 6 7 8 int count = 0 ; int main ( void ) { int count = 0 ; :: count = 1 ; // set global count to 1 count = 2 ; // set local count to 2 return 0 ; } Declaration # Function definition # Template declaration # Explicit template instantiation # Explicit template specialization # Namespace definition # Linkage specification # extern \"C\" is a linkage-specification Attribute declaration (attr ;) (since C++11) # Empty declaration (;) (since C++11) # A function declaration without a decl-specifier-seq: # Specifiers # Declaration Specifiers # typedef: typedef is used to give data type a new name e.g. 1 typedef unsigned char BYTE ; Type Specifiers # class ABC enum Abc char name Declarators # Initialization # Functions # In-built # strncpy(): char * strncpy ( char * destination, const char * source, size_t num ); copies characters from string Lambda # Pass by Value # Pass by Reference # Statements # Expression Statements # Compound Statements # Selection Statements # Label Statements # Iteration Statements # Jump Statements # Declaration Statements # Try Blocks # Atomic and Synchronized Blocks # Classes # Templates # STL (Standard Template Library) # The Standard Template Library (STL) is a set of C++ template classes to provide common programming data structures and functions such as lists, stacks, arrays, etc. It is a library of container classes, algorithms and iterators. Exceptions # Misc # namespace vs include # https://stackoverflow.com/questions/389922/c-namespace-and-include REST API # https://msdn.microsoft.com/en-us/magazine/dn342869.aspx Microservice Based on REST # https://medium.com/audelabs/modern-c-micro-service-implementation-rest-api-b499ffeaf898 https://martinfowler.com/articles/microservices.html https://dev.otto.de/2016/03/20/why-microservices/","title":"C/C++"},{"location":"Languages/c_cpp/#cc","text":"C/C++ Introduction The Features of C++ as a Language Basic concepts Compiler Machine Code Assembly Code Linker Different kinds of files Source Code (.c/.cpp) Header Files (.h) Object Files (.o/.obj) Binary Executable Library Files (.a) Shared Library Files (.so) Preprocessor Preprocessor Directives define include if, #elif, #else, #ifdef, #ifndef, #endif GNU Compile Collection (GCC) Compile using gcc Background Flow gcc Misc is there any need to compile the .h files? How to organize things between .h files and .c/.cpp file? Should is include/import .c/.cpp file instead .h because it just works fine? C++ Keywords Expressions Operators Declaration Function definition Template declaration Explicit template instantiation Explicit template specialization Namespace definition Linkage specification Attribute declaration (attr ;) (since C++11) Empty declaration (;) (since C++11) A function declaration without a decl-specifier-seq: Specifiers Declaration Specifiers Type Specifiers Declarators Initialization Functions In-built Lambda Pass by Value Pass by Reference Statements Expression Statements Compound Statements Selection Statements Label Statements Iteration Statements Jump Statements Declaration Statements Try Blocks Atomic and Synchronized Blocks Classes Templates STL (Standard Template Library) Exceptions Misc namespace vs include REST API Microservice Based on REST","title":"C/C++"},{"location":"Languages/c_cpp/#introduction","text":"Source-0 Source-1 Source-2 Source-3","title":"Introduction"},{"location":"Languages/c_cpp/#the-features-of-c-as-a-language","text":"is an open ISO-standardized language. For a time, C++ had no official standard and was maintained by a de-facto standard, however since 1998, C++ is standardized by a committee of the ISO. Their page may be accessed here . is a compiled language. C++ compiles directly to a machine's native code, allowing it to be one of the fastest languages in the world, if optimized. is a strongly-typed unsafe language. C++ is a language that expects the programmer to know what he or she is doing, but allows for incredible amounts of control as a result. supports both manifest and inferred typing. As of the latest C++ standard, C++ supports both manifest and inferred typing, allowing flexibility and a means of avoiding verbosity where desired. supports both static and dynamic type checking. C++ allows type conversions to be checked either at compile-time or at run-time, again offering another degree of flexibility. Most C++ type checking is, however, static. offers many paradigm choices. C++ offers remarkable support for procedural, generic, and object-oriented programming paradigms, with many other paradigms being possible as well. is portable. As one of the most frequently used languages in the world and as an open language, C++ has a wide range of compilers that run on many different platforms that support it. Code that exclusively uses C++'s standard library will run on many platforms with few to no changes. is upwards compatible with C C++, being a language that directly builds off C, is compatible with almost all C code. C++ can use C libraries with few to no modifications of the libraries' code. has incredible library support. A search for \"library\" on the popular project-management website SourceForge will yield over 3000 results for C++ libraries. A link to the results of the search may be found here .","title":"The Features of C++ as a Language"},{"location":"Languages/c_cpp/#basic-concepts","text":"Source-GCC Source-Compile-Cycle","title":"Basic concepts"},{"location":"Languages/c_cpp/#compiler","text":"Compiler compiles/translates all the source code into machine readable (raw binary/machine) code. - creates a .o file (relocatable machine code for module func.c) from the output of the preprocessor - can see the assembly code in func.o using either objdump or gdb","title":"Compiler"},{"location":"Languages/c_cpp/#machine-code","text":"aka Raw binary code e.g. 1 8B 0E 34 12","title":"Machine Code"},{"location":"Languages/c_cpp/#assembly-code","text":"Assembly is one level higher that is semi readable without having to memorize a bunch of hex or binary codes allows you to use symbolic names for addresses and do some simple math in creating the code. e.g. 1 MOV CX, 1234H","title":"Assembly Code"},{"location":"Languages/c_cpp/#linker","text":"Linker links together a number of object files to produce a binary file which can be directly executed. - creates an executable file (a.out file) from one or more .o files and .a or .so files (static or dynamic libraries) - can use objdump (or in gdb the disass command) to disassemble the code in the executable","title":"Linker"},{"location":"Languages/c_cpp/#different-kinds-of-files","text":"","title":"Different kinds of files"},{"location":"Languages/c_cpp/#source-code-ccpp","text":"contain function definitions","title":"Source Code (.c/.cpp)"},{"location":"Languages/c_cpp/#header-files-h","text":"contain function declarations (also known as function prototypes) and various preprocessor statements used to allow source code files to access externally-defined functions","title":"Header Files (.h)"},{"location":"Languages/c_cpp/#object-files-oobj","text":"(output of compiler & input to the linker) These files are produced as the output of the compiler consist of function definitions in binary form are not executable by themselves Object files end in \".o\" by convention can see the assembly code in func.o using either objdump or gdb","title":"Object Files (.o/.obj)"},{"location":"Languages/c_cpp/#binary-executable","text":"(output of linker & executable with resolved reference) produced as the output of a program called a \"linker\" linker links together a number of object files to produce a binary file which can be directly executed have no special suffix on Unix operating systems (or .out default suffix) have .exe suffix on Windows operating systems generates assembler output (a.out, default name)","title":"Binary Executable"},{"location":"Languages/c_cpp/#library-files-a","text":"files are archives are groups of objects or static libraries input to the linker similar to the .jar or .dll files","title":"Library Files (.a)"},{"location":"Languages/c_cpp/#shared-library-files-so","text":"","title":"Shared Library Files (.so)"},{"location":"Languages/c_cpp/#preprocessor","text":"Before the C compiler starts compiling a source code file, the file is processed by a preprocessor automatically invoked by compiler before compilation expands the source code file by incorporating the pre-processor files (#include <.h/.c/.cpp>) included in the source code either it creates a real file or creates modified source code in memory for short time before being sent to the compiler","title":"Preprocessor"},{"location":"Languages/c_cpp/#preprocessor-directives","text":"Declarative","title":"Preprocessor Directives"},{"location":"Languages/c_cpp/#define","text":"mainly used to define constants 1 #define BIGNUM 1000000","title":"define"},{"location":"Languages/c_cpp/#include","text":"used to access function definitions defined outside of a source code file 1 2 #include <stdio.h> #include \"somelocalcode.h\" or 1 #include \"somelocalcode.c\" Conditional","title":"include"},{"location":"Languages/c_cpp/#if-elif-else-ifdef-ifndef-endif","text":"","title":"if, #elif, #else, #ifdef, #ifndef, #endif"},{"location":"Languages/c_cpp/#gnu-compile-collection-gcc","text":"aka GNU C Compiler","title":"GNU Compile Collection (GCC)"},{"location":"Languages/c_cpp/#compile-using-gcc","text":"","title":"Compile using gcc"},{"location":"Languages/c_cpp/#background","text":"Lets say func.h have a function declarations : hello() and we are defining it in func.c file (func.c does not have main() function) now want to use the hello() function in source.c, so we can include func.h file, or func.c file (BAD IDEA)","title":"Background"},{"location":"Languages/c_cpp/#flow","text":"func.c & source.c both need #include \"func.h\" , because func.c is defining the code which backs the hello() function source.c is using/calling the hello() function and invoking its behavior, so it has to know the behavior & memory size need to allocate for the same but it does not need the actual definition/implementation of hello() yet the compiler will generate .o (object file, compiled but not executable) file func.o from func.c, and source.o from source.c which includes main method and unresolved reference of hello() function 1 2 gcc -c func.c gcc -c source.c here, func.o & source.o are not self executable because func.o doesn't have main() , and source.o have one un-resolved reference now, comes the linker linker will combine the two object files func.o & source.o into an executable file now it connects the dot between both the object files & resolves the un-resolved reference now, at run time, program can jump to the correct location 1 gcc func.o source.o -o program","title":"Flow"},{"location":"Languages/c_cpp/#gcc-misc","text":"","title":"gcc Misc"},{"location":"Languages/c_cpp/#is-there-any-need-to-compile-the-h-files","text":"No.","title":"is there any need to compile the .h files?"},{"location":"Languages/c_cpp/#how-to-organize-things-between-h-files-and-ccpp-file","text":"Put as much as you can in the .c and as little as possible in the .h. The includes in the .c are only included when that one file is compiled, but the includes for the .h have to be included by every file that uses it.","title":"How to organize things between .h files and .c/.cpp file?"},{"location":"Languages/c_cpp/#should-is-includeimport-ccpp-file-instead-h-because-it-just-works-fine","text":"No. Lets say .c file has definition of class Foo & this .c/.cpp file are used/referenced by multiple source code files (compilation units), then those all source code files will have definition of class Foo multiple times and which may cause a problem because linker will get confused and throw error.","title":"Should is include/import .c/.cpp file instead .h because it just works fine?"},{"location":"Languages/c_cpp/#c-keywords","text":"","title":"C++ Keywords"},{"location":"Languages/c_cpp/#expressions","text":"","title":"Expressions"},{"location":"Languages/c_cpp/#operators","text":"Arrow --> : Used to access classes, structure, or union member using pointer. e.g. 1 Dot . : Used to access classes, structure, or union member. e.g. 1 Scope Resolution :: : Qualifies the abstracted/hidden member/names e.g. 1 2 3 4 5 6 7 8 int count = 0 ; int main ( void ) { int count = 0 ; :: count = 1 ; // set global count to 1 count = 2 ; // set local count to 2 return 0 ; }","title":"Operators"},{"location":"Languages/c_cpp/#declaration","text":"","title":"Declaration"},{"location":"Languages/c_cpp/#function-definition","text":"","title":"Function definition"},{"location":"Languages/c_cpp/#template-declaration","text":"","title":"Template declaration"},{"location":"Languages/c_cpp/#explicit-template-instantiation","text":"","title":"Explicit template instantiation"},{"location":"Languages/c_cpp/#explicit-template-specialization","text":"","title":"Explicit template specialization"},{"location":"Languages/c_cpp/#namespace-definition","text":"","title":"Namespace definition"},{"location":"Languages/c_cpp/#linkage-specification","text":"extern \"C\" is a linkage-specification","title":"Linkage specification"},{"location":"Languages/c_cpp/#attribute-declaration-attr-since-c11","text":"","title":"Attribute declaration (attr ;) (since C++11)"},{"location":"Languages/c_cpp/#empty-declaration-since-c11","text":"","title":"Empty declaration (;) (since C++11)"},{"location":"Languages/c_cpp/#a-function-declaration-without-a-decl-specifier-seq","text":"","title":"A function declaration without a decl-specifier-seq:"},{"location":"Languages/c_cpp/#specifiers","text":"","title":"Specifiers"},{"location":"Languages/c_cpp/#declaration-specifiers","text":"typedef: typedef is used to give data type a new name e.g. 1 typedef unsigned char BYTE ;","title":"Declaration Specifiers"},{"location":"Languages/c_cpp/#type-specifiers","text":"class ABC enum Abc char name","title":"Type Specifiers"},{"location":"Languages/c_cpp/#declarators","text":"","title":"Declarators"},{"location":"Languages/c_cpp/#initialization","text":"","title":"Initialization"},{"location":"Languages/c_cpp/#functions","text":"","title":"Functions"},{"location":"Languages/c_cpp/#in-built","text":"strncpy(): char * strncpy ( char * destination, const char * source, size_t num ); copies characters from string","title":"In-built"},{"location":"Languages/c_cpp/#lambda","text":"","title":"Lambda"},{"location":"Languages/c_cpp/#pass-by-value","text":"","title":"Pass by Value"},{"location":"Languages/c_cpp/#pass-by-reference","text":"","title":"Pass by Reference"},{"location":"Languages/c_cpp/#statements","text":"","title":"Statements"},{"location":"Languages/c_cpp/#expression-statements","text":"","title":"Expression Statements"},{"location":"Languages/c_cpp/#compound-statements","text":"","title":"Compound Statements"},{"location":"Languages/c_cpp/#selection-statements","text":"","title":"Selection Statements"},{"location":"Languages/c_cpp/#label-statements","text":"","title":"Label Statements"},{"location":"Languages/c_cpp/#iteration-statements","text":"","title":"Iteration Statements"},{"location":"Languages/c_cpp/#jump-statements","text":"","title":"Jump Statements"},{"location":"Languages/c_cpp/#declaration-statements","text":"","title":"Declaration Statements"},{"location":"Languages/c_cpp/#try-blocks","text":"","title":"Try Blocks"},{"location":"Languages/c_cpp/#atomic-and-synchronized-blocks","text":"","title":"Atomic and Synchronized Blocks"},{"location":"Languages/c_cpp/#classes","text":"","title":"Classes"},{"location":"Languages/c_cpp/#templates","text":"","title":"Templates"},{"location":"Languages/c_cpp/#stl-standard-template-library","text":"The Standard Template Library (STL) is a set of C++ template classes to provide common programming data structures and functions such as lists, stacks, arrays, etc. It is a library of container classes, algorithms and iterators.","title":"STL (Standard Template Library)"},{"location":"Languages/c_cpp/#exceptions","text":"","title":"Exceptions"},{"location":"Languages/c_cpp/#misc","text":"","title":"Misc"},{"location":"Languages/c_cpp/#namespace-vs-include","text":"https://stackoverflow.com/questions/389922/c-namespace-and-include","title":"namespace vs include"},{"location":"Languages/c_cpp/#rest-api","text":"https://msdn.microsoft.com/en-us/magazine/dn342869.aspx","title":"REST API"},{"location":"Languages/c_cpp/#microservice-based-on-rest","text":"https://medium.com/audelabs/modern-c-micro-service-implementation-rest-api-b499ffeaf898 https://martinfowler.com/articles/microservices.html https://dev.otto.de/2016/03/20/why-microservices/","title":"Microservice Based on REST"},{"location":"Languages/markdown/","text":"Markdown # Markdown Math Inline Block Sources: 1. https://ia.net/writer/support/general/markdown-guide 2. https://www.markdownguide.org/basic-syntax/ 3. https://support.typora.io/Markdown-Reference/ Escape Dollar sign issue in Jupyter notebook 1 ### use of `$` <!--`$`--> (End of String Anchors) Math # $ $ Inline # uses single $ syntax: $\\theta$ output: \\theta \\theta Syntax 1 $p(x|y) = \\frac{p(y|x)p(x)}{p(y)}$, \\(p(x|y) = \\frac{p(y|x)p(x)}{p(y)}\\). Output p(x|y) = \\frac{p(y|x)p(x)}{p(y)} p(x|y) = \\frac{p(y|x)p(x)}{p(y)} , p(x|y) = \\frac{p(y|x)p(x)}{p(y)} p(x|y) = \\frac{p(y|x)p(x)}{p(y)} . Block # Double Dollar Syntax 1 2 3 $$ \\operatorname{ker} f=\\{g\\in G:f(g)=e_{H}\\}{\\mbox{.}} $$ Output \\operatorname{ker} f=\\{g\\in G:f(g)=e_{H}\\}{\\mbox{.}} \\operatorname{ker} f=\\{g\\in G:f(g)=e_{H}\\}{\\mbox{.}}","title":"Markdown"},{"location":"Languages/markdown/#markdown","text":"Markdown Math Inline Block Sources: 1. https://ia.net/writer/support/general/markdown-guide 2. https://www.markdownguide.org/basic-syntax/ 3. https://support.typora.io/Markdown-Reference/ Escape Dollar sign issue in Jupyter notebook 1 ### use of `$` <!--`$`--> (End of String Anchors)","title":"Markdown"},{"location":"Languages/markdown/#math","text":"$ $","title":"Math"},{"location":"Languages/markdown/#inline","text":"uses single $ syntax: $\\theta$ output: \\theta \\theta Syntax 1 $p(x|y) = \\frac{p(y|x)p(x)}{p(y)}$, \\(p(x|y) = \\frac{p(y|x)p(x)}{p(y)}\\). Output p(x|y) = \\frac{p(y|x)p(x)}{p(y)} p(x|y) = \\frac{p(y|x)p(x)}{p(y)} , p(x|y) = \\frac{p(y|x)p(x)}{p(y)} p(x|y) = \\frac{p(y|x)p(x)}{p(y)} .","title":"Inline"},{"location":"Languages/markdown/#block","text":"Double Dollar Syntax 1 2 3 $$ \\operatorname{ker} f=\\{g\\in G:f(g)=e_{H}\\}{\\mbox{.}} $$ Output \\operatorname{ker} f=\\{g\\in G:f(g)=e_{H}\\}{\\mbox{.}} \\operatorname{ker} f=\\{g\\in G:f(g)=e_{H}\\}{\\mbox{.}}","title":"Block"},{"location":"Languages/golang/go_basics/","text":"Golang Basics # Golang Basics Introduction How Go is different than other languages Install Setup Run Memory Management Stack vs Heap Garbage Collection Keywords package import type const var func struct Declaration Data Type bool int string UTF-8 and string literals byte Code points, characters, and runes Data Structure Array Slice Map Types & interfaces interface{} The representation of an interface Reflection The first law of reflection - Reflection goes from interface value to reflection object The second law of reflection - Reflection goes from reflection object to interface value The third law of reflection - To modify a reflection object, the value must be settable Statements Control Flows if, else for switch goto range Functions First Class User-defined Higher-order Closures Multiple Return Values Variadic Anonymous/Lambda Access Modifier Struct Methods Interface Constructor goroutine go defer panic recover goroutine vs threads Channels Channel Buffering Channel Synchromization Channel Directions select Timeouts Non-blocking Channel operations Closing Channels range over channels Modules go mod Semantic Import Versioning (SEMVER) Package main package Vs directory Built-in Packages timer & ticker Import Inbuilt Package Intra Package Inter Package Remote Package Cyclic json Encoding Decoding Generic JSON with interface{} Creating arbitrary data Encoding arbitrary data Decoding arbitrary data Json & Reference Type (pointers, slices, maps) Streaming Encoders and Decoders Misc regexp Errors Error Handling Testing Testcase Benchmarks Examples Skipping Subtests and Sub-benchmarks Main Run Coverage Debug gdb Delve Profiling pprof CPU Profiling - stack sampling (vs instrumentation) Heap Profiling - allocation profiling Block Trace Deploy CircleCI Run Test Misc generate Advance Testing assert Extra Best Practices Naming Convetions Code Organization Documentation Godoc Best Practices Concepts Compilation Static Typing Pointer Garbage Collection Debugging gdb Delve Profiling Design Patterns References Surprises Introduction # intially developed at Google - 2007 by Robert Griesemer, Rob Pike, and Ken Thompson is a statically-typed language with syntax similar to that of C provides garbage collection type safety dynamic-typing capability many advanced built-in types such as variable length arrays key-value maps heap a rich standard library is expressive, concise, clean, and efficient its concurrency mechanisms make it easy to write programs that get the most out of multi core and networked machines compiles quickly to machine code yet has the convenience of garbage collection and the power of run-time reflection runtime reflection in go garbage collection in go How Go is different than other languages # native concurrency support - i.e. at language-level different GC design single executable - copy/paste & deploy no dynamic/linked libraries keep lang. simple & expressive directly compiles to machine code - no virtual runtime or interpreter concept interior pointer concept Install # Setup # Run # Memory Management # https://deepu.tech/memory-management-in-golang/ https://medium.com/eureka-engineering/understanding-allocations-in-go-stack-heap-memory-9a2631b5035d Stack vs Heap # https://stackoverflow.com/questions/10866195/stack-vs-heap-allocation-of-structs-in-go-and-how-they-relate-to-garbage-collec Garbage Collection # https://blog.golang.org/ismmkeynote https://golang.org/doc/faq#garbage_collection Keywords # package # import # type # const # var # func # struct # Declaration # Data Type # bool # int # string # https://blog.golang.org/strings In Go, a string is in effect a read-only slice of bytes string holds arbitrary bytes It is not required to hold Unicode text, UTF-8 text, or any other predefined format string literal that uses \\xNN notation to define a string constant holding some peculiar byte values Of course, bytes range from hexadecimal values 00 through FF, inclusive 1 2 3 const sample = \"\\xbd\\xb2\\x3d\\xbc\\x20\\xe2\\x8c\\x98\" fmt . Println ( sample ) // out: \ufffd\ufffd=\ufffd \u2318 // in our sample string are not valid ASCII, not even valid UTF-8, printing the string directly will produce ugly output NOTE : indexing/iterating over string gives bytes and not characters in Go that also means, when we store a character in a string, we store its byte representation UTF-8 and string literals # placeOfInterest = \u2318 Unicode character value U+2318 1 2 3 4 5 6 7 8 9 const placeOfInterest = `\u2318` fmt . Printf ( \"%s\" , placeOfInterest ) // plain string: \u2318 fmt . Printf ( \"%+q\" , placeOfInterest ) // quoted string: \"\\u2318\" for i := 0 ; i < len ( placeOfInterest ); i ++ { fmt . Printf ( \"%x \" , placeOfInterest [ i ]) // hex bytes: e2 8c 98 } NOTE : Source code in Go is defined to be UTF-8 text; no other representation is allowed byte # Code points, characters, and runes # https://blog.golang.org/strings Given lower case Latin letter 'A': a A: unicode code point: U+0061 lower case grave-accented letter 'A', \u00e0 B: unicode code point: U+00E0 grave accent code point (that sign above a): C: unicode code point: U+0300 Then \u00e0 == A + C or \u00e0 == B code point == rune code point is a bit of mouthful, hence go introduces shorter term rune rune meaning: any of the characters of any of several alphabets character when we store a character value in a string, we store its byte-at-a-time representation In general, a character may be represented by a number of different sequences of code points, and therefore different sequences of UTF-8 bytes The concept of character in computing is therefore ambiguous, or at least confusing, so we use it with care NOTE : Go team have been very careful so far in how we use the words \"byte\" and \"character\" That's partly because strings hold bytes, and partly because the idea of \"character\" is a little hard/ambiguous to define Data Structure # Array # 1 2 3 4 5 var a [ 5 ] int fmt . Println ( \"emp:\" , a ) a [ 4 ] = 100 fmt . Println ( \"set:\" , a ) fmt . Println ( \"get:\" , a [ 4 ]) Slice # 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 s := make ([] string , 3 ) fmt . Println ( \"emp:\" , s ) s [ 0 ] = \"a\" s [ 1 ] = \"b\" s [ 2 ] = \"c\" fmt . Println ( \"set:\" , s ) fmt . Println ( \"get:\" , s [ 2 ]) fmt . Println ( \"len:\" , len ( s )) s = append ( s , \"d\" ) s = append ( s , \"e\" , \"f\" ) c := make ([] string , len ( s )) copy ( c , s ) l := s [ 2 : 5 ] t := [] string { \"g\" , \"h\" , \"i\" } // TODO: len vs size?? Map # 1 2 3 4 5 6 7 8 9 10 11 12 13 14 m := make ( map [ string ] int ) m [ \"k1\" ] = 7 v1 := m [ \"k1\" ] fmt . Println ( \"len:\" , len ( m )) delete ( m , \"k2\" ) // no error isThere , value := m [ \"k2\" ] // out: false, 0 isThere , value := m [ \"k1\" ] // out: true, 7 n := map [ string ] int { \"foo\" : 1 , \"bar\" : 2 } n := map [ string ] interface {}{ \"foo\" : 1 , \"bar\" : \"some string\" } Types & interfaces # Go is statically typed Every variable has a static type, that is, exactly one type known and fixed at compile time: int, float32, *MyType, []byte, and so on interface is one important category of type - which represent fixed/minimal sets of methods If we declare 1 2 3 4 type MyInt int var i int var j MyInt then i has type int and j has type MyInt variables i and j have distinct static types and, although they have the same underlying type, they cannot be assigned to one another without a conversion An interface variable can store any concrete (non-interface) value as long as that value implements the interface's methods. 1 2 3 4 5 var r io . Reader r = os . Stdin r = bufio . NewReader ( r ) r = new ( bytes . Buffer ) // and so on Here r can exhibit lot more methods than Read() of io.Reader . Its not limited to methods provided by interface io.Reader . An extremely important example of an interface type is the empty interface: interface{} interface{} # A.K.A. Empty Interface It represents the empty set of methods and is satisfied by any value at all, since any value has zero or more methods. interface{} (empty interface) type describes an interface with zero methods Every Go type implements at least zero methods therefore satisfies the empty interface so we can receive any go data type in an empty interface (similar to object/var in other langs.) 1 2 3 4 5 6 var i interface{} i = \"a string\" i = 2011 i = 2.777 fmt.Println() https://github.com/toransahu/go-misc/blob/master/interfac/main.go A variable of interface type always has the same static type, and even though at run time the value stored in the interface variable may change type, that value will always satisfy the interface. The representation of an interface # A variable of type interface type stores a pair: a concrete value assigned to the variable, and that concrete values's type/type-descriptor. e.g. 1 2 3 4 5 6 var r io . Reader tty , err := os . OpenFile ( \"/dev/tty\" , os . O_RDWR , 0 ) if err != nil { return nil , err } r = tty here r contains pair (value, concrete type) == (tty, *os.File). As a interface variable also stores the type details, we can do things like this: 1 2 3 4 5 6 7 8 9 10 11 12 var w io . Writer w = r .( io . Writer ) // The expression in this assignment is a type assertion // it asserts is that the item inside r also implements io.Writer, and so we can assign it to w // or var s interface {} s = \"hello\" ss := s .( string ) // The expression in this assignment is a type assertion // or ss , ok := s .( string ) this is called type assertion . Reflection # Reflection in computing is the ability of a program to examine its own structure, particularly through types; it's a form of metaprogramming. It's also a great source of confusion. reflection builds on the type system The first law of reflection - Reflection goes from interface value to reflection object # At the basic level, reflection is just a mechanism to examine the type and value pair stored inside an interface variable there are two types we need to know about in package reflect: Type and Value Those two types give access to the contents of an interface variable, and two simple functions, called reflect.TypeOf() and reflect.ValueOf() tbd The second law of reflection - Reflection goes from reflection object to interface value # tbd The third law of reflection - To modify a reflection object, the value must be settable # tbd Statements # Control Flows # if , else # 1 2 3 4 5 6 7 if num := 9 ; num < 0 { fmt . Println ( num , \"is negative\" ) } else if num < 10 { fmt . Println ( num , \"has 1 digit\" ) } else { fmt . Println ( num , \"has multiple digits\" ) } NOTE : there is no ternary ? if else condition for # for is the only looping construct in Go for have 3-4 pattern 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 // like while loop i := 1 for i <= 3 { fmt . Println ( i ) i = i + 1 } // regular for loop for j := 7 ; j <= 9 ; j ++ { fmt . Println ( j ) } for n := 0 ; n <= 5 ; n ++ { if n % 2 == 0 { continue } fmt . Println ( n ) } // infinite loop (or with break for { fmt . Println ( \"loop\" ) break } switch # 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 // regular switch case i := 2 fmt . Print ( \"Write \" , i , \" as \" ) switch i { case 1 : fmt . Println ( \"one\" ) case 2 : fmt . Println ( \"two\" ) case 3 : fmt . Println ( \"three\" ) } // TODO: switch time . Now (). Weekday () { case time . Saturday , time . Sunday : fmt . Println ( \"It's the weekend\" ) default : fmt . Println ( \"It's a weekday\" ) } // regular if else t := time . Now () switch { case t . Hour () < 12 : fmt . Println ( \"It's before noon\" ) default : fmt . Println ( \"It's after noon\" ) } // interface type assertion whatAmI := func ( i interface {}) { switch t := i .( type ) { case bool : fmt . Println ( \"I'm a bool\" ) case int : fmt . Println ( \"I'm an int\" ) default : fmt . Printf ( \"Don't know type %T\\n\" , t ) } } whatAmI ( true ) whatAmI ( 1 ) whatAmI ( \"hey\" ) goto # why its still valid in a new-gen prog. lang. ?? range # 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 nums := [] int { 2 , 3 , 4 } sum := 0 // range on arrays and slices provides both the index and value for each entry for _ , num := range nums { sum += num } for i , num := range nums { if num == 3 { fmt . Println ( \"index:\" , i ) } } // range on map iterates over key/value pairs kvs := map [ string ] string { \"a\" : \"apple\" , \"b\" : \"banana\" } for k , v := range kvs { fmt . Printf ( \"%s -> %s\\n\" , k , v ) } // range can also iterate over just the keys of a map for k := range kvs { fmt . Println ( \"key:\" , k ) } // range on strings iterates over Unicode code points // The first value is the starting byte index of the rune and the second the rune itself. for i , c := range \"go\" { fmt . Println ( i , c ) } // out: 0 103 1 111 Functions # src: https://golang.org/doc/codewalk/functions/ First Class # User-defined # Higher-order # a function which accepts another function as a arg Closures # a concept of having access of outer scope in an Anonynous function, function literals are closures: they inherit the scope of the function in which they are declared same as python, javascript Multiple Return Values # similar to pl/sql procedures 1 func () string , int { return \"Yes\" , 1 } Variadic # variable/arbitrary numbers of arguments similar to *args variable [space] ... concateanted to type vardiac arg is always a slice builtin e.g. fmt.Println 1 2 3 4 5 6 7 8 9 10 11 12 func sum ( nums ... int ) int { total := 0 for _ , number := range nums { total += number } return total } func main () { sum ( 1 , 2 ) sum ( 1 , 2 , 3 ) } can directly pass a slice like 1 2 num_slice := [] int { 1 , 2 , 3 } sum ( num_slice ... ) Anonymous/Lambda # 1 2 3 func ( msg string ) { fmt . Println ( msg ) }( \"Some message\" ) Access Modifier # depends on CASE of the func if starts with Capital case --> Public should have comment/doc string else private Struct # regular struct in go, struct are alternative to classes 1 2 3 4 5 6 7 8 9 10 11 type Vehicle struct { Make string `json:make` // additional json tag for de/serialization Fuel string Engine Engine owner string // not an exportable/exposed/public key/object } type Engine struct { Stroke string HorsePower string } Methods # 1 2 3 4 5 6 7 func ( v * Vehicle ) Start () ( string , error ) { return \"Vrooom\" , nil } engine := Engine { \"Four\" , \"1000\" } car := Vehicle { \"Tesla\" , \"Li\" , engine , \"Toran\" } res , err := car . Start () Interface # Interfaces are named collections of method signatures https://gobyexample.com/interfaces Constructor # goroutine # A goroutine is a lightweight thread of execution go # 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 func f ( from string ) { for i := 0 ; i < 3 ; i ++ { fmt . Println ( from , \":\" , i ) } } func main () { f ( \"direct\" ) // direct call go f ( \"goroutine\" ) // goroutine call go func ( msg string ){ fmt . Println ( msg ) }( \"goroutine from anonymous func\" ) fmt . Scanln () // input from stdio fmt . Println ( \"done\" ) } // out: direct : 0 direct : 1 direct : 2 goroutine : 0 goroutine from anonymous func goroutine : 1 goroutine : 2 < enter > done by going through above mentioned example: if we want to invoke the function f as a goroutine we call it using go statement func f will execute concurrently with the calling/main one we can also start a goroutine for an anonymous func our two function calls are running asynchronously in separate goroutines now we see output of blocking/synchronous call first then the interleaved output of two goroutines (output order may vary system to system) defer # tbd https://blog.golang.org/defer-panic-and-recover panic # recover # goroutine vs threads # can run more number of goroutines on a typical system than can threads goroutine are managed by go runtime, and have been designed to be lightweight thus the startup time is low its lightweight because it gets assigned very minimal memory, and that could be increased on demand that called growable segmented stack goroutine comes with in-built primitives called channels to communicate safely between themselves goroutines are multiplexed onto a small number of OS threads, rather than 1:1 mapping Channels # https://gobyexample.com/channels Channels are the pipes that connect concurrent goroutines You can send values into channels from one goroutine and receive those values into another goroutine 1 2 3 4 5 messages := make ( chan string ) // create a channel named message go func () { messages <- \"ping\" }() // from go routine `send` a string value/msg \"ping\" to the channel msg := <- messages // `receive` value/msg from the channel fmt . Println ( msg ) fmt . Println ( msg .( string ) // assert type NOTE : By default sends and receives block until both the sender and receiver are ready. This property allowed us to wait at the end of our program for the \"ping\" message without having to use any other synchronization. meaning that, whenever there will be receiver (and is ready to receive the value) then only sender can send the value/msg to the channel see: https://github.com/toransahu/go-misc/blob/master/goroutines/channels/channels.go#L30 hence we can say, by default go channels are unbufferred Channel Buffering # we checked above NOTE about readiness of senders & receivers channel buffering is to alter that nature & keep the value/msg in the buffer Buffered channels accept a limited number of values without a corresponding receiver for those values 1 2 3 4 5 message := make ( chan string , 2 ) // here 2 is capacity of the buffer message <- \"value 1\" message <- \"value 2\" fmt . Println ( <- message ) fmt . Println ( <- message ) https://github.com/toransahu/go-misc/blob/master/goroutines/channels/buffered/buffered_channels.go Channel Synchromization # tbd https://gobyexample.com/channel-synchronization Channel Directions # tbd https://gobyexample.com/channel-directions select # tbd https://gobyexample.com/select Timeouts # tbd https://gobyexample.com/timeouts Non-blocking Channel operations # tbd https://gobyexample.com/non-blocking-channel-operations Closing Channels # tbd https://gobyexample.com/closing-channels range over channels # tbd https://gobyexample.com/range-over-channels Modules # A module is a collection of related Go packages that are versioned together as a single unit. Ref: https://github.com/golang/go/wiki/Modules#modules https://github.com/golang/go/wiki/Modules go mod # to manage [versioned] dependencies was out with go 1.11 with preliminary/provisionary support & target to finalizing the feature for 1.14 (considering all feedbacks since 1.11-1.13) don't need to live the code in GOPATH creates/uses go.mod file the initial prototype vgo was announced in February 2018 other alternatives were: dep, gom etc. management of interdependencies: vgo's controversial algo uses the oldest common version to support stability this may while discard the acceptance of any security bug fixed in newer version e.g. package A uses B and B uses D's atleast 1.0 version package A also uses C and C uses D's either 1.0 or 1.1 version then, as per vgo vgo versioned go manages all the algorithm of versioning the go projects/packages/modules cmd vgo build is capable of generating go.mod creates/manages versioned cache packages inside GOPATH/src/v there are support for more than one module in repository, but general idea is one As of Go 1.11, the go command enables the use of modules when the current directory or any parent directory has a go.mod, provided the directory is outside GOPATH/src. (Inside GOPATH/src, for compatibility, the go command still runs in the old GOPATH mode, even if a go.mod is found. See the go command documentation for details.) Starting in Go 1.13, module mode will be the default for all development. In addition to go.mod, the go command maintains a file named go.sum containing the expected cryptographic hashes of the content of specific module versions to maintain the intigrity of the go.mod Semantic Import Versioning (SEMVER) # way to handle major dependency changes like: v1 to v2, v3,... API/interface changes how /github.com/toransahu/log (contains all v1.x.x) & /github.com/toransahu/log/v2 (contains all v2.x.x) Ref: https://research.swtch.com/vgo-import Package # main # package Vs directory # Built-in Packages # timer & ticker # tbd https://gobyexample.com/timers https://gobyexample.com/tickers continue exploring other packages from here Import # Inbuilt Package # Intra Package # Inter Package # Remote Package # Cyclic # json # Encoding # func Marshal(v interface{}) ([]byte, error) Only data structures that can be represented as valid JSON will be encoded: JSON objects only support strings as keys; to encode a Go map type it must be of the form map[string]T (where T is any Go type supported by the json package). TODO: Channel, complex, and function types cannot be encoded. TODO: Cyclic data structures are not supported; they will cause Marshal to go into an infinite loop. Pointers will be encoded as the values they point to (or 'null' if the pointer is nil) The json package only accesses the exported fields of struct types (those that begin with an uppercase letter). Therefore only the the exported fields of a struct will be present in the JSON output. TODO: what if alias is also provided? we can also provide json:\"alias\" for each field in the struct so that those fields will be encoded as per aliases shortcut to generate those: vim-go: visual select -> <leader> GoAddTags GoLand: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 // data structure, Message type Message struct { Name string `json:\"name\"` Body string `json:\"body\"` Time int64 `json:\"time\"` } // an instance of Message m := Message { \"Alice\" , \"Hello\" , 1294706395881547000 } // encoding m b , err := json . Marshal ( m ) // If all is well, err will be nil and b will be a []byte containing this JSON data b == [] byte ( `{\"Name\":\"Alice\",\"Body\":\"Hello\",\"Time\":1294706395881547000}` ) https://github.com/toransahu/go-misc/blob/master/json/encoding.go Decoding # https://github.com/toransahu/go-misc/blob/master/json/decoding.go Generic JSON with interface{} # The json package uses map[string]interface{} and []interface{} values to store arbitrary JSON objects and arrays it will happily unmarshal any valid JSON blob into a plain interface{} value default concrete Go types are: bool for JSON booleans float64 for JSON numbers string for JSON strings nil for JSON null Creating arbitrary data # 1 2 3 i := map [ string ] interface {}{ \"id\" : 1111 , \"name\" : \"Toran\" } fmt . Println ( i ) } Encoding arbitrary data # 1 bytes := json . Marshal ( i ) Decoding arbitrary data # https://github.com/toransahu/go-misc/blob/master/json/json_test.go#L33 1 2 var i interface {} json . Unmarshal ( bytes , & i ) Json & Reference Type (pointers, slices, maps) # https://blog.golang.org/json-and-go https://github.com/toransahu/go-misc/blob/master/jsons/reference_type.go#L53 Streaming Encoders and Decoders # https://blog.golang.org/json-and-go Misc # https://vsupalov.com/go-json-omitempty/ regexp # Ref: https://golang.org/pkg/regexp/ https://github.com/google/re2/wiki/Syntax https://shapeshed.com/golang-regexp/ Errors # https://gobyexample.com/errors Error Handling # https://blog.golang.org/error-handling-and-go Testing # test module should named as *_test.go under packages FIXME: better to organize test modules in a separate package - similar to Java?? Testcase # Benchmarks # Examples # Skipping # Subtests and Sub-benchmarks # Main # Run # a test file ~/go/src/github.com/toransahu/go-misc/json on \ue0a0 master! \u231a 14:18:45 $ go test json_test.go config.go encoding.go ok command-line-arguments 0.003s all test files under a package ~/go/src/github.com/toransahu/go-misc/json on \ue0a0 master! \u231a 14:18:56 $ go test PASS ok github.com/toransahu/go-misc/json 0.002s all test files in all packages tbd specific test function of a test file tbd specific test function/file with pattern/regex tbd Coverage # https://blog.alexellis.io/golang-writing-unit-tests/ src: https://golang.org/pkg/testing/#hdr-Subtests_and_Sub_benchmarks https://stackoverflow.com/questions/16935965/how-to-run-test-cases-in-a-specified-file Debug # gdb # https://golang.org/doc/gdb Delve # Profiling # pprof # https://blog.golang.org/pprof CPU Profiling - stack sampling (vs instrumentation) # Heap Profiling - allocation profiling # Block # Trace # Deploy # CircleCI # Run Test # Misc # generate # automatically generate golang code for a particular purpose like print name of memebers in a struct stringr https://blog.golang.org/generate can set header/preprocessor (with commands) in the go file to do the job on each build Advance Testing # assert # 3rd party packages for assert.* https://github.com/stretchr/testify Extra # https://talks.golang.org/2012/10things.slide#3 Best Practices # Naming Convetions # https://golang.org/doc/effective_go.html#names https://blog.golang.org/package-names Code Organization # Package main test https://blog.golang.org/organizing-go-code inside a directory all modules with only single package name can only execute/run main package module hence main package func main should be declared to run the main package module Documentation # to document a type, variable, constant, function, or even a package, write a regular comment directly preceding its declaration, with no intervening blank line a complete sentence begins with the name of the element similar to python 's Docstring & java 's Javadoc but simpler than them src: https://blog.golang.org/godoc-documenting-go-code 1 2 3 4 5 6 7 8 9 10 // Package sort provides primitives for sorting slices and user-defined // collections. package sort ... // Fprint formats using the default formats for its operands and writes to w. // Spaces are added between operands when neither is a string. // It returns the number of bytes written and any write error encountered. func Fprint ( w io . Writer , a ... interface {}) ( n int , err error ) { extra: do something like this to achieve this notable keywords: Deprecated: BUG(<contact person>) Godoc # Best Practices # Concepts # Compilation # Static Typing # Pointer # Garbage Collection # Go language features, goals, and use cases have forced to rethink the entire garbage collection stack and have led to a surprising place Go programs have hundreds of thousands of stacks They are managed by the Go scheduler and are always preempted at GC safepoints The Go scheduler multiplexes Go routines onto OS threads which hopefully run with one OS thread per HW thread We manage the stacks and their size by copying them and updating pointers in the stack. It's a local operation so it scales fairly well. Ref: https://blog.golang.org/ismmkeynote https://www.ardanlabs.com/blog/2018/12/garbage-collection-in-go-part1-semantics.html Debugging # gdb # https://golang.org/doc/gdb https://astaxie.gitbooks.io/build-web-application-with-golang/content/en/11.2.html Delve # Profiling # https://blog.golang.org/pprof Design Patterns # Functional options for friendly APIs Self referential functions and design References # https://golang.org/doc/code.html https://github.com/golang/go/wiki/Learn https://go.dev/about/#best-practices-h2 Surprises # https://medium.com/@karel_3d/things-that-surprised-me-in-go-47bccce94558 https://utcc.utoronto.ca/~cks/space/blog/programming/GoInteriorPointerGC https://dave.cheney.net/2017/04/29/there-is-no-pass-by-reference-in-go","title":"Golang Basics"},{"location":"Languages/golang/go_basics/#golang-basics","text":"Golang Basics Introduction How Go is different than other languages Install Setup Run Memory Management Stack vs Heap Garbage Collection Keywords package import type const var func struct Declaration Data Type bool int string UTF-8 and string literals byte Code points, characters, and runes Data Structure Array Slice Map Types & interfaces interface{} The representation of an interface Reflection The first law of reflection - Reflection goes from interface value to reflection object The second law of reflection - Reflection goes from reflection object to interface value The third law of reflection - To modify a reflection object, the value must be settable Statements Control Flows if, else for switch goto range Functions First Class User-defined Higher-order Closures Multiple Return Values Variadic Anonymous/Lambda Access Modifier Struct Methods Interface Constructor goroutine go defer panic recover goroutine vs threads Channels Channel Buffering Channel Synchromization Channel Directions select Timeouts Non-blocking Channel operations Closing Channels range over channels Modules go mod Semantic Import Versioning (SEMVER) Package main package Vs directory Built-in Packages timer & ticker Import Inbuilt Package Intra Package Inter Package Remote Package Cyclic json Encoding Decoding Generic JSON with interface{} Creating arbitrary data Encoding arbitrary data Decoding arbitrary data Json & Reference Type (pointers, slices, maps) Streaming Encoders and Decoders Misc regexp Errors Error Handling Testing Testcase Benchmarks Examples Skipping Subtests and Sub-benchmarks Main Run Coverage Debug gdb Delve Profiling pprof CPU Profiling - stack sampling (vs instrumentation) Heap Profiling - allocation profiling Block Trace Deploy CircleCI Run Test Misc generate Advance Testing assert Extra Best Practices Naming Convetions Code Organization Documentation Godoc Best Practices Concepts Compilation Static Typing Pointer Garbage Collection Debugging gdb Delve Profiling Design Patterns References Surprises","title":"Golang Basics"},{"location":"Languages/golang/go_basics/#introduction","text":"intially developed at Google - 2007 by Robert Griesemer, Rob Pike, and Ken Thompson is a statically-typed language with syntax similar to that of C provides garbage collection type safety dynamic-typing capability many advanced built-in types such as variable length arrays key-value maps heap a rich standard library is expressive, concise, clean, and efficient its concurrency mechanisms make it easy to write programs that get the most out of multi core and networked machines compiles quickly to machine code yet has the convenience of garbage collection and the power of run-time reflection runtime reflection in go garbage collection in go","title":"Introduction"},{"location":"Languages/golang/go_basics/#how-go-is-different-than-other-languages","text":"native concurrency support - i.e. at language-level different GC design single executable - copy/paste & deploy no dynamic/linked libraries keep lang. simple & expressive directly compiles to machine code - no virtual runtime or interpreter concept interior pointer concept","title":"How Go is different than other languages"},{"location":"Languages/golang/go_basics/#install","text":"","title":"Install"},{"location":"Languages/golang/go_basics/#setup","text":"","title":"Setup"},{"location":"Languages/golang/go_basics/#run","text":"","title":"Run"},{"location":"Languages/golang/go_basics/#memory-management","text":"https://deepu.tech/memory-management-in-golang/ https://medium.com/eureka-engineering/understanding-allocations-in-go-stack-heap-memory-9a2631b5035d","title":"Memory Management"},{"location":"Languages/golang/go_basics/#stack-vs-heap","text":"https://stackoverflow.com/questions/10866195/stack-vs-heap-allocation-of-structs-in-go-and-how-they-relate-to-garbage-collec","title":"Stack vs Heap"},{"location":"Languages/golang/go_basics/#garbage-collection","text":"https://blog.golang.org/ismmkeynote https://golang.org/doc/faq#garbage_collection","title":"Garbage Collection"},{"location":"Languages/golang/go_basics/#keywords","text":"","title":"Keywords"},{"location":"Languages/golang/go_basics/#package","text":"","title":"package"},{"location":"Languages/golang/go_basics/#import","text":"","title":"import"},{"location":"Languages/golang/go_basics/#type","text":"","title":"type"},{"location":"Languages/golang/go_basics/#const","text":"","title":"const"},{"location":"Languages/golang/go_basics/#var","text":"","title":"var"},{"location":"Languages/golang/go_basics/#func","text":"","title":"func"},{"location":"Languages/golang/go_basics/#struct","text":"","title":"struct"},{"location":"Languages/golang/go_basics/#declaration","text":"","title":"Declaration"},{"location":"Languages/golang/go_basics/#data-type","text":"","title":"Data Type"},{"location":"Languages/golang/go_basics/#bool","text":"","title":"bool"},{"location":"Languages/golang/go_basics/#int","text":"","title":"int"},{"location":"Languages/golang/go_basics/#string","text":"https://blog.golang.org/strings In Go, a string is in effect a read-only slice of bytes string holds arbitrary bytes It is not required to hold Unicode text, UTF-8 text, or any other predefined format string literal that uses \\xNN notation to define a string constant holding some peculiar byte values Of course, bytes range from hexadecimal values 00 through FF, inclusive 1 2 3 const sample = \"\\xbd\\xb2\\x3d\\xbc\\x20\\xe2\\x8c\\x98\" fmt . Println ( sample ) // out: \ufffd\ufffd=\ufffd \u2318 // in our sample string are not valid ASCII, not even valid UTF-8, printing the string directly will produce ugly output NOTE : indexing/iterating over string gives bytes and not characters in Go that also means, when we store a character in a string, we store its byte representation","title":"string"},{"location":"Languages/golang/go_basics/#utf-8-and-string-literals","text":"placeOfInterest = \u2318 Unicode character value U+2318 1 2 3 4 5 6 7 8 9 const placeOfInterest = `\u2318` fmt . Printf ( \"%s\" , placeOfInterest ) // plain string: \u2318 fmt . Printf ( \"%+q\" , placeOfInterest ) // quoted string: \"\\u2318\" for i := 0 ; i < len ( placeOfInterest ); i ++ { fmt . Printf ( \"%x \" , placeOfInterest [ i ]) // hex bytes: e2 8c 98 } NOTE : Source code in Go is defined to be UTF-8 text; no other representation is allowed","title":"UTF-8 and string literals"},{"location":"Languages/golang/go_basics/#byte","text":"","title":"byte"},{"location":"Languages/golang/go_basics/#code-points-characters-and-runes","text":"https://blog.golang.org/strings Given lower case Latin letter 'A': a A: unicode code point: U+0061 lower case grave-accented letter 'A', \u00e0 B: unicode code point: U+00E0 grave accent code point (that sign above a): C: unicode code point: U+0300 Then \u00e0 == A + C or \u00e0 == B code point == rune code point is a bit of mouthful, hence go introduces shorter term rune rune meaning: any of the characters of any of several alphabets character when we store a character value in a string, we store its byte-at-a-time representation In general, a character may be represented by a number of different sequences of code points, and therefore different sequences of UTF-8 bytes The concept of character in computing is therefore ambiguous, or at least confusing, so we use it with care NOTE : Go team have been very careful so far in how we use the words \"byte\" and \"character\" That's partly because strings hold bytes, and partly because the idea of \"character\" is a little hard/ambiguous to define","title":"Code points, characters, and runes"},{"location":"Languages/golang/go_basics/#data-structure","text":"","title":"Data Structure"},{"location":"Languages/golang/go_basics/#array","text":"1 2 3 4 5 var a [ 5 ] int fmt . Println ( \"emp:\" , a ) a [ 4 ] = 100 fmt . Println ( \"set:\" , a ) fmt . Println ( \"get:\" , a [ 4 ])","title":"Array"},{"location":"Languages/golang/go_basics/#slice","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 s := make ([] string , 3 ) fmt . Println ( \"emp:\" , s ) s [ 0 ] = \"a\" s [ 1 ] = \"b\" s [ 2 ] = \"c\" fmt . Println ( \"set:\" , s ) fmt . Println ( \"get:\" , s [ 2 ]) fmt . Println ( \"len:\" , len ( s )) s = append ( s , \"d\" ) s = append ( s , \"e\" , \"f\" ) c := make ([] string , len ( s )) copy ( c , s ) l := s [ 2 : 5 ] t := [] string { \"g\" , \"h\" , \"i\" } // TODO: len vs size??","title":"Slice"},{"location":"Languages/golang/go_basics/#map","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 m := make ( map [ string ] int ) m [ \"k1\" ] = 7 v1 := m [ \"k1\" ] fmt . Println ( \"len:\" , len ( m )) delete ( m , \"k2\" ) // no error isThere , value := m [ \"k2\" ] // out: false, 0 isThere , value := m [ \"k1\" ] // out: true, 7 n := map [ string ] int { \"foo\" : 1 , \"bar\" : 2 } n := map [ string ] interface {}{ \"foo\" : 1 , \"bar\" : \"some string\" }","title":"Map"},{"location":"Languages/golang/go_basics/#types-interfaces","text":"Go is statically typed Every variable has a static type, that is, exactly one type known and fixed at compile time: int, float32, *MyType, []byte, and so on interface is one important category of type - which represent fixed/minimal sets of methods If we declare 1 2 3 4 type MyInt int var i int var j MyInt then i has type int and j has type MyInt variables i and j have distinct static types and, although they have the same underlying type, they cannot be assigned to one another without a conversion An interface variable can store any concrete (non-interface) value as long as that value implements the interface's methods. 1 2 3 4 5 var r io . Reader r = os . Stdin r = bufio . NewReader ( r ) r = new ( bytes . Buffer ) // and so on Here r can exhibit lot more methods than Read() of io.Reader . Its not limited to methods provided by interface io.Reader . An extremely important example of an interface type is the empty interface: interface{}","title":"Types &amp; interfaces"},{"location":"Languages/golang/go_basics/#interface","text":"A.K.A. Empty Interface It represents the empty set of methods and is satisfied by any value at all, since any value has zero or more methods. interface{} (empty interface) type describes an interface with zero methods Every Go type implements at least zero methods therefore satisfies the empty interface so we can receive any go data type in an empty interface (similar to object/var in other langs.) 1 2 3 4 5 6 var i interface{} i = \"a string\" i = 2011 i = 2.777 fmt.Println() https://github.com/toransahu/go-misc/blob/master/interfac/main.go A variable of interface type always has the same static type, and even though at run time the value stored in the interface variable may change type, that value will always satisfy the interface.","title":"interface{}"},{"location":"Languages/golang/go_basics/#the-representation-of-an-interface","text":"A variable of type interface type stores a pair: a concrete value assigned to the variable, and that concrete values's type/type-descriptor. e.g. 1 2 3 4 5 6 var r io . Reader tty , err := os . OpenFile ( \"/dev/tty\" , os . O_RDWR , 0 ) if err != nil { return nil , err } r = tty here r contains pair (value, concrete type) == (tty, *os.File). As a interface variable also stores the type details, we can do things like this: 1 2 3 4 5 6 7 8 9 10 11 12 var w io . Writer w = r .( io . Writer ) // The expression in this assignment is a type assertion // it asserts is that the item inside r also implements io.Writer, and so we can assign it to w // or var s interface {} s = \"hello\" ss := s .( string ) // The expression in this assignment is a type assertion // or ss , ok := s .( string ) this is called type assertion .","title":"The representation of an interface"},{"location":"Languages/golang/go_basics/#reflection","text":"Reflection in computing is the ability of a program to examine its own structure, particularly through types; it's a form of metaprogramming. It's also a great source of confusion. reflection builds on the type system","title":"Reflection"},{"location":"Languages/golang/go_basics/#the-first-law-of-reflection-reflection-goes-from-interface-value-to-reflection-object","text":"At the basic level, reflection is just a mechanism to examine the type and value pair stored inside an interface variable there are two types we need to know about in package reflect: Type and Value Those two types give access to the contents of an interface variable, and two simple functions, called reflect.TypeOf() and reflect.ValueOf() tbd","title":"The first law of reflection - Reflection goes from interface value to reflection object"},{"location":"Languages/golang/go_basics/#the-second-law-of-reflection-reflection-goes-from-reflection-object-to-interface-value","text":"tbd","title":"The second law of reflection - Reflection goes from reflection object to interface value"},{"location":"Languages/golang/go_basics/#the-third-law-of-reflection-to-modify-a-reflection-object-the-value-must-be-settable","text":"tbd","title":"The third law of reflection - To modify a reflection object, the value must be settable"},{"location":"Languages/golang/go_basics/#statements","text":"","title":"Statements"},{"location":"Languages/golang/go_basics/#control-flows","text":"","title":"Control Flows"},{"location":"Languages/golang/go_basics/#if-else","text":"1 2 3 4 5 6 7 if num := 9 ; num < 0 { fmt . Println ( num , \"is negative\" ) } else if num < 10 { fmt . Println ( num , \"has 1 digit\" ) } else { fmt . Println ( num , \"has multiple digits\" ) } NOTE : there is no ternary ? if else condition","title":"if, else"},{"location":"Languages/golang/go_basics/#for","text":"for is the only looping construct in Go for have 3-4 pattern 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 // like while loop i := 1 for i <= 3 { fmt . Println ( i ) i = i + 1 } // regular for loop for j := 7 ; j <= 9 ; j ++ { fmt . Println ( j ) } for n := 0 ; n <= 5 ; n ++ { if n % 2 == 0 { continue } fmt . Println ( n ) } // infinite loop (or with break for { fmt . Println ( \"loop\" ) break }","title":"for"},{"location":"Languages/golang/go_basics/#switch","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 // regular switch case i := 2 fmt . Print ( \"Write \" , i , \" as \" ) switch i { case 1 : fmt . Println ( \"one\" ) case 2 : fmt . Println ( \"two\" ) case 3 : fmt . Println ( \"three\" ) } // TODO: switch time . Now (). Weekday () { case time . Saturday , time . Sunday : fmt . Println ( \"It's the weekend\" ) default : fmt . Println ( \"It's a weekday\" ) } // regular if else t := time . Now () switch { case t . Hour () < 12 : fmt . Println ( \"It's before noon\" ) default : fmt . Println ( \"It's after noon\" ) } // interface type assertion whatAmI := func ( i interface {}) { switch t := i .( type ) { case bool : fmt . Println ( \"I'm a bool\" ) case int : fmt . Println ( \"I'm an int\" ) default : fmt . Printf ( \"Don't know type %T\\n\" , t ) } } whatAmI ( true ) whatAmI ( 1 ) whatAmI ( \"hey\" )","title":"switch"},{"location":"Languages/golang/go_basics/#goto","text":"why its still valid in a new-gen prog. lang. ??","title":"goto"},{"location":"Languages/golang/go_basics/#range","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 nums := [] int { 2 , 3 , 4 } sum := 0 // range on arrays and slices provides both the index and value for each entry for _ , num := range nums { sum += num } for i , num := range nums { if num == 3 { fmt . Println ( \"index:\" , i ) } } // range on map iterates over key/value pairs kvs := map [ string ] string { \"a\" : \"apple\" , \"b\" : \"banana\" } for k , v := range kvs { fmt . Printf ( \"%s -> %s\\n\" , k , v ) } // range can also iterate over just the keys of a map for k := range kvs { fmt . Println ( \"key:\" , k ) } // range on strings iterates over Unicode code points // The first value is the starting byte index of the rune and the second the rune itself. for i , c := range \"go\" { fmt . Println ( i , c ) } // out: 0 103 1 111","title":"range"},{"location":"Languages/golang/go_basics/#functions","text":"src: https://golang.org/doc/codewalk/functions/","title":"Functions"},{"location":"Languages/golang/go_basics/#first-class","text":"","title":"First Class"},{"location":"Languages/golang/go_basics/#user-defined","text":"","title":"User-defined"},{"location":"Languages/golang/go_basics/#higher-order","text":"a function which accepts another function as a arg","title":"Higher-order"},{"location":"Languages/golang/go_basics/#closures","text":"a concept of having access of outer scope in an Anonynous function, function literals are closures: they inherit the scope of the function in which they are declared same as python, javascript","title":"Closures"},{"location":"Languages/golang/go_basics/#multiple-return-values","text":"similar to pl/sql procedures 1 func () string , int { return \"Yes\" , 1 }","title":"Multiple Return Values"},{"location":"Languages/golang/go_basics/#variadic","text":"variable/arbitrary numbers of arguments similar to *args variable [space] ... concateanted to type vardiac arg is always a slice builtin e.g. fmt.Println 1 2 3 4 5 6 7 8 9 10 11 12 func sum ( nums ... int ) int { total := 0 for _ , number := range nums { total += number } return total } func main () { sum ( 1 , 2 ) sum ( 1 , 2 , 3 ) } can directly pass a slice like 1 2 num_slice := [] int { 1 , 2 , 3 } sum ( num_slice ... )","title":"Variadic"},{"location":"Languages/golang/go_basics/#anonymouslambda","text":"1 2 3 func ( msg string ) { fmt . Println ( msg ) }( \"Some message\" )","title":"Anonymous/Lambda"},{"location":"Languages/golang/go_basics/#access-modifier","text":"depends on CASE of the func if starts with Capital case --> Public should have comment/doc string else private","title":"Access Modifier"},{"location":"Languages/golang/go_basics/#struct_1","text":"regular struct in go, struct are alternative to classes 1 2 3 4 5 6 7 8 9 10 11 type Vehicle struct { Make string `json:make` // additional json tag for de/serialization Fuel string Engine Engine owner string // not an exportable/exposed/public key/object } type Engine struct { Stroke string HorsePower string }","title":"Struct"},{"location":"Languages/golang/go_basics/#methods","text":"1 2 3 4 5 6 7 func ( v * Vehicle ) Start () ( string , error ) { return \"Vrooom\" , nil } engine := Engine { \"Four\" , \"1000\" } car := Vehicle { \"Tesla\" , \"Li\" , engine , \"Toran\" } res , err := car . Start ()","title":"Methods"},{"location":"Languages/golang/go_basics/#interface_1","text":"Interfaces are named collections of method signatures https://gobyexample.com/interfaces","title":"Interface"},{"location":"Languages/golang/go_basics/#constructor","text":"","title":"Constructor"},{"location":"Languages/golang/go_basics/#goroutine","text":"A goroutine is a lightweight thread of execution","title":"goroutine"},{"location":"Languages/golang/go_basics/#go","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 func f ( from string ) { for i := 0 ; i < 3 ; i ++ { fmt . Println ( from , \":\" , i ) } } func main () { f ( \"direct\" ) // direct call go f ( \"goroutine\" ) // goroutine call go func ( msg string ){ fmt . Println ( msg ) }( \"goroutine from anonymous func\" ) fmt . Scanln () // input from stdio fmt . Println ( \"done\" ) } // out: direct : 0 direct : 1 direct : 2 goroutine : 0 goroutine from anonymous func goroutine : 1 goroutine : 2 < enter > done by going through above mentioned example: if we want to invoke the function f as a goroutine we call it using go statement func f will execute concurrently with the calling/main one we can also start a goroutine for an anonymous func our two function calls are running asynchronously in separate goroutines now we see output of blocking/synchronous call first then the interleaved output of two goroutines (output order may vary system to system)","title":"go"},{"location":"Languages/golang/go_basics/#defer","text":"tbd https://blog.golang.org/defer-panic-and-recover","title":"defer"},{"location":"Languages/golang/go_basics/#panic","text":"","title":"panic"},{"location":"Languages/golang/go_basics/#recover","text":"","title":"recover"},{"location":"Languages/golang/go_basics/#goroutine-vs-threads","text":"can run more number of goroutines on a typical system than can threads goroutine are managed by go runtime, and have been designed to be lightweight thus the startup time is low its lightweight because it gets assigned very minimal memory, and that could be increased on demand that called growable segmented stack goroutine comes with in-built primitives called channels to communicate safely between themselves goroutines are multiplexed onto a small number of OS threads, rather than 1:1 mapping","title":"goroutine vs threads"},{"location":"Languages/golang/go_basics/#channels","text":"https://gobyexample.com/channels Channels are the pipes that connect concurrent goroutines You can send values into channels from one goroutine and receive those values into another goroutine 1 2 3 4 5 messages := make ( chan string ) // create a channel named message go func () { messages <- \"ping\" }() // from go routine `send` a string value/msg \"ping\" to the channel msg := <- messages // `receive` value/msg from the channel fmt . Println ( msg ) fmt . Println ( msg .( string ) // assert type NOTE : By default sends and receives block until both the sender and receiver are ready. This property allowed us to wait at the end of our program for the \"ping\" message without having to use any other synchronization. meaning that, whenever there will be receiver (and is ready to receive the value) then only sender can send the value/msg to the channel see: https://github.com/toransahu/go-misc/blob/master/goroutines/channels/channels.go#L30 hence we can say, by default go channels are unbufferred","title":"Channels"},{"location":"Languages/golang/go_basics/#channel-buffering","text":"we checked above NOTE about readiness of senders & receivers channel buffering is to alter that nature & keep the value/msg in the buffer Buffered channels accept a limited number of values without a corresponding receiver for those values 1 2 3 4 5 message := make ( chan string , 2 ) // here 2 is capacity of the buffer message <- \"value 1\" message <- \"value 2\" fmt . Println ( <- message ) fmt . Println ( <- message ) https://github.com/toransahu/go-misc/blob/master/goroutines/channels/buffered/buffered_channels.go","title":"Channel Buffering"},{"location":"Languages/golang/go_basics/#channel-synchromization","text":"tbd https://gobyexample.com/channel-synchronization","title":"Channel Synchromization"},{"location":"Languages/golang/go_basics/#channel-directions","text":"tbd https://gobyexample.com/channel-directions","title":"Channel Directions"},{"location":"Languages/golang/go_basics/#select","text":"tbd https://gobyexample.com/select","title":"select"},{"location":"Languages/golang/go_basics/#timeouts","text":"tbd https://gobyexample.com/timeouts","title":"Timeouts"},{"location":"Languages/golang/go_basics/#non-blocking-channel-operations","text":"tbd https://gobyexample.com/non-blocking-channel-operations","title":"Non-blocking Channel operations"},{"location":"Languages/golang/go_basics/#closing-channels","text":"tbd https://gobyexample.com/closing-channels","title":"Closing Channels"},{"location":"Languages/golang/go_basics/#range-over-channels","text":"tbd https://gobyexample.com/range-over-channels","title":"range over channels"},{"location":"Languages/golang/go_basics/#modules","text":"A module is a collection of related Go packages that are versioned together as a single unit. Ref: https://github.com/golang/go/wiki/Modules#modules https://github.com/golang/go/wiki/Modules","title":"Modules"},{"location":"Languages/golang/go_basics/#go-mod","text":"to manage [versioned] dependencies was out with go 1.11 with preliminary/provisionary support & target to finalizing the feature for 1.14 (considering all feedbacks since 1.11-1.13) don't need to live the code in GOPATH creates/uses go.mod file the initial prototype vgo was announced in February 2018 other alternatives were: dep, gom etc. management of interdependencies: vgo's controversial algo uses the oldest common version to support stability this may while discard the acceptance of any security bug fixed in newer version e.g. package A uses B and B uses D's atleast 1.0 version package A also uses C and C uses D's either 1.0 or 1.1 version then, as per vgo vgo versioned go manages all the algorithm of versioning the go projects/packages/modules cmd vgo build is capable of generating go.mod creates/manages versioned cache packages inside GOPATH/src/v there are support for more than one module in repository, but general idea is one As of Go 1.11, the go command enables the use of modules when the current directory or any parent directory has a go.mod, provided the directory is outside GOPATH/src. (Inside GOPATH/src, for compatibility, the go command still runs in the old GOPATH mode, even if a go.mod is found. See the go command documentation for details.) Starting in Go 1.13, module mode will be the default for all development. In addition to go.mod, the go command maintains a file named go.sum containing the expected cryptographic hashes of the content of specific module versions to maintain the intigrity of the go.mod","title":"go mod"},{"location":"Languages/golang/go_basics/#semantic-import-versioning-semver","text":"way to handle major dependency changes like: v1 to v2, v3,... API/interface changes how /github.com/toransahu/log (contains all v1.x.x) & /github.com/toransahu/log/v2 (contains all v2.x.x) Ref: https://research.swtch.com/vgo-import","title":"Semantic Import Versioning (SEMVER)"},{"location":"Languages/golang/go_basics/#package_1","text":"","title":"Package"},{"location":"Languages/golang/go_basics/#main","text":"","title":"main"},{"location":"Languages/golang/go_basics/#package-vs-directory","text":"","title":"package Vs directory"},{"location":"Languages/golang/go_basics/#built-in-packages","text":"","title":"Built-in Packages"},{"location":"Languages/golang/go_basics/#timer-ticker","text":"tbd https://gobyexample.com/timers https://gobyexample.com/tickers continue exploring other packages from here","title":"timer &amp; ticker"},{"location":"Languages/golang/go_basics/#import_1","text":"","title":"Import"},{"location":"Languages/golang/go_basics/#inbuilt-package","text":"","title":"Inbuilt Package"},{"location":"Languages/golang/go_basics/#intra-package","text":"","title":"Intra Package"},{"location":"Languages/golang/go_basics/#inter-package","text":"","title":"Inter Package"},{"location":"Languages/golang/go_basics/#remote-package","text":"","title":"Remote Package"},{"location":"Languages/golang/go_basics/#cyclic","text":"","title":"Cyclic"},{"location":"Languages/golang/go_basics/#json","text":"","title":"json"},{"location":"Languages/golang/go_basics/#encoding","text":"func Marshal(v interface{}) ([]byte, error) Only data structures that can be represented as valid JSON will be encoded: JSON objects only support strings as keys; to encode a Go map type it must be of the form map[string]T (where T is any Go type supported by the json package). TODO: Channel, complex, and function types cannot be encoded. TODO: Cyclic data structures are not supported; they will cause Marshal to go into an infinite loop. Pointers will be encoded as the values they point to (or 'null' if the pointer is nil) The json package only accesses the exported fields of struct types (those that begin with an uppercase letter). Therefore only the the exported fields of a struct will be present in the JSON output. TODO: what if alias is also provided? we can also provide json:\"alias\" for each field in the struct so that those fields will be encoded as per aliases shortcut to generate those: vim-go: visual select -> <leader> GoAddTags GoLand: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 // data structure, Message type Message struct { Name string `json:\"name\"` Body string `json:\"body\"` Time int64 `json:\"time\"` } // an instance of Message m := Message { \"Alice\" , \"Hello\" , 1294706395881547000 } // encoding m b , err := json . Marshal ( m ) // If all is well, err will be nil and b will be a []byte containing this JSON data b == [] byte ( `{\"Name\":\"Alice\",\"Body\":\"Hello\",\"Time\":1294706395881547000}` ) https://github.com/toransahu/go-misc/blob/master/json/encoding.go","title":"Encoding"},{"location":"Languages/golang/go_basics/#decoding","text":"https://github.com/toransahu/go-misc/blob/master/json/decoding.go","title":"Decoding"},{"location":"Languages/golang/go_basics/#generic-json-with-interface","text":"The json package uses map[string]interface{} and []interface{} values to store arbitrary JSON objects and arrays it will happily unmarshal any valid JSON blob into a plain interface{} value default concrete Go types are: bool for JSON booleans float64 for JSON numbers string for JSON strings nil for JSON null","title":"Generic JSON with interface{}"},{"location":"Languages/golang/go_basics/#creating-arbitrary-data","text":"1 2 3 i := map [ string ] interface {}{ \"id\" : 1111 , \"name\" : \"Toran\" } fmt . Println ( i ) }","title":"Creating arbitrary data"},{"location":"Languages/golang/go_basics/#encoding-arbitrary-data","text":"1 bytes := json . Marshal ( i )","title":"Encoding arbitrary data"},{"location":"Languages/golang/go_basics/#decoding-arbitrary-data","text":"https://github.com/toransahu/go-misc/blob/master/json/json_test.go#L33 1 2 var i interface {} json . Unmarshal ( bytes , & i )","title":"Decoding arbitrary data"},{"location":"Languages/golang/go_basics/#json-reference-type-pointers-slices-maps","text":"https://blog.golang.org/json-and-go https://github.com/toransahu/go-misc/blob/master/jsons/reference_type.go#L53","title":"Json &amp; Reference Type (pointers, slices, maps)"},{"location":"Languages/golang/go_basics/#streaming-encoders-and-decoders","text":"https://blog.golang.org/json-and-go","title":"Streaming Encoders and Decoders"},{"location":"Languages/golang/go_basics/#misc","text":"https://vsupalov.com/go-json-omitempty/","title":"Misc"},{"location":"Languages/golang/go_basics/#regexp","text":"Ref: https://golang.org/pkg/regexp/ https://github.com/google/re2/wiki/Syntax https://shapeshed.com/golang-regexp/","title":"regexp"},{"location":"Languages/golang/go_basics/#errors","text":"https://gobyexample.com/errors","title":"Errors"},{"location":"Languages/golang/go_basics/#error-handling","text":"https://blog.golang.org/error-handling-and-go","title":"Error Handling"},{"location":"Languages/golang/go_basics/#testing","text":"test module should named as *_test.go under packages FIXME: better to organize test modules in a separate package - similar to Java??","title":"Testing"},{"location":"Languages/golang/go_basics/#testcase","text":"","title":"Testcase"},{"location":"Languages/golang/go_basics/#benchmarks","text":"","title":"Benchmarks"},{"location":"Languages/golang/go_basics/#examples","text":"","title":"Examples"},{"location":"Languages/golang/go_basics/#skipping","text":"","title":"Skipping"},{"location":"Languages/golang/go_basics/#subtests-and-sub-benchmarks","text":"","title":"Subtests and Sub-benchmarks"},{"location":"Languages/golang/go_basics/#main_1","text":"","title":"Main"},{"location":"Languages/golang/go_basics/#run_1","text":"a test file ~/go/src/github.com/toransahu/go-misc/json on \ue0a0 master! \u231a 14:18:45 $ go test json_test.go config.go encoding.go ok command-line-arguments 0.003s all test files under a package ~/go/src/github.com/toransahu/go-misc/json on \ue0a0 master! \u231a 14:18:56 $ go test PASS ok github.com/toransahu/go-misc/json 0.002s all test files in all packages tbd specific test function of a test file tbd specific test function/file with pattern/regex tbd","title":"Run"},{"location":"Languages/golang/go_basics/#coverage","text":"https://blog.alexellis.io/golang-writing-unit-tests/ src: https://golang.org/pkg/testing/#hdr-Subtests_and_Sub_benchmarks https://stackoverflow.com/questions/16935965/how-to-run-test-cases-in-a-specified-file","title":"Coverage"},{"location":"Languages/golang/go_basics/#debug","text":"","title":"Debug"},{"location":"Languages/golang/go_basics/#gdb","text":"https://golang.org/doc/gdb","title":"gdb"},{"location":"Languages/golang/go_basics/#delve","text":"","title":"Delve"},{"location":"Languages/golang/go_basics/#profiling","text":"","title":"Profiling"},{"location":"Languages/golang/go_basics/#pprof","text":"https://blog.golang.org/pprof","title":"pprof"},{"location":"Languages/golang/go_basics/#cpu-profiling-stack-sampling-vs-instrumentation","text":"","title":"CPU Profiling - stack sampling (vs instrumentation)"},{"location":"Languages/golang/go_basics/#heap-profiling-allocation-profiling","text":"","title":"Heap Profiling - allocation profiling"},{"location":"Languages/golang/go_basics/#block","text":"","title":"Block"},{"location":"Languages/golang/go_basics/#trace","text":"","title":"Trace"},{"location":"Languages/golang/go_basics/#deploy","text":"","title":"Deploy"},{"location":"Languages/golang/go_basics/#circleci","text":"","title":"CircleCI"},{"location":"Languages/golang/go_basics/#run-test","text":"","title":"Run Test"},{"location":"Languages/golang/go_basics/#misc_1","text":"","title":"Misc"},{"location":"Languages/golang/go_basics/#generate","text":"automatically generate golang code for a particular purpose like print name of memebers in a struct stringr https://blog.golang.org/generate can set header/preprocessor (with commands) in the go file to do the job on each build","title":"generate"},{"location":"Languages/golang/go_basics/#advance-testing","text":"","title":"Advance Testing"},{"location":"Languages/golang/go_basics/#assert","text":"3rd party packages for assert.* https://github.com/stretchr/testify","title":"assert"},{"location":"Languages/golang/go_basics/#extra","text":"https://talks.golang.org/2012/10things.slide#3","title":"Extra"},{"location":"Languages/golang/go_basics/#best-practices","text":"","title":"Best Practices"},{"location":"Languages/golang/go_basics/#naming-convetions","text":"https://golang.org/doc/effective_go.html#names https://blog.golang.org/package-names","title":"Naming Convetions"},{"location":"Languages/golang/go_basics/#code-organization","text":"Package main test https://blog.golang.org/organizing-go-code inside a directory all modules with only single package name can only execute/run main package module hence main package func main should be declared to run the main package module","title":"Code Organization"},{"location":"Languages/golang/go_basics/#documentation","text":"to document a type, variable, constant, function, or even a package, write a regular comment directly preceding its declaration, with no intervening blank line a complete sentence begins with the name of the element similar to python 's Docstring & java 's Javadoc but simpler than them src: https://blog.golang.org/godoc-documenting-go-code 1 2 3 4 5 6 7 8 9 10 // Package sort provides primitives for sorting slices and user-defined // collections. package sort ... // Fprint formats using the default formats for its operands and writes to w. // Spaces are added between operands when neither is a string. // It returns the number of bytes written and any write error encountered. func Fprint ( w io . Writer , a ... interface {}) ( n int , err error ) { extra: do something like this to achieve this notable keywords: Deprecated: BUG(<contact person>)","title":"Documentation"},{"location":"Languages/golang/go_basics/#godoc","text":"","title":"Godoc"},{"location":"Languages/golang/go_basics/#best-practices_1","text":"","title":"Best Practices"},{"location":"Languages/golang/go_basics/#concepts","text":"","title":"Concepts"},{"location":"Languages/golang/go_basics/#compilation","text":"","title":"Compilation"},{"location":"Languages/golang/go_basics/#static-typing","text":"","title":"Static Typing"},{"location":"Languages/golang/go_basics/#pointer","text":"","title":"Pointer"},{"location":"Languages/golang/go_basics/#garbage-collection_1","text":"Go language features, goals, and use cases have forced to rethink the entire garbage collection stack and have led to a surprising place Go programs have hundreds of thousands of stacks They are managed by the Go scheduler and are always preempted at GC safepoints The Go scheduler multiplexes Go routines onto OS threads which hopefully run with one OS thread per HW thread We manage the stacks and their size by copying them and updating pointers in the stack. It's a local operation so it scales fairly well. Ref: https://blog.golang.org/ismmkeynote https://www.ardanlabs.com/blog/2018/12/garbage-collection-in-go-part1-semantics.html","title":"Garbage Collection"},{"location":"Languages/golang/go_basics/#debugging","text":"","title":"Debugging"},{"location":"Languages/golang/go_basics/#gdb_1","text":"https://golang.org/doc/gdb https://astaxie.gitbooks.io/build-web-application-with-golang/content/en/11.2.html","title":"gdb"},{"location":"Languages/golang/go_basics/#delve_1","text":"","title":"Delve"},{"location":"Languages/golang/go_basics/#profiling_1","text":"https://blog.golang.org/pprof","title":"Profiling"},{"location":"Languages/golang/go_basics/#design-patterns","text":"Functional options for friendly APIs Self referential functions and design","title":"Design Patterns"},{"location":"Languages/golang/go_basics/#references","text":"https://golang.org/doc/code.html https://github.com/golang/go/wiki/Learn https://go.dev/about/#best-practices-h2","title":"References"},{"location":"Languages/golang/go_basics/#surprises","text":"https://medium.com/@karel_3d/things-that-surprised-me-in-go-47bccce94558 https://utcc.utoronto.ca/~cks/space/blog/programming/GoInteriorPointerGC https://dave.cheney.net/2017/04/29/there-is-no-pass-by-reference-in-go","title":"Surprises"},{"location":"Languages/java/java_basics/","text":"Java Basics # Java Basics POJO Bean String Immutability String Pool dynamic method dispatch abstract class Can static, private, final methods be overridden? difference between interface and abstract class? achieve Multiple Inheritance in JAVA? What is the difference between inner class and nested class? final class, final method What is final field, static field What is singleton class When can you override clone method of Object class What is an extensible framework? why JAVA is platform independent? Can we have multiple constructors in a class super and this ? What is the difference between String Builder and String Buffer? What is the difference between == and .equals() ? The internal working of Hash-map in java? POJO # Bean # Ref: https://stackoverflow.com/questions/3295496/what-is-a-javabean-exactly Spec: https://www.oracle.com/java/technologies/javase/javabeans-spec.html","title":"Java Basics"},{"location":"Languages/java/java_basics/#java-basics","text":"Java Basics POJO Bean String Immutability String Pool dynamic method dispatch abstract class Can static, private, final methods be overridden? difference between interface and abstract class? achieve Multiple Inheritance in JAVA? What is the difference between inner class and nested class? final class, final method What is final field, static field What is singleton class When can you override clone method of Object class What is an extensible framework? why JAVA is platform independent? Can we have multiple constructors in a class super and this ? What is the difference between String Builder and String Buffer? What is the difference between == and .equals() ? The internal working of Hash-map in java?","title":"Java Basics"},{"location":"Languages/java/java_basics/#pojo","text":"","title":"POJO"},{"location":"Languages/java/java_basics/#bean","text":"Ref: https://stackoverflow.com/questions/3295496/what-is-a-javabean-exactly Spec: https://www.oracle.com/java/technologies/javase/javabeans-spec.html","title":"Bean"},{"location":"Languages/java/spring_framework/","text":"Spring Framework # Spring Framework Spring Bean Spring Container Aspect Oriented Spring Bean # vs JavaBean Spring Container # Aspect Oriented # #","title":"Spring Framework"},{"location":"Languages/java/spring_framework/#spring-framework","text":"Spring Framework Spring Bean Spring Container Aspect Oriented","title":"Spring Framework"},{"location":"Languages/java/spring_framework/#spring-bean","text":"vs JavaBean","title":"Spring Bean"},{"location":"Languages/java/spring_framework/#spring-container","text":"","title":"Spring Container"},{"location":"Languages/java/spring_framework/#aspect-oriented","text":"","title":"Aspect Oriented"},{"location":"Languages/java/spring_framework/#_1","text":"","title":""},{"location":"Languages/python/python_advanced/","text":"Python Advanced # Python Advanced Regular Expressions Basic Patterns use of . use of * use of + use of ? use of \\ use of \\t, \\r, \\n use of \\b (start and end of word anchors) use of \\B use of \\s use of \\d use of \\D use of \\w use of \\W use of [] use of () use of ^ (Start of String Anchors) use of $ $--> (End of String Anchors) use of | use of syntax (? ...) Builtin Functions: re.search re.group re.findall re.sub re.compile extra options to above functions Examples: Multithreading & Multiprocessing GIL - Global Interpreter Lock Some terminologies Concurrency Parallelism process thread Multi-threading Note: Use of thread Facts Multi-processing pipe What is .join()? daemon Files File Handling Using try, except, finally Using with JSON Operations import json flask: jsonify() Shipping Python Solution .pyc .pyo .exe PyInstaller py2exe Python Version Management Setting default Python Version By creating softlink to /usr/bin/python file By aliasing python By setting PATH env variable Anaconda Linux Windows virtualenv pipenv Installation Usage create venv create venv with specific python version install/uninstall dependencies lock dependencies for production install dependencies only for development install dependencies with other options run command within venv remove venv see venv details/path enter venv shell clean venv update venv security check Python Package Distribution (PyPi) Project Structure Pre-requisites Build Test Distribution Final Distribution Rebuild Test (Test) Package (Final) Package Advanced - Run Package in Command Line FAQ Python Linter & PEP-8 Python Templates PyCharm Live & File Templates Vim Template Regular Expressions # Basic Patterns # use of . # 1 - matches any character use of * # 1 - matches 0 or more repetition of a char/set use of + # 1 - matches 1 or more repetition of a char/set use of ? # 1 2 3 4 5 - matches 0 or 1 repetition of a char/set - also works as a Non-greedy pattern (with repeaters) - means in string '<b>Hello</b>' pattern r'<.*>' will match the whole string instead of '<b>' - but if pattern is used as r'<.*?>' then will match '<b>' only - AKA pcre (Perl Compatible Regular Expression) use of \\ # 1 2 - sign of speciality - used before any special chars, to match that char use of \\t , \\r , \\n # 1 2 3 4 - \\t matches a tab - \\r matches a carriage return (line break) in Mac, \\n\\r in Windows - \\n matches a line break ( carriage return) in Linux & Windows - note: On \"old\" printers, \\r sent the print head back to the start of the line, and \\n advanced the paper by one line. Both were therefore necessary to start printing on the next line. use of \\b (start and end of word anchors) # 1 2 3 4 5 - matches the boundary between word and non-word chars - matches the position called word boundaries - match has zero length - usually before (including start of the line) and after a word -e.g. <here>apple<here> use of \\B # 1 2 - opposite of \\b - matches every position where \\b does not use of \\s # use of \\d # use of \\D # use of \\w # use of \\W # use of [] # 1 2 - dash - inside [] - dot . inside [] use of () # use of ^ (Start of String Anchors) # 1 - with square bracket (set of chars) use of $ (End of String Anchors) # use of | # use of syntax ( ? ... ) # 1 2 3 4 5 6 - Lookarounds - +ve Lookahead - -ve Lookahead - +ve Lookbehind - -ve Lookbehind - Non-Capturing Group Builtin Functions: # re.search # re.group # re.findall # 1 2 - with files - with groups re.sub # re.compile # extra options to above functions # Examples: # Repetitions Leftmost and largest Square Brackets (Set of chars) Email example Group Extraction Greedy vs Non-Greedy Substitution 1 2 3 4 5 6 7 8 9 import re str = \"This#is#$ % a &%name%$ #\" r = re . sub ( r '(?<=[\\w])([\\W]+)(?=[\\w])' , ' ' , str ) print ( r ) r = re . sub ( r '(?<=[A-Za-z0-9])([^A-Za-z0-9]+)(?=[A-Za-z0-9])' , ' ' , str ) print ( r ) r = re . sub ( r 'q(?=u)' , ' ' , 'quit' ) print ( r ) remove white spaces from starting of the line but line break 1 2 3 4 regex = r '^[^\\S\\n]+' #or if regex supports PCRE regex = r '^[\\h]+' Credits: https://www.rexegg.com/regex-disambiguation.html#lookarounds www.regular-expressions.info Multithreading & Multiprocessing # GIL - Global Interpreter Lock # Is a mutex (mutual exclusion attribute) in python protects python objects from multiple threads i.e. prevents multiple threads to execute python (byte)codes at once inorder to protect access to python objects i.e. provides lock to protect shared mutable state The lock is necessary because CPython's Interpreter or memory management is not thread safe for example, when two threads simultaneously increment the reference count of the same object, the reference count could end up being incremented only once instead of twice. hence, GIL is here to make python thread-safe, wherever needed GIL is controversial because due to it python's multithreading lack few features like python's multithreaded codes cannot utilize multiprocessor system the longer operations like I/O, image processing happens outside the GIL it is only bottleneck for codes which enter GIL for longer time GIL can causes scheduling IO-bound threads ahead of a CPU-bound threads inshort, GIL is only bad for multi-core CPU - bound thread operations each forked process have separate GIL Jython, IronPython does not have GIL writing a C extension needs GIL in Cython the GIL exists, but can be released temporarily using a \"with\" statement - read more extra: https://stackoverflow.com/questions/1294382/what-is-a-global-interpreter-lock-gil Note : cython & CPython are different Some terminologies # Concurrency # When two or more task can start, run & complete in overlapping time periods. It doesn't necessarily mean they'll ever be running at the same instant. e.g. Multi-tasking on a single core Parallelism # When two are more tasks are executed simultaneously. process # is an instance of a program running in a computer can contain one or more threads has its independent memory space are spawned by creating a Process() object and then calling its start() method thread # is a sequence of instructions within a process as a light-weight process all threads shares the same memory space of the process Multi-threading # can be implemented to speedup the program using module : threading thread-based parallelism concurrency CPython implementation uses a python construct GIL (global interpreter lock). GIL makes sure that at a time only single thread can execute. a thread acquires GIL --> executes just for a while --> passes GIL to next thread happens very quickly that human cannot detect, and creates illusion of multiple thread running simultaneously. in reality all threads works turn by turn into the same core of CPU parallel CPU computation not possible due to GIL but parallel IO operations are possible (it releases GIL on IO) GIL passing is an overhead here. Note: # can turn off GIL - dirty practice this will result in messed memory management need to be very careful while writting semaphores & mutex properly Use of thread # in GUI apps to keep UI threads responsive IO tasks (network IO or filesystem IO) Facts # using multiple-threads for CPU bound tasks will result in worse performance than a single thread Multi-processing # used to speedup CPU bound tasks using module: multiprocessing process-based parallelism module results in full CPU utilization Inter-process communication can be achieved using queues and pipes * pipe # is a duplex(two way) communication channel 1 2 3 4 5 6 7 8 9 from multiprocessing import Process def f ( name ): print ( 'hello' , name ) if __name__ == '__main__' : p = Process ( target = f , args = ( 'bob' ,)) p . start () p . join () Out: hello bob 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 from multiprocessing import Process import os def info ( title ): print ( title ) print ( 'module name:' , __name__ ) print ( 'parent process:' , os . getppid ()) print ( 'process id:' , os . getpid ()) def f ( name ): info ( 'function f' ) print ( 'hello' , name ) if __name__ == '__main__' : info ( 'main line' ) p = Process ( target = f , args = ( 'bob' ,)) p . start () p . join () Out: 1 2 3 4 5 6 7 8 9 main line module name: __main__ parent process: 65 process id: 66 function f module name: __main__ parent process: 66 process id: 80 hello bob What is .join()? # The join() method, when used with threading or multiprocessing, is not related to str.join() it's not actually concatenating anything together It just means \"wait for this [thread/process] to complete\" The name join is used because the multiprocessing module's API is meant to look as similar to the threading module's API The reason why is called join is that is joining the processes into a single one. Note: * By default, when the main process is ready to exit, it will implicitly call join() on all running multiprocessing Process instances * This isn't as clearly stated in the multiprocessing docs as it should be, but it is mentioned in the Programming Guidelines section. * non-daemonic processes will be automatically be joined. * can override this behavior by setting the daemon flag on the Process to True prior to starting the process: 1 2 3 4 5 6 7 8 9 10 11 12 from multiprocessing import Process import os def say_hello (): print ( \"Hello\" ) p = Process ( target = say_hello ) p . daemon = True p . start () # Both parent and child will exit here, since the main process has completed. # the child process will be terminated as soon as the main process completes: Out: Hello daemon # In Linux: A daemon is a long-running background process that answers requests for services. There are also some other processes like orphan & zombie The process has daemon flag a Boolean value This must be set before start() is called The initial value is inherited from the creating process When a process exits, it attempts to terminate all of its daemonic child processes Files # File Handling # How Using try , except , finally Using with (pythonic way) Why to manage resources effeciently file descriptors (fd) are limited at OS level, so we can save fd by closing after use Using try , except , finally # the step finally is important here for us to use finally we need to write whole try , except , finally block in each languages finally is important because we need f.close() here to close the file descriptor with 100% guarantee file descriptors (fd) are limited at OS level, so we can save fd by closing after use code: 1 2 3 4 5 6 7 try : f = open ( 'csv.txt' , 'r' , encoding = 'utf-8' ) f . write ( 'abc' ) except : pass finally : f . close () Using with # Read here code: 1 2 with open ( 'csv.txt' , 'r' , encoding = 'utf-8' ) as f : f . readline () JSON Operations # import json # flask: jsonify() # Shipping Python Solution # never ship .py files find /path/to/your/files -type f -name \"*.py\" -delete can ship .pyc or .pyo files .pyc # compiled size(.py) < size(.pyc) .pyo # compiled + optimized removed doc strings does not contain \"set the current line to ...\" bytecode instructions for faster performance renaming .pyo to .pyc works fine find \"ETHEREAL_DIR\"/Ray/src/ -type f -name \"*.pyo\" -exec bash -c 'mv $0 ${0/.pyo/.pyc}' {} \\; size(.pyo) < size(.pyc) python -O -m compileall /path/to/your/files Note - After this, you may get some import issues .exe # Source http://docs.python-guide.org/en/latest/shipping/freezing/ https://pyinstaller.readthedocs.io/en/v3.3.1/ Note: Freezing Python code on Linux into a Windows executable was only once supported in PyInstaller and later dropped All solutions need MS Visual C++ dll to be installed on target machine, except py2app. Only Pyinstaller makes self-executable exe that bundles the dll when passing --onefile to Configure.py. PyInstaller # Uses the OS support to load the dynamic libraries, thus ensures full compatibility available for windows, linux, macOS etc. 1 pyinstaller -i <icon.ico> --windowed --onefile <python_file.py> py2exe # Python Version Management # Setting default Python Version # By creating softlink to /usr/bin/python file # not recommanded will affect other applications using another version of python can be done like: 1 2 3 ln -l /usr/bin/python /usr/bin/python3.6 ln -l /usr/bin/python /home/toran/anaconda/bin/python3.6 By aliasing python # not recommanded will conflict when python will encounter in other commands like which python python manage.py runserver can be done like: 1 alias python = ' /home/toran/anaconda/bin/python3.6 ` By setting PATH env variable # recommanded with anaconda works on particular shell only for persistency; include the command in .bashrc or .zshrc file can be done like: 1 export PATH = \"/home/toran/anaconda3/bin:<dollar>PATH\" Anaconda # install it from official site you can also choose miniconda over regular anaconda can be used in production env/ deployments etc. Linux # set anaconda python as default 1 export PATH = \"/home/toran/anaconda3/bin:<dollar>PATH\" Windows # set anaconda python as default 1 2 3 4 C: \\P rogramData \\M iniconda3 \\S cripts \\a ctivate #if using git bash . /c/ProgramData/Miniconda3/Scripts/activate virtualenv # pipenv # officially recommended dependency management system automagically installs dependencies in venv keep record in Pipfile packages section dev-packages section keeps record of dependencies with specific version in Pipfile.lock file if requirements.txt already exists, installs dep from there installs & uninstalls deps Installation # 1 pip install pipenv Usage # create venv # 1 pipenv install create venv with specific python version # 1 2 3 pipenv --two install pipenv --three install will initialize the venv with that python version install/uninstall dependencies # 1 2 3 pipenv install nose2 pipenv uninstall nose2 lock dependencies for production # 1 pipenv lock - will keep record of all the deps in Pipfile.lock file with their specific versions install dependencies only for development # 1 pipenv install --dev nose2 will keep this record in [dev-packages] section and will do not install it by default to install dev packages; need to do: 1 pipenv install --dev install dependencies with other options # --dev : Install both develop and default packages from Pipfile.lock. --system : Use the system pip command rather than the one from your virtualenv. --ignore-pipfile : Ignore the Pipfile and install from the Pipfile.lock. --skip-lock : Ignore the Pipfile.lock and install from the Pipfile. In addition, do not write out a Pipfile.lock reflecting changes to the Pipfile. run command within venv # 1 pipenv run which python remove venv # 1 pipenv --rm see venv details/path # 1 pipenv --venv enter venv shell # 1 pipenv shell clean venv # uninstall all the packages not recorded in Pipfile 1 pipenv clean update venv # locks & then update all the packages 1 pipenv update security check # checks for compliance with PEP 508 Dependency specification for Python Software Packages as well as package safety 1 pipenv check Python Package Distribution (PyPi) # Project Structure # 1 2 3 4 5 6 7 8 /example_pkg /pkg1 __init__.py /pkg2 /pkg2_1 setup.py LICENSE README.md where - __init__.py should contain a variable name='example_pkg' - setup.py should look like this gist - where classifiers can be like this or gist - README.md should be present, which will define the long_description about the package - LICENSE should be accurate with the list - packages - if want to includes all the packages/subpackages - use defaultone packages=setuptools.find_packages(), - else define manually like - packages=['pkg1', 'pkg2', 'pkg2.pkg2_1'] Note: After installing the package, you able to import packages with names listed in packages var. Pre-requisites # 1 2 3 4 5 6 #setuptools & wheel - to create build in .whl as well as .tar.gz. format python3 -m pip install --user --upgrade setuptools wheel #twine - to upload build in pypi server python3 -m pip install --user --upgrade twine Build # change directory where setup.py is & run 1 python3 setup.py sdist bdist_wheel output will be like 1 2 3 dist/ example_pkg-0.0.1-py3-none-any.whl example_pkg-0.0.1.tar.gz Test Distribution # 1 twine upload --repository-url https://test.pypi.org/legacy/ dist/* Final Distribution # 1 2 3 4 5 twine upload --repository-url https://upload.test.pypi.org/legacy/ dist/* #OR twine upload dist/* Rebuild # delete build , dist , & *.egg-info files then build again Test # (Test) Package # 1 python3 -m pip install --index-url https://test.pypi.org/simple/ example_pkg (Final) Package # 1 python3 -m pip install example_pkg Advanced - Run Package in Command Line # Source: http://python-packaging.readthedocs.io/en/latest/command-line-scripts.html FAQ # You're not allowed File already exists File already exists, change version particular line is not know/right Python Linter & PEP-8 # I found flake8, autopep8 & yapf similar but I found black best among them quotes: double no extra line change only draw back is bad list slice formating create issue: https://github.com/python/black/issues Python Templates # PyCharm Live & File Templates # https://gist.github.com/toransahu/6080cbf2cc1a8808f19bd0eccafc5ef0 Vim Template # https://github.com/toransahu/vim-template/blob/master/templates/%3Dtemplate%3D.py","title":"Python Advanced"},{"location":"Languages/python/python_advanced/#python-advanced","text":"Python Advanced Regular Expressions Basic Patterns use of . use of * use of + use of ? use of \\ use of \\t, \\r, \\n use of \\b (start and end of word anchors) use of \\B use of \\s use of \\d use of \\D use of \\w use of \\W use of [] use of () use of ^ (Start of String Anchors) use of $ $--> (End of String Anchors) use of | use of syntax (? ...) Builtin Functions: re.search re.group re.findall re.sub re.compile extra options to above functions Examples: Multithreading & Multiprocessing GIL - Global Interpreter Lock Some terminologies Concurrency Parallelism process thread Multi-threading Note: Use of thread Facts Multi-processing pipe What is .join()? daemon Files File Handling Using try, except, finally Using with JSON Operations import json flask: jsonify() Shipping Python Solution .pyc .pyo .exe PyInstaller py2exe Python Version Management Setting default Python Version By creating softlink to /usr/bin/python file By aliasing python By setting PATH env variable Anaconda Linux Windows virtualenv pipenv Installation Usage create venv create venv with specific python version install/uninstall dependencies lock dependencies for production install dependencies only for development install dependencies with other options run command within venv remove venv see venv details/path enter venv shell clean venv update venv security check Python Package Distribution (PyPi) Project Structure Pre-requisites Build Test Distribution Final Distribution Rebuild Test (Test) Package (Final) Package Advanced - Run Package in Command Line FAQ Python Linter & PEP-8 Python Templates PyCharm Live & File Templates Vim Template","title":"Python Advanced"},{"location":"Languages/python/python_advanced/#regular-expressions","text":"","title":"Regular Expressions"},{"location":"Languages/python/python_advanced/#basic-patterns","text":"","title":"Basic Patterns"},{"location":"Languages/python/python_advanced/#use-of","text":"1 - matches any character","title":"use of ."},{"location":"Languages/python/python_advanced/#use-of_1","text":"1 - matches 0 or more repetition of a char/set","title":"use of *"},{"location":"Languages/python/python_advanced/#use-of_2","text":"1 - matches 1 or more repetition of a char/set","title":"use of +"},{"location":"Languages/python/python_advanced/#use-of_3","text":"1 2 3 4 5 - matches 0 or 1 repetition of a char/set - also works as a Non-greedy pattern (with repeaters) - means in string '<b>Hello</b>' pattern r'<.*>' will match the whole string instead of '<b>' - but if pattern is used as r'<.*?>' then will match '<b>' only - AKA pcre (Perl Compatible Regular Expression)","title":"use of ?"},{"location":"Languages/python/python_advanced/#use-of_4","text":"1 2 - sign of speciality - used before any special chars, to match that char","title":"use of \\"},{"location":"Languages/python/python_advanced/#use-of-t-r-n","text":"1 2 3 4 - \\t matches a tab - \\r matches a carriage return (line break) in Mac, \\n\\r in Windows - \\n matches a line break ( carriage return) in Linux & Windows - note: On \"old\" printers, \\r sent the print head back to the start of the line, and \\n advanced the paper by one line. Both were therefore necessary to start printing on the next line.","title":"use of \\t, \\r, \\n"},{"location":"Languages/python/python_advanced/#use-of-b-start-and-end-of-word-anchors","text":"1 2 3 4 5 - matches the boundary between word and non-word chars - matches the position called word boundaries - match has zero length - usually before (including start of the line) and after a word -e.g. <here>apple<here>","title":"use of \\b (start and end of word anchors)"},{"location":"Languages/python/python_advanced/#use-of-b","text":"1 2 - opposite of \\b - matches every position where \\b does not","title":"use of \\B"},{"location":"Languages/python/python_advanced/#use-of-s","text":"","title":"use of \\s"},{"location":"Languages/python/python_advanced/#use-of-d","text":"","title":"use of \\d"},{"location":"Languages/python/python_advanced/#use-of-d_1","text":"","title":"use of \\D"},{"location":"Languages/python/python_advanced/#use-of-w","text":"","title":"use of \\w"},{"location":"Languages/python/python_advanced/#use-of-w_1","text":"","title":"use of \\W"},{"location":"Languages/python/python_advanced/#use-of_5","text":"1 2 - dash - inside [] - dot . inside []","title":"use of []"},{"location":"Languages/python/python_advanced/#use-of_6","text":"","title":"use of ()"},{"location":"Languages/python/python_advanced/#use-of-start-of-string-anchors","text":"1 - with square bracket (set of chars)","title":"use of ^ (Start of String Anchors)"},{"location":"Languages/python/python_advanced/#use-of-end-of-string-anchors","text":"","title":"use of $ $-->  (End of String Anchors)"},{"location":"Languages/python/python_advanced/#use-of_7","text":"","title":"use of |"},{"location":"Languages/python/python_advanced/#use-of-syntax","text":"1 2 3 4 5 6 - Lookarounds - +ve Lookahead - -ve Lookahead - +ve Lookbehind - -ve Lookbehind - Non-Capturing Group","title":"use of syntax (? ...)"},{"location":"Languages/python/python_advanced/#builtin-functions","text":"","title":"Builtin Functions:"},{"location":"Languages/python/python_advanced/#research","text":"","title":"re.search"},{"location":"Languages/python/python_advanced/#regroup","text":"","title":"re.group"},{"location":"Languages/python/python_advanced/#refindall","text":"1 2 - with files - with groups","title":"re.findall"},{"location":"Languages/python/python_advanced/#resub","text":"","title":"re.sub"},{"location":"Languages/python/python_advanced/#recompile","text":"","title":"re.compile"},{"location":"Languages/python/python_advanced/#extra-options-to-above-functions","text":"","title":"extra options to above functions"},{"location":"Languages/python/python_advanced/#examples","text":"Repetitions Leftmost and largest Square Brackets (Set of chars) Email example Group Extraction Greedy vs Non-Greedy Substitution 1 2 3 4 5 6 7 8 9 import re str = \"This#is#$ % a &%name%$ #\" r = re . sub ( r '(?<=[\\w])([\\W]+)(?=[\\w])' , ' ' , str ) print ( r ) r = re . sub ( r '(?<=[A-Za-z0-9])([^A-Za-z0-9]+)(?=[A-Za-z0-9])' , ' ' , str ) print ( r ) r = re . sub ( r 'q(?=u)' , ' ' , 'quit' ) print ( r ) remove white spaces from starting of the line but line break 1 2 3 4 regex = r '^[^\\S\\n]+' #or if regex supports PCRE regex = r '^[\\h]+' Credits: https://www.rexegg.com/regex-disambiguation.html#lookarounds www.regular-expressions.info","title":"Examples:"},{"location":"Languages/python/python_advanced/#multithreading-multiprocessing","text":"","title":"Multithreading &amp; Multiprocessing"},{"location":"Languages/python/python_advanced/#gil-global-interpreter-lock","text":"Is a mutex (mutual exclusion attribute) in python protects python objects from multiple threads i.e. prevents multiple threads to execute python (byte)codes at once inorder to protect access to python objects i.e. provides lock to protect shared mutable state The lock is necessary because CPython's Interpreter or memory management is not thread safe for example, when two threads simultaneously increment the reference count of the same object, the reference count could end up being incremented only once instead of twice. hence, GIL is here to make python thread-safe, wherever needed GIL is controversial because due to it python's multithreading lack few features like python's multithreaded codes cannot utilize multiprocessor system the longer operations like I/O, image processing happens outside the GIL it is only bottleneck for codes which enter GIL for longer time GIL can causes scheduling IO-bound threads ahead of a CPU-bound threads inshort, GIL is only bad for multi-core CPU - bound thread operations each forked process have separate GIL Jython, IronPython does not have GIL writing a C extension needs GIL in Cython the GIL exists, but can be released temporarily using a \"with\" statement - read more extra: https://stackoverflow.com/questions/1294382/what-is-a-global-interpreter-lock-gil Note : cython & CPython are different","title":"GIL - Global Interpreter Lock"},{"location":"Languages/python/python_advanced/#some-terminologies","text":"","title":"Some terminologies"},{"location":"Languages/python/python_advanced/#concurrency","text":"When two or more task can start, run & complete in overlapping time periods. It doesn't necessarily mean they'll ever be running at the same instant. e.g. Multi-tasking on a single core","title":"Concurrency"},{"location":"Languages/python/python_advanced/#parallelism","text":"When two are more tasks are executed simultaneously.","title":"Parallelism"},{"location":"Languages/python/python_advanced/#process","text":"is an instance of a program running in a computer can contain one or more threads has its independent memory space are spawned by creating a Process() object and then calling its start() method","title":"process"},{"location":"Languages/python/python_advanced/#thread","text":"is a sequence of instructions within a process as a light-weight process all threads shares the same memory space of the process","title":"thread"},{"location":"Languages/python/python_advanced/#multi-threading","text":"can be implemented to speedup the program using module : threading thread-based parallelism concurrency CPython implementation uses a python construct GIL (global interpreter lock). GIL makes sure that at a time only single thread can execute. a thread acquires GIL --> executes just for a while --> passes GIL to next thread happens very quickly that human cannot detect, and creates illusion of multiple thread running simultaneously. in reality all threads works turn by turn into the same core of CPU parallel CPU computation not possible due to GIL but parallel IO operations are possible (it releases GIL on IO) GIL passing is an overhead here.","title":"Multi-threading"},{"location":"Languages/python/python_advanced/#note","text":"can turn off GIL - dirty practice this will result in messed memory management need to be very careful while writting semaphores & mutex properly","title":"Note:"},{"location":"Languages/python/python_advanced/#use-of-thread","text":"in GUI apps to keep UI threads responsive IO tasks (network IO or filesystem IO)","title":"Use of thread"},{"location":"Languages/python/python_advanced/#facts","text":"using multiple-threads for CPU bound tasks will result in worse performance than a single thread","title":"Facts"},{"location":"Languages/python/python_advanced/#multi-processing","text":"used to speedup CPU bound tasks using module: multiprocessing process-based parallelism module results in full CPU utilization Inter-process communication can be achieved using queues and pipes *","title":"Multi-processing"},{"location":"Languages/python/python_advanced/#pipe","text":"is a duplex(two way) communication channel 1 2 3 4 5 6 7 8 9 from multiprocessing import Process def f ( name ): print ( 'hello' , name ) if __name__ == '__main__' : p = Process ( target = f , args = ( 'bob' ,)) p . start () p . join () Out: hello bob 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 from multiprocessing import Process import os def info ( title ): print ( title ) print ( 'module name:' , __name__ ) print ( 'parent process:' , os . getppid ()) print ( 'process id:' , os . getpid ()) def f ( name ): info ( 'function f' ) print ( 'hello' , name ) if __name__ == '__main__' : info ( 'main line' ) p = Process ( target = f , args = ( 'bob' ,)) p . start () p . join () Out: 1 2 3 4 5 6 7 8 9 main line module name: __main__ parent process: 65 process id: 66 function f module name: __main__ parent process: 66 process id: 80 hello bob","title":"pipe"},{"location":"Languages/python/python_advanced/#what-is-join","text":"The join() method, when used with threading or multiprocessing, is not related to str.join() it's not actually concatenating anything together It just means \"wait for this [thread/process] to complete\" The name join is used because the multiprocessing module's API is meant to look as similar to the threading module's API The reason why is called join is that is joining the processes into a single one. Note: * By default, when the main process is ready to exit, it will implicitly call join() on all running multiprocessing Process instances * This isn't as clearly stated in the multiprocessing docs as it should be, but it is mentioned in the Programming Guidelines section. * non-daemonic processes will be automatically be joined. * can override this behavior by setting the daemon flag on the Process to True prior to starting the process: 1 2 3 4 5 6 7 8 9 10 11 12 from multiprocessing import Process import os def say_hello (): print ( \"Hello\" ) p = Process ( target = say_hello ) p . daemon = True p . start () # Both parent and child will exit here, since the main process has completed. # the child process will be terminated as soon as the main process completes: Out: Hello","title":"What is .join()?"},{"location":"Languages/python/python_advanced/#daemon","text":"In Linux: A daemon is a long-running background process that answers requests for services. There are also some other processes like orphan & zombie The process has daemon flag a Boolean value This must be set before start() is called The initial value is inherited from the creating process When a process exits, it attempts to terminate all of its daemonic child processes","title":"daemon"},{"location":"Languages/python/python_advanced/#files","text":"","title":"Files"},{"location":"Languages/python/python_advanced/#file-handling","text":"How Using try , except , finally Using with (pythonic way) Why to manage resources effeciently file descriptors (fd) are limited at OS level, so we can save fd by closing after use","title":"File Handling"},{"location":"Languages/python/python_advanced/#using-try-except-finally","text":"the step finally is important here for us to use finally we need to write whole try , except , finally block in each languages finally is important because we need f.close() here to close the file descriptor with 100% guarantee file descriptors (fd) are limited at OS level, so we can save fd by closing after use code: 1 2 3 4 5 6 7 try : f = open ( 'csv.txt' , 'r' , encoding = 'utf-8' ) f . write ( 'abc' ) except : pass finally : f . close ()","title":"Using try, except, finally"},{"location":"Languages/python/python_advanced/#using-with","text":"Read here code: 1 2 with open ( 'csv.txt' , 'r' , encoding = 'utf-8' ) as f : f . readline ()","title":"Using with"},{"location":"Languages/python/python_advanced/#json-operations","text":"","title":"JSON Operations"},{"location":"Languages/python/python_advanced/#import-json","text":"","title":"import json"},{"location":"Languages/python/python_advanced/#flask-jsonify","text":"","title":"flask: jsonify()"},{"location":"Languages/python/python_advanced/#shipping-python-solution","text":"never ship .py files find /path/to/your/files -type f -name \"*.py\" -delete can ship .pyc or .pyo files","title":"Shipping Python Solution"},{"location":"Languages/python/python_advanced/#pyc","text":"compiled size(.py) < size(.pyc)","title":".pyc"},{"location":"Languages/python/python_advanced/#pyo","text":"compiled + optimized removed doc strings does not contain \"set the current line to ...\" bytecode instructions for faster performance renaming .pyo to .pyc works fine find \"ETHEREAL_DIR\"/Ray/src/ -type f -name \"*.pyo\" -exec bash -c 'mv $0 ${0/.pyo/.pyc}' {} \\; size(.pyo) < size(.pyc) python -O -m compileall /path/to/your/files Note - After this, you may get some import issues","title":".pyo"},{"location":"Languages/python/python_advanced/#exe","text":"Source http://docs.python-guide.org/en/latest/shipping/freezing/ https://pyinstaller.readthedocs.io/en/v3.3.1/ Note: Freezing Python code on Linux into a Windows executable was only once supported in PyInstaller and later dropped All solutions need MS Visual C++ dll to be installed on target machine, except py2app. Only Pyinstaller makes self-executable exe that bundles the dll when passing --onefile to Configure.py.","title":".exe"},{"location":"Languages/python/python_advanced/#pyinstaller","text":"Uses the OS support to load the dynamic libraries, thus ensures full compatibility available for windows, linux, macOS etc. 1 pyinstaller -i <icon.ico> --windowed --onefile <python_file.py>","title":"PyInstaller"},{"location":"Languages/python/python_advanced/#py2exe","text":"","title":"py2exe"},{"location":"Languages/python/python_advanced/#python-version-management","text":"","title":"Python Version Management"},{"location":"Languages/python/python_advanced/#setting-default-python-version","text":"","title":"Setting default Python Version"},{"location":"Languages/python/python_advanced/#by-creating-softlink-to-usrbinpython-file","text":"not recommanded will affect other applications using another version of python can be done like: 1 2 3 ln -l /usr/bin/python /usr/bin/python3.6 ln -l /usr/bin/python /home/toran/anaconda/bin/python3.6","title":"By creating softlink to /usr/bin/python file"},{"location":"Languages/python/python_advanced/#by-aliasing-python","text":"not recommanded will conflict when python will encounter in other commands like which python python manage.py runserver can be done like: 1 alias python = ' /home/toran/anaconda/bin/python3.6 `","title":"By aliasing python"},{"location":"Languages/python/python_advanced/#by-setting-path-env-variable","text":"recommanded with anaconda works on particular shell only for persistency; include the command in .bashrc or .zshrc file can be done like: 1 export PATH = \"/home/toran/anaconda3/bin:<dollar>PATH\"","title":"By setting PATH env variable"},{"location":"Languages/python/python_advanced/#anaconda","text":"install it from official site you can also choose miniconda over regular anaconda can be used in production env/ deployments etc.","title":"Anaconda"},{"location":"Languages/python/python_advanced/#linux","text":"set anaconda python as default 1 export PATH = \"/home/toran/anaconda3/bin:<dollar>PATH\"","title":"Linux"},{"location":"Languages/python/python_advanced/#windows","text":"set anaconda python as default 1 2 3 4 C: \\P rogramData \\M iniconda3 \\S cripts \\a ctivate #if using git bash . /c/ProgramData/Miniconda3/Scripts/activate","title":"Windows"},{"location":"Languages/python/python_advanced/#virtualenv","text":"","title":"virtualenv"},{"location":"Languages/python/python_advanced/#pipenv","text":"officially recommended dependency management system automagically installs dependencies in venv keep record in Pipfile packages section dev-packages section keeps record of dependencies with specific version in Pipfile.lock file if requirements.txt already exists, installs dep from there installs & uninstalls deps","title":"pipenv"},{"location":"Languages/python/python_advanced/#installation","text":"1 pip install pipenv","title":"Installation"},{"location":"Languages/python/python_advanced/#usage","text":"","title":"Usage"},{"location":"Languages/python/python_advanced/#create-venv","text":"1 pipenv install","title":"create venv"},{"location":"Languages/python/python_advanced/#create-venv-with-specific-python-version","text":"1 2 3 pipenv --two install pipenv --three install will initialize the venv with that python version","title":"create venv with specific python version"},{"location":"Languages/python/python_advanced/#installuninstall-dependencies","text":"1 2 3 pipenv install nose2 pipenv uninstall nose2","title":"install/uninstall dependencies"},{"location":"Languages/python/python_advanced/#lock-dependencies-for-production","text":"1 pipenv lock - will keep record of all the deps in Pipfile.lock file with their specific versions","title":"lock dependencies for production"},{"location":"Languages/python/python_advanced/#install-dependencies-only-for-development","text":"1 pipenv install --dev nose2 will keep this record in [dev-packages] section and will do not install it by default to install dev packages; need to do: 1 pipenv install --dev","title":"install dependencies only for development"},{"location":"Languages/python/python_advanced/#install-dependencies-with-other-options","text":"--dev : Install both develop and default packages from Pipfile.lock. --system : Use the system pip command rather than the one from your virtualenv. --ignore-pipfile : Ignore the Pipfile and install from the Pipfile.lock. --skip-lock : Ignore the Pipfile.lock and install from the Pipfile. In addition, do not write out a Pipfile.lock reflecting changes to the Pipfile.","title":"install dependencies with other options"},{"location":"Languages/python/python_advanced/#run-command-within-venv","text":"1 pipenv run which python","title":"run command within venv"},{"location":"Languages/python/python_advanced/#remove-venv","text":"1 pipenv --rm","title":"remove venv"},{"location":"Languages/python/python_advanced/#see-venv-detailspath","text":"1 pipenv --venv","title":"see venv details/path"},{"location":"Languages/python/python_advanced/#enter-venv-shell","text":"1 pipenv shell","title":"enter venv shell"},{"location":"Languages/python/python_advanced/#clean-venv","text":"uninstall all the packages not recorded in Pipfile 1 pipenv clean","title":"clean venv"},{"location":"Languages/python/python_advanced/#update-venv","text":"locks & then update all the packages 1 pipenv update","title":"update venv"},{"location":"Languages/python/python_advanced/#security-check","text":"checks for compliance with PEP 508 Dependency specification for Python Software Packages as well as package safety 1 pipenv check","title":"security check"},{"location":"Languages/python/python_advanced/#python-package-distribution-pypi","text":"","title":"Python Package Distribution (PyPi)"},{"location":"Languages/python/python_advanced/#project-structure","text":"1 2 3 4 5 6 7 8 /example_pkg /pkg1 __init__.py /pkg2 /pkg2_1 setup.py LICENSE README.md where - __init__.py should contain a variable name='example_pkg' - setup.py should look like this gist - where classifiers can be like this or gist - README.md should be present, which will define the long_description about the package - LICENSE should be accurate with the list - packages - if want to includes all the packages/subpackages - use defaultone packages=setuptools.find_packages(), - else define manually like - packages=['pkg1', 'pkg2', 'pkg2.pkg2_1'] Note: After installing the package, you able to import packages with names listed in packages var.","title":"Project Structure"},{"location":"Languages/python/python_advanced/#pre-requisites","text":"1 2 3 4 5 6 #setuptools & wheel - to create build in .whl as well as .tar.gz. format python3 -m pip install --user --upgrade setuptools wheel #twine - to upload build in pypi server python3 -m pip install --user --upgrade twine","title":"Pre-requisites"},{"location":"Languages/python/python_advanced/#build","text":"change directory where setup.py is & run 1 python3 setup.py sdist bdist_wheel output will be like 1 2 3 dist/ example_pkg-0.0.1-py3-none-any.whl example_pkg-0.0.1.tar.gz","title":"Build"},{"location":"Languages/python/python_advanced/#test-distribution","text":"1 twine upload --repository-url https://test.pypi.org/legacy/ dist/*","title":"Test Distribution"},{"location":"Languages/python/python_advanced/#final-distribution","text":"1 2 3 4 5 twine upload --repository-url https://upload.test.pypi.org/legacy/ dist/* #OR twine upload dist/*","title":"Final Distribution"},{"location":"Languages/python/python_advanced/#rebuild","text":"delete build , dist , & *.egg-info files then build again","title":"Rebuild"},{"location":"Languages/python/python_advanced/#test","text":"","title":"Test"},{"location":"Languages/python/python_advanced/#test-package","text":"1 python3 -m pip install --index-url https://test.pypi.org/simple/ example_pkg","title":"(Test) Package"},{"location":"Languages/python/python_advanced/#final-package","text":"1 python3 -m pip install example_pkg","title":"(Final)  Package"},{"location":"Languages/python/python_advanced/#advanced-run-package-in-command-line","text":"Source: http://python-packaging.readthedocs.io/en/latest/command-line-scripts.html","title":"Advanced - Run Package in Command Line"},{"location":"Languages/python/python_advanced/#faq","text":"You're not allowed File already exists File already exists, change version particular line is not know/right","title":"FAQ"},{"location":"Languages/python/python_advanced/#python-linter-pep-8","text":"I found flake8, autopep8 & yapf similar but I found black best among them quotes: double no extra line change only draw back is bad list slice formating create issue: https://github.com/python/black/issues","title":"Python Linter &amp; PEP-8"},{"location":"Languages/python/python_advanced/#python-templates","text":"","title":"Python Templates"},{"location":"Languages/python/python_advanced/#pycharm-live-file-templates","text":"https://gist.github.com/toransahu/6080cbf2cc1a8808f19bd0eccafc5ef0","title":"PyCharm Live &amp; File Templates"},{"location":"Languages/python/python_advanced/#vim-template","text":"https://github.com/toransahu/vim-template/blob/master/templates/%3Dtemplate%3D.py","title":"Vim Template"},{"location":"Languages/python/python_basics/","text":"Python Basics # Python Basics Introduction The Zen of Python (PEP 20) Easter Egg Implementations Internal PEP 8 - Style Guide for Python Code Features of Python are: Compilers Interpreters Compiler characteristics: Interpreter characteristics: Python interpreted/compiled? Compiled Language Interpreted Language How python works? Note What is .pyc file? What is .pyo file? Memory Management Data types in python? Data structure in python? 4 Built in Collection & heapq modules Abstract data type we can create are Data Types Strings String formatting What is doc string? byte string vs unicode string Intro 3 things unicode List Intro Why? When? Features List Generator List comprehension List Flattening Randomize items of list in place How to insert an element in between a list? Diif between append and extend method in list? enumerate() Tuple Why? When? Properties Expanding Tuple Dictionary When to use dict & set? dict vs list lookup performance? dict comprehension dict with order {} vs dict() Variables Global vs Local vs Non-Local Variable Global Variable Global Variables in Nested Functions Local Variable Non-Local Variable Operators in and or xor is not Generators xrange vs range? Statements assert yield return Compound Statements with Example #1 : file handling Example #2: Expressions lambda Operators Ternary Operator [Operator Overloading Using Magic Methods] Functions eval() partial() lambda map() filter() reduce() zip() Inverse a Matrix (list/tuple) Decorators Nested function Closures As per JavaScript Function Factory Parameter passing in python Passed As Default Parameters Variable Number of Parameters Exception Handling Inbuilt Exceptions AssertionError AttributeError EOFError ImportError IndentationError IndexError KeyError NameError NotImplementedError OSError RecursionError SyntaxError TypeError UnicodeError ValueError ZeroDivisionError User-defined Exception Performance Misc is vs == operator Pickling Unpickling Pickling Unpickling Comparision with JSON Monkey Patching Uses Duck Typing Copy Regular Reference (Hard Copy) Shallow copy Deep copy Introduction # The Zen of Python (PEP 20) # Beautiful is better than ugly. Explicit is better than implicit. Simple is better than complex. Complex is better than complicated. Flat is better than nested. Sparse is better than dense. Readability counts. Special cases aren't special enough to break the rules. Although practicality beats purity. Errors should never pass silently. Unless explicitly silenced. In the face of ambiguity, refuse the temptation to guess. There should be one-- and preferably only one --obvious way to do it. Although that way may not be obvious at first unless you're Dutch. Now is better than never. Although never is often better than right now. If the implementation is hard to explain, it's a bad idea. If the implementation is easy to explain, it may be a good idea. Namespaces are one honking great idea -- let's do more of those! Easter Egg # In computer software and media, an Easter egg is an intentional inside joke, hidden message or image, or secret feature of a work The Zen of python can be listed by importing this 1 >>> import this Implementations # CPython (Default) Jython IronPython RPython (PyPy) Internal # Runs on a single process by forking Each process have seperate GIL Python can run only one thread at once Context Manager PEP 8 - Style Guide for Python Code # Package: lowercase, '_' is discouraged, use hyphen '-' instead Module: lowercase + '_' Class: Initial Capital, Camal Case Function: lowercase, Valid word, '_' Method: self, lowercase, Valid word, '_' Variable: Same as functions Local: Global: Static: Constant: Generally Module Level, All CAPS + '_' Private Attribute: To avoid name clashes with subclasses, Leading dunder Protected Attribute: For non-public methods and instance variables, Leading underscore '_' Exception Name: Same as class NOTE: Never use the characters 'l' (lowercase letter el), 'O' (uppercase letter oh), or 'I' (uppercase letter eye) as single character variable names. Features of Python are: # Unique Interpreted Language Dynamic type system but strongly typed (changing data type need explicit conversion) emphasizes on code readability - lesser line of code/syntax Supports multiple programming paradigms, including object-oriented, imperative, functional and procedural Has community based developement model Dynamic name resolution/ late binding: unlike compiled languages, the name of method, variable is lookedup by name at runtime Data types are strongly and dynamically typed. Mixing incompatible types (e.g. attempting to add a string and a number) causes an exception to be raised, so errors are caught sooner. Python contains advanced programming features such as generators and list comprehensions. Lambda functions Multiple inheritance Common A variety of basic data types are available: numbers (floating point, complex, and unlimited-length long integers), strings (both ASCII and Unicode), lists, and dictionaries. Python supports object-oriented programming with classes and multiple inheritance. Code can be grouped into modules and packages. The language supports raising and catching exceptions, resulting in cleaner error handling. Python's automatic memory management frees you from having to manually allocate and free memory in your code. Compilers # A type of translator. Compiler analyze the whole source code at once and translate it to another language. i.e. machine code Interpreters # An interpreter is also a program that translates a high-level language into a low-level one (but not directly into machine code), and it does it at the moment the program is run. It takes the program, one line at a time, and translates each line before running it: It translates the first line and runs it, then translates the second line and runs it etc. Compiler characteristics: # spends a lot of time analyzing and processing the program the resulting executable is some form of machine- specific binary code the computer hardware interprets (executes) the resulting code program execution is fast Interpreter characteristics: # relatively little time is spent analyzing and processing the program the resulting code is some sort of intermediate code the resulting code is interpreted by another program program execution is relatively slow Python interpreted/compiled? # Compiled Language # A compiled language turn human-readable code into machine code (a string of binary numbers), which are directly executed by OS & CPU. Interpreted Language # A language which is in non-machine code form just before its execution. In general an interpreted prog. language turn human-readable code into non-machine code (byte-code), which are then line by line executed by virtual machine. Interpreted/Compiled is not property of language, its property of implementation. Its byte-code INTERPRETED, because .py is first COMPILED/translated to .pyc (a byte-code language, a non-machine language, executed by python virtual machine, not by OS / CPU). How python works? # run module i.e. .py file .py loaded into memory parsing in order .py compiled to bytecode .pyc file (which is not binary machinecode) compilation is translation step .pyc (.i.e. bytecode) is low-level platform -independent version of source-code require more work than CPU instructions if .pyc of source code is present, compilation step will be skipped by checking time-stamp of .py & .pyc if python cannot create .pyc, then bytecode will be created in-memory then routed/shipped to python virtual machine PVM for execution pre-installed / inbuilt it is runtime engine of python .pyc is also way of shipping python code without source-code Note # no initial/explicit compilation phase compiles at runtime only and then executes in single step able to produce executable, frozen binaries using py2exe PyInstaller freeze What is .pyc file? # Low-level Platform-independent Bytecode What is .pyo file? # in addition to .pyc, .pyo removes all the comments & docs(i guess) Memory Management # Basics: Python's memory allocation and deallocation method is automatic. involves a private heap the heap contains all the objects & data structures managed by python \"memory manager MM\" interpreter manages this all no user control heap space allocation for objects & buffers are performed on-demand by MM c memory management libs works in-behind: malloc(), calloc(), realloc(), free() at low level, raw memory allocator allocates enough memory for all the data on top of low-level, object-specific allocators allocates memory as per object's policies e.g. for integer: speed tradeoff Memory De-allocation Strategies: Reference Counting Was only option Prior Python 2.0 When an object gets created and referenced, it counts the number of times the object is referenced by some other objects When reference is removed, the reference count for the object is decremented When the reference count becomes zero, the object is deallocated. Extremely efficient but Have limitations like: Cannot handle reference cycle Reference Cycle: When there is no way to reach an object but its reference count is still greater than zero e.g. 1 2 list1 = [] list1 . append ( list1 ) Examples, where the reference count increases: assignment operator argument passing appending an object to a list (object's reference count will be increased). 1 2 3 4 5 6 7 8 9 10 11 12 13 foo = [] # 2 references, 1 from the foo var and 1 from getrefcount print ( sys . getrefcount ( foo )) def bar ( a ): # 4 references # from the foo var, function argument, getrefcount and Python's function stack print ( sys . getrefcount ( a )) bar ( foo ) # 2 references, the function scope is destroyed print ( sys . getrefcount ( foo )) Garbage Collection Introduced after Python 2.0 it contains reference counting as well as garbage collector Automatic / Scheduled: The \"Reference Counting\" mechanism was not able to deallocate objects in few cases like: Reference Cycle How reference counting is solved by garbage collection it is a scheduled task based on a threshold threshold = allocations - de-allocations The GC classifies container objects into three generations. Every new object starts in the first generation. If an object survives a garbage collection round, it moves to the older (higher) generation. Lower generations are collected more often than higher. Because most of the newly created objects die young, it improves GC performance and reduces the GC pause time. Source : read Pro Python for better understanding https://rushter.com/blog/python-garbage-collector/ whenever threshold is reached, garbage collector starts identifying memory spaces which are garbage garbage? the memory spaces which are un-reachable to python objects e.g. 1 2 3 4 5 6 7 8 import gc gc . disable () obj1 = { \"val\" : 1 } obj2 = { \"val\" : 2 } obj1 [ \"obj2\" ] = obj2 obj2 [ \"obj1\" ] = obj1 del obj1 , obj2 - it is most important to identify a memory space whether it is a garbage or not - otherwise it will lead to memory leak - memory leak means, automatically data loss Note: Automatic garbage collection will not run if your Python device is running out of memory Manual / Explicitly 1 2 3 4 5 6 import gc # get_count() returns a tuple of (threshold, no. of objects allocated, no. of objects de-allocated) print ( gc . get_count ()) # With no arguments, run a full collection gc . collect () https://docs.python.org/3/c-api/memory.html pass by value pass by reference change reference change value behaviour of mutable and immutable Data types in python? # data type: set of data with predefined values. primitive: integers floating char string user-defined Data structure in python? # Data Structure: are special format/structures to store & organize data. 4 Built in # Sequence data types: Ordered Sequence: List Tuple Set Dictionary We consider string more as a data type. Collection & heapq modules # provides additional data structure collections: dequeue ordereddict heapq: priority queue heap Abstract data type we can create are # linear: linked list stack queue hash table non-linear: tree graph Data Types # Strings # String formatting # 1 2 3 4 5 6 7 8 9 10 11 12 i = 1 v = 'a' print ( \"Value at index {0} is {1} \" . format ( i , v )) print ( \"Value at index {} is {} \" . format ( i , v )) print ( \"Value at index %d is %s \" % ( i , v )) # my favorite way before Python 3.6.4 emp = { 'name' : 'toran' , 'age' : 26 , 'mobile' : '8602431733' } print ( \"My name is {name} and I'm {age} years old. You can contact me at {mobile} \" . format ( ** emp )) # in Python 3.6.4 print ( f 'The value of i is { i } and value of v is { v } ' ) What is doc string? # way of associating document with modules, functions, class, methods describes what it does instead how first line should heading (start with capital, end with dot), then gap of one-line, then desc byte string vs unicode string # Intro # there are a lot of encodings available world-wide e.g. ASCII, CP-1252 (windows), Mac-greek.. etc computer only understands bit, bytes e.g. in ASCII: 65 is -a, 97 is A HOW TO REPRESENT ALL LANGUAGEs IN SAME FILE? 3 things # str python object byte string, computer native array of bytes unicode, some encoded text unicode # one encoding, all chars represent a char as 4-byte number: 4*8 = 32; UTF-32 a lot memory freak similarly, 2*8 = 16; UTF-16 UTF-8 a variable-length encoding system for Unicode till 128, ASCII & UTF-8 is same, uses 1 byte uses 2 bytes for latin read more at 1 , 2 List # Intro # A data structure/type to store objects/data/items Ordered collection: stores in ordered way i.e. using index from 0 Variable length: dynamic sized Mutable: can change any existing element in run-time Preferred for homogenious collection, but can store heterogenious data types inside. many attribute/member methods: Why? When? # when dynamic data structure is benificial like: appending, removing, altering use when implementing buffer, stack, queues Features # List Generator # generates iterable items on demand build up in memory xrange in Python 2.x i.e. range() in Python 3.x is example of generator Advantages: No need to wait until all the elements have been generated before we use them in python 2.x, range returns a list while xrange returns a generator e.g. 1 2 3 4 5 6 7 8 def first_n ( n ): num = 0 while ( num < n ): yield num num += 1 for i in first_n ( 5 ): print ( i ) List comprehension # 1 2 l = [ i for i in range ( 0 , 10 )] print ( l ) 1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] List Flattening # 1 2 l = [[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ] ] flat_list = [ item for sublist in l for item in sublist ] which is equivalent to 1 2 3 4 5 flat_list = [] for sublist in l : for item in sublist : flat_list . append ( item ) Randomize items of list in place # Code: 1 2 3 4 5 6 from random import shuffle l = [ 1 , 2 , 3 , 4 ] shuffle ( l ) print ( l ) shuffle ( l ) print ( l ) Out: 1 2 [3, 2, 1, 4] [4, 1, 2, 3] How to insert an element in between a list? # Code: 1 2 3 l = [ 1 , 2 , 3 , 5 , 6 , 7 ] l . insert ( 3 , 4 ) l Out: 1 [1, 2, 3, 4, 5, 6, 7] Diif between append and extend method in list? # append - appends object at the end extend - extends list by appending elements from iterables Code: 1 2 3 4 5 6 7 8 l = [ 1 , 2 , 3 ] a = [ 4 , 5 ] e = [ 6 , 7 , 8 ] l . append ( a ) l . extend ( e ) print ( l ) Out: 1 [1, 2, 3, [4, 5], 6, 7, 8] enumerate() # 1 2 3 4 for i , v in enumerate ([ 'f' , 's' , 't' ]): print ( \"Value at index %d is %s \" % ( i , v )) print ( \"Value at index {0} is {1} \" % ( i , v )) print ( \"Value at index {} is {} \" . format ( i , v )) Tuple # A fixed data structure/type to store objects/data/items Ordered collection: stores in ordered way i.e. using index from 0 Fixed length: cannot change length of a tuple, cannot append, pop an element Immutable: cannot change any existing element in run-time Preferred for heterogenious collection, but can store homoge data types inside. Why? When? # when a collection of values will not change i.e. in case of functions args. use when fixed structure is benificial like: heavy memory intensive work, api, server can be used as key in dictionary due to its fixed structure use when need to store a db table data and want to maintain column structure Properties # 1 2 3 4 5 6 7 8 9 10 l = [ 1 , 2 , ( 3 , 'a' ), 'b' , [ 4 , 5 ]] t = ( 1 , 2 ,[ 3 , 'a' ], 'b' , ( 4 , 5 ) ) #Lets try to make changes in l & t # l[2][0] = 1 #does not work # l[2] = 3 #works # t[2] = 3 #does not work # t[2][0] = 1 #works Expanding Tuple # used to pass tuple elements as function parameter 1 2 3 4 5 6 7 8 9 10 11 12 13 14 t = ( 1 , 2 , 3 , 4 , 5 ) # simple def bar ( a , b , c , d , e ): print ( a , b , c , d , e ) bar ( * t ) # in general def foo ( * args ): for arg in args : print ( arg ) foo ( * t ) Dictionary # Is map type of data structure which holds a key value pair. Unordered collection: does not maintain order When to use dict & set? # When data is labelled Use a dictionary when you have a set of unique keys that map to values. Use a set to store an unordered unique set of items. dict vs list lookup performance? # dict - O(1) due to hashing list - O(n) dict comprehension # Code: 1 2 3 d = { i : i * i for i in range ( 0 , 10 )} print ( d ) Out: 1 {0: 0, 1: 1, 2: 4, 3: 9, 4: 16, 5: 25, 6: 36, 7: 49, 8: 64, 9: 81} dict with order # maintains the order of elements in which they were inserted 1 2 3 from collections import OrderedDict d = OrderedDict ({ 2 : 'second' , 1 : 'first' }) d . items () {} vs dict() # tl;dr use {} over dict() whenever possible {} uses transient memory allocation, and frees the memory after value being consumed if {} value is assigned to a variable, then memory is persist dict() is slower (takes atleast double the time) Ref: - https://stackoverflow.com/questions/664118/whats-the-difference-between-dict-and - https://doughellmann.com/posts/the-performance-impact-of-using-dict-instead-of-in-cpython-2-7-2/ Variables # Global vs Local vs Non-Local Variable # src: https://www.python-course.eu/python3_global_vs_local_variables.php not same as other languages (by default global) by default all are local (if you need, declare global) 1 2 3 4 5 6 7 def f (): s = \"I'm local\" print ( s ) s = \"I'm global\" f () print ( s ) Out: 1 2 I'm local I'm global Global Variable # How to access global variable inside a function: 1 2 3 4 5 6 7 8 9 def f (): # print(s) #error : cannot access a global variable directly global s # keyword global will give access to outer s s = \"I'm local\" # value of global s has been changed print ( s ) s = \"I'm global\" f () print ( s ) Out: 1 2 I'm local I'm local Global Variables in Nested Functions # keyword global inside inner function will not access the upper function level variable instead, it will create a variable in main scope to make it possible there is one more keyword : nonlocal 1 2 3 4 5 6 7 8 9 10 11 12 def f (): x = 42 def g (): global x x = 43 print ( \"Before calling g: \" + str ( x )) print ( \"Calling g now:\" ) g () print ( \"After calling g: \" + str ( x )) f () print ( \"x in main: \" + str ( x )) Out: 1 2 3 4 Before calling g: 42 Calling g now: After calling g: 42 x in main: 43 Local Variable # variable defined inside a function are local to that function Non-Local Variable # introduced in Python 3 different than global can only be used inside of nested functions has to be defined in the enclosing/upper function scope 1 2 3 4 5 6 7 8 9 10 11 12 def f (): y = 42 def g (): nonlocal y y = 43 print ( \"Before calling g: \" + str ( y )) print ( \"Calling g now:\" ) g () print ( \"After calling g: \" + str ( y )) f () print ( \"y in main: \" + str ( y )) # this will create error Out: 1 2 3 4 Before calling g: 42 Calling g now: After calling g: 43 NameError: name 'y' is not defined Operators # in # Searching Time Complexity: (Depends on type of operand) List Avg: O(n) Dict/Set Avg: O(1) Worst: O(n) magic/member method: __contains__(<element>) and # or # xor # is # not # Generators # xrange vs range? # python3: * xrange is renamed to range. python2: * same result but xrange is more memory efficient * range creates iterable list (in python2) * while xrange creates xrange object and generate list of demand Statements # assert # A statement Used to check an expectation Works on logical condition If true, return nothing, if false raise AssertionError exception yield # A statement Does not end a function Returns value to its caller suspends the function and then return value to its caller then resume the function Continues with next line of statement uses: in generators like range return # A statement Ends function Returns value to caller Compound Statements # Compound statements contain (groups of) other statements they affect or control the execution of those other statements in some way contains multi line code block e.g. if , while , for , def , class , with with # The with statement is used to wrap the execution of a block with methods defined by a context manager with statement allows the execution of initialization and finalization code around a block of code i.e. try / finally + context manager having methods __enter__() & __exit__() read more Example #1 : file handling # 1 2 3 4 5 6 7 # automatically acquring `csv.txt` file and does not allows others to acquire it with open ( 'csv.txt' , 'r' , encoding = 'utf-8' ) as f : # do some operations on f # do some operations more on f # if any exception occur, closes the file before exception is caught and shown by interpreter # automatically closed/released `csv.txt` file for others with statement opens a file or acquires a resource then do some block of codes then closes the file or releases the resource if any exception occur, during operations closes the file before exception is caught and shown by interpreter thats how we are better than try, except, finally I/O operation : GIL free Example #2: # 1 2 with A () as a , B () as b : suite is equivalent to 1 2 3 with A () as a : with B () as b : suite - thats how, it does not need help of GIL Expressions # lambda # Operators # Ternary Operator # [on-true] if [expression] else [on-flase] Code: 1 2 3 x , y = 23 , 50 big = x if x > y else y print ( big ) Out: 1 50 [Operator Overloading Using Magic Methods] # Read in Python-OOPs notebook Functions # Source https://docs.python.org/3/library/functions.html eval() # Source: https://www.programiz.com/python-programming/methods/built-in/eval a built-in function to evaluate the python expression writter in string form 1 2 3 4 5 6 7 8 9 str = \"lambda x: x**2\" square = eval ( str ) square ( 2 ) # returns 4 str_list = \"[1,2,3]\" eval ( str_list ) # returns a list str_dict = \"{'a':1, 'b':[2,3]}\" eval ( str_dict ) # returns a dict - eval takes 3 parameters - expression: string - globals: dict (used for namespace) - locals: any mapping object partial() # a closure or a nested function used to fulfill the cases when we need to provide some/few fixed parameters to any functions need to import from functools import partial partial always takes functions as first parameter e.g. 1 2 3 4 5 6 7 8 from functools import partial def foo ( a , b , c = 10 ): print ( f \"I'm foo with { a } , { b } , and { c } \" ) foo_partial = partial ( foo , 1 , 2 ) foo_partial () lambda # format: lambda arg1, arg2, ...argN : expression using arguments anonymous function single-line statement expression in-place function definition can be stored in a variable syntax: 1 lambda x : return x * x Scopes: to make Jump Tables nested lambda loop in lambda using map() map() # signature: map(aFunction, aSequence) applies a passed-in function to each item in an iterable object python2 :returns a list containing all the function call results python3 :returns an iterator of type map object Syntax 1 2 sqrs = list ( map ( lambda x : x * x , range ( 0 , 10 ))) print ( sqrs ) Out: 1 [0, 1, 4, 9, 16, 25, 36, 49, 64, 81] filter() # signature: filter(aFunction, aSequence) applies a passed-in function to each item in an iterable object python2 :returns a list of items for whose function call returns True python3 :returns an iterator of type map object for the items whose function call returns True Syntax 1 2 evens = list ( filter ( lambda x : x % 2 == 0 , range ( 0 , 10 ))) print ( evens ) Out: 1 [0, 2, 4, 6, 8] reduce() # (Dropped out in Python 3.x) * signature: filter(aFunction, aSequence) * applies a passed-in function to each item in an iterable object * python2 :returns a single value * python3 : dropped out zip() # 1 2 3 list1 = [ 'A' , 'B' , 'C' ] and list2 = [ 10 , 20 , 30 ] . zip ( list1 , list2 ) # results in a list of tuples say [('A',10),('B',20),('C',30)] Inverse a Matrix (list/tuple) # 1 2 3 4 5 6 m = [[ 1 , 2 , 3 , 4 ], [ 5 , 6 , 7 , 8 ], [ 9 , 10 , 11 , 12 ] ] m_inverse = list ( zip ( * m )) # here *m is expanding of list/tuple, mostly used in passing tuple as func parameters Decorators # are a thin wrapper arround any function or any class there are also decorators for classes (read in Python-OOPs) benifits: do not need to decorate each method manually when we apply a decorator to any function a few lines code in before start (if any) of the fuction call will get executed and/or a few lines code in after end (if any) of the fuction call will get executed types of function decorator made out of a nested functions made out of a class decorator with param decorator over decorator order of decorators Using wraps from functools The way we have defined decorators so far hasn't taken into account that the attributes 1 2 3 __name__ ( name of the function ), __doc__ ( the docstring ) and __module__ ( The module in which the function is defined ) of the original functions will be lost after the decoration. simplest form of e.g. to create a custom decorator is: 1 2 3 4 5 6 7 8 9 def exec_time ( some_func ): print ( 'this is start time' ) some_func () print ( 'this is end time' ) @exec_time def foo (): print ( 'running foo' ) - gist: https://gist.github.com/toransahu/7ac4c7f139e78d15b74ca0ce6e17cf85 Nested function # 1 2 3 4 5 6 7 8 def outer_func (): x = 5 def inner_func ( y = 3 ): return ( x + y ) return inner_func a = outer_func () print ( a ()) # 8 Closures # is a concept - not a function a few may refer it as a nested function The local function is able to reference the outer scope through closures. Closures maintain references to objects from the earlier scope. As per JavaScript # A closure is the combination of a function bundled together (enclosed) with references to its surrounding state (the lexical environment). In other words, a closure gives you access to an outer function\u2019s scope from an inner function. In JavaScript, closures are created every time a function is created, at function creation time. Function Factory # these are functions that return other functions the returned functions are specialized Function Factory takes in argument(s), creates local function that creates its own argument(s) and also uses the argument(s) passed to the function factory this is possible with closures 1 2 3 4 5 6 7 8 9 10 11 12 def multiply_by ( num ): def multiply_by_num ( k ): return num * k return multiply_by_num five = multiply_by ( 5 ) print ( five ( 2 )) # 10 print ( five ( 4 )) # 20 decimal = multiply_by ( 10 ) print ( decimal ( 20 )) # 200 print ( decimal ( 3 )) # 30 Parameter passing in python # Passed As # by default, all the parameters (arguments) are passed \u00e2\u0080\u009cby reference\u00e2\u0080\u009d to the functions numbers, strings, tuples (i.e. immutables) are passed by value Default Parameters # Non-default parameters comes before default parameters following will give SyntaxError 1 2 def add ( a , b = 3 , c ): return a + b + c 1 SyntaxError: non-default argument follows default argument Variable Number of Parameters # *args **kwarg Exception Handling # Syntax: 1 2 3 4 5 6 7 8 9 10 try : # do something except IOError as e : # handle except ValueError : # handle except : # handle finally : # do final work can also put an else block after all the except block (will be executed if no exception occurs) Inbuilt Exceptions # All the buil-in exceptions are subclass of BaseException AssertionError # raised when assert statement fails AttributeError # EOFError # raised when the input() function hits an end-of-file condition without reading any data ImportError # raised when there is some trouble loading mudules IndentationError # wrong indentation IndexError # raised when a sequence is out of range KeyError # when key not found in Dict NameError # when a local or global name is not found NotImplementedError # when a abstract/interface method lacks real implementation in sub-class OSError # when there is some OS level failure like, file not found , disk full RecursionError # when intrepreter detects maximum recursion depth SyntaxError # when parser encounters some syntax error 1 print 1 1 SyntaxError: Missing parentheses in call to 'print' TypeError # when an operation or function is applied to an inappropriate object e.g. when index is not an int addition of int + str 1 a = 1 + 'abc' 1 TypeError: unsupported operand type(s) for +: 'int' and 'str' UnicodeError # when a unicode related encoding/decoding error occurs ValueError # when a built-in operations or function receives an argument that has the right type but an inappropriate value. 1 int ( 'abc' ) 1 ValueError: invalid literal for int() with base 10: 'abc' ZeroDivisionError # when 2nd arg in division or modulo operation is zero User-defined Exception # make a class inherit the Exception class syntax: 1 2 3 4 5 6 7 8 9 10 11 12 # define Python user-defined exceptions class Error ( Exception ): \"\"\"Base class for other exceptions\"\"\" pass class ValueTooSmallError ( Error ): \"\"\"Raised when the input value is too small\"\"\" pass class ValueTooLargeError ( Error ): \"\"\"Raised when the input value is too large\"\"\" pass Performance # https://wiki.python.org/moin/TimeComplexity Misc # is vs == operator # == compares for values i.e. checks that 2 arguments have the same value 1 2 3 4 5 6 7 l1 = [ 1 , 2 , 3 ] l2 = l1 l3 = [ 1 , 2 , 3 ] l1 == l2 # returns True l1 == l3 # returns True l3 == l2 # returns True is checks if operand1 is exact copy of operand2 i.e. checks that 2 arguments refer to the same object 1 2 3 4 5 6 7 l1 = [ 1 , 2 , 3 ] l2 = l1 l3 = [ 1 , 2 , 3 ] l1 is l2 # returns True l1 is l3 # returns False l3 is l2 # returns False Pickling Unpickling # Pickling # python object hierarchy is converted into byte stream aka serialization, marshal, flattening can say 'binary serialization format' module: pickle Unpickling # byte-like objects or binary files are converted back into objects hierarchy opposite of pickling Comparision with JSON # JSON is a text serialization format it outputs unicode text, and most of the time it is then encoded to 'utf-8' JSON text is human readable which pickle is not widely used outside the python, while pickle is python-specific Monkey Patching # an evil hack ;) It's simply the dynamic replacement of attributes of a class/module at runtime. its possible because classes are mutable & methods are just attributes Also, we can replace classes and functions in a module Uses # for testing purpose, replace a function which calls a heavy API with a dummy one Code: 1 2 3 4 5 6 7 8 9 10 11 class MyClass : def f ( self ): print ( \"f()\" ) def monkey_f ( self ): print ( \"monkey_f()\" ) MyClass . f = monkey_f obj = MyClass () obj . f () # here, definition of f has been replace with def of monkey_f #obj.monkey_f() Out: 1 monkey_f() Duck Typing # EAFP: Easier to Ask Forgiveness than Permission Tag line definition: If an object can quack & fly, then its a duck. Do not worry about, if this object has this attribute or not, just try it inside try: block. If work then great, else handle the error. Opposite is LBYL: Look Before You Leap. (Check if it is possible or not, then try) e.g. https://gist.github.com/toransahu/337c287f8ead0d663c13b96d4b8fb7d2 Copy # Regular Reference (Hard Copy) # Copies reference of original only not value. Changes in copy will also reflect in original. i.e. id of both would be same The difference between shallow and deep copying is only relevant for compound objects (objects that contain other objects, like lists or class instances) Shallow copy # A shallow copy constructs a new compound object and then (to the extent possible) inserts references into it to the objects found in the original. Copies top level data & references other level objects into new Changes in top level not reflect in orignal Changes in other level objects reflect in orignal Means ids of nested/child objects will remain same in both the copies doesn't slow downs programs refer example Deep copy # A deep copy constructs a new compound object and then, recursively, inserts copies into it of the objects found in the deforiginal. slow downs programs 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 print ( \"Regular reference Example \\n \" ) print ( \"Ops on mutable\" ) l1 = [ 1 , 2 , 3 ] l2 = l1 # l2 have reference of l1 # changes in regular reference affects original data l2 . append ( 4 ) print ( \"l1 =\" , l1 ) print ( \"l2 =\" , l2 ) print ( \"id(l1) =\" , id ( l1 )) print ( \"id(l2) =\" , id ( l2 )) print ( \"Ops on Immutable\" ) s1 = \"abcd\" s2 = s1 print ( id ( s1 )) print ( id ( s2 )) print ( s1 ) print ( s2 ) s1 = \"efgh\" print ( id ( s1 )) print ( id ( s2 )) print ( s1 , s2 ) Out: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Regular reference Example Ops on mutable l1 = [1, 2, 3, 4] l2 = [1, 2, 3, 4] id(l1) = 140486838971464 id(l2) = 140486838971464 Ops on Immutable 140486838172392 140486838172392 abcd abcd 140486838172784 140486838172392 efgh abcd 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 print ( \" \\n Shallow Copy Example - Manual \\n \" ) l0 = [ 1 , 2 , 3 ] l3 = [ 1 , l0 ] l4 = list ( l3 ) print ( \"l3 = \" , l3 ) print ( \"l4 = \" , l4 ) print ( \"id(l3) =\" , id ( l3 )) print ( \"id(l4) = \" , id ( l4 )) print ( \"l3 == l4\" , l3 == l4 ) #checks value-wise print ( \"l3 is l4\" , l3 is l4 ) #checks object identity-wise print ( \"id(l3[1]) =\" , id ( l3 [ 1 ])) print ( \"id(l4[1]) = \" , id ( l4 [ 1 ])) print ( \"l3[1] == l4[1]\" , l3 [ 1 ] == l4 [ 1 ]) print ( \"l3[1] is l4[1]\" , l3 [ 1 ] is l4 [ 1 ]) print ( \"Size of l3 = \" , sys . getsizeof ( l3 )) print ( \"Size of l4 = \" , sys . getsizeof ( l4 )) print ( \"Size of l3[1] = \" , sys . getsizeof ( l3 [ 1 ])) print ( \"Size of l4[1] = \" , sys . getsizeof ( l4 [ 1 ])) Out: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 Shallow Copy Example - Manual l3 = [1, [1, 2, 3]] l4 = [1, [1, 2, 3]] id(l3) = 140486838605384 id(l4) = 140486838202312 l3 == l4 True l3 is l4 False id(l3[1]) = 140486839016584 id(l4[1]) = 140486839016584 l3[1] == l4[1] True l3[1] is l4[1] True --------------------------------------------------------------------------- NameError Traceback (most recent call last) <ipython-input-14-607e640db462> in <module>() 19 print(\"l3[1] is l4[1]\", l3[1] is l4[1]) 20 ---> 21 print(\"Size of l3 = \",sys.getsizeof(l3)) 22 print(\"Size of l4 = \",sys.getsizeof(l4)) 23 NameError: name 'sys' is not defined Here, * values of l3 and l4 are same * but ids of l3 and l4 are different * nested/child object of l3 is not directly copied to l4 instead the reference of that child is provided. * the id of l3[1] & l4[1] are same, means changes in l4[0] will affect l3[0]. * sizes of l3 & l4 are different * sizes of l3[1] & l4[1] are different 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 print ( \" \\n Shallow Copy Example - copy.copy()\" ) import copy import sys l3 = [ 1 , l0 ] l4 = copy . copy ( l3 ) # some value of l3 are copied to l4 and reference of some are passed to l4 print ( \"l3 = \" , l3 ) print ( \"l4 = \" , l4 ) print ( \"id(l3) =\" , id ( l3 )) print ( \"id(l4) = \" , id ( l4 )) print ( \"l3 == l4\" , l3 == l4 ) #checks value-wise print ( \"l3 is l4\" , l3 is l4 ) #checks object identity-wise print ( \"id(l3[1]) =\" , id ( l3 [ 1 ])) print ( \"id(l4[1]) = \" , id ( l4 [ 1 ])) print ( \"l3[1] == l4[1]\" , l3 [ 1 ] == l4 [ 1 ]) print ( \"l3[1] is l4[1]\" , l3 [ 1 ] is l4 [ 1 ]) print ( \"Size of l3 = \" , sys . getsizeof ( l3 )) print ( \"Size of l4 = \" , sys . getsizeof ( l4 )) print ( \"Size of l3[1] = \" , sys . getsizeof ( l3 [ 1 ])) print ( \"Size of l4[1] = \" , sys . getsizeof ( l4 [ 1 ])) Out: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Shallow Copy Example - copy.copy() l3 = [1, [1, 2, 3]] l4 = [1, [1, 2, 3]] id(l3) = 140486838153032 id(l4) = 140486838151368 l3 == l4 True l3 is l4 False id(l3[1]) = 140486839016584 id(l4[1]) = 140486839016584 l3[1] == l4[1] True l3[1] is l4[1] True Size of l3 = 80 Size of l4 = 104 Size of l3[1] = 88 Size of l4[1] = 88 Here, * Results are same as Manual shallow copy. Except, * sizes of l3 & l4 are different * sizes of l3[1] & l4[1] are same 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 print ( \" \\n Deep Copy Example - Manual\" ) import copy import sys l5 = [ 1 , l0 ] l6 = [ 1 , list ( l0 )] # all value of l5 are copied to l6 print ( \"l5 = \" , l5 ) print ( \"l6 = \" , l6 ) print ( \"id(l5) =\" , id ( l5 )) print ( \"id(l6) = \" , id ( l6 )) print ( \"l5 == l6\" , l5 == l6 ) #checks value-wise print ( \"l5 is l6\" , l5 is l6 ) #checks object identity-wise print ( \"id(l5[1]) =\" , id ( l5 [ 1 ])) print ( \"id(l6[1]) = \" , id ( l6 [ 1 ])) print ( \"l5[1] == l6[1]\" , l5 [ 1 ] == l6 [ 1 ]) print ( \"l5[1] is l6[1]\" , l5 [ 1 ] is l6 [ 1 ]) print ( \"Size of l5 = \" , sys . getsizeof ( l5 )) print ( \"Size of l6 = \" , sys . getsizeof ( l6 )) print ( \"Size of l5[1] = \" , sys . getsizeof ( l5 [ 1 ])) print ( \"Size of l6[1] = \" , sys . getsizeof ( l6 [ 1 ])) Out: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Deep Copy Example - Manual l5 = [1, [1, 2, 3]] l6 = [1, [1, 2, 3]] id(l5) = 140486838605384 id(l6) = 140486838226760 l5 == l6 True l5 is l6 False id(l5[1]) = 140486839016584 id(l6[1]) = 140486838253000 l5[1] == l6[1] True l5[1] is l6[1] False Size of l5 = 80 Size of l6 = 80 Size of l5[1] = 88 Size of l6[1] = 112 Here, * values of l5 and l6 are same * but ids of l5 and l6 are different * nested/child object of l5 is directly copied to l6 instead of passing the reference * the id of l5[1] & l6[1] are different, means changes in l6[0] will not affect l5[0] * sizes of l5 & l6 are same * sizes of l5[1] & l6[1] are different 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 print ( \" \\n Deep Copy Example - copy.deepcopy()\" ) import copy import sys l5 = [ 1 , l0 ] l6 = copy . deepcopy ( l5 ) # all value of l5 are copied to l6 print ( \"l5 = \" , l5 ) print ( \"l6 = \" , l6 ) print ( \"id(l5) =\" , id ( l5 )) print ( \"id(l6) = \" , id ( l6 )) print ( \"l5 == l6\" , l5 == l6 ) #checks value-wise print ( \"l5 is l6\" , l5 is l6 ) #checks object identity-wise print ( \"id(l5[1]) =\" , id ( l5 [ 1 ])) print ( \"id(l6[1]) = \" , id ( l6 [ 1 ])) print ( \"l5[1] == l6[1]\" , l5 [ 1 ] == l6 [ 1 ]) print ( \"l5[1] is l6[1]\" , l5 [ 1 ] is l6 [ 1 ]) print ( \"Size of l5 = \" , sys . getsizeof ( l5 )) print ( \"Size of l6 = \" , sys . getsizeof ( l6 )) print ( \"Size of l5[1] = \" , sys . getsizeof ( l5 [ 1 ])) print ( \"Size of l6[1] = \" , sys . getsizeof ( l6 [ 1 ])) Out: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Deep Copy Example - copy.deepcopy() l5 = [1, [1, 2, 3]] l6 = [1, [1, 2, 3]] id(l5) = 140486838138056 id(l6) = 140486838211656 l5 == l6 True l5 is l6 False id(l5[1]) = 140486839016584 id(l6[1]) = 140486838168328 l5[1] == l6[1] True l5[1] is l6[1] False Size of l5 = 80 Size of l6 = 96 Size of l5[1] = 88 Size of l6[1] = 96 Here, * Results are same as Manual Deep copy. Except, * sizes of l5 & l6 are different * sizes of l5[1] & l6[1] are different Two problems often exist with deep copy operations that don\u2019t exist with shallow copy operations: Recursive objects (compound objects that, directly or indirectly, contain a reference to themselves) may cause a recursive loop. Because deep copy copies everything it may copy too much, such as data which is intended to be shared between copies.","title":"Python Basics"},{"location":"Languages/python/python_basics/#python-basics","text":"Python Basics Introduction The Zen of Python (PEP 20) Easter Egg Implementations Internal PEP 8 - Style Guide for Python Code Features of Python are: Compilers Interpreters Compiler characteristics: Interpreter characteristics: Python interpreted/compiled? Compiled Language Interpreted Language How python works? Note What is .pyc file? What is .pyo file? Memory Management Data types in python? Data structure in python? 4 Built in Collection & heapq modules Abstract data type we can create are Data Types Strings String formatting What is doc string? byte string vs unicode string Intro 3 things unicode List Intro Why? When? Features List Generator List comprehension List Flattening Randomize items of list in place How to insert an element in between a list? Diif between append and extend method in list? enumerate() Tuple Why? When? Properties Expanding Tuple Dictionary When to use dict & set? dict vs list lookup performance? dict comprehension dict with order {} vs dict() Variables Global vs Local vs Non-Local Variable Global Variable Global Variables in Nested Functions Local Variable Non-Local Variable Operators in and or xor is not Generators xrange vs range? Statements assert yield return Compound Statements with Example #1 : file handling Example #2: Expressions lambda Operators Ternary Operator [Operator Overloading Using Magic Methods] Functions eval() partial() lambda map() filter() reduce() zip() Inverse a Matrix (list/tuple) Decorators Nested function Closures As per JavaScript Function Factory Parameter passing in python Passed As Default Parameters Variable Number of Parameters Exception Handling Inbuilt Exceptions AssertionError AttributeError EOFError ImportError IndentationError IndexError KeyError NameError NotImplementedError OSError RecursionError SyntaxError TypeError UnicodeError ValueError ZeroDivisionError User-defined Exception Performance Misc is vs == operator Pickling Unpickling Pickling Unpickling Comparision with JSON Monkey Patching Uses Duck Typing Copy Regular Reference (Hard Copy) Shallow copy Deep copy","title":"Python Basics"},{"location":"Languages/python/python_basics/#introduction","text":"","title":"Introduction"},{"location":"Languages/python/python_basics/#the-zen-of-python-pep-20","text":"Beautiful is better than ugly. Explicit is better than implicit. Simple is better than complex. Complex is better than complicated. Flat is better than nested. Sparse is better than dense. Readability counts. Special cases aren't special enough to break the rules. Although practicality beats purity. Errors should never pass silently. Unless explicitly silenced. In the face of ambiguity, refuse the temptation to guess. There should be one-- and preferably only one --obvious way to do it. Although that way may not be obvious at first unless you're Dutch. Now is better than never. Although never is often better than right now. If the implementation is hard to explain, it's a bad idea. If the implementation is easy to explain, it may be a good idea. Namespaces are one honking great idea -- let's do more of those!","title":"The Zen of Python (PEP 20)"},{"location":"Languages/python/python_basics/#easter-egg","text":"In computer software and media, an Easter egg is an intentional inside joke, hidden message or image, or secret feature of a work The Zen of python can be listed by importing this 1 >>> import this","title":"Easter Egg"},{"location":"Languages/python/python_basics/#implementations","text":"CPython (Default) Jython IronPython RPython (PyPy)","title":"Implementations"},{"location":"Languages/python/python_basics/#internal","text":"Runs on a single process by forking Each process have seperate GIL Python can run only one thread at once Context Manager","title":"Internal"},{"location":"Languages/python/python_basics/#pep-8-style-guide-for-python-code","text":"Package: lowercase, '_' is discouraged, use hyphen '-' instead Module: lowercase + '_' Class: Initial Capital, Camal Case Function: lowercase, Valid word, '_' Method: self, lowercase, Valid word, '_' Variable: Same as functions Local: Global: Static: Constant: Generally Module Level, All CAPS + '_' Private Attribute: To avoid name clashes with subclasses, Leading dunder Protected Attribute: For non-public methods and instance variables, Leading underscore '_' Exception Name: Same as class NOTE: Never use the characters 'l' (lowercase letter el), 'O' (uppercase letter oh), or 'I' (uppercase letter eye) as single character variable names.","title":"PEP 8 - Style Guide for Python Code"},{"location":"Languages/python/python_basics/#features-of-python-are","text":"Unique Interpreted Language Dynamic type system but strongly typed (changing data type need explicit conversion) emphasizes on code readability - lesser line of code/syntax Supports multiple programming paradigms, including object-oriented, imperative, functional and procedural Has community based developement model Dynamic name resolution/ late binding: unlike compiled languages, the name of method, variable is lookedup by name at runtime Data types are strongly and dynamically typed. Mixing incompatible types (e.g. attempting to add a string and a number) causes an exception to be raised, so errors are caught sooner. Python contains advanced programming features such as generators and list comprehensions. Lambda functions Multiple inheritance Common A variety of basic data types are available: numbers (floating point, complex, and unlimited-length long integers), strings (both ASCII and Unicode), lists, and dictionaries. Python supports object-oriented programming with classes and multiple inheritance. Code can be grouped into modules and packages. The language supports raising and catching exceptions, resulting in cleaner error handling. Python's automatic memory management frees you from having to manually allocate and free memory in your code.","title":"Features of Python are:"},{"location":"Languages/python/python_basics/#compilers","text":"A type of translator. Compiler analyze the whole source code at once and translate it to another language. i.e. machine code","title":"Compilers"},{"location":"Languages/python/python_basics/#interpreters","text":"An interpreter is also a program that translates a high-level language into a low-level one (but not directly into machine code), and it does it at the moment the program is run. It takes the program, one line at a time, and translates each line before running it: It translates the first line and runs it, then translates the second line and runs it etc.","title":"Interpreters"},{"location":"Languages/python/python_basics/#compiler-characteristics","text":"spends a lot of time analyzing and processing the program the resulting executable is some form of machine- specific binary code the computer hardware interprets (executes) the resulting code program execution is fast","title":"Compiler characteristics:"},{"location":"Languages/python/python_basics/#interpreter-characteristics","text":"relatively little time is spent analyzing and processing the program the resulting code is some sort of intermediate code the resulting code is interpreted by another program program execution is relatively slow","title":"Interpreter characteristics:"},{"location":"Languages/python/python_basics/#python-interpretedcompiled","text":"","title":"Python interpreted/compiled?"},{"location":"Languages/python/python_basics/#compiled-language","text":"A compiled language turn human-readable code into machine code (a string of binary numbers), which are directly executed by OS & CPU.","title":"Compiled Language"},{"location":"Languages/python/python_basics/#interpreted-language","text":"A language which is in non-machine code form just before its execution. In general an interpreted prog. language turn human-readable code into non-machine code (byte-code), which are then line by line executed by virtual machine. Interpreted/Compiled is not property of language, its property of implementation. Its byte-code INTERPRETED, because .py is first COMPILED/translated to .pyc (a byte-code language, a non-machine language, executed by python virtual machine, not by OS / CPU).","title":"Interpreted Language"},{"location":"Languages/python/python_basics/#how-python-works","text":"run module i.e. .py file .py loaded into memory parsing in order .py compiled to bytecode .pyc file (which is not binary machinecode) compilation is translation step .pyc (.i.e. bytecode) is low-level platform -independent version of source-code require more work than CPU instructions if .pyc of source code is present, compilation step will be skipped by checking time-stamp of .py & .pyc if python cannot create .pyc, then bytecode will be created in-memory then routed/shipped to python virtual machine PVM for execution pre-installed / inbuilt it is runtime engine of python .pyc is also way of shipping python code without source-code","title":"How python works?"},{"location":"Languages/python/python_basics/#note","text":"no initial/explicit compilation phase compiles at runtime only and then executes in single step able to produce executable, frozen binaries using py2exe PyInstaller freeze","title":"Note"},{"location":"Languages/python/python_basics/#what-is-pyc-file","text":"Low-level Platform-independent Bytecode","title":"What is .pyc file?"},{"location":"Languages/python/python_basics/#what-is-pyo-file","text":"in addition to .pyc, .pyo removes all the comments & docs(i guess)","title":"What is .pyo file?"},{"location":"Languages/python/python_basics/#memory-management","text":"Basics: Python's memory allocation and deallocation method is automatic. involves a private heap the heap contains all the objects & data structures managed by python \"memory manager MM\" interpreter manages this all no user control heap space allocation for objects & buffers are performed on-demand by MM c memory management libs works in-behind: malloc(), calloc(), realloc(), free() at low level, raw memory allocator allocates enough memory for all the data on top of low-level, object-specific allocators allocates memory as per object's policies e.g. for integer: speed tradeoff Memory De-allocation Strategies: Reference Counting Was only option Prior Python 2.0 When an object gets created and referenced, it counts the number of times the object is referenced by some other objects When reference is removed, the reference count for the object is decremented When the reference count becomes zero, the object is deallocated. Extremely efficient but Have limitations like: Cannot handle reference cycle Reference Cycle: When there is no way to reach an object but its reference count is still greater than zero e.g. 1 2 list1 = [] list1 . append ( list1 ) Examples, where the reference count increases: assignment operator argument passing appending an object to a list (object's reference count will be increased). 1 2 3 4 5 6 7 8 9 10 11 12 13 foo = [] # 2 references, 1 from the foo var and 1 from getrefcount print ( sys . getrefcount ( foo )) def bar ( a ): # 4 references # from the foo var, function argument, getrefcount and Python's function stack print ( sys . getrefcount ( a )) bar ( foo ) # 2 references, the function scope is destroyed print ( sys . getrefcount ( foo )) Garbage Collection Introduced after Python 2.0 it contains reference counting as well as garbage collector Automatic / Scheduled: The \"Reference Counting\" mechanism was not able to deallocate objects in few cases like: Reference Cycle How reference counting is solved by garbage collection it is a scheduled task based on a threshold threshold = allocations - de-allocations The GC classifies container objects into three generations. Every new object starts in the first generation. If an object survives a garbage collection round, it moves to the older (higher) generation. Lower generations are collected more often than higher. Because most of the newly created objects die young, it improves GC performance and reduces the GC pause time. Source : read Pro Python for better understanding https://rushter.com/blog/python-garbage-collector/ whenever threshold is reached, garbage collector starts identifying memory spaces which are garbage garbage? the memory spaces which are un-reachable to python objects e.g. 1 2 3 4 5 6 7 8 import gc gc . disable () obj1 = { \"val\" : 1 } obj2 = { \"val\" : 2 } obj1 [ \"obj2\" ] = obj2 obj2 [ \"obj1\" ] = obj1 del obj1 , obj2 - it is most important to identify a memory space whether it is a garbage or not - otherwise it will lead to memory leak - memory leak means, automatically data loss Note: Automatic garbage collection will not run if your Python device is running out of memory Manual / Explicitly 1 2 3 4 5 6 import gc # get_count() returns a tuple of (threshold, no. of objects allocated, no. of objects de-allocated) print ( gc . get_count ()) # With no arguments, run a full collection gc . collect () https://docs.python.org/3/c-api/memory.html pass by value pass by reference change reference change value behaviour of mutable and immutable","title":"Memory Management"},{"location":"Languages/python/python_basics/#data-types-in-python","text":"data type: set of data with predefined values. primitive: integers floating char string user-defined","title":"Data types in python?"},{"location":"Languages/python/python_basics/#data-structure-in-python","text":"Data Structure: are special format/structures to store & organize data.","title":"Data structure in python?"},{"location":"Languages/python/python_basics/#4-built-in","text":"Sequence data types: Ordered Sequence: List Tuple Set Dictionary We consider string more as a data type.","title":"4 Built in"},{"location":"Languages/python/python_basics/#collection-heapq-modules","text":"provides additional data structure collections: dequeue ordereddict heapq: priority queue heap","title":"Collection &amp; heapq modules"},{"location":"Languages/python/python_basics/#abstract-data-type-we-can-create-are","text":"linear: linked list stack queue hash table non-linear: tree graph","title":"Abstract data type we can create are"},{"location":"Languages/python/python_basics/#data-types","text":"","title":"Data Types"},{"location":"Languages/python/python_basics/#strings","text":"","title":"Strings"},{"location":"Languages/python/python_basics/#string-formatting","text":"1 2 3 4 5 6 7 8 9 10 11 12 i = 1 v = 'a' print ( \"Value at index {0} is {1} \" . format ( i , v )) print ( \"Value at index {} is {} \" . format ( i , v )) print ( \"Value at index %d is %s \" % ( i , v )) # my favorite way before Python 3.6.4 emp = { 'name' : 'toran' , 'age' : 26 , 'mobile' : '8602431733' } print ( \"My name is {name} and I'm {age} years old. You can contact me at {mobile} \" . format ( ** emp )) # in Python 3.6.4 print ( f 'The value of i is { i } and value of v is { v } ' )","title":"String formatting"},{"location":"Languages/python/python_basics/#what-is-doc-string","text":"way of associating document with modules, functions, class, methods describes what it does instead how first line should heading (start with capital, end with dot), then gap of one-line, then desc","title":"What is doc string?"},{"location":"Languages/python/python_basics/#byte-string-vs-unicode-string","text":"","title":"byte string vs unicode string"},{"location":"Languages/python/python_basics/#intro","text":"there are a lot of encodings available world-wide e.g. ASCII, CP-1252 (windows), Mac-greek.. etc computer only understands bit, bytes e.g. in ASCII: 65 is -a, 97 is A HOW TO REPRESENT ALL LANGUAGEs IN SAME FILE?","title":"Intro"},{"location":"Languages/python/python_basics/#3-things","text":"str python object byte string, computer native array of bytes unicode, some encoded text","title":"3 things"},{"location":"Languages/python/python_basics/#unicode","text":"one encoding, all chars represent a char as 4-byte number: 4*8 = 32; UTF-32 a lot memory freak similarly, 2*8 = 16; UTF-16 UTF-8 a variable-length encoding system for Unicode till 128, ASCII & UTF-8 is same, uses 1 byte uses 2 bytes for latin read more at 1 , 2","title":"unicode"},{"location":"Languages/python/python_basics/#list","text":"","title":"List"},{"location":"Languages/python/python_basics/#intro_1","text":"A data structure/type to store objects/data/items Ordered collection: stores in ordered way i.e. using index from 0 Variable length: dynamic sized Mutable: can change any existing element in run-time Preferred for homogenious collection, but can store heterogenious data types inside. many attribute/member methods:","title":"Intro"},{"location":"Languages/python/python_basics/#why-when","text":"when dynamic data structure is benificial like: appending, removing, altering use when implementing buffer, stack, queues","title":"Why? When?"},{"location":"Languages/python/python_basics/#features","text":"","title":"Features"},{"location":"Languages/python/python_basics/#list-generator","text":"generates iterable items on demand build up in memory xrange in Python 2.x i.e. range() in Python 3.x is example of generator Advantages: No need to wait until all the elements have been generated before we use them in python 2.x, range returns a list while xrange returns a generator e.g. 1 2 3 4 5 6 7 8 def first_n ( n ): num = 0 while ( num < n ): yield num num += 1 for i in first_n ( 5 ): print ( i )","title":"List Generator"},{"location":"Languages/python/python_basics/#list-comprehension","text":"1 2 l = [ i for i in range ( 0 , 10 )] print ( l ) 1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]","title":"List comprehension"},{"location":"Languages/python/python_basics/#list-flattening","text":"1 2 l = [[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ] ] flat_list = [ item for sublist in l for item in sublist ] which is equivalent to 1 2 3 4 5 flat_list = [] for sublist in l : for item in sublist : flat_list . append ( item )","title":"List Flattening"},{"location":"Languages/python/python_basics/#randomize-items-of-list-in-place","text":"Code: 1 2 3 4 5 6 from random import shuffle l = [ 1 , 2 , 3 , 4 ] shuffle ( l ) print ( l ) shuffle ( l ) print ( l ) Out: 1 2 [3, 2, 1, 4] [4, 1, 2, 3]","title":"Randomize items of list in place"},{"location":"Languages/python/python_basics/#how-to-insert-an-element-in-between-a-list","text":"Code: 1 2 3 l = [ 1 , 2 , 3 , 5 , 6 , 7 ] l . insert ( 3 , 4 ) l Out: 1 [1, 2, 3, 4, 5, 6, 7]","title":"How to insert an element in between a list?"},{"location":"Languages/python/python_basics/#diif-between-append-and-extend-method-in-list","text":"append - appends object at the end extend - extends list by appending elements from iterables Code: 1 2 3 4 5 6 7 8 l = [ 1 , 2 , 3 ] a = [ 4 , 5 ] e = [ 6 , 7 , 8 ] l . append ( a ) l . extend ( e ) print ( l ) Out: 1 [1, 2, 3, [4, 5], 6, 7, 8]","title":"Diif  between append and extend method in list?"},{"location":"Languages/python/python_basics/#enumerate","text":"1 2 3 4 for i , v in enumerate ([ 'f' , 's' , 't' ]): print ( \"Value at index %d is %s \" % ( i , v )) print ( \"Value at index {0} is {1} \" % ( i , v )) print ( \"Value at index {} is {} \" . format ( i , v ))","title":"enumerate()"},{"location":"Languages/python/python_basics/#tuple","text":"A fixed data structure/type to store objects/data/items Ordered collection: stores in ordered way i.e. using index from 0 Fixed length: cannot change length of a tuple, cannot append, pop an element Immutable: cannot change any existing element in run-time Preferred for heterogenious collection, but can store homoge data types inside.","title":"Tuple"},{"location":"Languages/python/python_basics/#why-when_1","text":"when a collection of values will not change i.e. in case of functions args. use when fixed structure is benificial like: heavy memory intensive work, api, server can be used as key in dictionary due to its fixed structure use when need to store a db table data and want to maintain column structure","title":"Why? When?"},{"location":"Languages/python/python_basics/#properties","text":"1 2 3 4 5 6 7 8 9 10 l = [ 1 , 2 , ( 3 , 'a' ), 'b' , [ 4 , 5 ]] t = ( 1 , 2 ,[ 3 , 'a' ], 'b' , ( 4 , 5 ) ) #Lets try to make changes in l & t # l[2][0] = 1 #does not work # l[2] = 3 #works # t[2] = 3 #does not work # t[2][0] = 1 #works","title":"Properties"},{"location":"Languages/python/python_basics/#expanding-tuple","text":"used to pass tuple elements as function parameter 1 2 3 4 5 6 7 8 9 10 11 12 13 14 t = ( 1 , 2 , 3 , 4 , 5 ) # simple def bar ( a , b , c , d , e ): print ( a , b , c , d , e ) bar ( * t ) # in general def foo ( * args ): for arg in args : print ( arg ) foo ( * t )","title":"Expanding Tuple"},{"location":"Languages/python/python_basics/#dictionary","text":"Is map type of data structure which holds a key value pair. Unordered collection: does not maintain order","title":"Dictionary"},{"location":"Languages/python/python_basics/#when-to-use-dict-set","text":"When data is labelled Use a dictionary when you have a set of unique keys that map to values. Use a set to store an unordered unique set of items.","title":"When to use dict &amp; set?"},{"location":"Languages/python/python_basics/#dict-vs-list-lookup-performance","text":"dict - O(1) due to hashing list - O(n)","title":"dict vs list lookup performance?"},{"location":"Languages/python/python_basics/#dict-comprehension","text":"Code: 1 2 3 d = { i : i * i for i in range ( 0 , 10 )} print ( d ) Out: 1 {0: 0, 1: 1, 2: 4, 3: 9, 4: 16, 5: 25, 6: 36, 7: 49, 8: 64, 9: 81}","title":"dict comprehension"},{"location":"Languages/python/python_basics/#dict-with-order","text":"maintains the order of elements in which they were inserted 1 2 3 from collections import OrderedDict d = OrderedDict ({ 2 : 'second' , 1 : 'first' }) d . items ()","title":"dict with order"},{"location":"Languages/python/python_basics/#vs-dict","text":"tl;dr use {} over dict() whenever possible {} uses transient memory allocation, and frees the memory after value being consumed if {} value is assigned to a variable, then memory is persist dict() is slower (takes atleast double the time) Ref: - https://stackoverflow.com/questions/664118/whats-the-difference-between-dict-and - https://doughellmann.com/posts/the-performance-impact-of-using-dict-instead-of-in-cpython-2-7-2/","title":"{} vs dict()"},{"location":"Languages/python/python_basics/#variables","text":"","title":"Variables"},{"location":"Languages/python/python_basics/#global-vs-local-vs-non-local-variable","text":"src: https://www.python-course.eu/python3_global_vs_local_variables.php not same as other languages (by default global) by default all are local (if you need, declare global) 1 2 3 4 5 6 7 def f (): s = \"I'm local\" print ( s ) s = \"I'm global\" f () print ( s ) Out: 1 2 I'm local I'm global","title":"Global vs Local vs Non-Local Variable"},{"location":"Languages/python/python_basics/#global-variable","text":"How to access global variable inside a function: 1 2 3 4 5 6 7 8 9 def f (): # print(s) #error : cannot access a global variable directly global s # keyword global will give access to outer s s = \"I'm local\" # value of global s has been changed print ( s ) s = \"I'm global\" f () print ( s ) Out: 1 2 I'm local I'm local","title":"Global Variable"},{"location":"Languages/python/python_basics/#global-variables-in-nested-functions","text":"keyword global inside inner function will not access the upper function level variable instead, it will create a variable in main scope to make it possible there is one more keyword : nonlocal 1 2 3 4 5 6 7 8 9 10 11 12 def f (): x = 42 def g (): global x x = 43 print ( \"Before calling g: \" + str ( x )) print ( \"Calling g now:\" ) g () print ( \"After calling g: \" + str ( x )) f () print ( \"x in main: \" + str ( x )) Out: 1 2 3 4 Before calling g: 42 Calling g now: After calling g: 42 x in main: 43","title":"Global Variables in Nested Functions"},{"location":"Languages/python/python_basics/#local-variable","text":"variable defined inside a function are local to that function","title":"Local Variable"},{"location":"Languages/python/python_basics/#non-local-variable","text":"introduced in Python 3 different than global can only be used inside of nested functions has to be defined in the enclosing/upper function scope 1 2 3 4 5 6 7 8 9 10 11 12 def f (): y = 42 def g (): nonlocal y y = 43 print ( \"Before calling g: \" + str ( y )) print ( \"Calling g now:\" ) g () print ( \"After calling g: \" + str ( y )) f () print ( \"y in main: \" + str ( y )) # this will create error Out: 1 2 3 4 Before calling g: 42 Calling g now: After calling g: 43 NameError: name 'y' is not defined","title":"Non-Local Variable"},{"location":"Languages/python/python_basics/#operators","text":"","title":"Operators"},{"location":"Languages/python/python_basics/#in","text":"Searching Time Complexity: (Depends on type of operand) List Avg: O(n) Dict/Set Avg: O(1) Worst: O(n) magic/member method: __contains__(<element>)","title":"in"},{"location":"Languages/python/python_basics/#and","text":"","title":"and"},{"location":"Languages/python/python_basics/#or","text":"","title":"or"},{"location":"Languages/python/python_basics/#xor","text":"","title":"xor"},{"location":"Languages/python/python_basics/#is","text":"","title":"is"},{"location":"Languages/python/python_basics/#not","text":"","title":"not"},{"location":"Languages/python/python_basics/#generators","text":"","title":"Generators"},{"location":"Languages/python/python_basics/#xrange-vs-range","text":"python3: * xrange is renamed to range. python2: * same result but xrange is more memory efficient * range creates iterable list (in python2) * while xrange creates xrange object and generate list of demand","title":"xrange vs range?"},{"location":"Languages/python/python_basics/#statements","text":"","title":"Statements"},{"location":"Languages/python/python_basics/#assert","text":"A statement Used to check an expectation Works on logical condition If true, return nothing, if false raise AssertionError exception","title":"assert"},{"location":"Languages/python/python_basics/#yield","text":"A statement Does not end a function Returns value to its caller suspends the function and then return value to its caller then resume the function Continues with next line of statement uses: in generators like range","title":"yield"},{"location":"Languages/python/python_basics/#return","text":"A statement Ends function Returns value to caller","title":"return"},{"location":"Languages/python/python_basics/#compound-statements","text":"Compound statements contain (groups of) other statements they affect or control the execution of those other statements in some way contains multi line code block e.g. if , while , for , def , class , with","title":"Compound Statements"},{"location":"Languages/python/python_basics/#with","text":"The with statement is used to wrap the execution of a block with methods defined by a context manager with statement allows the execution of initialization and finalization code around a block of code i.e. try / finally + context manager having methods __enter__() & __exit__() read more","title":"with"},{"location":"Languages/python/python_basics/#example-1-file-handling","text":"1 2 3 4 5 6 7 # automatically acquring `csv.txt` file and does not allows others to acquire it with open ( 'csv.txt' , 'r' , encoding = 'utf-8' ) as f : # do some operations on f # do some operations more on f # if any exception occur, closes the file before exception is caught and shown by interpreter # automatically closed/released `csv.txt` file for others with statement opens a file or acquires a resource then do some block of codes then closes the file or releases the resource if any exception occur, during operations closes the file before exception is caught and shown by interpreter thats how we are better than try, except, finally I/O operation : GIL free","title":"Example #1 : file handling"},{"location":"Languages/python/python_basics/#example-2","text":"1 2 with A () as a , B () as b : suite is equivalent to 1 2 3 with A () as a : with B () as b : suite - thats how, it does not need help of GIL","title":"Example #2:"},{"location":"Languages/python/python_basics/#expressions","text":"","title":"Expressions"},{"location":"Languages/python/python_basics/#lambda","text":"","title":"lambda"},{"location":"Languages/python/python_basics/#operators_1","text":"","title":"Operators"},{"location":"Languages/python/python_basics/#ternary-operator","text":"[on-true] if [expression] else [on-flase] Code: 1 2 3 x , y = 23 , 50 big = x if x > y else y print ( big ) Out: 1 50","title":"Ternary Operator"},{"location":"Languages/python/python_basics/#operator-overloading-using-magic-methods","text":"Read in Python-OOPs notebook","title":"[Operator Overloading Using Magic Methods]"},{"location":"Languages/python/python_basics/#functions","text":"Source https://docs.python.org/3/library/functions.html","title":"Functions"},{"location":"Languages/python/python_basics/#eval","text":"Source: https://www.programiz.com/python-programming/methods/built-in/eval a built-in function to evaluate the python expression writter in string form 1 2 3 4 5 6 7 8 9 str = \"lambda x: x**2\" square = eval ( str ) square ( 2 ) # returns 4 str_list = \"[1,2,3]\" eval ( str_list ) # returns a list str_dict = \"{'a':1, 'b':[2,3]}\" eval ( str_dict ) # returns a dict - eval takes 3 parameters - expression: string - globals: dict (used for namespace) - locals: any mapping object","title":"eval()"},{"location":"Languages/python/python_basics/#partial","text":"a closure or a nested function used to fulfill the cases when we need to provide some/few fixed parameters to any functions need to import from functools import partial partial always takes functions as first parameter e.g. 1 2 3 4 5 6 7 8 from functools import partial def foo ( a , b , c = 10 ): print ( f \"I'm foo with { a } , { b } , and { c } \" ) foo_partial = partial ( foo , 1 , 2 ) foo_partial ()","title":"partial()"},{"location":"Languages/python/python_basics/#lambda_1","text":"format: lambda arg1, arg2, ...argN : expression using arguments anonymous function single-line statement expression in-place function definition can be stored in a variable syntax: 1 lambda x : return x * x Scopes: to make Jump Tables nested lambda loop in lambda using map()","title":"lambda"},{"location":"Languages/python/python_basics/#map","text":"signature: map(aFunction, aSequence) applies a passed-in function to each item in an iterable object python2 :returns a list containing all the function call results python3 :returns an iterator of type map object Syntax 1 2 sqrs = list ( map ( lambda x : x * x , range ( 0 , 10 ))) print ( sqrs ) Out: 1 [0, 1, 4, 9, 16, 25, 36, 49, 64, 81]","title":"map()"},{"location":"Languages/python/python_basics/#filter","text":"signature: filter(aFunction, aSequence) applies a passed-in function to each item in an iterable object python2 :returns a list of items for whose function call returns True python3 :returns an iterator of type map object for the items whose function call returns True Syntax 1 2 evens = list ( filter ( lambda x : x % 2 == 0 , range ( 0 , 10 ))) print ( evens ) Out: 1 [0, 2, 4, 6, 8]","title":"filter()"},{"location":"Languages/python/python_basics/#reduce","text":"(Dropped out in Python 3.x) * signature: filter(aFunction, aSequence) * applies a passed-in function to each item in an iterable object * python2 :returns a single value * python3 : dropped out","title":"reduce()"},{"location":"Languages/python/python_basics/#zip","text":"1 2 3 list1 = [ 'A' , 'B' , 'C' ] and list2 = [ 10 , 20 , 30 ] . zip ( list1 , list2 ) # results in a list of tuples say [('A',10),('B',20),('C',30)]","title":"zip()"},{"location":"Languages/python/python_basics/#inverse-a-matrix-listtuple","text":"1 2 3 4 5 6 m = [[ 1 , 2 , 3 , 4 ], [ 5 , 6 , 7 , 8 ], [ 9 , 10 , 11 , 12 ] ] m_inverse = list ( zip ( * m )) # here *m is expanding of list/tuple, mostly used in passing tuple as func parameters","title":"Inverse a Matrix (list/tuple)"},{"location":"Languages/python/python_basics/#decorators","text":"are a thin wrapper arround any function or any class there are also decorators for classes (read in Python-OOPs) benifits: do not need to decorate each method manually when we apply a decorator to any function a few lines code in before start (if any) of the fuction call will get executed and/or a few lines code in after end (if any) of the fuction call will get executed types of function decorator made out of a nested functions made out of a class decorator with param decorator over decorator order of decorators Using wraps from functools The way we have defined decorators so far hasn't taken into account that the attributes 1 2 3 __name__ ( name of the function ), __doc__ ( the docstring ) and __module__ ( The module in which the function is defined ) of the original functions will be lost after the decoration. simplest form of e.g. to create a custom decorator is: 1 2 3 4 5 6 7 8 9 def exec_time ( some_func ): print ( 'this is start time' ) some_func () print ( 'this is end time' ) @exec_time def foo (): print ( 'running foo' ) - gist: https://gist.github.com/toransahu/7ac4c7f139e78d15b74ca0ce6e17cf85","title":"Decorators"},{"location":"Languages/python/python_basics/#nested-function","text":"1 2 3 4 5 6 7 8 def outer_func (): x = 5 def inner_func ( y = 3 ): return ( x + y ) return inner_func a = outer_func () print ( a ()) # 8","title":"Nested function"},{"location":"Languages/python/python_basics/#closures","text":"is a concept - not a function a few may refer it as a nested function The local function is able to reference the outer scope through closures. Closures maintain references to objects from the earlier scope.","title":"Closures"},{"location":"Languages/python/python_basics/#as-per-javascript","text":"A closure is the combination of a function bundled together (enclosed) with references to its surrounding state (the lexical environment). In other words, a closure gives you access to an outer function\u2019s scope from an inner function. In JavaScript, closures are created every time a function is created, at function creation time.","title":"As per JavaScript"},{"location":"Languages/python/python_basics/#function-factory","text":"these are functions that return other functions the returned functions are specialized Function Factory takes in argument(s), creates local function that creates its own argument(s) and also uses the argument(s) passed to the function factory this is possible with closures 1 2 3 4 5 6 7 8 9 10 11 12 def multiply_by ( num ): def multiply_by_num ( k ): return num * k return multiply_by_num five = multiply_by ( 5 ) print ( five ( 2 )) # 10 print ( five ( 4 )) # 20 decimal = multiply_by ( 10 ) print ( decimal ( 20 )) # 200 print ( decimal ( 3 )) # 30","title":"Function Factory"},{"location":"Languages/python/python_basics/#parameter-passing-in-python","text":"","title":"Parameter passing in python"},{"location":"Languages/python/python_basics/#passed-as","text":"by default, all the parameters (arguments) are passed \u00e2\u0080\u009cby reference\u00e2\u0080\u009d to the functions numbers, strings, tuples (i.e. immutables) are passed by value","title":"Passed As"},{"location":"Languages/python/python_basics/#default-parameters","text":"Non-default parameters comes before default parameters following will give SyntaxError 1 2 def add ( a , b = 3 , c ): return a + b + c 1 SyntaxError: non-default argument follows default argument","title":"Default Parameters"},{"location":"Languages/python/python_basics/#variable-number-of-parameters","text":"*args **kwarg","title":"Variable Number of Parameters"},{"location":"Languages/python/python_basics/#exception-handling","text":"Syntax: 1 2 3 4 5 6 7 8 9 10 try : # do something except IOError as e : # handle except ValueError : # handle except : # handle finally : # do final work can also put an else block after all the except block (will be executed if no exception occurs)","title":"Exception Handling"},{"location":"Languages/python/python_basics/#inbuilt-exceptions","text":"All the buil-in exceptions are subclass of BaseException","title":"Inbuilt Exceptions"},{"location":"Languages/python/python_basics/#assertionerror","text":"raised when assert statement fails","title":"AssertionError"},{"location":"Languages/python/python_basics/#attributeerror","text":"","title":"AttributeError"},{"location":"Languages/python/python_basics/#eoferror","text":"raised when the input() function hits an end-of-file condition without reading any data","title":"EOFError"},{"location":"Languages/python/python_basics/#importerror","text":"raised when there is some trouble loading mudules","title":"ImportError"},{"location":"Languages/python/python_basics/#indentationerror","text":"wrong indentation","title":"IndentationError"},{"location":"Languages/python/python_basics/#indexerror","text":"raised when a sequence is out of range","title":"IndexError"},{"location":"Languages/python/python_basics/#keyerror","text":"when key not found in Dict","title":"KeyError"},{"location":"Languages/python/python_basics/#nameerror","text":"when a local or global name is not found","title":"NameError"},{"location":"Languages/python/python_basics/#notimplementederror","text":"when a abstract/interface method lacks real implementation in sub-class","title":"NotImplementedError"},{"location":"Languages/python/python_basics/#oserror","text":"when there is some OS level failure like, file not found , disk full","title":"OSError"},{"location":"Languages/python/python_basics/#recursionerror","text":"when intrepreter detects maximum recursion depth","title":"RecursionError"},{"location":"Languages/python/python_basics/#syntaxerror","text":"when parser encounters some syntax error 1 print 1 1 SyntaxError: Missing parentheses in call to 'print'","title":"SyntaxError"},{"location":"Languages/python/python_basics/#typeerror","text":"when an operation or function is applied to an inappropriate object e.g. when index is not an int addition of int + str 1 a = 1 + 'abc' 1 TypeError: unsupported operand type(s) for +: 'int' and 'str'","title":"TypeError"},{"location":"Languages/python/python_basics/#unicodeerror","text":"when a unicode related encoding/decoding error occurs","title":"UnicodeError"},{"location":"Languages/python/python_basics/#valueerror","text":"when a built-in operations or function receives an argument that has the right type but an inappropriate value. 1 int ( 'abc' ) 1 ValueError: invalid literal for int() with base 10: 'abc'","title":"ValueError"},{"location":"Languages/python/python_basics/#zerodivisionerror","text":"when 2nd arg in division or modulo operation is zero","title":"ZeroDivisionError"},{"location":"Languages/python/python_basics/#user-defined-exception","text":"make a class inherit the Exception class syntax: 1 2 3 4 5 6 7 8 9 10 11 12 # define Python user-defined exceptions class Error ( Exception ): \"\"\"Base class for other exceptions\"\"\" pass class ValueTooSmallError ( Error ): \"\"\"Raised when the input value is too small\"\"\" pass class ValueTooLargeError ( Error ): \"\"\"Raised when the input value is too large\"\"\" pass","title":"User-defined Exception"},{"location":"Languages/python/python_basics/#performance","text":"https://wiki.python.org/moin/TimeComplexity","title":"Performance"},{"location":"Languages/python/python_basics/#misc","text":"","title":"Misc"},{"location":"Languages/python/python_basics/#is-vs-operator","text":"== compares for values i.e. checks that 2 arguments have the same value 1 2 3 4 5 6 7 l1 = [ 1 , 2 , 3 ] l2 = l1 l3 = [ 1 , 2 , 3 ] l1 == l2 # returns True l1 == l3 # returns True l3 == l2 # returns True is checks if operand1 is exact copy of operand2 i.e. checks that 2 arguments refer to the same object 1 2 3 4 5 6 7 l1 = [ 1 , 2 , 3 ] l2 = l1 l3 = [ 1 , 2 , 3 ] l1 is l2 # returns True l1 is l3 # returns False l3 is l2 # returns False","title":"is vs == operator"},{"location":"Languages/python/python_basics/#pickling-unpickling","text":"","title":"Pickling Unpickling"},{"location":"Languages/python/python_basics/#pickling","text":"python object hierarchy is converted into byte stream aka serialization, marshal, flattening can say 'binary serialization format' module: pickle","title":"Pickling"},{"location":"Languages/python/python_basics/#unpickling","text":"byte-like objects or binary files are converted back into objects hierarchy opposite of pickling","title":"Unpickling"},{"location":"Languages/python/python_basics/#comparision-with-json","text":"JSON is a text serialization format it outputs unicode text, and most of the time it is then encoded to 'utf-8' JSON text is human readable which pickle is not widely used outside the python, while pickle is python-specific","title":"Comparision with JSON"},{"location":"Languages/python/python_basics/#monkey-patching","text":"an evil hack ;) It's simply the dynamic replacement of attributes of a class/module at runtime. its possible because classes are mutable & methods are just attributes Also, we can replace classes and functions in a module","title":"Monkey Patching"},{"location":"Languages/python/python_basics/#uses","text":"for testing purpose, replace a function which calls a heavy API with a dummy one Code: 1 2 3 4 5 6 7 8 9 10 11 class MyClass : def f ( self ): print ( \"f()\" ) def monkey_f ( self ): print ( \"monkey_f()\" ) MyClass . f = monkey_f obj = MyClass () obj . f () # here, definition of f has been replace with def of monkey_f #obj.monkey_f() Out: 1 monkey_f()","title":"Uses"},{"location":"Languages/python/python_basics/#duck-typing","text":"EAFP: Easier to Ask Forgiveness than Permission Tag line definition: If an object can quack & fly, then its a duck. Do not worry about, if this object has this attribute or not, just try it inside try: block. If work then great, else handle the error. Opposite is LBYL: Look Before You Leap. (Check if it is possible or not, then try) e.g. https://gist.github.com/toransahu/337c287f8ead0d663c13b96d4b8fb7d2","title":"Duck Typing"},{"location":"Languages/python/python_basics/#copy","text":"","title":"Copy"},{"location":"Languages/python/python_basics/#regular-reference-hard-copy","text":"Copies reference of original only not value. Changes in copy will also reflect in original. i.e. id of both would be same The difference between shallow and deep copying is only relevant for compound objects (objects that contain other objects, like lists or class instances)","title":"Regular Reference (Hard Copy)"},{"location":"Languages/python/python_basics/#shallow-copy","text":"A shallow copy constructs a new compound object and then (to the extent possible) inserts references into it to the objects found in the original. Copies top level data & references other level objects into new Changes in top level not reflect in orignal Changes in other level objects reflect in orignal Means ids of nested/child objects will remain same in both the copies doesn't slow downs programs refer example","title":"Shallow copy"},{"location":"Languages/python/python_basics/#deep-copy","text":"A deep copy constructs a new compound object and then, recursively, inserts copies into it of the objects found in the deforiginal. slow downs programs 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 print ( \"Regular reference Example \\n \" ) print ( \"Ops on mutable\" ) l1 = [ 1 , 2 , 3 ] l2 = l1 # l2 have reference of l1 # changes in regular reference affects original data l2 . append ( 4 ) print ( \"l1 =\" , l1 ) print ( \"l2 =\" , l2 ) print ( \"id(l1) =\" , id ( l1 )) print ( \"id(l2) =\" , id ( l2 )) print ( \"Ops on Immutable\" ) s1 = \"abcd\" s2 = s1 print ( id ( s1 )) print ( id ( s2 )) print ( s1 ) print ( s2 ) s1 = \"efgh\" print ( id ( s1 )) print ( id ( s2 )) print ( s1 , s2 ) Out: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Regular reference Example Ops on mutable l1 = [1, 2, 3, 4] l2 = [1, 2, 3, 4] id(l1) = 140486838971464 id(l2) = 140486838971464 Ops on Immutable 140486838172392 140486838172392 abcd abcd 140486838172784 140486838172392 efgh abcd 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 print ( \" \\n Shallow Copy Example - Manual \\n \" ) l0 = [ 1 , 2 , 3 ] l3 = [ 1 , l0 ] l4 = list ( l3 ) print ( \"l3 = \" , l3 ) print ( \"l4 = \" , l4 ) print ( \"id(l3) =\" , id ( l3 )) print ( \"id(l4) = \" , id ( l4 )) print ( \"l3 == l4\" , l3 == l4 ) #checks value-wise print ( \"l3 is l4\" , l3 is l4 ) #checks object identity-wise print ( \"id(l3[1]) =\" , id ( l3 [ 1 ])) print ( \"id(l4[1]) = \" , id ( l4 [ 1 ])) print ( \"l3[1] == l4[1]\" , l3 [ 1 ] == l4 [ 1 ]) print ( \"l3[1] is l4[1]\" , l3 [ 1 ] is l4 [ 1 ]) print ( \"Size of l3 = \" , sys . getsizeof ( l3 )) print ( \"Size of l4 = \" , sys . getsizeof ( l4 )) print ( \"Size of l3[1] = \" , sys . getsizeof ( l3 [ 1 ])) print ( \"Size of l4[1] = \" , sys . getsizeof ( l4 [ 1 ])) Out: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 Shallow Copy Example - Manual l3 = [1, [1, 2, 3]] l4 = [1, [1, 2, 3]] id(l3) = 140486838605384 id(l4) = 140486838202312 l3 == l4 True l3 is l4 False id(l3[1]) = 140486839016584 id(l4[1]) = 140486839016584 l3[1] == l4[1] True l3[1] is l4[1] True --------------------------------------------------------------------------- NameError Traceback (most recent call last) <ipython-input-14-607e640db462> in <module>() 19 print(\"l3[1] is l4[1]\", l3[1] is l4[1]) 20 ---> 21 print(\"Size of l3 = \",sys.getsizeof(l3)) 22 print(\"Size of l4 = \",sys.getsizeof(l4)) 23 NameError: name 'sys' is not defined Here, * values of l3 and l4 are same * but ids of l3 and l4 are different * nested/child object of l3 is not directly copied to l4 instead the reference of that child is provided. * the id of l3[1] & l4[1] are same, means changes in l4[0] will affect l3[0]. * sizes of l3 & l4 are different * sizes of l3[1] & l4[1] are different 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 print ( \" \\n Shallow Copy Example - copy.copy()\" ) import copy import sys l3 = [ 1 , l0 ] l4 = copy . copy ( l3 ) # some value of l3 are copied to l4 and reference of some are passed to l4 print ( \"l3 = \" , l3 ) print ( \"l4 = \" , l4 ) print ( \"id(l3) =\" , id ( l3 )) print ( \"id(l4) = \" , id ( l4 )) print ( \"l3 == l4\" , l3 == l4 ) #checks value-wise print ( \"l3 is l4\" , l3 is l4 ) #checks object identity-wise print ( \"id(l3[1]) =\" , id ( l3 [ 1 ])) print ( \"id(l4[1]) = \" , id ( l4 [ 1 ])) print ( \"l3[1] == l4[1]\" , l3 [ 1 ] == l4 [ 1 ]) print ( \"l3[1] is l4[1]\" , l3 [ 1 ] is l4 [ 1 ]) print ( \"Size of l3 = \" , sys . getsizeof ( l3 )) print ( \"Size of l4 = \" , sys . getsizeof ( l4 )) print ( \"Size of l3[1] = \" , sys . getsizeof ( l3 [ 1 ])) print ( \"Size of l4[1] = \" , sys . getsizeof ( l4 [ 1 ])) Out: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Shallow Copy Example - copy.copy() l3 = [1, [1, 2, 3]] l4 = [1, [1, 2, 3]] id(l3) = 140486838153032 id(l4) = 140486838151368 l3 == l4 True l3 is l4 False id(l3[1]) = 140486839016584 id(l4[1]) = 140486839016584 l3[1] == l4[1] True l3[1] is l4[1] True Size of l3 = 80 Size of l4 = 104 Size of l3[1] = 88 Size of l4[1] = 88 Here, * Results are same as Manual shallow copy. Except, * sizes of l3 & l4 are different * sizes of l3[1] & l4[1] are same 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 print ( \" \\n Deep Copy Example - Manual\" ) import copy import sys l5 = [ 1 , l0 ] l6 = [ 1 , list ( l0 )] # all value of l5 are copied to l6 print ( \"l5 = \" , l5 ) print ( \"l6 = \" , l6 ) print ( \"id(l5) =\" , id ( l5 )) print ( \"id(l6) = \" , id ( l6 )) print ( \"l5 == l6\" , l5 == l6 ) #checks value-wise print ( \"l5 is l6\" , l5 is l6 ) #checks object identity-wise print ( \"id(l5[1]) =\" , id ( l5 [ 1 ])) print ( \"id(l6[1]) = \" , id ( l6 [ 1 ])) print ( \"l5[1] == l6[1]\" , l5 [ 1 ] == l6 [ 1 ]) print ( \"l5[1] is l6[1]\" , l5 [ 1 ] is l6 [ 1 ]) print ( \"Size of l5 = \" , sys . getsizeof ( l5 )) print ( \"Size of l6 = \" , sys . getsizeof ( l6 )) print ( \"Size of l5[1] = \" , sys . getsizeof ( l5 [ 1 ])) print ( \"Size of l6[1] = \" , sys . getsizeof ( l6 [ 1 ])) Out: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Deep Copy Example - Manual l5 = [1, [1, 2, 3]] l6 = [1, [1, 2, 3]] id(l5) = 140486838605384 id(l6) = 140486838226760 l5 == l6 True l5 is l6 False id(l5[1]) = 140486839016584 id(l6[1]) = 140486838253000 l5[1] == l6[1] True l5[1] is l6[1] False Size of l5 = 80 Size of l6 = 80 Size of l5[1] = 88 Size of l6[1] = 112 Here, * values of l5 and l6 are same * but ids of l5 and l6 are different * nested/child object of l5 is directly copied to l6 instead of passing the reference * the id of l5[1] & l6[1] are different, means changes in l6[0] will not affect l5[0] * sizes of l5 & l6 are same * sizes of l5[1] & l6[1] are different 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 print ( \" \\n Deep Copy Example - copy.deepcopy()\" ) import copy import sys l5 = [ 1 , l0 ] l6 = copy . deepcopy ( l5 ) # all value of l5 are copied to l6 print ( \"l5 = \" , l5 ) print ( \"l6 = \" , l6 ) print ( \"id(l5) =\" , id ( l5 )) print ( \"id(l6) = \" , id ( l6 )) print ( \"l5 == l6\" , l5 == l6 ) #checks value-wise print ( \"l5 is l6\" , l5 is l6 ) #checks object identity-wise print ( \"id(l5[1]) =\" , id ( l5 [ 1 ])) print ( \"id(l6[1]) = \" , id ( l6 [ 1 ])) print ( \"l5[1] == l6[1]\" , l5 [ 1 ] == l6 [ 1 ]) print ( \"l5[1] is l6[1]\" , l5 [ 1 ] is l6 [ 1 ]) print ( \"Size of l5 = \" , sys . getsizeof ( l5 )) print ( \"Size of l6 = \" , sys . getsizeof ( l6 )) print ( \"Size of l5[1] = \" , sys . getsizeof ( l5 [ 1 ])) print ( \"Size of l6[1] = \" , sys . getsizeof ( l6 [ 1 ])) Out: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Deep Copy Example - copy.deepcopy() l5 = [1, [1, 2, 3]] l6 = [1, [1, 2, 3]] id(l5) = 140486838138056 id(l6) = 140486838211656 l5 == l6 True l5 is l6 False id(l5[1]) = 140486839016584 id(l6[1]) = 140486838168328 l5[1] == l6[1] True l5[1] is l6[1] False Size of l5 = 80 Size of l6 = 96 Size of l5[1] = 88 Size of l6[1] = 96 Here, * Results are same as Manual Deep copy. Except, * sizes of l5 & l6 are different * sizes of l5[1] & l6[1] are different Two problems often exist with deep copy operations that don\u2019t exist with shallow copy operations: Recursive objects (compound objects that, directly or indirectly, contain a reference to themselves) may cause a recursive loop. Because deep copy copies everything it may copy too much, such as data which is intended to be shared between copies.","title":"Deep copy"},{"location":"Languages/python/python_django/","text":"Python Django # Python Django Intro Why should we use Django MVC Architecture Models Fields Field Options Views Templates Project Structure WSGI: Web Server Gateway Interface Rename Django Project Workflow django-admin.py Vs manage.py Uses Clean Database Load initial data / fixtures dump data / save DB data Check production readyness Run Dev server makemigrations into files migrate changes into db makemigrations into files Clean Migration Files Features Database Set CharSet MySQL Prerequisites Do Django models support multiple-column primary keys? Does Django support NoSQL databases? Using Multiple Databases in Django Migrations in Django ORM - Object Relational Mapper QuerySet Features: Iteration Slicing len() list() bool() QuerySet Operations: Field Lookups Custom Lookups & Transforms queries which do not return QuerySet all() distinct() filter() select_for_update() raw() F() order_by() exclude() annotate() reverse() values() values_list() extra() only() defer() dates() datetimes() none() union() intersection() difference() select_related() prefetch_related() using() 1 + N Problems Models Query Manager Manager names Custom Manager Inheritance style in django? Abstract base class Multi-table Inheritance Proxy models Extend User Model (Custom User Model) Using a Proxy Model Using One-To-One Link with a User Model (Profile) Creating a Custom User Model Extending AbstractBaseUser Creating a Custom User Model Extending AbstractUser Conclusion class Meta Options Views Function Based Generic View Class Based View Class Based Generic View URLs using view function using class based view including app urls django 2.0: using path django <=1.9: using url Static files In Production In Developement Templates Middlewares Custom Middleware Function Based Custom Middleware Class Based Custom Middleware Settings Place in settings.py Separated into production & development environments like STRICT SEPERATION FROM CODE Python Decouple Signals Built-in Signals Request/Response Signals Way of connecting/registering signals using <signal>.connect using receiver() decorator Asynchronous Signals Using Celery Installation Task Queue System - Celery Message Broker System - RabbitMQ Setup Code Enable Celery Write Task Celery vs RabiitMQ State Management Theory Stateless Session Anonymous Session Enabling the session Configuring The Session Engine Using Database-Backed Sessions Using Cached Sessions Using File-Based Sessions Using Cookie-Based Sessions Cookies What Why How Uses: Cache Why Architecture - Django Production Servers Web Server nginx (Pronounced as: Engine X) Application Server gunicorn (Pronounced as: gee-unicorn) DB Asynchronous Task Queue Celery Cron jobs Message Broker Solutions RabbitMQ Redis Amazon SQS Caching Solution Memcached Monitoring Graphite Statsd Sentry - logging New Relic Supervisor Stack Flow Working Deployment - Production Environment How to deploy django application in Production checklist critical settings Environment Related Settings HTTPS Performance Optimizations Error Reporting Testing Run Test Cases Security CSRF - Cross Site Request Forgery Why CSRF? Is it required in REST Django REST Framewok Some reasons you might want to use Django REST framework: REST API Serializers Implementations Relations Writable Nested Serializers class Meta Options Misc Views Custom/Disable Request Methods in ViewSet mapping for views from ViewSet using as_view() adding custom routes/action to the existing ViewSet URLs Routers actions using base_name: how to use namespace (provided in url patterns) adding custom routes/action urls to the existing Router or urlpatterns API Versioning Configurations Uses Allow CORS (Cross Origin Resource Sharing) in DRF Using Custom Middleware class Using package django-cors-headers Creating Schema from API Setting Media URL & ROOT Site Wide App Based Permission request methods level permissions (in ViewSet) objects level permission Exception/Http response Authentication Basic Auth Session Auth Token Auth JWT (JSON Web Token) django-rest-auth (Register, Login, Logout, Reset, Change..) djoser django-templated-mail Testing Testing ViewSet using: Security Vulnerabilities CSRF (Cross Site Request Forgery) XSS (Cross Site Script) Vulnerables Web Client Local/ Session Storage Cookies (JWT/Auth) Misc Running Multiple Host (website) from Single Django Project Get request URL string Get slugs from url Microservice Based on REST To Do: * Project Structure * REST App * App * Working: Re-phrase * Django Form * UUID Source: * https://docs.djangoproject.com/en/1.11/contents/ * https://docs.djangoproject.com/en/1.11/topics/ * https://www.ibm.com/developerworks/library/os-django/index.html * https://www.edureka.co/blog/django-tutorial/ Intro # Django is an open source Web development framework for the Python Designed to be loosely coupled and tightly cohesive, meaning that different parts of the framework, while connected to one another, are not dependent on one another DRY principle Why should we use Django # modular ease the administration by auto-generated web admin pre-packaged APIs template system to avoid code duplication enables to define URL for gives function seperates business logic from HTML everything is in python MVC Architecture # MTV - Model, Template, View Similar to MVC Models # Describes database schema & data structure Fields # BinaryField BigAutoField BigIntegerField CharField DateField DateTimeField DecimalField DurationFields EmailField IntegerField BooleanField TextField ImageField FileField FilePathField FloatField GenericIPAddressField AutoField ForeignKey ManytoOne solution ManyToMany OneToOne SlugField TimeField URLField UUIDField Field Options # blank null primary_key max_length choices validators db_columns db_index db_tablespace default editable help_text error_messages auto_now auto_now_add unique unique_for_date unique_for_month unique_for_year verbose_name Views # Same as Controller in MVC Controls what a user sees it retrieves data from appropriate model, executes any calculation on data and pass it to template 5 Module Name: HttpResponse, template.render(templatename),from django.shortcuts import render(req,temp_name,context), get_object_or_404, Templates # Same as View in MVC describes how user sees data & info describes how tha data recieved from views should be changed/formatted for display on the page Note: According to Django the framework + URL config feature itself is known as Controller. Project Structure # 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 Project (just a container)/ | |-----project/ | |--__init__.py | |--settings.py | |--urls.py | \u2514--wsgi.py | |-----app1/ | | | |-----app2-RESTful/ | | | |-----manage.py | | | \u2514-----db.sqlite3 Project: main project package settings.py: python module (module level) represents django settings urls.py: represents site-wide urls configuration (includes apps also) wsgi.py: Web-server-gateway-interface: djangos's primary deployement platform wsgi.py gets created on \"startproject\" contains \"application\" callable & \"DJANGO_SETTINGS_MODULE\" django setting eviroment variable DJANGO_SETTINGS_MODULE: locates setting file, default: project_name.settings.py application: used by server to communicate with code, default: get_wsgi_application() App1 App2- REST-ful WSGI: Web Server Gateway Interface # an interface between web server & application contains some statements, set of rules its not a software/library/framework WSGI compliant server will able to communicate with a WSGI compliant web app in WSGI, WSGI application has to be callable & it needs to be given to web server, so web server can call web application whenever it receives a request Rename Django Project # Your django project structure 1 2 3 4 5 6 7 ProjectName/ manage.py ProjectName/ __init__.py settings.py urls.py wsgi.py changes required at 4 places settings.py 1 2 ROOT_URLCONF = 'NewProjectName.urls' WSGI_APPLICATION = 'NewProjectName.wsgi.application' wsgi.py 1 os.environ.setdefault(\"DJANGO_SETTINGS_MODULE\", \"NewProjectName.settings\") manage.py 1 os.environ.setdefault(\"DJANGO_SETTINGS_MODULE\", \"NewProjectName.settings\") Workflow # runserver --> web-server gateway interface WSGI--> DJANGO_SETTING_MODULE env var -->by default project.Setting.py (site folder) --> ROOT_URLCONF --> url.py (site url file location) --> Browser --> URL --> DNS + PortNo from application (protocol based)--> IP + port --> http request--> Server -->web-server gateway interface WSGI--> DJANGO_SETTING_MODULE env var -->by default project.Setting.py (site folder) --> ROOT_URLCONF --> url.py (site url file location) --> urlpattern --> url scanning --> matched view --> model + http response + template --> web page django-admin.py Vs manage.py # Django-admin.py: It is a Django's command line utility for administrative tasks. Manage.py: It is an automatically created file in each Django project. It is a thin wrapper around the Django-admin.py. Uses # Clean Database # 1 python manage.py flush Load initial data / fixtures # src: https://coderwall.com/p/mvsoyg/django-dumpdata-and-loaddata 1 2 3 #prerequisites- <app_dir>/<fixtures>/<fixture_file.json> #else provide fixture files path python manage.py loaddata <fixture_file_name> dump data / save DB data # src: https://coderwall.com/p/mvsoyg/django-dumpdata-and-loaddata 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 #whole db python manage.py dumpdata > [ db.json ] #app wise python manage.py dumpdata [ app_name ] > [ app.json ] #table/model wise python manage.py dumpdata [ app.model ( in small )] > [ app_model.json ] #exclude some table python manage.py dumpdata --exclude [ app.model ( in small )] > [ db.json ] #specify indentation python manage.py dumpdata --indent 4 > [ db.json ] #specify output format python manage.py dumpdata --format [ json/xml/yaml ] > [ db.json ] #backup whole db fresh (without any Integrity Issue) python manage.py dumpdata --exclude auth.permission --exclude contenttypes > db.json Check production readyness # 1 python manage.py check --deploy Run Dev server # 1 python manage.py runserver 0 .0.0.0:8000 makemigrations into files # 1 python manage.py makemifrations <app_name> migrate changes into db # 1 python manage.py migrate makemigrations into files # 1 python manage.py makemifrations <app_name> Clean Migration Files # Features # Admin Interface (CRUD: Create, Retrieve, Update, Delete) Templating Form Handling Internationalization Session, User management, role-based permissions ORM (Object-Relational Mapping) Testing Framework Best Documentation Database # site folder setting.py DATABASES fill dict entries ENGINE: type of db django.db.backends.sqlite3 django.db.backends.mysql django.db.backends.postgresql_psycopg2 django.db.backends.oracle NAME: name of database USERNAME(optional) PASSWORD(optional) HOST(optional) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 DATABASES = { 'sqlite3' : { 'ENGINE' : 'django.db.backends.sqlite3' , 'NAME' : os . path . join ( BASE_DIR , 'db.sqlite3' ), }, 'default' : { 'ENGINE' : 'django.db.backends.mysql' , 'NAME' : 'employees' , 'USER' : 'test' , 'HOST' : 'localhost' , 'PORT' : '' , } } (optional) are required in case of DB other than sqlite. Set CharSet # By deafult django uses latin1 , its better to use unicode utf8 to support all types of language characters. So, Put this in settings.py: 1 DATABASE_OPTIONS = dict ( charset = \"utf8\" ) MySQL # Prerequisites # 1 2 sudo apt install python-dev libmysqlclient-dev pipenv install mysqlclient login to root 1 sudo mysql -u root -p create another user & grant all access 1 2 3 use ` <db_name> ` CREATE USER 'dev' @ 'localhost' IDENTIFIED BY 'password' ; GRANT ALL PRIVILEGES ON *.* TO 'dev' @ 'localhost' WITH GRANT OPTION ; use dev user in django install workbench (optional) 1 sudo apt install mysql-workbench Do Django models support multiple-column primary keys? # Ans No. Only single-column primary keys are supported. But using the unique_together model option we can achieve it. Does Django support NoSQL databases? # Ans No. Not officially. But can using 3rd party forks like Django non-rel. Using Multiple Databases in Django # Different ways of using multiple databases: * QuerySet's \"using\" Method 1 2 3 4 5 6 7 8 # This will run on the 'default' database. Author . objects . all () # So will this. Author . objects . using ( 'default' ) . all () # This will run on the 'other' database. Author . objects . using ( 'other' ) . all () Model.save()'s \"using\" Parameter 1 2 3 choice_one = Choice . objects . get ( pk = 1 ) choice_one . text = \"New Text\" choice_one . save ( using = \"Polls_DB\" ) Database Routing Migrations in Django # applies changes in models to database tables like deleteing/adding models/fields commands makemigrations creates migration files as per changes in models inside app-->migrations-->0001_initial.py, contains Migration class with all the operations/changes does not applies changes in DB 1 python manage . py makemigrations migrate applies migration files to DB 1 python manage . py migrate sqlmigrate generates sql from migration files 1 python manage . py sqlmigrate polls_app 0001 _initial ORM - Object Relational Mapper # defines your data model entirely in python provides rich & dynamic database-access API QuerySet Features: # Iteration # 1 2 for e in Entry . objects . all (): print ( e . headline ) Slicing # len() # 1 record_count = len ( Entry . objects . all ()) # will return length of result list list() # 1 entry_list = list ( Entry . objects . all ()) # convert to list bool() # 1 bool ( Entry . objects . filter ( age = 21 )) # same as EXISTS, will return True if there are results QuerySet Operations: # Field Lookups # Parameter passed using \",\" comma == AND For OR, use exclude() field__gt: greater than field__gte: greater than equal to field__lt: less than field__lte: less than equal to list slicing [:5] == for starting 5 records list slicing some_queryset.reverse()[:5] == for last 5 records field__exact field__iexact: non-casesensitive match field__contains : Entry.objects.get(headline__contains='Lennon'), same as LIKE %Lennon% field__icontains : Entry.objects.get(headline__icontains='LeNnon'), same as ILIKE %LenNon% field__in : Entry.objects.filter(id__in=[1, 3, 4]), same as SELECT ... WHERE id IN (1, 3, 4); field__startswith: Entry.objects.filter(headline__startswith='Lennon') field__istartswith: Entry.objects.filter(headline__istartswith='LeNnon') field__endswith: Entry.objects.filter(headline__endswith='Lennon') field__iendswith: Entry.objects.filter(headline__iendswith='Lennon') field__range : Entry.objects.filter(pub_date__range=(start_date, end_date)), same as SELECT ... WHERE pub_date BETWEEN '2005-01-01' and '2005-03-31'; field__date: Entry.objects.filter(pub_date__date=datetime.date(2005, 1, 1)) Entry.objects.filter(pub_date__date__gt=datetime.date(2005, 1, 1)) year month day week week_day quarter time : Entry.objects.filter(pub_date__time=datetime.time(14, 30)) hour minute second isnull : Entry.objects.filter(pub_date__isnull=True), same as SELECT ... WHERE pub_date IS NULL; regex : Entry.objects.get(title__regex=r'^(An?|The) +') iregex : Entry.objects.get(title__iregex=r'^(An?|The) +') Custom Lookups & Transforms # e.g. : title__slug='first-blog' TODO queries which do not return QuerySet # get() returns object MultipleObjectsError get_or_create() update_or_create() bulk_create() count() return count in_bulk() iterator() latest() earliest() first() last() aggreagte() returns dict exists() return True/False delete() returns the number of objects deleted and a dictionary with the number of deletions per object type. update() returns the number of objects updated all() # same as SELECT * distinct() # same as SELECT DISTINCT 1 2 3 Author . objects . distinct () Entry . objects . order_by ( 'blog' ) . distinct ( 'blog' ) # write parameters in same order in both Entry . objects . order_by ( 'blog' ) . distinct ( 'blog' ) # if 'blog' is foreign Model, then by deafult order_by will take it as 'blog__name', so explicity define it as 'blog__id' or 'blog__pk', else will not produce any result filter() # select_for_update() # will lock the row(s) till end of transaction raw() # raw(raw_query, params=None, translations=None) F() # for updating (increment/decrement) column value without fetching the current value in python memory 1 2 3 from django.db.models import F User . object . filter ( pk = 1 ) . update ( salary = F ( 'salary' ) + 1000 ) # here F is usefull order_by() # 1 2 3 User . objects . filter ( age = 21 ) . order_by ( '-salary' , 'name' ) # negative salary means in descending order User . objects . filter ( age = 21 ) . order_by ( '?' ) # random order (expensive) User . objects . order_by ( 'id' ) exclude() # 1 Entry . objects . exclude ( pub_date__gt = datetime . date ( 2005 , 1 , 3 )) # __gt: greater than annotate() # 1 2 3 from django.db.models import Count u = User . objects . annotate ( Count ( ' reverse() # to reverse the order values() # returns dictionary object instead list query result 1 2 Blog . objects . values () Blog . objects . values ( 'id' , 'name' ) values_list() # same as values() but returns tuple extra() # extra(select=None, where=None, params=None, tables=None, order_by=None, select_params=None) Sometimes, the Django query syntax by itself can\u2019t easily express a complex WHERE clause. For these edge cases, Django provides the extra() QuerySet modifier \u2014 a hook for injecting specific clauses into the SQL generated by a QuerySet. only() # opposite to defer name those, which should not get deferred except rest defer() # to defer some fields from a large data base passing field to it will not load those columns in queryset from DB, but we can access to field if we need by calling we can never defer pk dates() # datetimes() # none() # returns null queryset instance of EmptyQuerySet union() # same as SELECT * FROM TABLE1 UNION SELECT * FROM TABLE2 queryset1.union(queryset2) intersection() # difference() # select_related() # returns Foreign key related objects without hitting database 1 2 3 4 5 6 7 8 9 10 11 12 13 #Hits the database. e = Entry . objects . get ( id = 5 ) #Hits the database again to get the related Blog object. b = e . blog #Hits the database. e = Entry . objects . select_related ( 'blog' ) . get ( id = 5 ) #Doesn't hit the database, because e.blog has been prepopulated #in the previous query. b = e . blog prefetch_related() # To solve the 1 + N problem in all types of ORM after Django 1.4 using() # for choosing databse for the queryset 1 + N Problems # TODO Models # Query Manager # A class An interface through which database query operations are performed to Django models by default Manager for each Model is objects e.g. Questions. object .all() Manager names # if you want to rename objects e.g. 1 2 3 4 5 6 7 8 9 from django.db import models class Person ( models . Model ): #... people = models . Manager () #Uses Person . objects . all () #AttributeError Person . people . all () Custom Manager # if you want to define some more query methods Extend models.Manager class Code: https://gist.github.com/toransahu/62cd045891656b90f7e18a492e9b81db Inheritance style in django? # Abstract base class # you define a base class model as a abstract class cannot instantiate cannot use as a regular model cannot create a table in db you want to reuse the code for attributes (fields/methods) of the base class into other models each child model will have their own table in db e.g. 1 2 3 4 5 6 7 8 9 class CommonInfo ( models . Model ): name = models . CharField ( max_length = 100 ) age = models . PositiveIntegerField () class Meta : abstract = True class Student ( CommonInfo ): home_group = models . CharField ( max_length = 5 ) Multi-table Inheritance # This style is used when subclassing an existing model & need each model to have its own database table e.g. 1 2 3 4 5 6 7 8 9 from django.db import models class Place ( models . Model ): name = models . CharField ( max_length = 50 ) address = models . CharField ( max_length = 80 ) class Restaurant ( Place ): serves_hot_dogs = models . BooleanField ( default = False ) serves_pizza = models . BooleanField ( default = False ) All of the fields of Place will also be available in Restaurant, although the data will reside in a different database table. So these are both possible: 1 2 Place.objects.filter ( name = \"Bob's Cafe\" ) Restaurant.objects.filter ( name = \"Bob's Cafe\" ) Note: The inheritance relationship introduces links between the child model and each of its parents (via an automatically-created OneToOneField) Proxy models # You can use this model, If you only want to modify the Python level behavior of the model (means any methods/functions), without changing the models fields unlike multi-table inheritance, if we only want to add some methods change the default manager change the default ordering and at the same time we don't want to create different tables for each model we can inherit the base model & can define child models as proxy e.g. 1 2 3 4 5 6 7 8 9 10 11 12 13 from django.db import models class Person ( models . Model ): first_name = models . CharField ( max_length = 30 ) last_name = models . CharField ( max_length = 30 ) class MyPerson ( Person ): class Meta : proxy = True def do_something ( self ): #... pass Extend User Model (Custom User Model) # Some modifications on top of Django's default User Model just to fit our web appplication. Source: https://simpleisbetterthancomplex.com/tutorial/2016/07/22/how-to-extend-django-user-model.html 4 Ways to extend existing User Model: Using a Proxy Model # Proxy Model: Model inheritance without creating a new table in database. Used to change the deafult behaviour of an existing model (e.g. methods, ordering by) without affecting exisitng database table When used: When you don't need to save extra information in the databse, but want to add extra methods or change query Manager Code: https://gist.github.com/toransahu/676d4a8c29b5cbd7737ff1c0a0b4dfc4 Using One-To-One Link with a User Model (Profile) # One-To-One Link: one to one relationship between two Django Models Both Models are normal django model implemented using models.OneToOneField(SomeModelHere) When Should Use when you need to store some extra information about the exisitng User Model that's not related to authentication process called as User Profile How to Use Create User model & Profile model Create OneToOneField in Profile model of User model Define signals so our Profile model will be automatically created/updated when we create/update User instances We'll use post_save signals for this purpose follow e.g. https://gist.github.com/toransahu/f0bd7313c24605ce92d38a7b09caf4b4 Creating a Custom User Model Extending AbstractBaseUser # Custom User Model Extending AbstractBaseUser a new User model inheriting AbtractBaseUser class require extra care & AUTH_USER_MODEL reference in settings.py ideally it should be done in starting of the project When Should Use when specific requirement in authentication process e.g. change identification token from username to emailId or mobile_number Code: https://gist.github.com/toransahu/4a6314f40676a75b0288953f5c0e8b1c Creating a Custom User Model Extending AbstractUser # Custom User Model Extending AbstractUser a new User model inheriting AbtractUser class require extra care & AUTH_USER_MODEL reference in settings.py ideally it should be done in starting of the project When Should Use when we are perfectly happy with how Django handles the authentication process and we don't want to change anything on it yet we want to add some extra information directly in the User model without having to create extra class like Profile Code: https://gist.github.com/toransahu/0ce910494dedb8b9f2774b751f45b559 Whenever we define custom User Model like this 1 2 3 4 5 from django.db import models from django.contrib.auth.models import AbstractUser class User ( AbstractUser ): name = models . CharField ( max_length = 100 , blank = True , null = True ) we need to specify custom user model in settings.py like 1 AUTH_USER_MODEL = \u2018 your_app . User ' and we can refer this User model in our code either as 1 User = get_user_model () or 1 User = settings . AUTH_USER_MODEL # use when define a foreign relationship, to make it resuable app Conclusion # Proxy Model: You are happy with everything Django User provide and don\u2019t need to store extra information. User Profile: You are happy with the way Django handles the auth and need to add some non-auth related attributes to the User. Custom User Model from AbstractBaseUser: The way Django handles auth doesn\u2019t fit your project. Custom User Model from AbstractUser: The way Django handles auth is a perfect fit for your project but still you want to add extra attributes without having to create a separate Model. class Meta Options # TODO: Views # Function Based Generic View # with template (by rendering) 1 2 3 4 from django.shortcuts import render def home ( request ): return render ( request , 'home.html' ) without template : using HttpResponse 1 2 3 4 from django.http import HttpResponse def home ( request ): return HttpResponse ( \"Hello World\" ) Need to write conditional branch for different HTTP request type like POST, GET, PUT Need to provide view method name in URL Disadvantage: Cannot extend Class Based View # Module: from django.views import View Inherit View class Need to define get(), post() like HTTP methods Need to provide ClassName.as_view() in URL Advantage: Can be extended by sub classes 1 2 3 4 5 6 7 from django.http import HttpResponse from django.views import View class MyView ( View ): def get ( self , request ): # <view logic> return HttpResponse ( 'result' ) Class Based Generic View # Module: from django.views.generic import ListView Can inherit ListView, TemplateView... class No need to define request handler methods set model attribute to Model Class Need to provide ClassName.as_view() in URL 1 2 3 4 5 from django.views.generic import ListView from books.models import Publisher class PublisherList ( ListView ): model = Publisher URLs # using view function # 1 2 urlpatterns = [ path ( '/' , views . home (), name = 'home' ),] using class based view # 1 2 urlpatterns = [ path ( '/' , views . IndexView . as_view (), name = 'index' ),] including app urls # 1 2 3 4 from django.urls import path , include urlpatterns = [ path ( '' , include ( 'home.urls' )),] django 2.0: using path # 1 from django.urls import path , include django <=1.9: using url # 1 from django.conf.urls import url Static files # In Production # set STATIC_ROOT in settings.py run manage.py collectstatic In Developement # 1 2 3 4 5 6 7 8 9 10 STATICFILES_FINDERS = ( 'django.contrib.staticfiles.finders.FileSystemFinder' , 'django.contrib.staticfiles.finders.AppDirectoriesFinder' ) STATICFILES_DIRS = [ os . path . join ( BASE_DIR , 'static' ), '/var/www/static/' , ] STATIC_URL = '/static/' Templates # Contains Markups JS, CSS, HTML, XML django tags Variables/Logic blocks {% extends 'home/base.html' %} {% load static %} Comments {# CSS files#} Middlewares # Middleware is framework of hooks to Django's request/response processing its light and low-level plugin system for making global changes in Django's input/output each middleware component in responsible for performing some specific task Some usage of middlewares in Django is: Session management User authentication Cross-site request forgery protection Content Gzipping, etc. Custom Middleware # a middleware factory (i.e. outer function or a class) is a callable it takes an argument called get_response get_response might be an actual Django view if the middleware is last listed else, get_response might be a next middleware and it returns a middleware (or ultimately a response ) a middleware is also a callable which takes an arg called request and it returns a response Function Based Custom Middleware # 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def simple_middleware ( get_response ): # One-time configuration and initialization. def middleware ( request ): # Code to be executed for each request before # the view (and later middleware) are called. response = get_response ( request ) # Code to be executed for each request/response after # the view is called. return response return middleware Class Based Custom Middleware # 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 class SimpleMiddleware : def __init__ ( self , get_response ): self . get_response = get_response # One-time configuration and initialization. def __call__ ( self , request ): # Code to be executed for each request before # the view (and later middleware) are called. response = self . get_response ( request ) # Code to be executed for each request/response after # the view is called. return response Settings # Place in settings.py # Separated into production & development environments like # 1 2 3 4 - base_settings.py - dev_settings.py - prod_settings.py - settings.py STRICT SEPERATION FROM CODE # Python Decouple # Source: https://pypi.org/project/python-decouple/ Usage Where the settings data are stored? Ini file Env file How it works? Understanding the CAST argument Signals # Signals are a strategy to allow decoupled applications to get notified when some event occurs. Source: https://simpleisbetterthancomplex.com/tutorial/2016/07/28/how-to-create-django-signals.html where to create <signals_module>.py ? anywhere recommended: inside apps Built-in Signals # django.db.models.signals.pre_init: receiver_function(sender, *args, **kwargs) django.db.models.signals.post_init: receiver_function(sender, instance) django.db.models.signals.pre_save: receiver_function(sender, instance, raw, using, update_fields) django.db.models.signals.post_save: receiver_function(sender, instance, created, raw, using, update_fields) django.db.models.signals.pre_delete: receiver_function(sender, instance, using) django.db.models.signals.post_delete: receiver_function(sender, instance, using) django.db.models.signals.m2m_changed: receiver_function(sender, instance, action, reverse, model, pk_set, using) Request/Response Signals # django.core.signals.request_started: receiver_function(sender, environ) django.core.signals.request_finished: receiver_function(sender, environ) django.core.signals.got_request_exception: receiver_function(sender, request) Way of connecting/registering signals # using <signal>.connect # Need to register signals inside ready() in AppConfig class in <app>.apps.py Need to define default_app_config = '<app>.apps.<App>Config' in <app>.__init__.py ignore if '<app>.apps.<App>Config' is inside INSTALLED_APPS insettings.py Issues: In django 2.0+, not working fine. Throwing error: AppRegistryNotReady: Apps aren't loaded yet. e.g.: https://gist.github.com/toransahu/c3870b4ad58bde5a9b9563f7e0883729 using receiver() decorator # Only need to import signals inside ready() in AppConfig class in <app>.apps.py Need to define default_app_config = '<app>.apps.<App>Config' in <app>.__init__.py ignore if '<app>.apps.<App>Config' is inside INSTALLED_APPS insettings.py No issues till yet e.g.: https://gist.github.com/toransahu/c3870b4ad58bde5a9b9563f7e0883729 Asynchronous Signals Using Celery # src https://simpleisbetterthancomplex.com/tutorial/2017/08/20/how-to-use-celery-with-django.html http://docs.celeryproject.org/en/latest/django/first-steps-with-django.html Installation # Task Queue System - Celery # 1 pip install celery Message Broker System - RabbitMQ # src: http://docs.celeryproject.org/en/latest/getting-started/brokers/rabbitmq.html#broker-rabbitmq 1 sudo apt install rabbitmq-server Setup # no need to define any setup, but can set credentials for rabbitmq-server Code # Enable Celery # Define celery instance in project level inside celery.py Load celery when django starts import the celery instance app to init .py in project package Write Task # code: https://gist.github.com/toransahu/d01c7374c5317a908b99ac03cf24cc11 create tasks.py inside django app celery searches for tasks inside tasks.py modules either import celery app instance & use 1 2 3 4 from backend.celery import app @app . task def foo (): pass or use shared_task 1 2 3 4 from celery import shared_task @shared_task def foo (): pass set broker_url inside settings.py every celery config variables start with CELERY_ 1 CELERY_BROKER_URL = 'amqp://localhost' start rabbitmq starts automatically on boot 1 2 sudo systemctl enable rabbitmq-server sudo systemctl start rabbitmq-server start celery worker need to run inside src folder need to be in virtual env 1 celery -A project_name worker -l info or create script to start celery cd ethereal-machines-backend/src (where manage.py is) vim start_celery.sh make sure to write #! /bin/bash in first line chmod a+x start_celery.sh start_celery.sh should look like this: https://gist.github.com/toransahu/31874def1bd661db676d0dfe02e1c651 Celery vs RabiitMQ # Celery is a queue Wrapper/Framework which takes away the complexity of having to manage the underlying AMQP mechanisms/architecture that come with operating RabbitMQ directly Celery is just a very high level of abstraction to implement the producer / consumer of events. It takes out several painful things you need to do to work for example with rabbitmq. Celery itself is not the queue. The events queues are stored in the system of your choice, celery helps you to work with such events without having to write the producer / consumer from scratch. State Management # Session: Anonymous Session Cookies Theory # Stateless # Meaning navigating from one web page to another will not retain infos of first one. There is no persistence between one request and the next, and there is no way the server can tell whether successive requests come from the same person e.g. HTTP, REST This lack of state is managed using sessions. Session # are a semi-permanent, two-way communication between your browser and the web server. The session framework lets you store and retrieve arbitrary data on a per-site-visitor basis It stores data on the server side and abstracts the sending and receiving of cookies can be implemented through middleware In client side cookies contain a session ID \u0093 not the data itself (unless youre using the cookie based backend) 1 2 3 4 5 6 7 INSTALLED_APPS = [ 'django.contrib.sessions' , ] MIDDLEWARE = [ 'django.contrib.sessions.middleware.SessionMiddleware' , ] Anonymous Session # to keep track of data relevant to your visit the web server can only record what you did, not who you are Enabling the session # Using middleware MIDDLEWARE_CLASSES in setting.py should contain 'django.contrib.sessions.middleware.SessionMiddleware' by deafult enabled Configuring The Session Engine # by default stored in database using the model django.contrib.sessions.models.Session can be configured to store session data on file system or in cache Using Database-Backed Sessions # need to add 'django.contrib.sessions' to your INSTALLED_APPS setting Using Cached Sessions # for better performance, for making web pages more responsive local memory cache backend doesnt retain data long enough to be a good choice use third-party like Memcached cache backend, else use file system or DB based session two different implementation: Only cache Set SESSION_ENGINE to \"django.contrib.sessions.backends.cache\" Cache + DB (Persistent) set SESSION_ENGINE to \"django.contrib.sessions.backends.cached_db\" every write to the cache will also be written to the database use the database if the data is not already in the cache Using File-Based Sessions # have to set the SESSION_ENGINE settings to \u009cdjango.contrib.sessions.backends.file\u009d Using Cookie-Based Sessions # Cookies # What # An HTTP cookie (web cookie, browser cookie) is a small piece of data that a server sends to the user's web browser. The browser may store it and send it back with the next request to the same server. module: http.cookies Why # Typically, it's used to tell if two requests came from the same browser \u2014 keeping a user logged-in, for example. How # It remembers stateful information for the stateless HTTP protocol. Uses: # Session management Logins, shopping carts, game scores, or anything else the server should remember Personalization User preferences, themes, and other settings Tracking Recording and analyzing user behavior Cache # Why # The performance of web sites and applications can be significantly improved by reusing previously fetched resources. Web caches reduce latency and network traffic and thus lessen the time needed to display a representation of a resource. By making use of HTTP caching, Web sites become more responsive. Architecture - Django Production Servers # Source : https://www.sayonetech.com/blog/how-host-your-django-project-production-server/#.WgSKV3VL9Xo Web Server # the outermost tier of the Backend(3-tiers) Apache, nginx, lighttpd, cherokee used as proxy, reverse proxy, load balancer, static data (css, html, images) dispatcher and cache it can't talk directly to Django applications nginx (Pronounced as: Engine X) # Application Server # the middle tier of the Backend(3-tiers) Gunicorn, mod_python, mod_wsgi, mod_uwsgi, FastCGI is used to handle all dynamic requests, basically based on URL pattern (view call) the Interface between the web server and the python app so that the app(or any python framework) understands the incoming requests create a Unix socket, and serve responses to nginx via the wsgi protocol - the socket passes data in both directions gunicorn (Pronounced as: gee-unicorn) # inspired from Ruby's Unicorn DB # the third tier of the Backend(3-tiers) MySQL/ Postgres/ Other databases Asynchronous Task Queue # Celery # Celery is an asynchronous task/job queue based on distributed message passing requires an external solution to send and receive messages i.e. Message Brokers like RabbitMQ, Redis focused on real-time operation, but supports scheduling as well Cron jobs # Message Broker Solutions # RabbitMQ # Redis # Amazon SQS # Caching Solution # Memcached # Monitoring # When our project is hosted, we need to monitor it using some tools to check its performance, its error logs and user interactions.We have some tools available for this. Graphite # Graphite provides real-time visualization and storage of numeric time-series data on an enterprise level. Statsd # A network daemon that runs on the Node.js platform and listens for statistics, like counters and timers, sent over UDP or TCP and sends aggregates to one or more pluggable backend services (e.g.,Graphite). Sentry - logging # Sentry is a modern error logging and aggregation platform. New Relic # A software analytics tool suite used by developers, ops, and software companies to understand how your applications are performing in development and production. Supervisor # a process control system a client/server system which allows its users to monitor and control a number of process in UNIX-like OS similar to launchd , daemontools , and runit monitors projects starts on boot Supervisord starts processes as its subprocesses, and can be configured to automatically restart them on a crash accurately shows the up/down times of the processes can asign priorities to the processes can group the processes Stack Flow # User requests from browser. Request reaches Nginx. If (request is staic) Nginx serves the request. Else if (request is dynamic) Nginx forwards the request to Application server (Gunicorn). Gunicorn receives the request, executes corresponding python (Flask) code. Gunicorn returns the response to Nginx. Nginx serves the response to the user. Working # You need both Nginx and Gunicorn (or something similar) for a proper Django deployment The complete answer is both Nginx and Gunicorn handle the request. Basically, Nginx will receive the request and if it's a dynamic request (generally based on URL patterns) then it will give that request to Gunicorn, which will process it, and then return a response to Nginx which then forwards the response back to the original client. Deployment - Production Environment # How to deploy django application in Production # https://devcenter.heroku.com/articles/getting-started-with-python#introduction https://developer.mozilla.org/en-US/docs/Learn/Server-side/Django/Deployment DEBUG = False change default SECRET_KEY (used for CRSF protection) and hide it somewhere else Run manage.py check --deploy (to check the default list of changes mentioned by django) checklist https://docs.djangoproject.com/en/1.10/howto/deployment/checklist/ checklist # in settings must be set properly for Django to provide the expected level of security; are expected to be different in each environment; enable optional security features; enable performance optimizations; provide error reporting. or Run manage.py check --deploy to list all the factors listed below take care of these things if releasing source code critical settings # SECRET_KEY Instead of hardcoding the secret key in your settings module, consider loading it from an environment variable or file 1 2 3 4 5 6 import os SECRET_KEY = os . environ [ 'SECRET_KEY' ] with open ( '/etc/secret_key.txt' ) as f : SECRET_KEY = f . read () . strip () avoid committing it to source control DEBUG : set to False Environment Related Settings # ALLOWED_HOSTS When DEBUG = False, Django doesn\u2019t work at all without a suitable value for ALLOWED_HOSTS. This setting is required to protect your site against some CSRF attacks CACHE change it for production performance otimization default for developement is 'local-memory caching' instead use cache servers like Memcached using 'cached sessions' Cache servers often have weak authentication. Make sure they only accept connections from your application servers. DATABASE Database passwords are very sensitive. Keep them in environment variable or in file same as SECRET_KEY For maximum security, make sure database servers only accept connections from your application servers. If you haven\u2019t set up backups for your database, do it right now! EMAIL_BACKEND and related settings If your site sends emails, these values need to be set correctly. modify the DEFAULT_FROM_EMAIL and SERVER_EMAIL settings By default, Django sends email from webmaster@localhost and root@localhost. STATIC_ROOT and STATIC_URL Static files are automatically served by the development server. In production, you must define a STATIC_ROOT directory where collectstatic will copy them. MEDIA_ROOT and MEDIA_URL Media files are uploaded by your users. They\u2019re untrusted! Make sure your web server never attempts to interpret them. For instance, if a user uploads a .php file, the web server shouldn\u2019t execute it. HTTPS # Any website which allows users to log in should enforce site-wide HTTPS to avoid transmitting access tokens in clear. In Django, access tokens include the login/password, the session cookie, and password reset tokens. Note: You can\u2019t do much to protect password reset tokens if you\u2019re sending them by email web server must redirect all HTTP traffic to HTTPS, and only transmit HTTPS requests to Django. because: the same session cookie is used for HTTP and HTTPS. Once you\u2019ve set up HTTPS, enable the following settings. CSRF_COOKIE_SECURE Set this to True to avoid transmitting the CSRF cookie over HTTP accidentally. SESSION_COOKIE_SECURE Set this to True to avoid transmitting the session cookie over HTTP accidentally. Performance Optimizations # DEBUG = False CONN_MAX_AGE TEMPLATES Enabling the cached template loader often improves performance drastically, as it avoids compiling each template every time it needs to be rendered. Error Reporting # LOGGING Review your logging configuration before putting your website in production, and check that it works as expected as soon as you have received some traffic Customize the default error views Django includes default views and templates for several HTTP error codes. You may want to override the default templates by creating the following templates in your root template directory: 404.html, 500.html, 403.html, and 400.html. Testing # Class level testing for each app 1 2 3 4 5 6 from django.test import TestCase class QuestionModelTests ( TestCase ): def test_was_published_recently_with_future_question ( self ): #do something self . assertIs ( future_question . was_published_recently (), False ) Run Test Cases # 1 python manage.py test appname Security # CSRF - Cross Site Request Forgery # Why CSRF? # CSRF attack happens in presence of state It really boils down to the browsers ability to automatically present login credentials for any request by sending along cookies. If a session id is stored in a cookie the browser will automatically send it along with all requests that go back to the original website. This means that an attacker doesn't actually have to know authentication details to take an action as the victim user. Rather, the attacker just has to trick the victims browser into making a request, and the credentials to authenticate the request will ride along for free. Is it required in REST # No, it will be useless piece of code because REST is stateless at client-side a cookie-less REST endpoint is completely immune from CSRF attacks if there is cookie used for authentication, then we need CSRF protection HTTP/BasicAuthentication will also need CSRF protection also, if any app uses any tech to store state of app at clientside, then its not a RESTful app src: https://security.stackexchange.com/questions/166724/should-i-use-csrf-protection-on-rest-api-endpoints Django REST Framewok # Django REST framework \"djangorestframework\" is powerful & flexibal toolkit for creating web APIs. - https://stackoverflow.com/questions/671118/what-exactly-is-restful-programming Some reasons you might want to use Django REST framework: # The Web browsable API is a huge usability win for your developers. Authentication policies including packages for OAuth1a and OAuth2. Serialization that supports both ORM and non-ORM data sources. Customizable all the way down - just use regular function-based views if you don't need the more powerful features. Extensive documentation, and great community support. REST API # Please refer to this . Serializers # Module: from rest_framework import serializers Provides a way to serialize & deserialize Model instances into representations like JSON. Serialization is mechanism of converting the state of an object into byte-stream, which can be displayed, stored. Deserialization is reverse mechanism of serialization. Note: In django its very similar to Django Form class and includes similar validation flags on the various fields, such as required, max_length and default. Implementations # Inherit classes serializers.Serializer need to write all the fields mentioned in Model (only those, which we want to use here) need to define create() & update() method to create/update new Model instance from validated data (representaion/JSON) serializers.ModelSerializer Inside Meta class mention model inside Meta class; model = Snippet i.e. Class fields = ('id', ....,'title') i.e. tuple Automatically implements default create() and update() methods Note : If we want to include url in fields, then the base_name (in case of routers) in urls.py should same as the name of Model in lower case, alternatively don't mention base_name serializers.HyperlinkedModelSerializer The only difference is, as in citation you included, that primary and foreign keys are represented by URLs that point to those resources, instead of just actual key values. The benefit is that you will not have to construct resource URLs in your frontend when you want to retrieve related objects. Relations # src: http://www.django-rest-framework.org/api-guide/relations/#serializer-relations Writable Nested Serializers # scr: http://www.django-rest-framework.org/api-guide/relations/#writable-nested-serializers code: https://gist.github.com/toransahu/221371c981c20f0b9c645019a53b90c7 by default django does not provides write access in case of nested model objects & their model serializers strategy create 2 models assign M2M/Foreign key field in one model write serializers for both the models keep foreign model serializers normal/default for other model serializer write custom code override create() method handle foreign field data explicitly create post instance from data iterate over list value of foreign field create instance of image append image instance to post instance's foreign field override update() method write normal viewset for only one : i.e. posts (both is optional) Issues faced: if nested fields are char if posting as application/json from django form : works fine if posting as multipart/form-data from django form: validation fails for required fields means data received is empty works fine with python code only iff data & files both are provided if nested fields are image cannot post using django form able to post using custom form: https://github.com/toransahu/multiple-file-upload able to post using python code class Meta Options # TODO: Misc # Note: after serializer.is_valid() we can't save serializer if we have already accessed serializer.data; to avoid this, access serializer.validated_data custom create & update in serializer - writable nested serializers datefield attribute: auto_now vs auto_now_add auto_now: Automatically set the field to now every time the object is saved Useful for \"last-modified\" timestamps cannot be overriden auto_now_add Automatically set the field to now when the object is first created Useful for creation of timestamps cannot be overriden Postman: nested: https://medium.com/@darilldrems/how-to-send-arrays-with-get-or-post-request-in-postman-f87ca70b154e Try: https://github.com/beda-software/drf-writable-nested https://github.com/alanjds/drf-nested-routers Base64 ImageField Views # Function Based View Using normal functions like in Django (without using any rest_framework feature) Modules: 1 2 3 4 from django.views.decorators.csrf import csrf_exempt from django.http import HttpResponse , JsonResponse from rest_framework.renderers import JSONRenderer from rest_framework.parsers import JSONParser Approach: write function with conditional branching for different http request methods write a _list function for listing all records (GET) & submiting a record (POST) write a _detail function for fetching, modifying or destroying a specific record by pk, ID or Name use JSON response & parser use @csrf_exempt decorator for safety code: https://gist.github.com/toransahu/1bad12b87dcd160c0de0d29d218d9bf6 Using @api_view() decorator Modules: 1 2 3 from rest_framework import status from rest_framework.decorators import api_view from rest_framework.response import Response Approach: write function with conditional branching for different http request methods write a _list function for listing all records (GET) & submiting a record (POST) write a _detail function for fetching, modifying or destroying a specific record by pk, ID or Name decorate with @api_view() with parameters like ['GET'], ['GET', 'POST'], ['GET', 'PUT', 'DELETE'] etc By default @api_view() takes GET method if nothing is mentioned code: https://gist.github.com/toransahu/5c99704ec721e461b5f8fa67776a6d74 Class Based View By Inheriting APIView class Modules: 1 2 3 4 from rest_framework.views import APIView from rest_framework.response import Response from rest_framework import status from django.http import Http404 Approach: write classes and define request handler methods in name of http request method for each http request method write a List class for listing all records (GET) & submiting a record (POST) write a Detail class for fetching, modifying or destroying a specific record by pk, ID or Name similar to Django's \"View\" class Note request handler methods receives REST's Request instance instead django's HttpRequest instance request handler methods may return REST's Response instance instead django's HttpResponse instance set few attributes like authentication_classes = (authentication.TokenAuthentication,) permission_classes = (permissions.IsAdminUser,) code: https://gist.github.com/toransahu/bcdb1a6beb5da0475c8c056837da940f Using mixins Modules: 1 2 from rest_framework import mixins from rest_framework import generics Approach: write classes and inherit generics.GenericAPIView & mixins as per use write a List class and inherit mixins.ListModelMixin, mixins.CreateModelMixin, generics.GenericAPIView write a Detail class and inherit mixins.RetrieveModelMixin, mixins.UpdateModelMixin, mixins.DestroyModelMixin, generics.GenericAPIView base class providescore functionality & mixin classes provides actions like .list(), .create(), .retrieve(), .update() and .destroy() Code: https://gist.github.com/toransahu/dfc2666307c1628042b99edaba630c07 Using generic class-based views Modules: 1 from rest_framework import generics Approach: write List class and inherit generics.ListCreateAPIView write Detail class and generics.RetrieveUpdateDestroyAPIView Code: https://gist.github.com/toransahu/ab21d8bf46f45616ccb9285dbd0b201a Using ViewSet Modules: from rest_framework import viewsets Approach: Only need to write a single class, named ModelNameViewSet and inhert viewsets.ModelViewSet facilitates router for URL writing Advantage: provides create, retrieve, update, and destroy in a single Class def DRY View code DRY URL code can also add custom endpoints as per our need, apart from regular create, retrieve, update, and destroy endpoint provided by ViewSet Code: https://gist.github.com/toransahu/c7d43776065aa6fda05d7389133c68f9 Custom/Disable Request Methods in ViewSet # By default ViewSets provides All the requests methods this technique will give power to override,disable those methods to do this use mixins with viewset.GenericViewSet , or override methods of ViewSet class url: https://gist.github.com/toransahu/e02fc30cd0e55e968971c46e59862acb mapping for views from ViewSet using as_view() # syntax {<method>:<action>} patterns {'get': 'list'} {'get': 'retrieve'} {'post': 'create'} {'put': 'update'} {'patch': 'partial_update'} {'delete': 'destroy'} adding custom routes/action to the existing ViewSet # https://gist.github.com/toransahu/95781bc23f39192276d011d0aa990470 http://www.django-rest-framework.org/api-guide/routers/#customizing-dynamic-routes URLs # if using include( ) dont provide namespace if providing namespace inside include define app_name = in app/urls.py Routers # REST framework adds support for automatic URL routing to Django, and provides you with a simple, quick and consistent way of wiring your view logic to a set of URLs. 1 2 3 4 5 6 7 8 9 from django.urls import path from .views import BlogViewSet from rest_framework.routers import DefaultRouter router = DefaultRouter () router . register ( '' , BlogViewSet , base_name = 'blogs' ) urlpatterns = router . urls Note: Try to keep base_name same as Model name, because view_name like blog-detail, blog-list comes from Model & not from app's name when we create custom actions (similar to -list, -detail) in any View using @detail_route(), we access that action using base_name in serializers. alternatively remove base_name actions using base_name: # <base_name>-list <base_name>-detail <base_name>-<any_custom> how to use namespace (provided in url patterns) # using from django.urls import reverse reverse(<namespace>:<base_name>-<action> or reverse(<namespace>:<model_name>-<action> adding custom routes/action urls to the existing Router or urlpatterns # https://gist.github.com/toransahu/95781bc23f39192276d011d0aa990470 API Versioning # Source e.g. Its always a good idea to version your API so that you can make changes in your API without disturbing your current clients. e.g. http://127.0.0.1:8000/api/v1/blogs/ Configurations # settings.py 1 2 3 REST_FRAMEWORK = { 'DEFAULT_VERSIONING_CLASS' : 'rest_framework.versioning.NamespaceVersioning' } urls.py 1 2 3 4 urlpatterns = [ path ( 'admin/' , admin . site . urls ), path ( 'api/v1/blogs/' , include ( 'blogs.urls' , namespace = 'v1' )), ] blogs/urls.py 1 2 3 4 5 6 app_name = 'blogs' router = DefaultRouter () router . register ( '' , BlogViewSet , base_name = 'blogs' ) urlpatterns = router . urls Uses # Once you have set versioning, django will be able to provide value of request.version else it will be None so, apply conditions ( if/else ) in View or serializers if there are changes in fields, then use versioning in serializers else, if there are only changes in some functionalities, (which current client cannot consume), then use versioning in veiws only, like: 1 2 3 4 def get_serializer_class ( self ): if self . request . version == 'v1' : return AccountSerializerVersion1 return AccountSerializer Allow CORS (Cross Origin Resource Sharing) in DRF # Source1 Source2 Using Custom Middleware class # need to define for each app python manage.py startapp app create app/cors.py and write 1 2 3 4 class CorsMiddleware ( object ): def process_response ( self , req , resp ): response [ \"Access-Control-Allow-Origin\" ] = \"*\" return response - MIDDLEWARE_CLASSES = ['app.CorsMiddleware'] Using package django-cors-headers # works site-wide pipenv install django-cors-headers in settings.py INSTALLED_APPS = ['corsheaders'] MIDDLEWARE_CLASSES = ['corsheaders.middleware.CorsMiddleware',] , keep in top as possible as Allow using any one CORS_ORIGIN_ALLOW_ALL = True OR CORS_ORIGIN_ALLOW_ALL = False CORS_ORIGIN_WHITELIST = ['http//:localhost:8000',] also set ALLOWED_HOSTS = ['192.168.1.121'] and run server like python manage.py runserver 192.168.1.121:8000 Creating Schema from API # provides all the details about api endpoint present in site-wide urls will show all the endpoints actions fields if any authentication is needed it will show schema accordingly; if not authenticated, it will show that much api endpoints only need to configure in site-wide urls.py source: http://www.django-rest-framework.org/api-guide/schemas/ pipenv install coreapi 1 2 3 4 5 6 7 8 from rest_framework.schemas import get_schema_view schema_view = get_schema_view ( title = \"Server Monitoring API\" ) urlpatterns = [ url ( '^<dollor_sign>' , schema_view ), ... ] Setting Media URL & ROOT # Site Wide # App Based # define MEDIA_URL & MEDIA_ROOT in settings.py 1 2 3 4 5 6 7 8 9 10 # Media files # https://timmyomahony.com/blog/static-vs-media-and-root-vs-path-in-django/ # the relative browser URL to be used when accessing our media files in the browser MEDIA_URL = 'media/' # the absolute path to the folder that will hold our user uploads # to get absolute path without hardcoding ENV_PATH = os . path . abspath ( os . path . dirname ( __file__ )) + os . sep + os . pardir MEDIA_ROOT = os . path . join ( ENV_PATH , 'media/' ) - add following code in app/urls.py 1 2 3 4 5 6 7 # adding media url from django.conf import settings # from backend.settings import DEBUG from django.conf.urls.static import static # If developement env if settings . DEBUG is True : urlpatterns += static ( settings . MEDIA_URL , document_root = settings . MEDIA_ROOT ) want to set upload_to directory dynamically: code: https://gist.github.com/toransahu/8f9407250cee7a729473335bdd7a3f3b here comes, signal things, pre_save, post_save Permission # request methods level permissions (in ViewSet) # this needs when \"a user must post only or admin must post only\" type of requirements comes url-view: https://gist.github.com/toransahu/e02fc30cd0e55e968971c46e59862acb url-permission: https://gist.github.com/toransahu/c40b625165a395fabf772700f3ab2e04 objects level permission # TODO Exception/Http response # When the permissions checks fail either a \"403 Forbidden\" or a \"401 Unauthorized\" response will be returned, according to the following rules: The request was successfully authenticated, but permission was denied. \u2014 An HTTP 403 Forbidden response will be returned. The request was not successfully authenticated, and the highest priority authentication class does not use WWW-Authenticate headers. \u2014 An HTTP 403 Forbidden response will be returned. The request was not successfully authenticated, and the highest priority authentication class does use WWW-Authenticate headers. \u2014 An HTTP 401 Unauthorized response, with an appropriate WWW-Authenticate header will be returned. Authentication # Source: http://www.django-rest-framework.org/api-guide/authentication/ Basic Auth # module: from rest_framework.authentication import BasicAuthentication By default used in DRF, whether you mention it in settings.py or views.py or not is only suitable for testing purpose, don't use in production if using in production you need to stick to Django Admin Login form page, can't use JSON need to be HTTPS Session Auth # module from rest_framework.authentication import SessionAuthentication TODO Token Auth # Source: https://stackoverflow.com/questions/14838128/django-rest-framework-token-authentication module `from rest_framework.authentication import TokenAuthentication Prerequisites add 'rest_framework.authtoken' to INSTALLED_APPS in settings.py mention TokenAuthentication class in views or function based 1 2 3 @authentication_classes ([ TokenAuthentication , ]) def abcd - detail (): pass class based 1 2 class Abcd (): authentication_classes = [ TokenAuthentication , ] settings.py 1 2 3 4 5 6 7 8 9 REST_FRAMEWORK = { 'DEFAULT_VERSIONING_CLASS' : 'rest_framework.versioning.NamespaceVersioning' , 'DEFAULT_AUTHENTICATION_CLASSES' : ( # 'rest_framework.authentication.BasicAuthentication', # 'rest_framework.authentication.SessionAuthentication', 'rest_framework.authentication.TokenAuthentication' , ), } Obtain Token DRF provides a view which returns a token on correct username & password. include following in urls.py 1 2 3 from rest_framework.authtoken.views import obtain_auth_token urlpatterns = [ path ( 'api-auth-token/' , obtain_auth_token ),] - call the view as: http POST 127.0.0.1:8000/api-token-auth/ username='admin' password='whatever' will get token like: 1 2 3 { \"token\" : \"blah_blah_blah\" } Use the Token in API call put the following in Header 1 key: Authorization value: Token token_value Please mind the space between Token & token_value http GET 127.0.0.1:8000/whatever 'Authorization: Token your_token_value' JWT (JSON Web Token) # src: http://getblimp.github.io/django-rest-framework-jwt/ quick replacement of Default Token Auth Basic Changes Needed: pip install djangorestframework-jwt add in REST_FRAMEWORK setting 'DEFAULT_AUTHENTICATION_CLASSES': ( 'rest_framework_jwt.authentication.JSONWebTokenAuthentication', ) add view in url 1 2 3 4 5 6 7 8 9 from rest_framework_jwt.views import obtain_jwt_token #... urlpatterns = [ '' , # ... url ( r '^api-token-auth/' , obtain_jwt_token ), ] jwt setting variables 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 JWT_AUTH = { 'JWT_ENCODE_HANDLER' : 'rest_framework_jwt.utils.jwt_encode_handler' , 'JWT_DECODE_HANDLER' : 'rest_framework_jwt.utils.jwt_decode_handler' , 'JWT_PAYLOAD_HANDLER' : 'rest_framework_jwt.utils.jwt_payload_handler' , 'JWT_PAYLOAD_GET_USER_ID_HANDLER' : 'rest_framework_jwt.utils.jwt_get_user_id_from_payload_handler' , 'JWT_RESPONSE_PAYLOAD_HANDLER' : 'rest_framework_jwt.utils.jwt_response_payload_handler' , #'JWT_SECRET_KEY': settings.SECRET_KEY, #will take from settings.py by default 'JWT_GET_USER_SECRET_KEY' : None , 'JWT_PUBLIC_KEY' : None , 'JWT_PRIVATE_KEY' : None , 'JWT_ALGORITHM' : 'HS256' , 'JWT_VERIFY' : True , 'JWT_VERIFY_EXPIRATION' : True , 'JWT_LEEWAY' : 0 , 'JWT_EXPIRATION_DELTA' : datetime . timedelta ( seconds = 300 ), 'JWT_AUDIENCE' : None , 'JWT_ISSUER' : None , 'JWT_ALLOW_REFRESH' : True , #allowing it to refresh 'JWT_REFRESH_EXPIRATION_DELTA' : datetime . timedelta ( days = 7 ), 'JWT_AUTH_HEADER_PREFIX' : 'Token' , 'JWT_AUTH_COOKIE' : None , } django-rest-auth (Register, Login, Logout, Reset, Change..) # src http://django-rest-auth.readthedocs.io/en/latest/api_endpoints.html https://michaelwashburnjr.com/django-user-authentication/ not recommanded, is not completely RESTful have issues with password/reset & password/reset/confirm/ depends on django-allauth for email things djoser # src: http://djoser.readthedocs.io/en/stable/sample_usage.html djoser uses following settings for email configuration (which is also used by django's default mail module from django.core.mail import send_mail 1 2 3 4 5 6 7 8 9 10 11 12 13 CONFIG_PATH = os . path . join ( ENV_PATH , '../configs/' ) EMAIL_USE_TLS = True #EMAIL_USE_SSL = True #Use any one from TLS, SSL EMAIL_BACKEND = 'django.core.mail.backends.smtp.EmailBackend' #default one, production #EMAIL_BACKEND = 'django.core.mail.backends.console.EmailBackend' #for development EMAIL_HOST = 'smtp.gmail.com' with open ( os . path . join ( CONFIG_PATH , 'email_pwd' )) as f : EMAIL_HOST_PASSWORD = f . readline () EMAIL_HOST_USER = 'noreply.etherealmachines@gmail.com' EMAIL_PORT = 587 DEFAULT_FROM_EMAIL = EMAIL_HOST_USER EMAIL_ADMIN = ( 'toran.ethereal@gmail.com' , ) - djoser setting variables 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 DJOSER = { 'PASSWORD_RESET_CONFIRM_URL' : 'auth/password/reset/confirm/ {uid} / {token} ' , 'ACTIVATION_URL' : 'auth/password/reset/confirm/ {uid} / {token} ' , 'EMAIL' : { 'activation' : 'djoser.email.ActivationEmail' , 'confirmation' : 'djoser.email.ConfirmationEmail' , 'password_reset' : 'djoser.email.PasswordResetEmail' , }, # DEFAULT; no need to define here 'SERIALIZERS' : { 'activation' : 'djoser.serializers.ActivationSerializer' , 'password_reset' : 'djoser.serializers.PasswordResetSerializer' , 'password_reset_confirm' : 'djoser.serializers.PasswordResetConfirmSerializer' , 'password_reset_confirm_retype' : 'djoser.serializers.PasswordResetConfirmRetypeSerializer' , 'set_password' : 'djoser.serializers.SetPasswordSerializer' , 'set_password_retype' : 'djoser.serializers.SetPasswordRetypeSerializer' , 'set_username' : 'djoser.serializers.SetUsernameSerializer' , 'set_username_retype' : 'djoser.serializers.SetUsernameRetypeSerializer' , 'user_create' : 'djoser.serializers.UserCreateSerializer' , 'user_delete' : 'djoser.serializers.UserDeleteSerializer' , 'user' : 'djoser.serializers.UserSerializer' , 'token' : 'djoser.serializers.TokenSerializer' , 'token_create' : 'djoser.serializers.TokenCreateSerializer' , }, } django-templated-mail # djoser uses it to create mail template & send mails few setting variables: DOMAIN SITE_NAME Testing # Testing ViewSet using: # APITestCase Token Authentication APIRequestFactory Source: https://gist.github.com/toransahu/706bd1de705e21f3be000e1517f7cae8 Security # https://stormpath.com/blog/where-to-store-your-jwts-cookies-vs-html5-web-storage http://kylebebak.github.io/post/django-rest-framework-auth-csrf django-jwt (JWT_AUTH_COOKIE ) is not safe https://github.com/GetBlimp/django-rest-framework-jwt/issues/338 if SessionAuthentication is enabled, then CSRF will be used (if ON) https://stackoverflow.com/questions/30871033/django-rest-framework-remove-csrf if JWTAuthentications is enabled, then CSRF is automatically disabled Vulnerabilities # CSRF (Cross Site Request Forgery) # XSS (Cross Site Script) # Vulnerables # Web Client Local/ Session Storage # XSS Cookies (JWT/Auth) # CSRF ajax call : https://gist.github.com/bengolder/aa9033efc8959dc38e5d Misc # Running Multiple Host (website) from Single Django Project # source: http://effbot.org/zone/django-multihost.htm Get request URL string # 1 2 3 4 5 6 from django.contrib.sites.shortcuts import get_current_site print ( 'query_params are:' , request . build_absolute_uri ()) #returns url with domain print ( request . get_full_path ()) #returns url without domain print ( get_current_site ( request ) . domain , request . get_host ()) #both returns host or domain Get slugs from url # if you have any - url like: http://127.0.0.1:8000/auth/password/reset/confirm/Mw/4vw-d9cdc8954482ecf8e253/ - url pattern like: path('password/reset/confirm/ / /', password_reset_confirm, name='password_reset_confirm_custom_get') then code to fetch uid & token will be: https://gist.github.com/toransahu/68663e302f87a9c32a1dcd0654408577 Microservice Based on REST # https://www.fullstackpython.com/microservices.html https://martinfowler.com/articles/microservices.html https://dev.otto.de/2016/03/20/why-microservices/ with flask: https://medium.com/@ssola/building-microservices-with-python-part-i-5240a8dcc2fb django to flask based microservice - https://medium.com/greedygame-media/how-we-broke-up-our-monolithic-django-service-into-microservices-8ad6ff4db9d4 https://blog.rapid7.com/2016/09/15/microservices-please-dont/","title":"Python Django"},{"location":"Languages/python/python_django/#python-django","text":"Python Django Intro Why should we use Django MVC Architecture Models Fields Field Options Views Templates Project Structure WSGI: Web Server Gateway Interface Rename Django Project Workflow django-admin.py Vs manage.py Uses Clean Database Load initial data / fixtures dump data / save DB data Check production readyness Run Dev server makemigrations into files migrate changes into db makemigrations into files Clean Migration Files Features Database Set CharSet MySQL Prerequisites Do Django models support multiple-column primary keys? Does Django support NoSQL databases? Using Multiple Databases in Django Migrations in Django ORM - Object Relational Mapper QuerySet Features: Iteration Slicing len() list() bool() QuerySet Operations: Field Lookups Custom Lookups & Transforms queries which do not return QuerySet all() distinct() filter() select_for_update() raw() F() order_by() exclude() annotate() reverse() values() values_list() extra() only() defer() dates() datetimes() none() union() intersection() difference() select_related() prefetch_related() using() 1 + N Problems Models Query Manager Manager names Custom Manager Inheritance style in django? Abstract base class Multi-table Inheritance Proxy models Extend User Model (Custom User Model) Using a Proxy Model Using One-To-One Link with a User Model (Profile) Creating a Custom User Model Extending AbstractBaseUser Creating a Custom User Model Extending AbstractUser Conclusion class Meta Options Views Function Based Generic View Class Based View Class Based Generic View URLs using view function using class based view including app urls django 2.0: using path django <=1.9: using url Static files In Production In Developement Templates Middlewares Custom Middleware Function Based Custom Middleware Class Based Custom Middleware Settings Place in settings.py Separated into production & development environments like STRICT SEPERATION FROM CODE Python Decouple Signals Built-in Signals Request/Response Signals Way of connecting/registering signals using <signal>.connect using receiver() decorator Asynchronous Signals Using Celery Installation Task Queue System - Celery Message Broker System - RabbitMQ Setup Code Enable Celery Write Task Celery vs RabiitMQ State Management Theory Stateless Session Anonymous Session Enabling the session Configuring The Session Engine Using Database-Backed Sessions Using Cached Sessions Using File-Based Sessions Using Cookie-Based Sessions Cookies What Why How Uses: Cache Why Architecture - Django Production Servers Web Server nginx (Pronounced as: Engine X) Application Server gunicorn (Pronounced as: gee-unicorn) DB Asynchronous Task Queue Celery Cron jobs Message Broker Solutions RabbitMQ Redis Amazon SQS Caching Solution Memcached Monitoring Graphite Statsd Sentry - logging New Relic Supervisor Stack Flow Working Deployment - Production Environment How to deploy django application in Production checklist critical settings Environment Related Settings HTTPS Performance Optimizations Error Reporting Testing Run Test Cases Security CSRF - Cross Site Request Forgery Why CSRF? Is it required in REST Django REST Framewok Some reasons you might want to use Django REST framework: REST API Serializers Implementations Relations Writable Nested Serializers class Meta Options Misc Views Custom/Disable Request Methods in ViewSet mapping for views from ViewSet using as_view() adding custom routes/action to the existing ViewSet URLs Routers actions using base_name: how to use namespace (provided in url patterns) adding custom routes/action urls to the existing Router or urlpatterns API Versioning Configurations Uses Allow CORS (Cross Origin Resource Sharing) in DRF Using Custom Middleware class Using package django-cors-headers Creating Schema from API Setting Media URL & ROOT Site Wide App Based Permission request methods level permissions (in ViewSet) objects level permission Exception/Http response Authentication Basic Auth Session Auth Token Auth JWT (JSON Web Token) django-rest-auth (Register, Login, Logout, Reset, Change..) djoser django-templated-mail Testing Testing ViewSet using: Security Vulnerabilities CSRF (Cross Site Request Forgery) XSS (Cross Site Script) Vulnerables Web Client Local/ Session Storage Cookies (JWT/Auth) Misc Running Multiple Host (website) from Single Django Project Get request URL string Get slugs from url Microservice Based on REST To Do: * Project Structure * REST App * App * Working: Re-phrase * Django Form * UUID Source: * https://docs.djangoproject.com/en/1.11/contents/ * https://docs.djangoproject.com/en/1.11/topics/ * https://www.ibm.com/developerworks/library/os-django/index.html * https://www.edureka.co/blog/django-tutorial/","title":"Python Django"},{"location":"Languages/python/python_django/#intro","text":"Django is an open source Web development framework for the Python Designed to be loosely coupled and tightly cohesive, meaning that different parts of the framework, while connected to one another, are not dependent on one another DRY principle","title":"Intro"},{"location":"Languages/python/python_django/#why-should-we-use-django","text":"modular ease the administration by auto-generated web admin pre-packaged APIs template system to avoid code duplication enables to define URL for gives function seperates business logic from HTML everything is in python","title":"Why should we use Django"},{"location":"Languages/python/python_django/#mvc-architecture","text":"MTV - Model, Template, View Similar to MVC","title":"MVC Architecture"},{"location":"Languages/python/python_django/#models","text":"Describes database schema & data structure","title":"Models"},{"location":"Languages/python/python_django/#fields","text":"BinaryField BigAutoField BigIntegerField CharField DateField DateTimeField DecimalField DurationFields EmailField IntegerField BooleanField TextField ImageField FileField FilePathField FloatField GenericIPAddressField AutoField ForeignKey ManytoOne solution ManyToMany OneToOne SlugField TimeField URLField UUIDField","title":"Fields"},{"location":"Languages/python/python_django/#field-options","text":"blank null primary_key max_length choices validators db_columns db_index db_tablespace default editable help_text error_messages auto_now auto_now_add unique unique_for_date unique_for_month unique_for_year verbose_name","title":"Field Options"},{"location":"Languages/python/python_django/#views","text":"Same as Controller in MVC Controls what a user sees it retrieves data from appropriate model, executes any calculation on data and pass it to template 5 Module Name: HttpResponse, template.render(templatename),from django.shortcuts import render(req,temp_name,context), get_object_or_404,","title":"Views"},{"location":"Languages/python/python_django/#templates","text":"Same as View in MVC describes how user sees data & info describes how tha data recieved from views should be changed/formatted for display on the page Note: According to Django the framework + URL config feature itself is known as Controller.","title":"Templates"},{"location":"Languages/python/python_django/#project-structure","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 Project (just a container)/ | |-----project/ | |--__init__.py | |--settings.py | |--urls.py | \u2514--wsgi.py | |-----app1/ | | | |-----app2-RESTful/ | | | |-----manage.py | | | \u2514-----db.sqlite3 Project: main project package settings.py: python module (module level) represents django settings urls.py: represents site-wide urls configuration (includes apps also) wsgi.py: Web-server-gateway-interface: djangos's primary deployement platform wsgi.py gets created on \"startproject\" contains \"application\" callable & \"DJANGO_SETTINGS_MODULE\" django setting eviroment variable DJANGO_SETTINGS_MODULE: locates setting file, default: project_name.settings.py application: used by server to communicate with code, default: get_wsgi_application() App1 App2- REST-ful","title":"Project Structure"},{"location":"Languages/python/python_django/#wsgi-web-server-gateway-interface","text":"an interface between web server & application contains some statements, set of rules its not a software/library/framework WSGI compliant server will able to communicate with a WSGI compliant web app in WSGI, WSGI application has to be callable & it needs to be given to web server, so web server can call web application whenever it receives a request","title":"WSGI: Web Server Gateway Interface"},{"location":"Languages/python/python_django/#rename-django-project","text":"Your django project structure 1 2 3 4 5 6 7 ProjectName/ manage.py ProjectName/ __init__.py settings.py urls.py wsgi.py changes required at 4 places settings.py 1 2 ROOT_URLCONF = 'NewProjectName.urls' WSGI_APPLICATION = 'NewProjectName.wsgi.application' wsgi.py 1 os.environ.setdefault(\"DJANGO_SETTINGS_MODULE\", \"NewProjectName.settings\") manage.py 1 os.environ.setdefault(\"DJANGO_SETTINGS_MODULE\", \"NewProjectName.settings\")","title":"Rename Django Project"},{"location":"Languages/python/python_django/#workflow","text":"runserver --> web-server gateway interface WSGI--> DJANGO_SETTING_MODULE env var -->by default project.Setting.py (site folder) --> ROOT_URLCONF --> url.py (site url file location) --> Browser --> URL --> DNS + PortNo from application (protocol based)--> IP + port --> http request--> Server -->web-server gateway interface WSGI--> DJANGO_SETTING_MODULE env var -->by default project.Setting.py (site folder) --> ROOT_URLCONF --> url.py (site url file location) --> urlpattern --> url scanning --> matched view --> model + http response + template --> web page","title":"Workflow"},{"location":"Languages/python/python_django/#django-adminpy-vs-managepy","text":"Django-admin.py: It is a Django's command line utility for administrative tasks. Manage.py: It is an automatically created file in each Django project. It is a thin wrapper around the Django-admin.py.","title":"django-admin.py Vs manage.py"},{"location":"Languages/python/python_django/#uses","text":"","title":"Uses"},{"location":"Languages/python/python_django/#clean-database","text":"1 python manage.py flush","title":"Clean Database"},{"location":"Languages/python/python_django/#load-initial-data-fixtures","text":"src: https://coderwall.com/p/mvsoyg/django-dumpdata-and-loaddata 1 2 3 #prerequisites- <app_dir>/<fixtures>/<fixture_file.json> #else provide fixture files path python manage.py loaddata <fixture_file_name>","title":"Load initial data / fixtures"},{"location":"Languages/python/python_django/#dump-data-save-db-data","text":"src: https://coderwall.com/p/mvsoyg/django-dumpdata-and-loaddata 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 #whole db python manage.py dumpdata > [ db.json ] #app wise python manage.py dumpdata [ app_name ] > [ app.json ] #table/model wise python manage.py dumpdata [ app.model ( in small )] > [ app_model.json ] #exclude some table python manage.py dumpdata --exclude [ app.model ( in small )] > [ db.json ] #specify indentation python manage.py dumpdata --indent 4 > [ db.json ] #specify output format python manage.py dumpdata --format [ json/xml/yaml ] > [ db.json ] #backup whole db fresh (without any Integrity Issue) python manage.py dumpdata --exclude auth.permission --exclude contenttypes > db.json","title":"dump data / save DB data"},{"location":"Languages/python/python_django/#check-production-readyness","text":"1 python manage.py check --deploy","title":"Check production readyness"},{"location":"Languages/python/python_django/#run-dev-server","text":"1 python manage.py runserver 0 .0.0.0:8000","title":"Run Dev server"},{"location":"Languages/python/python_django/#makemigrations-into-files","text":"1 python manage.py makemifrations <app_name>","title":"makemigrations into files"},{"location":"Languages/python/python_django/#migrate-changes-into-db","text":"1 python manage.py migrate","title":"migrate changes into db"},{"location":"Languages/python/python_django/#makemigrations-into-files_1","text":"1 python manage.py makemifrations <app_name>","title":"makemigrations into files"},{"location":"Languages/python/python_django/#clean-migration-files","text":"","title":"Clean Migration Files"},{"location":"Languages/python/python_django/#features","text":"Admin Interface (CRUD: Create, Retrieve, Update, Delete) Templating Form Handling Internationalization Session, User management, role-based permissions ORM (Object-Relational Mapping) Testing Framework Best Documentation","title":"Features"},{"location":"Languages/python/python_django/#database","text":"site folder setting.py DATABASES fill dict entries ENGINE: type of db django.db.backends.sqlite3 django.db.backends.mysql django.db.backends.postgresql_psycopg2 django.db.backends.oracle NAME: name of database USERNAME(optional) PASSWORD(optional) HOST(optional) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 DATABASES = { 'sqlite3' : { 'ENGINE' : 'django.db.backends.sqlite3' , 'NAME' : os . path . join ( BASE_DIR , 'db.sqlite3' ), }, 'default' : { 'ENGINE' : 'django.db.backends.mysql' , 'NAME' : 'employees' , 'USER' : 'test' , 'HOST' : 'localhost' , 'PORT' : '' , } } (optional) are required in case of DB other than sqlite.","title":"Database"},{"location":"Languages/python/python_django/#set-charset","text":"By deafult django uses latin1 , its better to use unicode utf8 to support all types of language characters. So, Put this in settings.py: 1 DATABASE_OPTIONS = dict ( charset = \"utf8\" )","title":"Set CharSet"},{"location":"Languages/python/python_django/#mysql","text":"","title":"MySQL"},{"location":"Languages/python/python_django/#prerequisites","text":"1 2 sudo apt install python-dev libmysqlclient-dev pipenv install mysqlclient login to root 1 sudo mysql -u root -p create another user & grant all access 1 2 3 use ` <db_name> ` CREATE USER 'dev' @ 'localhost' IDENTIFIED BY 'password' ; GRANT ALL PRIVILEGES ON *.* TO 'dev' @ 'localhost' WITH GRANT OPTION ; use dev user in django install workbench (optional) 1 sudo apt install mysql-workbench","title":"Prerequisites"},{"location":"Languages/python/python_django/#do-django-models-support-multiple-column-primary-keys","text":"Ans No. Only single-column primary keys are supported. But using the unique_together model option we can achieve it.","title":"Do Django models support multiple-column primary keys?"},{"location":"Languages/python/python_django/#does-django-support-nosql-databases","text":"Ans No. Not officially. But can using 3rd party forks like Django non-rel.","title":"Does Django support NoSQL databases?"},{"location":"Languages/python/python_django/#using-multiple-databases-in-django","text":"Different ways of using multiple databases: * QuerySet's \"using\" Method 1 2 3 4 5 6 7 8 # This will run on the 'default' database. Author . objects . all () # So will this. Author . objects . using ( 'default' ) . all () # This will run on the 'other' database. Author . objects . using ( 'other' ) . all () Model.save()'s \"using\" Parameter 1 2 3 choice_one = Choice . objects . get ( pk = 1 ) choice_one . text = \"New Text\" choice_one . save ( using = \"Polls_DB\" ) Database Routing","title":"Using Multiple Databases in Django"},{"location":"Languages/python/python_django/#migrations-in-django","text":"applies changes in models to database tables like deleteing/adding models/fields commands makemigrations creates migration files as per changes in models inside app-->migrations-->0001_initial.py, contains Migration class with all the operations/changes does not applies changes in DB 1 python manage . py makemigrations migrate applies migration files to DB 1 python manage . py migrate sqlmigrate generates sql from migration files 1 python manage . py sqlmigrate polls_app 0001 _initial","title":"Migrations in Django"},{"location":"Languages/python/python_django/#orm-object-relational-mapper","text":"defines your data model entirely in python provides rich & dynamic database-access API","title":"ORM - Object Relational Mapper"},{"location":"Languages/python/python_django/#queryset-features","text":"","title":"QuerySet Features:"},{"location":"Languages/python/python_django/#iteration","text":"1 2 for e in Entry . objects . all (): print ( e . headline )","title":"Iteration"},{"location":"Languages/python/python_django/#slicing","text":"","title":"Slicing"},{"location":"Languages/python/python_django/#len","text":"1 record_count = len ( Entry . objects . all ()) # will return length of result list","title":"len()"},{"location":"Languages/python/python_django/#list","text":"1 entry_list = list ( Entry . objects . all ()) # convert to list","title":"list()"},{"location":"Languages/python/python_django/#bool","text":"1 bool ( Entry . objects . filter ( age = 21 )) # same as EXISTS, will return True if there are results","title":"bool()"},{"location":"Languages/python/python_django/#queryset-operations","text":"","title":"QuerySet Operations:"},{"location":"Languages/python/python_django/#field-lookups","text":"Parameter passed using \",\" comma == AND For OR, use exclude() field__gt: greater than field__gte: greater than equal to field__lt: less than field__lte: less than equal to list slicing [:5] == for starting 5 records list slicing some_queryset.reverse()[:5] == for last 5 records field__exact field__iexact: non-casesensitive match field__contains : Entry.objects.get(headline__contains='Lennon'), same as LIKE %Lennon% field__icontains : Entry.objects.get(headline__icontains='LeNnon'), same as ILIKE %LenNon% field__in : Entry.objects.filter(id__in=[1, 3, 4]), same as SELECT ... WHERE id IN (1, 3, 4); field__startswith: Entry.objects.filter(headline__startswith='Lennon') field__istartswith: Entry.objects.filter(headline__istartswith='LeNnon') field__endswith: Entry.objects.filter(headline__endswith='Lennon') field__iendswith: Entry.objects.filter(headline__iendswith='Lennon') field__range : Entry.objects.filter(pub_date__range=(start_date, end_date)), same as SELECT ... WHERE pub_date BETWEEN '2005-01-01' and '2005-03-31'; field__date: Entry.objects.filter(pub_date__date=datetime.date(2005, 1, 1)) Entry.objects.filter(pub_date__date__gt=datetime.date(2005, 1, 1)) year month day week week_day quarter time : Entry.objects.filter(pub_date__time=datetime.time(14, 30)) hour minute second isnull : Entry.objects.filter(pub_date__isnull=True), same as SELECT ... WHERE pub_date IS NULL; regex : Entry.objects.get(title__regex=r'^(An?|The) +') iregex : Entry.objects.get(title__iregex=r'^(An?|The) +')","title":"Field Lookups"},{"location":"Languages/python/python_django/#custom-lookups-transforms","text":"e.g. : title__slug='first-blog' TODO","title":"Custom Lookups &amp; Transforms"},{"location":"Languages/python/python_django/#queries-which-do-not-return-queryset","text":"get() returns object MultipleObjectsError get_or_create() update_or_create() bulk_create() count() return count in_bulk() iterator() latest() earliest() first() last() aggreagte() returns dict exists() return True/False delete() returns the number of objects deleted and a dictionary with the number of deletions per object type. update() returns the number of objects updated","title":"queries which do not return QuerySet"},{"location":"Languages/python/python_django/#all","text":"same as SELECT *","title":"all()"},{"location":"Languages/python/python_django/#distinct","text":"same as SELECT DISTINCT 1 2 3 Author . objects . distinct () Entry . objects . order_by ( 'blog' ) . distinct ( 'blog' ) # write parameters in same order in both Entry . objects . order_by ( 'blog' ) . distinct ( 'blog' ) # if 'blog' is foreign Model, then by deafult order_by will take it as 'blog__name', so explicity define it as 'blog__id' or 'blog__pk', else will not produce any result","title":"distinct()"},{"location":"Languages/python/python_django/#filter","text":"","title":"filter()"},{"location":"Languages/python/python_django/#select_for_update","text":"will lock the row(s) till end of transaction","title":"select_for_update()"},{"location":"Languages/python/python_django/#raw","text":"raw(raw_query, params=None, translations=None)","title":"raw()"},{"location":"Languages/python/python_django/#f","text":"for updating (increment/decrement) column value without fetching the current value in python memory 1 2 3 from django.db.models import F User . object . filter ( pk = 1 ) . update ( salary = F ( 'salary' ) + 1000 ) # here F is usefull","title":"F()"},{"location":"Languages/python/python_django/#order_by","text":"1 2 3 User . objects . filter ( age = 21 ) . order_by ( '-salary' , 'name' ) # negative salary means in descending order User . objects . filter ( age = 21 ) . order_by ( '?' ) # random order (expensive) User . objects . order_by ( 'id' )","title":"order_by()"},{"location":"Languages/python/python_django/#exclude","text":"1 Entry . objects . exclude ( pub_date__gt = datetime . date ( 2005 , 1 , 3 )) # __gt: greater than","title":"exclude()"},{"location":"Languages/python/python_django/#annotate","text":"1 2 3 from django.db.models import Count u = User . objects . annotate ( Count ( '","title":"annotate()"},{"location":"Languages/python/python_django/#reverse","text":"to reverse the order","title":"reverse()"},{"location":"Languages/python/python_django/#values","text":"returns dictionary object instead list query result 1 2 Blog . objects . values () Blog . objects . values ( 'id' , 'name' )","title":"values()"},{"location":"Languages/python/python_django/#values_list","text":"same as values() but returns tuple","title":"values_list()"},{"location":"Languages/python/python_django/#extra","text":"extra(select=None, where=None, params=None, tables=None, order_by=None, select_params=None) Sometimes, the Django query syntax by itself can\u2019t easily express a complex WHERE clause. For these edge cases, Django provides the extra() QuerySet modifier \u2014 a hook for injecting specific clauses into the SQL generated by a QuerySet.","title":"extra()"},{"location":"Languages/python/python_django/#only","text":"opposite to defer name those, which should not get deferred except rest","title":"only()"},{"location":"Languages/python/python_django/#defer","text":"to defer some fields from a large data base passing field to it will not load those columns in queryset from DB, but we can access to field if we need by calling we can never defer pk","title":"defer()"},{"location":"Languages/python/python_django/#dates","text":"","title":"dates()"},{"location":"Languages/python/python_django/#datetimes","text":"","title":"datetimes()"},{"location":"Languages/python/python_django/#none","text":"returns null queryset instance of EmptyQuerySet","title":"none()"},{"location":"Languages/python/python_django/#union","text":"same as SELECT * FROM TABLE1 UNION SELECT * FROM TABLE2 queryset1.union(queryset2)","title":"union()"},{"location":"Languages/python/python_django/#intersection","text":"","title":"intersection()"},{"location":"Languages/python/python_django/#difference","text":"","title":"difference()"},{"location":"Languages/python/python_django/#select_related","text":"returns Foreign key related objects without hitting database 1 2 3 4 5 6 7 8 9 10 11 12 13 #Hits the database. e = Entry . objects . get ( id = 5 ) #Hits the database again to get the related Blog object. b = e . blog #Hits the database. e = Entry . objects . select_related ( 'blog' ) . get ( id = 5 ) #Doesn't hit the database, because e.blog has been prepopulated #in the previous query. b = e . blog","title":"select_related()"},{"location":"Languages/python/python_django/#prefetch_related","text":"To solve the 1 + N problem in all types of ORM after Django 1.4","title":"prefetch_related()"},{"location":"Languages/python/python_django/#using","text":"for choosing databse for the queryset","title":"using()"},{"location":"Languages/python/python_django/#1-n-problems","text":"TODO","title":"1 + N Problems"},{"location":"Languages/python/python_django/#models_1","text":"","title":"Models"},{"location":"Languages/python/python_django/#query-manager","text":"A class An interface through which database query operations are performed to Django models by default Manager for each Model is objects e.g. Questions. object .all()","title":"Query Manager"},{"location":"Languages/python/python_django/#manager-names","text":"if you want to rename objects e.g. 1 2 3 4 5 6 7 8 9 from django.db import models class Person ( models . Model ): #... people = models . Manager () #Uses Person . objects . all () #AttributeError Person . people . all ()","title":"Manager names"},{"location":"Languages/python/python_django/#custom-manager","text":"if you want to define some more query methods Extend models.Manager class Code: https://gist.github.com/toransahu/62cd045891656b90f7e18a492e9b81db","title":"Custom Manager"},{"location":"Languages/python/python_django/#inheritance-style-in-django","text":"","title":"Inheritance style in django?"},{"location":"Languages/python/python_django/#abstract-base-class","text":"you define a base class model as a abstract class cannot instantiate cannot use as a regular model cannot create a table in db you want to reuse the code for attributes (fields/methods) of the base class into other models each child model will have their own table in db e.g. 1 2 3 4 5 6 7 8 9 class CommonInfo ( models . Model ): name = models . CharField ( max_length = 100 ) age = models . PositiveIntegerField () class Meta : abstract = True class Student ( CommonInfo ): home_group = models . CharField ( max_length = 5 )","title":"Abstract base class"},{"location":"Languages/python/python_django/#multi-table-inheritance","text":"This style is used when subclassing an existing model & need each model to have its own database table e.g. 1 2 3 4 5 6 7 8 9 from django.db import models class Place ( models . Model ): name = models . CharField ( max_length = 50 ) address = models . CharField ( max_length = 80 ) class Restaurant ( Place ): serves_hot_dogs = models . BooleanField ( default = False ) serves_pizza = models . BooleanField ( default = False ) All of the fields of Place will also be available in Restaurant, although the data will reside in a different database table. So these are both possible: 1 2 Place.objects.filter ( name = \"Bob's Cafe\" ) Restaurant.objects.filter ( name = \"Bob's Cafe\" ) Note: The inheritance relationship introduces links between the child model and each of its parents (via an automatically-created OneToOneField)","title":"Multi-table Inheritance"},{"location":"Languages/python/python_django/#proxy-models","text":"You can use this model, If you only want to modify the Python level behavior of the model (means any methods/functions), without changing the models fields unlike multi-table inheritance, if we only want to add some methods change the default manager change the default ordering and at the same time we don't want to create different tables for each model we can inherit the base model & can define child models as proxy e.g. 1 2 3 4 5 6 7 8 9 10 11 12 13 from django.db import models class Person ( models . Model ): first_name = models . CharField ( max_length = 30 ) last_name = models . CharField ( max_length = 30 ) class MyPerson ( Person ): class Meta : proxy = True def do_something ( self ): #... pass","title":"Proxy models"},{"location":"Languages/python/python_django/#extend-user-model-custom-user-model","text":"Some modifications on top of Django's default User Model just to fit our web appplication. Source: https://simpleisbetterthancomplex.com/tutorial/2016/07/22/how-to-extend-django-user-model.html 4 Ways to extend existing User Model:","title":"Extend User Model (Custom User Model)"},{"location":"Languages/python/python_django/#using-a-proxy-model","text":"Proxy Model: Model inheritance without creating a new table in database. Used to change the deafult behaviour of an existing model (e.g. methods, ordering by) without affecting exisitng database table When used: When you don't need to save extra information in the databse, but want to add extra methods or change query Manager Code: https://gist.github.com/toransahu/676d4a8c29b5cbd7737ff1c0a0b4dfc4","title":"Using a Proxy Model"},{"location":"Languages/python/python_django/#using-one-to-one-link-with-a-user-model-profile","text":"One-To-One Link: one to one relationship between two Django Models Both Models are normal django model implemented using models.OneToOneField(SomeModelHere) When Should Use when you need to store some extra information about the exisitng User Model that's not related to authentication process called as User Profile How to Use Create User model & Profile model Create OneToOneField in Profile model of User model Define signals so our Profile model will be automatically created/updated when we create/update User instances We'll use post_save signals for this purpose follow e.g. https://gist.github.com/toransahu/f0bd7313c24605ce92d38a7b09caf4b4","title":"Using One-To-One Link with a User Model (Profile)"},{"location":"Languages/python/python_django/#creating-a-custom-user-model-extending-abstractbaseuser","text":"Custom User Model Extending AbstractBaseUser a new User model inheriting AbtractBaseUser class require extra care & AUTH_USER_MODEL reference in settings.py ideally it should be done in starting of the project When Should Use when specific requirement in authentication process e.g. change identification token from username to emailId or mobile_number Code: https://gist.github.com/toransahu/4a6314f40676a75b0288953f5c0e8b1c","title":"Creating a Custom User Model Extending AbstractBaseUser"},{"location":"Languages/python/python_django/#creating-a-custom-user-model-extending-abstractuser","text":"Custom User Model Extending AbstractUser a new User model inheriting AbtractUser class require extra care & AUTH_USER_MODEL reference in settings.py ideally it should be done in starting of the project When Should Use when we are perfectly happy with how Django handles the authentication process and we don't want to change anything on it yet we want to add some extra information directly in the User model without having to create extra class like Profile Code: https://gist.github.com/toransahu/0ce910494dedb8b9f2774b751f45b559 Whenever we define custom User Model like this 1 2 3 4 5 from django.db import models from django.contrib.auth.models import AbstractUser class User ( AbstractUser ): name = models . CharField ( max_length = 100 , blank = True , null = True ) we need to specify custom user model in settings.py like 1 AUTH_USER_MODEL = \u2018 your_app . User ' and we can refer this User model in our code either as 1 User = get_user_model () or 1 User = settings . AUTH_USER_MODEL # use when define a foreign relationship, to make it resuable app","title":"Creating a Custom User Model Extending AbstractUser"},{"location":"Languages/python/python_django/#conclusion","text":"Proxy Model: You are happy with everything Django User provide and don\u2019t need to store extra information. User Profile: You are happy with the way Django handles the auth and need to add some non-auth related attributes to the User. Custom User Model from AbstractBaseUser: The way Django handles auth doesn\u2019t fit your project. Custom User Model from AbstractUser: The way Django handles auth is a perfect fit for your project but still you want to add extra attributes without having to create a separate Model.","title":"Conclusion"},{"location":"Languages/python/python_django/#class-meta-options","text":"TODO:","title":"class Meta Options"},{"location":"Languages/python/python_django/#views_1","text":"","title":"Views"},{"location":"Languages/python/python_django/#function-based-generic-view","text":"with template (by rendering) 1 2 3 4 from django.shortcuts import render def home ( request ): return render ( request , 'home.html' ) without template : using HttpResponse 1 2 3 4 from django.http import HttpResponse def home ( request ): return HttpResponse ( \"Hello World\" ) Need to write conditional branch for different HTTP request type like POST, GET, PUT Need to provide view method name in URL Disadvantage: Cannot extend","title":"Function Based Generic View"},{"location":"Languages/python/python_django/#class-based-view","text":"Module: from django.views import View Inherit View class Need to define get(), post() like HTTP methods Need to provide ClassName.as_view() in URL Advantage: Can be extended by sub classes 1 2 3 4 5 6 7 from django.http import HttpResponse from django.views import View class MyView ( View ): def get ( self , request ): # <view logic> return HttpResponse ( 'result' )","title":"Class Based View"},{"location":"Languages/python/python_django/#class-based-generic-view","text":"Module: from django.views.generic import ListView Can inherit ListView, TemplateView... class No need to define request handler methods set model attribute to Model Class Need to provide ClassName.as_view() in URL 1 2 3 4 5 from django.views.generic import ListView from books.models import Publisher class PublisherList ( ListView ): model = Publisher","title":"Class Based Generic View"},{"location":"Languages/python/python_django/#urls","text":"","title":"URLs"},{"location":"Languages/python/python_django/#using-view-function","text":"1 2 urlpatterns = [ path ( '/' , views . home (), name = 'home' ),]","title":"using view function"},{"location":"Languages/python/python_django/#using-class-based-view","text":"1 2 urlpatterns = [ path ( '/' , views . IndexView . as_view (), name = 'index' ),]","title":"using class based view"},{"location":"Languages/python/python_django/#including-app-urls","text":"1 2 3 4 from django.urls import path , include urlpatterns = [ path ( '' , include ( 'home.urls' )),]","title":"including app urls"},{"location":"Languages/python/python_django/#django-20-using-path","text":"1 from django.urls import path , include","title":"django 2.0: using path"},{"location":"Languages/python/python_django/#django-19-using-url","text":"1 from django.conf.urls import url","title":"django &lt;=1.9: using url"},{"location":"Languages/python/python_django/#static-files","text":"","title":"Static files"},{"location":"Languages/python/python_django/#in-production","text":"set STATIC_ROOT in settings.py run manage.py collectstatic","title":"In Production"},{"location":"Languages/python/python_django/#in-developement","text":"1 2 3 4 5 6 7 8 9 10 STATICFILES_FINDERS = ( 'django.contrib.staticfiles.finders.FileSystemFinder' , 'django.contrib.staticfiles.finders.AppDirectoriesFinder' ) STATICFILES_DIRS = [ os . path . join ( BASE_DIR , 'static' ), '/var/www/static/' , ] STATIC_URL = '/static/'","title":"In Developement"},{"location":"Languages/python/python_django/#templates_1","text":"Contains Markups JS, CSS, HTML, XML django tags Variables/Logic blocks {% extends 'home/base.html' %} {% load static %} Comments {# CSS files#}","title":"Templates"},{"location":"Languages/python/python_django/#middlewares","text":"Middleware is framework of hooks to Django's request/response processing its light and low-level plugin system for making global changes in Django's input/output each middleware component in responsible for performing some specific task Some usage of middlewares in Django is: Session management User authentication Cross-site request forgery protection Content Gzipping, etc.","title":"Middlewares"},{"location":"Languages/python/python_django/#custom-middleware","text":"a middleware factory (i.e. outer function or a class) is a callable it takes an argument called get_response get_response might be an actual Django view if the middleware is last listed else, get_response might be a next middleware and it returns a middleware (or ultimately a response ) a middleware is also a callable which takes an arg called request and it returns a response","title":"Custom Middleware"},{"location":"Languages/python/python_django/#function-based-custom-middleware","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def simple_middleware ( get_response ): # One-time configuration and initialization. def middleware ( request ): # Code to be executed for each request before # the view (and later middleware) are called. response = get_response ( request ) # Code to be executed for each request/response after # the view is called. return response return middleware","title":"Function Based Custom Middleware"},{"location":"Languages/python/python_django/#class-based-custom-middleware","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 class SimpleMiddleware : def __init__ ( self , get_response ): self . get_response = get_response # One-time configuration and initialization. def __call__ ( self , request ): # Code to be executed for each request before # the view (and later middleware) are called. response = self . get_response ( request ) # Code to be executed for each request/response after # the view is called. return response","title":"Class Based Custom Middleware"},{"location":"Languages/python/python_django/#settings","text":"","title":"Settings"},{"location":"Languages/python/python_django/#place-in-settingspy","text":"","title":"Place in settings.py"},{"location":"Languages/python/python_django/#separated-into-production-development-environments-like","text":"1 2 3 4 - base_settings.py - dev_settings.py - prod_settings.py - settings.py","title":"Separated into production &amp; development environments like"},{"location":"Languages/python/python_django/#strict-seperation-from-code","text":"","title":"STRICT SEPERATION FROM CODE"},{"location":"Languages/python/python_django/#python-decouple","text":"Source: https://pypi.org/project/python-decouple/ Usage Where the settings data are stored? Ini file Env file How it works? Understanding the CAST argument","title":"Python Decouple"},{"location":"Languages/python/python_django/#signals","text":"Signals are a strategy to allow decoupled applications to get notified when some event occurs. Source: https://simpleisbetterthancomplex.com/tutorial/2016/07/28/how-to-create-django-signals.html where to create <signals_module>.py ? anywhere recommended: inside apps","title":"Signals"},{"location":"Languages/python/python_django/#built-in-signals","text":"django.db.models.signals.pre_init: receiver_function(sender, *args, **kwargs) django.db.models.signals.post_init: receiver_function(sender, instance) django.db.models.signals.pre_save: receiver_function(sender, instance, raw, using, update_fields) django.db.models.signals.post_save: receiver_function(sender, instance, created, raw, using, update_fields) django.db.models.signals.pre_delete: receiver_function(sender, instance, using) django.db.models.signals.post_delete: receiver_function(sender, instance, using) django.db.models.signals.m2m_changed: receiver_function(sender, instance, action, reverse, model, pk_set, using)","title":"Built-in Signals"},{"location":"Languages/python/python_django/#requestresponse-signals","text":"django.core.signals.request_started: receiver_function(sender, environ) django.core.signals.request_finished: receiver_function(sender, environ) django.core.signals.got_request_exception: receiver_function(sender, request)","title":"Request/Response Signals"},{"location":"Languages/python/python_django/#way-of-connectingregistering-signals","text":"","title":"Way of connecting/registering signals"},{"location":"Languages/python/python_django/#using-signalconnect","text":"Need to register signals inside ready() in AppConfig class in <app>.apps.py Need to define default_app_config = '<app>.apps.<App>Config' in <app>.__init__.py ignore if '<app>.apps.<App>Config' is inside INSTALLED_APPS insettings.py Issues: In django 2.0+, not working fine. Throwing error: AppRegistryNotReady: Apps aren't loaded yet. e.g.: https://gist.github.com/toransahu/c3870b4ad58bde5a9b9563f7e0883729","title":"using &lt;signal&gt;.connect"},{"location":"Languages/python/python_django/#using-receiver-decorator","text":"Only need to import signals inside ready() in AppConfig class in <app>.apps.py Need to define default_app_config = '<app>.apps.<App>Config' in <app>.__init__.py ignore if '<app>.apps.<App>Config' is inside INSTALLED_APPS insettings.py No issues till yet e.g.: https://gist.github.com/toransahu/c3870b4ad58bde5a9b9563f7e0883729","title":"using receiver() decorator"},{"location":"Languages/python/python_django/#asynchronous-signals-using-celery","text":"src https://simpleisbetterthancomplex.com/tutorial/2017/08/20/how-to-use-celery-with-django.html http://docs.celeryproject.org/en/latest/django/first-steps-with-django.html","title":"Asynchronous Signals Using Celery"},{"location":"Languages/python/python_django/#installation","text":"","title":"Installation"},{"location":"Languages/python/python_django/#task-queue-system-celery","text":"1 pip install celery","title":"Task Queue System - Celery"},{"location":"Languages/python/python_django/#message-broker-system-rabbitmq","text":"src: http://docs.celeryproject.org/en/latest/getting-started/brokers/rabbitmq.html#broker-rabbitmq 1 sudo apt install rabbitmq-server","title":"Message Broker System - RabbitMQ"},{"location":"Languages/python/python_django/#setup","text":"no need to define any setup, but can set credentials for rabbitmq-server","title":"Setup"},{"location":"Languages/python/python_django/#code","text":"","title":"Code"},{"location":"Languages/python/python_django/#enable-celery","text":"Define celery instance in project level inside celery.py Load celery when django starts import the celery instance app to init .py in project package","title":"Enable Celery"},{"location":"Languages/python/python_django/#write-task","text":"code: https://gist.github.com/toransahu/d01c7374c5317a908b99ac03cf24cc11 create tasks.py inside django app celery searches for tasks inside tasks.py modules either import celery app instance & use 1 2 3 4 from backend.celery import app @app . task def foo (): pass or use shared_task 1 2 3 4 from celery import shared_task @shared_task def foo (): pass set broker_url inside settings.py every celery config variables start with CELERY_ 1 CELERY_BROKER_URL = 'amqp://localhost' start rabbitmq starts automatically on boot 1 2 sudo systemctl enable rabbitmq-server sudo systemctl start rabbitmq-server start celery worker need to run inside src folder need to be in virtual env 1 celery -A project_name worker -l info or create script to start celery cd ethereal-machines-backend/src (where manage.py is) vim start_celery.sh make sure to write #! /bin/bash in first line chmod a+x start_celery.sh start_celery.sh should look like this: https://gist.github.com/toransahu/31874def1bd661db676d0dfe02e1c651","title":"Write Task"},{"location":"Languages/python/python_django/#celery-vs-rabiitmq","text":"Celery is a queue Wrapper/Framework which takes away the complexity of having to manage the underlying AMQP mechanisms/architecture that come with operating RabbitMQ directly Celery is just a very high level of abstraction to implement the producer / consumer of events. It takes out several painful things you need to do to work for example with rabbitmq. Celery itself is not the queue. The events queues are stored in the system of your choice, celery helps you to work with such events without having to write the producer / consumer from scratch.","title":"Celery vs RabiitMQ"},{"location":"Languages/python/python_django/#state-management","text":"Session: Anonymous Session Cookies","title":"State Management"},{"location":"Languages/python/python_django/#theory","text":"","title":"Theory"},{"location":"Languages/python/python_django/#stateless","text":"Meaning navigating from one web page to another will not retain infos of first one. There is no persistence between one request and the next, and there is no way the server can tell whether successive requests come from the same person e.g. HTTP, REST This lack of state is managed using sessions.","title":"Stateless"},{"location":"Languages/python/python_django/#session","text":"are a semi-permanent, two-way communication between your browser and the web server. The session framework lets you store and retrieve arbitrary data on a per-site-visitor basis It stores data on the server side and abstracts the sending and receiving of cookies can be implemented through middleware In client side cookies contain a session ID \u0093 not the data itself (unless youre using the cookie based backend) 1 2 3 4 5 6 7 INSTALLED_APPS = [ 'django.contrib.sessions' , ] MIDDLEWARE = [ 'django.contrib.sessions.middleware.SessionMiddleware' , ]","title":"Session"},{"location":"Languages/python/python_django/#anonymous-session","text":"to keep track of data relevant to your visit the web server can only record what you did, not who you are","title":"Anonymous Session"},{"location":"Languages/python/python_django/#enabling-the-session","text":"Using middleware MIDDLEWARE_CLASSES in setting.py should contain 'django.contrib.sessions.middleware.SessionMiddleware' by deafult enabled","title":"Enabling the session"},{"location":"Languages/python/python_django/#configuring-the-session-engine","text":"by default stored in database using the model django.contrib.sessions.models.Session can be configured to store session data on file system or in cache","title":"Configuring The Session Engine"},{"location":"Languages/python/python_django/#using-database-backed-sessions","text":"need to add 'django.contrib.sessions' to your INSTALLED_APPS setting","title":"Using Database-Backed Sessions"},{"location":"Languages/python/python_django/#using-cached-sessions","text":"for better performance, for making web pages more responsive local memory cache backend doesnt retain data long enough to be a good choice use third-party like Memcached cache backend, else use file system or DB based session two different implementation: Only cache Set SESSION_ENGINE to \"django.contrib.sessions.backends.cache\" Cache + DB (Persistent) set SESSION_ENGINE to \"django.contrib.sessions.backends.cached_db\" every write to the cache will also be written to the database use the database if the data is not already in the cache","title":"Using Cached Sessions"},{"location":"Languages/python/python_django/#using-file-based-sessions","text":"have to set the SESSION_ENGINE settings to \u009cdjango.contrib.sessions.backends.file\u009d","title":"Using File-Based Sessions"},{"location":"Languages/python/python_django/#using-cookie-based-sessions","text":"","title":"Using Cookie-Based Sessions"},{"location":"Languages/python/python_django/#cookies","text":"","title":"Cookies"},{"location":"Languages/python/python_django/#what","text":"An HTTP cookie (web cookie, browser cookie) is a small piece of data that a server sends to the user's web browser. The browser may store it and send it back with the next request to the same server. module: http.cookies","title":"What"},{"location":"Languages/python/python_django/#why","text":"Typically, it's used to tell if two requests came from the same browser \u2014 keeping a user logged-in, for example.","title":"Why"},{"location":"Languages/python/python_django/#how","text":"It remembers stateful information for the stateless HTTP protocol.","title":"How"},{"location":"Languages/python/python_django/#uses_1","text":"Session management Logins, shopping carts, game scores, or anything else the server should remember Personalization User preferences, themes, and other settings Tracking Recording and analyzing user behavior","title":"Uses:"},{"location":"Languages/python/python_django/#cache","text":"","title":"Cache"},{"location":"Languages/python/python_django/#why_1","text":"The performance of web sites and applications can be significantly improved by reusing previously fetched resources. Web caches reduce latency and network traffic and thus lessen the time needed to display a representation of a resource. By making use of HTTP caching, Web sites become more responsive.","title":"Why"},{"location":"Languages/python/python_django/#architecture-django-production-servers","text":"Source : https://www.sayonetech.com/blog/how-host-your-django-project-production-server/#.WgSKV3VL9Xo","title":"Architecture - Django Production Servers"},{"location":"Languages/python/python_django/#web-server","text":"the outermost tier of the Backend(3-tiers) Apache, nginx, lighttpd, cherokee used as proxy, reverse proxy, load balancer, static data (css, html, images) dispatcher and cache it can't talk directly to Django applications","title":"Web Server"},{"location":"Languages/python/python_django/#nginx-pronounced-as-engine-x","text":"","title":"nginx (Pronounced as: Engine X)"},{"location":"Languages/python/python_django/#application-server","text":"the middle tier of the Backend(3-tiers) Gunicorn, mod_python, mod_wsgi, mod_uwsgi, FastCGI is used to handle all dynamic requests, basically based on URL pattern (view call) the Interface between the web server and the python app so that the app(or any python framework) understands the incoming requests create a Unix socket, and serve responses to nginx via the wsgi protocol - the socket passes data in both directions","title":"Application Server"},{"location":"Languages/python/python_django/#gunicorn-pronounced-as-gee-unicorn","text":"inspired from Ruby's Unicorn","title":"gunicorn (Pronounced as: gee-unicorn)"},{"location":"Languages/python/python_django/#db","text":"the third tier of the Backend(3-tiers) MySQL/ Postgres/ Other databases","title":"DB"},{"location":"Languages/python/python_django/#asynchronous-task-queue","text":"","title":"Asynchronous Task Queue"},{"location":"Languages/python/python_django/#celery","text":"Celery is an asynchronous task/job queue based on distributed message passing requires an external solution to send and receive messages i.e. Message Brokers like RabbitMQ, Redis focused on real-time operation, but supports scheduling as well","title":"Celery"},{"location":"Languages/python/python_django/#cron-jobs","text":"","title":"Cron jobs"},{"location":"Languages/python/python_django/#message-broker-solutions","text":"","title":"Message Broker Solutions"},{"location":"Languages/python/python_django/#rabbitmq","text":"","title":"RabbitMQ"},{"location":"Languages/python/python_django/#redis","text":"","title":"Redis"},{"location":"Languages/python/python_django/#amazon-sqs","text":"","title":"Amazon SQS"},{"location":"Languages/python/python_django/#caching-solution","text":"","title":"Caching Solution"},{"location":"Languages/python/python_django/#memcached","text":"","title":"Memcached"},{"location":"Languages/python/python_django/#monitoring","text":"When our project is hosted, we need to monitor it using some tools to check its performance, its error logs and user interactions.We have some tools available for this.","title":"Monitoring"},{"location":"Languages/python/python_django/#graphite","text":"Graphite provides real-time visualization and storage of numeric time-series data on an enterprise level.","title":"Graphite"},{"location":"Languages/python/python_django/#statsd","text":"A network daemon that runs on the Node.js platform and listens for statistics, like counters and timers, sent over UDP or TCP and sends aggregates to one or more pluggable backend services (e.g.,Graphite).","title":"Statsd"},{"location":"Languages/python/python_django/#sentry-logging","text":"Sentry is a modern error logging and aggregation platform.","title":"Sentry - logging"},{"location":"Languages/python/python_django/#new-relic","text":"A software analytics tool suite used by developers, ops, and software companies to understand how your applications are performing in development and production.","title":"New Relic"},{"location":"Languages/python/python_django/#supervisor","text":"a process control system a client/server system which allows its users to monitor and control a number of process in UNIX-like OS similar to launchd , daemontools , and runit monitors projects starts on boot Supervisord starts processes as its subprocesses, and can be configured to automatically restart them on a crash accurately shows the up/down times of the processes can asign priorities to the processes can group the processes","title":"Supervisor"},{"location":"Languages/python/python_django/#stack-flow","text":"User requests from browser. Request reaches Nginx. If (request is staic) Nginx serves the request. Else if (request is dynamic) Nginx forwards the request to Application server (Gunicorn). Gunicorn receives the request, executes corresponding python (Flask) code. Gunicorn returns the response to Nginx. Nginx serves the response to the user.","title":"Stack Flow"},{"location":"Languages/python/python_django/#working","text":"You need both Nginx and Gunicorn (or something similar) for a proper Django deployment The complete answer is both Nginx and Gunicorn handle the request. Basically, Nginx will receive the request and if it's a dynamic request (generally based on URL patterns) then it will give that request to Gunicorn, which will process it, and then return a response to Nginx which then forwards the response back to the original client.","title":"Working"},{"location":"Languages/python/python_django/#deployment-production-environment","text":"","title":"Deployment - Production Environment"},{"location":"Languages/python/python_django/#how-to-deploy-django-application-in-production","text":"https://devcenter.heroku.com/articles/getting-started-with-python#introduction https://developer.mozilla.org/en-US/docs/Learn/Server-side/Django/Deployment DEBUG = False change default SECRET_KEY (used for CRSF protection) and hide it somewhere else Run manage.py check --deploy (to check the default list of changes mentioned by django) checklist https://docs.djangoproject.com/en/1.10/howto/deployment/checklist/","title":"How to deploy django application in Production"},{"location":"Languages/python/python_django/#checklist","text":"in settings must be set properly for Django to provide the expected level of security; are expected to be different in each environment; enable optional security features; enable performance optimizations; provide error reporting. or Run manage.py check --deploy to list all the factors listed below take care of these things if releasing source code","title":"checklist"},{"location":"Languages/python/python_django/#critical-settings","text":"SECRET_KEY Instead of hardcoding the secret key in your settings module, consider loading it from an environment variable or file 1 2 3 4 5 6 import os SECRET_KEY = os . environ [ 'SECRET_KEY' ] with open ( '/etc/secret_key.txt' ) as f : SECRET_KEY = f . read () . strip () avoid committing it to source control DEBUG : set to False","title":"critical settings"},{"location":"Languages/python/python_django/#environment-related-settings","text":"ALLOWED_HOSTS When DEBUG = False, Django doesn\u2019t work at all without a suitable value for ALLOWED_HOSTS. This setting is required to protect your site against some CSRF attacks CACHE change it for production performance otimization default for developement is 'local-memory caching' instead use cache servers like Memcached using 'cached sessions' Cache servers often have weak authentication. Make sure they only accept connections from your application servers. DATABASE Database passwords are very sensitive. Keep them in environment variable or in file same as SECRET_KEY For maximum security, make sure database servers only accept connections from your application servers. If you haven\u2019t set up backups for your database, do it right now! EMAIL_BACKEND and related settings If your site sends emails, these values need to be set correctly. modify the DEFAULT_FROM_EMAIL and SERVER_EMAIL settings By default, Django sends email from webmaster@localhost and root@localhost. STATIC_ROOT and STATIC_URL Static files are automatically served by the development server. In production, you must define a STATIC_ROOT directory where collectstatic will copy them. MEDIA_ROOT and MEDIA_URL Media files are uploaded by your users. They\u2019re untrusted! Make sure your web server never attempts to interpret them. For instance, if a user uploads a .php file, the web server shouldn\u2019t execute it.","title":"Environment Related Settings"},{"location":"Languages/python/python_django/#https","text":"Any website which allows users to log in should enforce site-wide HTTPS to avoid transmitting access tokens in clear. In Django, access tokens include the login/password, the session cookie, and password reset tokens. Note: You can\u2019t do much to protect password reset tokens if you\u2019re sending them by email web server must redirect all HTTP traffic to HTTPS, and only transmit HTTPS requests to Django. because: the same session cookie is used for HTTP and HTTPS. Once you\u2019ve set up HTTPS, enable the following settings. CSRF_COOKIE_SECURE Set this to True to avoid transmitting the CSRF cookie over HTTP accidentally. SESSION_COOKIE_SECURE Set this to True to avoid transmitting the session cookie over HTTP accidentally.","title":"HTTPS"},{"location":"Languages/python/python_django/#performance-optimizations","text":"DEBUG = False CONN_MAX_AGE TEMPLATES Enabling the cached template loader often improves performance drastically, as it avoids compiling each template every time it needs to be rendered.","title":"Performance Optimizations"},{"location":"Languages/python/python_django/#error-reporting","text":"LOGGING Review your logging configuration before putting your website in production, and check that it works as expected as soon as you have received some traffic Customize the default error views Django includes default views and templates for several HTTP error codes. You may want to override the default templates by creating the following templates in your root template directory: 404.html, 500.html, 403.html, and 400.html.","title":"Error Reporting"},{"location":"Languages/python/python_django/#testing","text":"Class level testing for each app 1 2 3 4 5 6 from django.test import TestCase class QuestionModelTests ( TestCase ): def test_was_published_recently_with_future_question ( self ): #do something self . assertIs ( future_question . was_published_recently (), False )","title":"Testing"},{"location":"Languages/python/python_django/#run-test-cases","text":"1 python manage.py test appname","title":"Run Test Cases"},{"location":"Languages/python/python_django/#security","text":"","title":"Security"},{"location":"Languages/python/python_django/#csrf-cross-site-request-forgery","text":"","title":"CSRF - Cross Site Request Forgery"},{"location":"Languages/python/python_django/#why-csrf","text":"CSRF attack happens in presence of state It really boils down to the browsers ability to automatically present login credentials for any request by sending along cookies. If a session id is stored in a cookie the browser will automatically send it along with all requests that go back to the original website. This means that an attacker doesn't actually have to know authentication details to take an action as the victim user. Rather, the attacker just has to trick the victims browser into making a request, and the credentials to authenticate the request will ride along for free.","title":"Why CSRF?"},{"location":"Languages/python/python_django/#is-it-required-in-rest","text":"No, it will be useless piece of code because REST is stateless at client-side a cookie-less REST endpoint is completely immune from CSRF attacks if there is cookie used for authentication, then we need CSRF protection HTTP/BasicAuthentication will also need CSRF protection also, if any app uses any tech to store state of app at clientside, then its not a RESTful app src: https://security.stackexchange.com/questions/166724/should-i-use-csrf-protection-on-rest-api-endpoints","title":"Is it required in REST"},{"location":"Languages/python/python_django/#django-rest-framewok","text":"Django REST framework \"djangorestframework\" is powerful & flexibal toolkit for creating web APIs. - https://stackoverflow.com/questions/671118/what-exactly-is-restful-programming","title":"Django REST Framewok"},{"location":"Languages/python/python_django/#some-reasons-you-might-want-to-use-django-rest-framework","text":"The Web browsable API is a huge usability win for your developers. Authentication policies including packages for OAuth1a and OAuth2. Serialization that supports both ORM and non-ORM data sources. Customizable all the way down - just use regular function-based views if you don't need the more powerful features. Extensive documentation, and great community support.","title":"Some reasons you might want to use Django REST framework:"},{"location":"Languages/python/python_django/#rest-api","text":"Please refer to this .","title":"REST API"},{"location":"Languages/python/python_django/#serializers","text":"Module: from rest_framework import serializers Provides a way to serialize & deserialize Model instances into representations like JSON. Serialization is mechanism of converting the state of an object into byte-stream, which can be displayed, stored. Deserialization is reverse mechanism of serialization. Note: In django its very similar to Django Form class and includes similar validation flags on the various fields, such as required, max_length and default.","title":"Serializers"},{"location":"Languages/python/python_django/#implementations","text":"Inherit classes serializers.Serializer need to write all the fields mentioned in Model (only those, which we want to use here) need to define create() & update() method to create/update new Model instance from validated data (representaion/JSON) serializers.ModelSerializer Inside Meta class mention model inside Meta class; model = Snippet i.e. Class fields = ('id', ....,'title') i.e. tuple Automatically implements default create() and update() methods Note : If we want to include url in fields, then the base_name (in case of routers) in urls.py should same as the name of Model in lower case, alternatively don't mention base_name serializers.HyperlinkedModelSerializer The only difference is, as in citation you included, that primary and foreign keys are represented by URLs that point to those resources, instead of just actual key values. The benefit is that you will not have to construct resource URLs in your frontend when you want to retrieve related objects.","title":"Implementations"},{"location":"Languages/python/python_django/#relations","text":"src: http://www.django-rest-framework.org/api-guide/relations/#serializer-relations","title":"Relations"},{"location":"Languages/python/python_django/#writable-nested-serializers","text":"scr: http://www.django-rest-framework.org/api-guide/relations/#writable-nested-serializers code: https://gist.github.com/toransahu/221371c981c20f0b9c645019a53b90c7 by default django does not provides write access in case of nested model objects & their model serializers strategy create 2 models assign M2M/Foreign key field in one model write serializers for both the models keep foreign model serializers normal/default for other model serializer write custom code override create() method handle foreign field data explicitly create post instance from data iterate over list value of foreign field create instance of image append image instance to post instance's foreign field override update() method write normal viewset for only one : i.e. posts (both is optional) Issues faced: if nested fields are char if posting as application/json from django form : works fine if posting as multipart/form-data from django form: validation fails for required fields means data received is empty works fine with python code only iff data & files both are provided if nested fields are image cannot post using django form able to post using custom form: https://github.com/toransahu/multiple-file-upload able to post using python code","title":"Writable Nested Serializers"},{"location":"Languages/python/python_django/#class-meta-options_1","text":"TODO:","title":"class Meta Options"},{"location":"Languages/python/python_django/#misc","text":"Note: after serializer.is_valid() we can't save serializer if we have already accessed serializer.data; to avoid this, access serializer.validated_data custom create & update in serializer - writable nested serializers datefield attribute: auto_now vs auto_now_add auto_now: Automatically set the field to now every time the object is saved Useful for \"last-modified\" timestamps cannot be overriden auto_now_add Automatically set the field to now when the object is first created Useful for creation of timestamps cannot be overriden Postman: nested: https://medium.com/@darilldrems/how-to-send-arrays-with-get-or-post-request-in-postman-f87ca70b154e Try: https://github.com/beda-software/drf-writable-nested https://github.com/alanjds/drf-nested-routers Base64 ImageField","title":"Misc"},{"location":"Languages/python/python_django/#views_2","text":"Function Based View Using normal functions like in Django (without using any rest_framework feature) Modules: 1 2 3 4 from django.views.decorators.csrf import csrf_exempt from django.http import HttpResponse , JsonResponse from rest_framework.renderers import JSONRenderer from rest_framework.parsers import JSONParser Approach: write function with conditional branching for different http request methods write a _list function for listing all records (GET) & submiting a record (POST) write a _detail function for fetching, modifying or destroying a specific record by pk, ID or Name use JSON response & parser use @csrf_exempt decorator for safety code: https://gist.github.com/toransahu/1bad12b87dcd160c0de0d29d218d9bf6 Using @api_view() decorator Modules: 1 2 3 from rest_framework import status from rest_framework.decorators import api_view from rest_framework.response import Response Approach: write function with conditional branching for different http request methods write a _list function for listing all records (GET) & submiting a record (POST) write a _detail function for fetching, modifying or destroying a specific record by pk, ID or Name decorate with @api_view() with parameters like ['GET'], ['GET', 'POST'], ['GET', 'PUT', 'DELETE'] etc By default @api_view() takes GET method if nothing is mentioned code: https://gist.github.com/toransahu/5c99704ec721e461b5f8fa67776a6d74 Class Based View By Inheriting APIView class Modules: 1 2 3 4 from rest_framework.views import APIView from rest_framework.response import Response from rest_framework import status from django.http import Http404 Approach: write classes and define request handler methods in name of http request method for each http request method write a List class for listing all records (GET) & submiting a record (POST) write a Detail class for fetching, modifying or destroying a specific record by pk, ID or Name similar to Django's \"View\" class Note request handler methods receives REST's Request instance instead django's HttpRequest instance request handler methods may return REST's Response instance instead django's HttpResponse instance set few attributes like authentication_classes = (authentication.TokenAuthentication,) permission_classes = (permissions.IsAdminUser,) code: https://gist.github.com/toransahu/bcdb1a6beb5da0475c8c056837da940f Using mixins Modules: 1 2 from rest_framework import mixins from rest_framework import generics Approach: write classes and inherit generics.GenericAPIView & mixins as per use write a List class and inherit mixins.ListModelMixin, mixins.CreateModelMixin, generics.GenericAPIView write a Detail class and inherit mixins.RetrieveModelMixin, mixins.UpdateModelMixin, mixins.DestroyModelMixin, generics.GenericAPIView base class providescore functionality & mixin classes provides actions like .list(), .create(), .retrieve(), .update() and .destroy() Code: https://gist.github.com/toransahu/dfc2666307c1628042b99edaba630c07 Using generic class-based views Modules: 1 from rest_framework import generics Approach: write List class and inherit generics.ListCreateAPIView write Detail class and generics.RetrieveUpdateDestroyAPIView Code: https://gist.github.com/toransahu/ab21d8bf46f45616ccb9285dbd0b201a Using ViewSet Modules: from rest_framework import viewsets Approach: Only need to write a single class, named ModelNameViewSet and inhert viewsets.ModelViewSet facilitates router for URL writing Advantage: provides create, retrieve, update, and destroy in a single Class def DRY View code DRY URL code can also add custom endpoints as per our need, apart from regular create, retrieve, update, and destroy endpoint provided by ViewSet Code: https://gist.github.com/toransahu/c7d43776065aa6fda05d7389133c68f9","title":"Views"},{"location":"Languages/python/python_django/#customdisable-request-methods-in-viewset","text":"By default ViewSets provides All the requests methods this technique will give power to override,disable those methods to do this use mixins with viewset.GenericViewSet , or override methods of ViewSet class url: https://gist.github.com/toransahu/e02fc30cd0e55e968971c46e59862acb","title":"Custom/Disable Request Methods in ViewSet"},{"location":"Languages/python/python_django/#mapping-for-views-from-viewset-using-as_view","text":"syntax {<method>:<action>} patterns {'get': 'list'} {'get': 'retrieve'} {'post': 'create'} {'put': 'update'} {'patch': 'partial_update'} {'delete': 'destroy'}","title":"mapping for views from ViewSet using as_view()"},{"location":"Languages/python/python_django/#adding-custom-routesaction-to-the-existing-viewset","text":"https://gist.github.com/toransahu/95781bc23f39192276d011d0aa990470 http://www.django-rest-framework.org/api-guide/routers/#customizing-dynamic-routes","title":"adding custom routes/action to the existing ViewSet"},{"location":"Languages/python/python_django/#urls_1","text":"if using include( ) dont provide namespace if providing namespace inside include define app_name = in app/urls.py","title":"URLs"},{"location":"Languages/python/python_django/#routers","text":"REST framework adds support for automatic URL routing to Django, and provides you with a simple, quick and consistent way of wiring your view logic to a set of URLs. 1 2 3 4 5 6 7 8 9 from django.urls import path from .views import BlogViewSet from rest_framework.routers import DefaultRouter router = DefaultRouter () router . register ( '' , BlogViewSet , base_name = 'blogs' ) urlpatterns = router . urls Note: Try to keep base_name same as Model name, because view_name like blog-detail, blog-list comes from Model & not from app's name when we create custom actions (similar to -list, -detail) in any View using @detail_route(), we access that action using base_name in serializers. alternatively remove base_name","title":"Routers"},{"location":"Languages/python/python_django/#actions-using-base_name","text":"<base_name>-list <base_name>-detail <base_name>-<any_custom>","title":"actions using base_name:"},{"location":"Languages/python/python_django/#how-to-use-namespace-provided-in-url-patterns","text":"using from django.urls import reverse reverse(<namespace>:<base_name>-<action> or reverse(<namespace>:<model_name>-<action>","title":"how to use namespace (provided in url patterns)"},{"location":"Languages/python/python_django/#adding-custom-routesaction-urls-to-the-existing-router-or-urlpatterns","text":"https://gist.github.com/toransahu/95781bc23f39192276d011d0aa990470","title":"adding custom routes/action urls to the existing Router or urlpatterns"},{"location":"Languages/python/python_django/#api-versioning","text":"Source e.g. Its always a good idea to version your API so that you can make changes in your API without disturbing your current clients. e.g. http://127.0.0.1:8000/api/v1/blogs/","title":"API Versioning"},{"location":"Languages/python/python_django/#configurations","text":"settings.py 1 2 3 REST_FRAMEWORK = { 'DEFAULT_VERSIONING_CLASS' : 'rest_framework.versioning.NamespaceVersioning' } urls.py 1 2 3 4 urlpatterns = [ path ( 'admin/' , admin . site . urls ), path ( 'api/v1/blogs/' , include ( 'blogs.urls' , namespace = 'v1' )), ] blogs/urls.py 1 2 3 4 5 6 app_name = 'blogs' router = DefaultRouter () router . register ( '' , BlogViewSet , base_name = 'blogs' ) urlpatterns = router . urls","title":"Configurations"},{"location":"Languages/python/python_django/#uses_2","text":"Once you have set versioning, django will be able to provide value of request.version else it will be None so, apply conditions ( if/else ) in View or serializers if there are changes in fields, then use versioning in serializers else, if there are only changes in some functionalities, (which current client cannot consume), then use versioning in veiws only, like: 1 2 3 4 def get_serializer_class ( self ): if self . request . version == 'v1' : return AccountSerializerVersion1 return AccountSerializer","title":"Uses"},{"location":"Languages/python/python_django/#allow-cors-cross-origin-resource-sharing-in-drf","text":"Source1 Source2","title":"Allow CORS (Cross Origin Resource Sharing) in DRF"},{"location":"Languages/python/python_django/#using-custom-middleware-class","text":"need to define for each app python manage.py startapp app create app/cors.py and write 1 2 3 4 class CorsMiddleware ( object ): def process_response ( self , req , resp ): response [ \"Access-Control-Allow-Origin\" ] = \"*\" return response - MIDDLEWARE_CLASSES = ['app.CorsMiddleware']","title":"Using Custom Middleware class"},{"location":"Languages/python/python_django/#using-package-django-cors-headers","text":"works site-wide pipenv install django-cors-headers in settings.py INSTALLED_APPS = ['corsheaders'] MIDDLEWARE_CLASSES = ['corsheaders.middleware.CorsMiddleware',] , keep in top as possible as Allow using any one CORS_ORIGIN_ALLOW_ALL = True OR CORS_ORIGIN_ALLOW_ALL = False CORS_ORIGIN_WHITELIST = ['http//:localhost:8000',] also set ALLOWED_HOSTS = ['192.168.1.121'] and run server like python manage.py runserver 192.168.1.121:8000","title":"Using package django-cors-headers"},{"location":"Languages/python/python_django/#creating-schema-from-api","text":"provides all the details about api endpoint present in site-wide urls will show all the endpoints actions fields if any authentication is needed it will show schema accordingly; if not authenticated, it will show that much api endpoints only need to configure in site-wide urls.py source: http://www.django-rest-framework.org/api-guide/schemas/ pipenv install coreapi 1 2 3 4 5 6 7 8 from rest_framework.schemas import get_schema_view schema_view = get_schema_view ( title = \"Server Monitoring API\" ) urlpatterns = [ url ( '^<dollor_sign>' , schema_view ), ... ]","title":"Creating Schema from API"},{"location":"Languages/python/python_django/#setting-media-url-root","text":"","title":"Setting Media URL &amp; ROOT"},{"location":"Languages/python/python_django/#site-wide","text":"","title":"Site Wide"},{"location":"Languages/python/python_django/#app-based","text":"define MEDIA_URL & MEDIA_ROOT in settings.py 1 2 3 4 5 6 7 8 9 10 # Media files # https://timmyomahony.com/blog/static-vs-media-and-root-vs-path-in-django/ # the relative browser URL to be used when accessing our media files in the browser MEDIA_URL = 'media/' # the absolute path to the folder that will hold our user uploads # to get absolute path without hardcoding ENV_PATH = os . path . abspath ( os . path . dirname ( __file__ )) + os . sep + os . pardir MEDIA_ROOT = os . path . join ( ENV_PATH , 'media/' ) - add following code in app/urls.py 1 2 3 4 5 6 7 # adding media url from django.conf import settings # from backend.settings import DEBUG from django.conf.urls.static import static # If developement env if settings . DEBUG is True : urlpatterns += static ( settings . MEDIA_URL , document_root = settings . MEDIA_ROOT ) want to set upload_to directory dynamically: code: https://gist.github.com/toransahu/8f9407250cee7a729473335bdd7a3f3b here comes, signal things, pre_save, post_save","title":"App Based"},{"location":"Languages/python/python_django/#permission","text":"","title":"Permission"},{"location":"Languages/python/python_django/#request-methods-level-permissions-in-viewset","text":"this needs when \"a user must post only or admin must post only\" type of requirements comes url-view: https://gist.github.com/toransahu/e02fc30cd0e55e968971c46e59862acb url-permission: https://gist.github.com/toransahu/c40b625165a395fabf772700f3ab2e04","title":"request methods level permissions (in ViewSet)"},{"location":"Languages/python/python_django/#objects-level-permission","text":"TODO","title":"objects level permission"},{"location":"Languages/python/python_django/#exceptionhttp-response","text":"When the permissions checks fail either a \"403 Forbidden\" or a \"401 Unauthorized\" response will be returned, according to the following rules: The request was successfully authenticated, but permission was denied. \u2014 An HTTP 403 Forbidden response will be returned. The request was not successfully authenticated, and the highest priority authentication class does not use WWW-Authenticate headers. \u2014 An HTTP 403 Forbidden response will be returned. The request was not successfully authenticated, and the highest priority authentication class does use WWW-Authenticate headers. \u2014 An HTTP 401 Unauthorized response, with an appropriate WWW-Authenticate header will be returned.","title":"Exception/Http response"},{"location":"Languages/python/python_django/#authentication","text":"Source: http://www.django-rest-framework.org/api-guide/authentication/","title":"Authentication"},{"location":"Languages/python/python_django/#basic-auth","text":"module: from rest_framework.authentication import BasicAuthentication By default used in DRF, whether you mention it in settings.py or views.py or not is only suitable for testing purpose, don't use in production if using in production you need to stick to Django Admin Login form page, can't use JSON need to be HTTPS","title":"Basic Auth"},{"location":"Languages/python/python_django/#session-auth","text":"module from rest_framework.authentication import SessionAuthentication TODO","title":"Session Auth"},{"location":"Languages/python/python_django/#token-auth","text":"Source: https://stackoverflow.com/questions/14838128/django-rest-framework-token-authentication module `from rest_framework.authentication import TokenAuthentication Prerequisites add 'rest_framework.authtoken' to INSTALLED_APPS in settings.py mention TokenAuthentication class in views or function based 1 2 3 @authentication_classes ([ TokenAuthentication , ]) def abcd - detail (): pass class based 1 2 class Abcd (): authentication_classes = [ TokenAuthentication , ] settings.py 1 2 3 4 5 6 7 8 9 REST_FRAMEWORK = { 'DEFAULT_VERSIONING_CLASS' : 'rest_framework.versioning.NamespaceVersioning' , 'DEFAULT_AUTHENTICATION_CLASSES' : ( # 'rest_framework.authentication.BasicAuthentication', # 'rest_framework.authentication.SessionAuthentication', 'rest_framework.authentication.TokenAuthentication' , ), } Obtain Token DRF provides a view which returns a token on correct username & password. include following in urls.py 1 2 3 from rest_framework.authtoken.views import obtain_auth_token urlpatterns = [ path ( 'api-auth-token/' , obtain_auth_token ),] - call the view as: http POST 127.0.0.1:8000/api-token-auth/ username='admin' password='whatever' will get token like: 1 2 3 { \"token\" : \"blah_blah_blah\" } Use the Token in API call put the following in Header 1 key: Authorization value: Token token_value Please mind the space between Token & token_value http GET 127.0.0.1:8000/whatever 'Authorization: Token your_token_value'","title":"Token Auth"},{"location":"Languages/python/python_django/#jwt-json-web-token","text":"src: http://getblimp.github.io/django-rest-framework-jwt/ quick replacement of Default Token Auth Basic Changes Needed: pip install djangorestframework-jwt add in REST_FRAMEWORK setting 'DEFAULT_AUTHENTICATION_CLASSES': ( 'rest_framework_jwt.authentication.JSONWebTokenAuthentication', ) add view in url 1 2 3 4 5 6 7 8 9 from rest_framework_jwt.views import obtain_jwt_token #... urlpatterns = [ '' , # ... url ( r '^api-token-auth/' , obtain_jwt_token ), ] jwt setting variables 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 JWT_AUTH = { 'JWT_ENCODE_HANDLER' : 'rest_framework_jwt.utils.jwt_encode_handler' , 'JWT_DECODE_HANDLER' : 'rest_framework_jwt.utils.jwt_decode_handler' , 'JWT_PAYLOAD_HANDLER' : 'rest_framework_jwt.utils.jwt_payload_handler' , 'JWT_PAYLOAD_GET_USER_ID_HANDLER' : 'rest_framework_jwt.utils.jwt_get_user_id_from_payload_handler' , 'JWT_RESPONSE_PAYLOAD_HANDLER' : 'rest_framework_jwt.utils.jwt_response_payload_handler' , #'JWT_SECRET_KEY': settings.SECRET_KEY, #will take from settings.py by default 'JWT_GET_USER_SECRET_KEY' : None , 'JWT_PUBLIC_KEY' : None , 'JWT_PRIVATE_KEY' : None , 'JWT_ALGORITHM' : 'HS256' , 'JWT_VERIFY' : True , 'JWT_VERIFY_EXPIRATION' : True , 'JWT_LEEWAY' : 0 , 'JWT_EXPIRATION_DELTA' : datetime . timedelta ( seconds = 300 ), 'JWT_AUDIENCE' : None , 'JWT_ISSUER' : None , 'JWT_ALLOW_REFRESH' : True , #allowing it to refresh 'JWT_REFRESH_EXPIRATION_DELTA' : datetime . timedelta ( days = 7 ), 'JWT_AUTH_HEADER_PREFIX' : 'Token' , 'JWT_AUTH_COOKIE' : None , }","title":"JWT (JSON Web Token)"},{"location":"Languages/python/python_django/#django-rest-auth-register-login-logout-reset-change","text":"src http://django-rest-auth.readthedocs.io/en/latest/api_endpoints.html https://michaelwashburnjr.com/django-user-authentication/ not recommanded, is not completely RESTful have issues with password/reset & password/reset/confirm/ depends on django-allauth for email things","title":"django-rest-auth (Register, Login, Logout, Reset, Change..)"},{"location":"Languages/python/python_django/#djoser","text":"src: http://djoser.readthedocs.io/en/stable/sample_usage.html djoser uses following settings for email configuration (which is also used by django's default mail module from django.core.mail import send_mail 1 2 3 4 5 6 7 8 9 10 11 12 13 CONFIG_PATH = os . path . join ( ENV_PATH , '../configs/' ) EMAIL_USE_TLS = True #EMAIL_USE_SSL = True #Use any one from TLS, SSL EMAIL_BACKEND = 'django.core.mail.backends.smtp.EmailBackend' #default one, production #EMAIL_BACKEND = 'django.core.mail.backends.console.EmailBackend' #for development EMAIL_HOST = 'smtp.gmail.com' with open ( os . path . join ( CONFIG_PATH , 'email_pwd' )) as f : EMAIL_HOST_PASSWORD = f . readline () EMAIL_HOST_USER = 'noreply.etherealmachines@gmail.com' EMAIL_PORT = 587 DEFAULT_FROM_EMAIL = EMAIL_HOST_USER EMAIL_ADMIN = ( 'toran.ethereal@gmail.com' , ) - djoser setting variables 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 DJOSER = { 'PASSWORD_RESET_CONFIRM_URL' : 'auth/password/reset/confirm/ {uid} / {token} ' , 'ACTIVATION_URL' : 'auth/password/reset/confirm/ {uid} / {token} ' , 'EMAIL' : { 'activation' : 'djoser.email.ActivationEmail' , 'confirmation' : 'djoser.email.ConfirmationEmail' , 'password_reset' : 'djoser.email.PasswordResetEmail' , }, # DEFAULT; no need to define here 'SERIALIZERS' : { 'activation' : 'djoser.serializers.ActivationSerializer' , 'password_reset' : 'djoser.serializers.PasswordResetSerializer' , 'password_reset_confirm' : 'djoser.serializers.PasswordResetConfirmSerializer' , 'password_reset_confirm_retype' : 'djoser.serializers.PasswordResetConfirmRetypeSerializer' , 'set_password' : 'djoser.serializers.SetPasswordSerializer' , 'set_password_retype' : 'djoser.serializers.SetPasswordRetypeSerializer' , 'set_username' : 'djoser.serializers.SetUsernameSerializer' , 'set_username_retype' : 'djoser.serializers.SetUsernameRetypeSerializer' , 'user_create' : 'djoser.serializers.UserCreateSerializer' , 'user_delete' : 'djoser.serializers.UserDeleteSerializer' , 'user' : 'djoser.serializers.UserSerializer' , 'token' : 'djoser.serializers.TokenSerializer' , 'token_create' : 'djoser.serializers.TokenCreateSerializer' , }, }","title":"djoser"},{"location":"Languages/python/python_django/#django-templated-mail","text":"djoser uses it to create mail template & send mails few setting variables: DOMAIN SITE_NAME","title":"django-templated-mail"},{"location":"Languages/python/python_django/#testing_1","text":"","title":"Testing"},{"location":"Languages/python/python_django/#testing-viewset-using","text":"APITestCase Token Authentication APIRequestFactory Source: https://gist.github.com/toransahu/706bd1de705e21f3be000e1517f7cae8","title":"Testing ViewSet using:"},{"location":"Languages/python/python_django/#security_1","text":"https://stormpath.com/blog/where-to-store-your-jwts-cookies-vs-html5-web-storage http://kylebebak.github.io/post/django-rest-framework-auth-csrf django-jwt (JWT_AUTH_COOKIE ) is not safe https://github.com/GetBlimp/django-rest-framework-jwt/issues/338 if SessionAuthentication is enabled, then CSRF will be used (if ON) https://stackoverflow.com/questions/30871033/django-rest-framework-remove-csrf if JWTAuthentications is enabled, then CSRF is automatically disabled","title":"Security"},{"location":"Languages/python/python_django/#vulnerabilities","text":"","title":"Vulnerabilities"},{"location":"Languages/python/python_django/#csrf-cross-site-request-forgery_1","text":"","title":"CSRF (Cross Site Request Forgery)"},{"location":"Languages/python/python_django/#xss-cross-site-script","text":"","title":"XSS (Cross Site Script)"},{"location":"Languages/python/python_django/#vulnerables","text":"","title":"Vulnerables"},{"location":"Languages/python/python_django/#web-client-local-session-storage","text":"XSS","title":"Web Client Local/ Session Storage"},{"location":"Languages/python/python_django/#cookies-jwtauth","text":"CSRF ajax call : https://gist.github.com/bengolder/aa9033efc8959dc38e5d","title":"Cookies (JWT/Auth)"},{"location":"Languages/python/python_django/#misc_1","text":"","title":"Misc"},{"location":"Languages/python/python_django/#running-multiple-host-website-from-single-django-project","text":"source: http://effbot.org/zone/django-multihost.htm","title":"Running Multiple Host (website) from Single Django Project"},{"location":"Languages/python/python_django/#get-request-url-string","text":"1 2 3 4 5 6 from django.contrib.sites.shortcuts import get_current_site print ( 'query_params are:' , request . build_absolute_uri ()) #returns url with domain print ( request . get_full_path ()) #returns url without domain print ( get_current_site ( request ) . domain , request . get_host ()) #both returns host or domain","title":"Get request URL string"},{"location":"Languages/python/python_django/#get-slugs-from-url","text":"if you have any - url like: http://127.0.0.1:8000/auth/password/reset/confirm/Mw/4vw-d9cdc8954482ecf8e253/ - url pattern like: path('password/reset/confirm/ / /', password_reset_confirm, name='password_reset_confirm_custom_get') then code to fetch uid & token will be: https://gist.github.com/toransahu/68663e302f87a9c32a1dcd0654408577","title":"Get slugs from url"},{"location":"Languages/python/python_django/#microservice-based-on-rest","text":"https://www.fullstackpython.com/microservices.html https://martinfowler.com/articles/microservices.html https://dev.otto.de/2016/03/20/why-microservices/ with flask: https://medium.com/@ssola/building-microservices-with-python-part-i-5240a8dcc2fb django to flask based microservice - https://medium.com/greedygame-media/how-we-broke-up-our-monolithic-django-service-into-microservices-8ad6ff4db9d4 https://blog.rapid7.com/2016/09/15/microservices-please-dont/","title":"Microservice Based on REST"},{"location":"Languages/python/python_oops/","text":"Python OOPs # Python OOPs OOPs Basics UML 2.x (Unified Modeling Language) ERD (Entity Relation Diagram) OOPs Features Abstraction Ex Encapsulation Ex. Difference between Abstraction and Encapsulation: Implementation in Class using Access Modifiers Polymorphism Static Polymorphism (Method-Overloading) Runtime Polymorphism (Method-Overriding) Inheritance Types Diamond Problem Need of super() Other relationships Association Weak Association (Aggregation) Strong Association (Composition) Aggregation Association vs Aggregation Composition (Not-Shared Association) Realization Dependency Implementations of OOPs class type metaclass __new__() __init__() __call__() instance Old vs New Style object keyword MRO DLR (Old) MRO Algorithm (Based on Old-Style Class) C3 New MRO Algorithm (Based on New-Style Class) Impossible Method Resolution Abstract Base Class (Abstract Class) Bottom Line Class Decorators Class Decorators versus metaclass Interface Variables Instance Variable Static or Class Variable Instance vs Class/Static attribute lookup order Methods Abstract Method Method vs Function Static Method What How Why Code Class Method What How Why Code Magic Method Class Internal Methods __new__ __init__ __call__ __get__ __set__ __del__ __slots__ __getattribute__ __getattr__ __getattr__ vs __getattribute__ __enter__ __exit__ __repr__ __str__ __format__ __iter__ __next__ __reversed__ __dir__ __setattr__ __delattr__ __len__ __contains__ __getitem__ Properties Descriptors Protocol Simple Implemetation Property implementation Misc Ways to call a class member function __slots__ : reduce RAM usage Unbound vs Bound Method Unbound Method (Simple Function in Python 3.x) Bound Method Closure TODO: stack queue class design class custom metaclass creation OOPs Basics # UML 2.x (Unified Modeling Language) # UML can be used for many diagrams other then ERD sequence diagram state diagram more for the funcionality of the application (what user can do, who does it, when he does it, before what step, what table he use to do it) other then the tables description. many more (http://agilemodeling.com/essays/umlDiagrams.htm) read more: https://www.omg.org/spec/UML https://en.wikipedia.org/wiki/Unified_Modeling_Language#UML_2 ERD (Entity Relation Diagram) # Gives image of how the tables should connect what fields are going to be on each table the tables connection, if many-to-many, one-to-many. OOPs Features # Inheritance Polymorphism Encapsulation Abstraction Abstraction # (Implementation hiding) * Core concept in all of computer science. * Without abstraction, we would still be programming in machine code or worse not have computers * Give names to things, so that the name captures the core of what a function or a whole program does. * Used to hide internal details and show only functionalities. * e.g : Any Verb Ex # Imagine a graphics library \"nicepic\" that contains pre-defined functions for: rectangles, squares, triangles, house, village. 1 2 import nicepic draw_house () Suppose an ATM. You simply insert your card and click some buttons and get the money. You dont know what is happening internally on press of these buttons. Encapsulation # (Information hiding) * Is a characteristic to bind data members and functions in single unit. * Is packing of data and functions operating on that data into a single component and restricting the access to some of the objects components. * Is a mechanism which represent the essential features without including implementation details. * e.g : Any Noun Ex. # A class is an example of encapsulation as it encapsulates all the data that is member functions,variables etc. Suppose there is a tree. Tree can have root, stem, branches, leaves, flowers and fruits. But in a single unit we call it a tree. Difference between Abstraction and Encapsulation: # Abstraction:Implementation hiding. Encapsulation:Information hiding. Implementation in Class using Access Modifiers # Python has no privacy model, there are no access modifiers like in C++, C# or Java. There are no truly 'protected' or 'private' attributes. Public: Protected members: Accessible outside the class and from its subclasses. By prefixing the name of your member with a single underscore Accessing using: obj._protected_mem Just a convention to show, the variable is protected 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 class Person : def __init__ ( self ): self . name = 'Toran' self . _lastname = 'Sahu' # protected self . __gender = 'Male' def PrintName ( self ): return self . name + ' ' + self . _lastname #Outside of class p = Person () print ( p . name ) # Out - Toran print ( p . PrintName ()) # Out - Toran Sahu # print(p._lastname) # Out - Sahu Private members: Add ____ (double underscore ) in front of the variable and function name Accessing private member : Using name mangling : obj._classname__private_mem 1 2 # print(p.__gender) # AttributeError - no attribute '__gender' print ( p . _Person__gender ) # name-mangling : Out - Male Polymorphism # In general: The ability to appear in many forms. Specifically: The ability to redefine methods for derived classes. Static Polymorphism (Method-Overloading) # Multiple times method with same name does not support in Python (its always takes the last definition) We can achieve this in python in Single method itself By passing default parameters or by using *arg , **kwargs Runtime Polymorphism (Method-Overriding) # Supports using Inheritance Inheritance # keyword: is-a sign: Types # Single Multilevel Multiple Hierchical Diamond Problem # resolved in python using order-preference (Method Resolution Order\" MRO) in multiplle inheritance preference order Search Path using Depth first search with linear search (old style python 2 classes) Search Path with optimization using Depth first search & linear search (new style python3 classes + python2 classes whicj inherits object ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 class A : def m ( self ): print ( \"m of A called\" ) class B ( A ): pass class C ( A ): def m ( self ): print ( \"m of C called\" ) class D ( B , C ): pass d = D () d . m () Out: 1 m of C called Need of super() # lets avoid referring to the base class explicitly (i.e. just one class above) e.g. python 3.x: super().__init__() python 2.x: super(<ChildClassName>, self.__init__()) 2 Basic Uses In class hierarchy with single inheritance used to refer to parent classes without naming them explicitly thus making code more maintainable similar in other languages Support cooperative multiple inheritance in dynamic execution environment unique to python only implements diamond diagram Magic happens like: m of A called printed once instead twice e.g. as shown below https://stackoverflow.com/questions/5033903/python-super-method-and-calling-alternatives applications useful for accessing inherited methods that have been overridden in a child class Note: uses MRO, in case of multiple inheritance, e.g. of class D Facts: if super().m() is used in a member functions of a sub-class D to call its parent's member function, where class D inherits class B and C then super().m() will instantiate both the parent class B and C and will call method m() in both also, if B and C 's method m() calls its super().m(), where B & C inherits class A then only one time m() of A will be called (the smartest decision) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 class A : def m ( self ): print ( \"m of A called\" ) class B ( A ): def m ( self ): print ( \"m of B called\" ) # A.m(self) super () . m () class C ( A ): def m ( self ): print ( \"m of C called\" ) # A.m(self) super () . m () class D ( B , C ): def m ( self ): print ( \"m of D called\" ) # B.m(self) # C.m(self) super () . m () d = D () d . m () Out: 1 2 3 4 m of D called m of B called m of C called m of A called Other relationships # src: https://javapapers.com/oops/association-aggregation-composition-abstraction-generalization-realization-dependency/ These relationships are totally implementation based how you want to function relationship between two or more classes & their instances Association Aggregation Composition Generalization Specialization Dependency Note: uses-a has-a / uses-a part-of (contains-a, consists-a) / has-a / uses-a i.e. a part-of relation can always use words like has-a, uses-a so, to find out a perfect relationship, try to approach in a top-to-down manner, to make the relationship more specific Association # keyword: uses-a sign: single line with arrow unidirectional if class A uses instance of class B arrow will point towards class B bidirectional explanation defines the multiplicity (cardianality) between objects one-to-one, one-to-many, many-to-one, many-to-many all classes/instances have their own life cycle (in general cases) no body have ownership over another (in general cases) e.g. house uses-an internet provider Weak Association (Aggregation) # explanation: Lets say we have class A & B class A has-an (or uses-an) instance class B means class A's any method uses instance of class B as a parameter or returns the instance or if an instance of class A calls a member function (an operation) of an instance of the class B or class A holds instance of class B but instance of class B doesn't get destroyed when instance of class B is created & passed to constructor of class A Strong Association (Composition) # explanation: Lets say we have class A & B class A has-an instance of class B / class B is part-of class A / class A consists of class B means class A holds instance of class B & the instance gets destroyed with destruction of instance of class A i.e. instance of class B is created & gets destroyed inside class A Aggregation # aka Shared Association Weak Association keyword: has-a (uses-a) sign: a line with hollow diamond \"whole\" end have a hollow diamond shaped arrow-head cardianality: one-to-one, one-to-many, many-to-many explanation Classes Within Classes When an object \u2018has-a\u2019 another object when one object is an attribute of another whole/part relationship (i.e. part of relationship) special form of Association e.g. a library has-a student a student can exist without a library a Text-editor has-a file opened if text editor is closed, file still exists conditional e.g. if parts of a car are reusable car uses-a/has-a engine if car is scrapped, engine still exists note: It\u2019s always safe to call a relationship an association,but if class A contains objects of class B , and is organizationally superior to class B , it\u2019s a good candidate for aggregation. Association vs Aggregation # The association link can replace aggregation link in every situation Composition (Not-Shared Association) # keyword: has-a (part-of, consists-of, composed-of) sign: a line with solid diamond unidirectional only \"whole\" end have a solid diamond shaped arrow-head cardianality: one-to-many, many-to-many explanation: When an object contains the other object (The part may belong to only one whole) if the contained object cannot exist without the existence of container object i.e. The lifetime of the part is the same as the lifetime of the whole. special form of Aggregation Stronger/restricted aggregation ownership relation e.g a house has-a room if house is destroyed, room also gets a class has (contains) students students cannot exist without a class a text editor has a buffer if text editor is closed, buffer also gets destroyed conditional e.g. if parts of a car are NOT reusable a car has-a wheel if a car is destroyed, wheel also gets destroyed Realization # Dependency # Implementations of OOPs # class # in python everything is object of some class classes are first-class objects they can be created at runtime, passed as parameters and returned from functions, and assigned to variables even class is object of something this something by default is type class we can create a class which is an object of type class read metaclass for deep knowledge 1 cls_a = type ( 'A' , ( object ,), dict ({ 'foo' : 2 , 'bar' : 3 })) # type(class_name_str, base_classes_tuple, body_dict) this way we can create a class at runtime type # type is the class of python classes 1 2 3 4 5 6 class Example : pass e = Example () e . __class__ # prints: <class __main__.Example> Example . __class__ # prints: <type 'type'> so in above code, e is object of class Example and Example is object of class type in python3 we can use type and class interchangabely which was not used in python2 so, whenever we write keyword class while defining a class, an object of class type gets created metaclass # source: https://eli.thegreenplace.net/2011/08/14/python-metaclasses-by-example/ https://realpython.com/python-metaclasses/#defining-a-class-dynamically a metaclass is defined as the class of a class Any class whose instances are themselves classes, is a metaclass so, type is also a kind of metaclass (which is by default for all python classes and mostly used), but not always we can define a metaclass for a class 1 2 3 4 class Example ( metaclass = SomeMetaclass ): # in python2 define metaclass here like # __metaclass__ = SomeMetaclass pass Since a metaclass is the class of a class it is used to construct classes (just as a class is used to construct objects) as we have already seen, class keyword invokes type function to create a class it was because, type is default metaclass in reality class keyword do followings when python encounters class definiton, it collects all the attributes in a dict when collection is over, python determines metaclass of the class, lets say SomeMetaclass using SomeClass.__metaclass__ then python calls Metaclass(class_name, (base_classes, ), body_dict) , where class_name is the name of the class (string ) (base_clasees, ) is the tuple of base classes, if it was empty in class definition, then by dafault it is object body_dict is a python dict contaning all the class attribute names metaclass defines structure/metadata of the class Methods of metaclass to create & initialize a class are: __new__() # used to perform some basic stuff like memory allocation called on creation of an object of the class __init__() # works as contructor in python used to perform initialization of data called on creation of an object of the class it is called after above methods __new__() so in case of creation of class (suppose A ), it is called when we call type() __call__() # it is called after above two methods in case of creation of class, it is called when an object (suppose a = A() ) of the class (created from above two steps) is created. instance # TODO: - confusion in magic method call orders - how a class instance are created - as a class is object of a metaclass 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 ```python #here class A A = type('A', (object,), {\"a\":1}) ``` - whenever we call any object/instance of any class, it calls the `__call__` of that class ```python a = A() #here A() will call the __call__ of metaclass type ``` - so, a is created by A(), where A() means calling `__call__` of `type` metaclass - if we provide A() parameters, like A(1,2), then `__call__(self, *args, **kwargs)__` handles those parameters - then inside metaclass's `__call__` , `__init__` of the class is called by passing all the parameters passed to `__call__` so, whenever we create an object/instance of a class, following things happens: metaclass's __new__ is called; creates memory (object) for class metaclass's __init__ is called; intializes class with its name and body when class, lets say Example is called Example(), then metaclass's __call__ is called to initialize the object of Example class, Example.__init__ is called Old vs New Style # in python realm, there are two varities of classes Old-Style object of a class is always a in python 2.x , classes are old-style by deafult due to compatibility issues in python 2.x, New-Style classes are created by inheriting other New-style classes or top-level type object here object is a keyword which is instance of class type , and we know that type is a metaclass and metaclass is a class of class hence instance of a metaclass is always a class hence object keyword is a kind of class, which is created already in python just to implement a New-Style class syntax are like 1 2 class A ( object ): pass New-Style object of a class is always a in python 3.x has new-style classes only hence no need to inherit object in class 1 2 class Example : pass introduced in python2.2 to unify the concepts of class and type in new-style type(x) and x.__class__ are always same a new-style class is simply a user-defined type more differences at https://stackoverflow.com/questions/4015417/python-class-inherits-object TODO: object keyword # object is a keyword which is instance of class type , and we know that type is a metaclass and metaclass is a class of class hence instance of a metaclass is always a class hence object keyword is a kind of class, which is created already in python just to implement a New-Style class MRO # (Method Resolution Order) - source: - https://makina-corpus.com/blog/metier/2014/python-tutorial-understanding-python-mro-class-search-path - class.__mro__ is only available in New-Style class Script: 1 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 class A : def m ( self ): print ( \"m of A called\" ) class B ( A ): def m ( self ): print ( \"m of B called\" ) class C ( A ): #def m(self): # print(\"m of C called\") pass class D ( B , C ): #def m(self): # print(\"m of D called\") pass d = D () d . m () Script: 2 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 class A1 (): # def who_am_i(self): # print(\"I am a A1\") pass class A2 (): def who_am_i ( self ): print ( \"I am a A2\" ) class B ( A1 , A2 ): # def who_am_i(self): # print(\"I am a B\") pass class C ( A2 ): def who_am_i ( self ): print ( \"I am a C\" ) class D ( B , C ): # def who_am_i(self): # print(\"I am a D\") pass d1 = D () d1 . who_am_i () DLR (Old) MRO Algorithm (Based on Old-Style Class) # depth first left to right uses depth-first search followed by linear search (left to right) we call this order search path in script 1 Looking in D If not found, looking in B If not found, looking un B first parent A If not found, going back in B others parents (none) If not found, looking in D others parents : C in script 2 search path will be D, B, A1, A2, C, A2 C3 New MRO Algorithm (Based on New-Style Class) # default in python3 works in python2 with classes who inherits object Algo defines search path same as old algorithm then simplifies the search path like this iterate through the original search path put pointer on current class check if any classes after the pointer are child of current class (in other words, check if any classes after the pointer inherits the current class) if yes, then remove the current class from search path if no, then move the pointer to next class in the right do the same till end of search path list in script 1 original search path will be D, B, A, C, A then simplified search path will be D, B, C, A in script 2 search path will be D, B, A1, A2, C, A2 then simplified search path should be D, B, A1, C, A2 Impossible Method Resolution # Script: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 class X (): def who_am_i ( self ): print ( \"I am a X\" ) class Y (): def who_am_i ( self ): print ( \"I am a Y\" ) class A ( X , Y ): def who_am_i ( self ): print ( \"I am a A\" ) class B ( Y , X ): def who_am_i ( self ): print ( \"I am a B\" ) class F ( A , B ): def who_am_i ( self ): print ( \"I am a F\" ) In Old MRO algo, search path is F, A, X, Y, B, Y, X In New MRO algo, simplified search path is F, A, B, Y, X but New MRO algo fails to give __mro__ and throws following exception 1 TypeError: Cannot create a consistent method resolution Abstract Base Class (Abstract Class) # Classes that contains one or more abstract methods as well as concrete methods. A normal class cannot have abstract methods. Cannot instantiate an abstract class that has abstract methods Subclass which implements all the abstract method can be instantiated Python Implementation: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 # Python 3.4+ from abc import ABC , abstractmethod class Abstract ( ABC ): @abstractmethod def foo ( self ): pass # Python 3.0+ from abc import ABCMeta , abstractmethod class Abstract ( metaclass = ABCMeta ): @abstractmethod def foo ( self ): pass # Python 2 from abc import ABCMeta , abstractmethod class Abstract : __metaclass__ = ABCMeta @abstractmethod def foo ( self ): pass Bottom Line # in python3: base class of classes are: object metaclass of classes are: type type of classes are: type classes are instance of class: type Class Decorators # e.g. https://github.com/toransahu/py-misc/blob/master/class_based_class_decorator_with_pre_post.py TODO: Class Decorators versus metaclass # source: https://jfine-python-classes.readthedocs.io/en/latest/decorators-versus-metaclass.html nothing different other than implementation style Interface # Doesn't need in Python (bcoz, it was need in other lang. to full-fill the Multiple inheritance) Concept: * A class with all the methods abstract * Contains Methods signature * Do not contain definition/method body * Constant variables (which must be Static + Final in other langs.) - in python there is no final or const keyword * Cannot instantiated * Behaviour/methods which must be implemented by classes * Class which implement interface should implement all the methods OR be an abstract class Python Implementation: 1 2 3 4 5 6 7 class Engine (): \"\"\" Engine Interface\"\"\" def ignition ( self ): raise NotImplementedError ( \"Ingnition should have implemented.\" ) def fuel ( self ): raise NotImplementedError ( \"Fuel should have implemented.\" ) Variables # Instance Variable # variable defined inside class methods different for different objects every object have its own copy Static or Class Variable # defined in class level shared by all the objects can be accessed by Class name as well as objects with the same name, there can be one class level variable and one instance level variable, see in e.g. in this case, Class.variable will definitely access the static var but instance.variable will access the instance variable accessing inside methods it can be accessed using Class.var & self.var if there is instance variable with the same name then self.var will access the instance variable 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 class A : variable = 'Static/class Variable' # static/class variable var = 'Static/class Var' # static/class variable def __init__ ( self ): self . variable = 'Instance Variable' def foo ( self ): print ( A . variable , self . variable , self . var ) print ( A . variable ) a = A () a . foo () Out: 1 2 3 4 Static/class Variable Instance Variable Static/class Var Static/class Variable Instance Variable Instance vs Class/Static attribute lookup order # instance/object > static/class > base class which is: a. dict ['x'] > type(a). dict ['x'] > type(a) Methods # Abstract Method # a method without definition (only declared) only signature In python: a method decorated with @abstractmethod Method vs Function # Method works exactly same as a simple function. But a method's first argument always receives the instance object: Code: 1 2 3 4 5 def outside_foo (): pass def outside_foo ( self ,): pass Static Method # What # an organization/stylistic feature functionality-wise same as normal module level functions except, module level func can access an instance and hence instance variables, but static method cannot designed to work on class attributes can be called using class as well as instance A static method has no self argument Never receive an automatic self argument, whether called through a class or an instance. can access/modify class or static variables using class_name.var How # two way of declaration using decorator @staticmethod [fn_name] = staticmethod([fn_name]) Why # to restrict access of instance attributes (unlike a normal method have access) to fix the access of static variable of a that particular class only because it is hardcoded like A.static_variable src: http://radek.io/2011/07/21/static-variables-and-methods-in-python/ Code # 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 class A : static_variable = 'Static/class Variable of class A' # static/class variable def __init__ ( self ): self . instance_variable = 'Instance Variable' @staticmethod def static_method (): print ( 'Inside static_method()' , A . static_variable ) # works # print('Inside static_method()', A.instance_variable) # error, static method can't access instance attributes # static method def way2 (): pass # making way2() a static method way2 = staticmethod ( way2 ) class B ( A ): static_variable = 'Static/class Variable of class B' # static/class variable # but a module level func can access an instance attribute def module_level_func (): a1 = A () print ( a1 . instance_variable ) a2 = A () a2 . static_method () A . static_method () B . static_method () # still accessing static_variable from class A Out: 1 2 3 Inside static_method() Static/class Variable of class A Inside static_method() Static/class Variable of class A Inside static_method() Static/class Variable of class A Class Method # What # an organization/stylistic feature not different from static method, only signature diff it receives one mandatory arguement: a class name it was called from apart from this first parameter, there is no any functionality diff between class & static method designed to work on class attributes can be called using class as well as instance can access/modify class or static variables using cls.var where cls is first parameter provided to a classmethod How # implemented using decorator @classmethod assigning to function i.e., method_name = classmethod(method_name) Why # to restrict access of instance attributes (unlike a normal method have access) to make code more maintanable than static methods Code # 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 class A : static_variable = 'Static/class Variable of class A' # static/class variable def __init__ ( self ): self . instance_variable = 'Instance Variable' @classmethod def class_method ( cls ): print ( 'Inside class_method()' , cls . static_variable ) # works # print('Inside class_method()', cls.instance_variable) # error, class method can't access instance attributes # static method def way2 ( cls ): pass # making way2() a static method way2 = classmethod ( way2 ) class B ( A ): static_variable = 'Static/class Variable of class B' # static/class variable # but a module level func can access an instance attribute def module_level_func (): a1 = A () print ( a1 . instance_variable ) a2 = A () a2 . class_method () A . class_method () B . class_method () # now it will access static variable of class B Out: 1 2 3 Inside class_method() Static/class Variable of class A Inside class_method() Static/class Variable of class A Inside class_method() Static/class Variable of class B Magic Method # TODO: Source: https://www.python-course.eu/python3_magic_methods.php https://rszalski.github.io/magicmethods/ Methods with dunders (+) : __add()__ (-) : __sub()__ Can be used for operator overloading, as 1 2 3 4 5 6 7 8 9 class Calc ( int ): def __init__ ( self , x ): self . x = x def __add__ ( self , other ): return self . x + other . x + 1 a = Calc ( 1 ) b = Calc ( 1 ) a + b Out: 1 3 Class Internal Methods # TODO: Source: http://www.diveintopython3.net/special-method-names.html https://rszalski.github.io/magicmethods/ Some famous magic methods / internal methods of a class __new__ # allocates memory to class instance __init__ # initializes instance with some values __call__ # called upon calling an instance (e.g. a = A(); a()) __get__ # get descriptor method __set__ # set descriptor method __del__ # del descriptor method __slots__ # allows us to explicitly declare data members (like properties) and deny the creation of __dict__ & __weakref__ (unless explicitly declared in __slots__ ) __getattribute__ # called when accessing any attribute via class instance 1 2 c = C () c . name __getattr__ # same as __getattribute__ but gets called only when attribiute is not found via __getattribute__ __getattr__ vs __getattribute__ # Source: http://www.diveintopython3.net/special-method-names.html __enter__ # __exit__ # __repr__ # __str__ # __format__ # __iter__ # __next__ # __reversed__ # __dir__ # __setattr__ # __delattr__ # __len__ # __contains__ # __getitem__ # Properties # Source: https://www.journaldev.com/14893/python-property-decorator https://docs.python.org/3/howto/descriptor.html#properties https://medium.com/shecodeafrica/managing-class-attributes-in-python-c42d501c5ee0 Why to manage access to an attribute to outer world can only allow get can only allow set can only allow del can perform something more while doing above operations to solve problem like this (dependent attribute value issue) e.g. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 class Example : def __init__ ( self , ap , bp ): self . _a = ap self . _b = bp self . c = self . _a + self . _b def get_c ( self ): return self . c e = Example ( 1 , 2 ) print ( e . _a ) print ( e . _b ) print ( e . c ) print ( e . get_c ()) e . _a = 5 print ( e . _a ) print ( e . c ) print ( e . get_c ()) solution e.g. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 class Example : def __init__ ( self , ap , bp ): self . _a = ap self . _b = bp @property def c ( self ): return self . _a + self . _b e = Example ( 1 , 2 ) print ( e . _a ) print ( e . _b ) print ( e . c ) e . _a = 5 print ( e . _a ) print ( e . c ) What property are built -in python decorators provides 3 methods get set del How property in python are implemented using Descriptors uses : pattern 1 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 class Example : def __init__ ( self , ap ): self . _a = ap @property def a ( self ): return self . _a @a . setter def a ( self , value ): self . _a = value @a . deleter def a ( self ): del self . _a e = Example ( 1 ) print ( e . a ) e . a = 5 print ( e . a ) del ( e . a ) print ( e . a ) #AttributeError: 'Example' object has no attribute '_a' uses : pattern 2 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 class Example : def __init__ ( self , ap ): self . _a = ap def get_a ( self ): return self . _a def set_a ( self , value ): self . _a = value def del_a ( self ): del self . _a # will give only get access a = property ( fget = get_a ) # will give only set access a = property ( fset = set_a ) # will give only del access a = property ( fdel = del_a ) #will give all access of var `a` a = property ( get_a , set_a , del_a ) #property(fget=None, fset=None, fdel=None, doc=None) e = Example ( 1 ) print ( e . a ) e . a = 5 print ( e . a ) del ( e . a ) print ( e . a ) # AttributeError: 'Example' object has no attribute '_a' Descriptors # TODO: Source: https://docs.python.org/3/howto/descriptor.html descriptor is an object attribute with \u201cbinding behavior\u201d, one whose attribute access has been overridden by methods in the descriptor protocol Those methods are get (), set (), and delete () Basic default behavior for attribute access is to get, set, or delete For instance, a.x has a lookup chain starting with a. dict ['x'], then type(a). dict ['x'], and continuing through the base classes of type(a) excluding metaclasses If there is descriptor x defined, then descriptor will override the lookup order and will become number one Use Cases They are the mechanism behind properties, methods, static methods, class methods, and super() They are used throughout Python itself to implement the new style classes introduced in version 2.2 Protocol # 1 2 3 4 5 descr . __get__ ( self , obj , type = None ) --> value descr . __set__ ( self , obj , value ) --> None descr . __delete__ ( self , obj ) --> None define any of the above in any class, and object will considered as descriptor and descriptor will override the default attribute lookup behavior (in class dictionary lookup) if object defines both __get__ and __set__ ; then its data descriptor if instance have attribute (means entry in instance dict) with same name as descriptor, then here lookup order will become data descriptor > instance dict if object defines only __get__ ; then its non-data descriptor in this case if instance have attribute with same name as descriptor, then here lookup order will become instance dict > data descriptor Simple Implemetation # TODO - Issue with __del__ ; not explained in python doc 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 class ExampleDescriptor : def __init__ ( self , val ): self . val = val def __get__ ( self , obj , objtype = None ): print ( \"getting\" ) return self . val def __set__ ( self , obj , val ): print ( \"setting\" ) self . val = val class Example : name = ExampleDescriptor ( \"toran\" ) if __name__ == \"__main__\" : e = Example () print ( e . name ) e . name = \"sahu\" print ( e . name ) Property implementation # 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 class Property ( object ): \"Emulate PyProperty_Type() in Objects/descrobject.c\" def __init__ ( self , fget = None , fset = None , fdel = None , doc = None ): self . fget = fget self . fset = fset self . fdel = fdel if doc is None and fget is not None : doc = fget . __doc__ self . __doc__ = doc def __get__ ( self , obj , objtype = None ): if obj is None : return self if self . fget is None : raise AttributeError ( \"unreadable attribute\" ) return self . fget ( obj ) def __set__ ( self , obj , value ): if self . fset is None : raise AttributeError ( \"can't set attribute\" ) self . fset ( obj , value ) def __delete__ ( self , obj ): if self . fdel is None : raise AttributeError ( \"can't delete attribute\" ) self . fdel ( obj ) def getter ( self , fget ): return type ( self )( fget , self . fset , self . fdel , self . __doc__ ) def setter ( self , fset ): return type ( self )( self . fget , fset , self . fdel , self . __doc__ ) def deleter ( self , fdel ): return type ( self )( self . fget , self . fset , fdel , self . __doc__ ) Misc # Ways to call a class member function # 1 2 a = A () a . foo ( args ) 1 2 a = A () A . foo ( a , args ) __slots__ : reduce RAM usage # source http://book.pythontips.com/en/latest/__slots__magic.html Unbound vs Bound Method # Unbound Method (Simple Function in Python 3.x) # Unbound means not bound to any instance i.e. callable using class name Bound Method # Bound methods are bound to some instance Need an instance to call it. Note: In Python 3.x, the notion of Unbound Method has been dropped and we treat it as Simple Function. Closure # A combination of code and scope. It's about nested function and the scope of the function Code: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 def startAt ( start ): def incrementBy ( inc ): return start + inc return incrementBy f = startAt ( 10 ) g = startAt ( 100 ) print ( f ( 1 ), g ( 2 )) # Using lamba def startAt ( start ): return lambda inc : start + inc f = startAt ( 10 ) g = startAt ( 100 ) print ( f ( 1 ), g ( 2 )) Out: 1 2 11 102 11 102","title":"Python OOPs"},{"location":"Languages/python/python_oops/#python-oops","text":"Python OOPs OOPs Basics UML 2.x (Unified Modeling Language) ERD (Entity Relation Diagram) OOPs Features Abstraction Ex Encapsulation Ex. Difference between Abstraction and Encapsulation: Implementation in Class using Access Modifiers Polymorphism Static Polymorphism (Method-Overloading) Runtime Polymorphism (Method-Overriding) Inheritance Types Diamond Problem Need of super() Other relationships Association Weak Association (Aggregation) Strong Association (Composition) Aggregation Association vs Aggregation Composition (Not-Shared Association) Realization Dependency Implementations of OOPs class type metaclass __new__() __init__() __call__() instance Old vs New Style object keyword MRO DLR (Old) MRO Algorithm (Based on Old-Style Class) C3 New MRO Algorithm (Based on New-Style Class) Impossible Method Resolution Abstract Base Class (Abstract Class) Bottom Line Class Decorators Class Decorators versus metaclass Interface Variables Instance Variable Static or Class Variable Instance vs Class/Static attribute lookup order Methods Abstract Method Method vs Function Static Method What How Why Code Class Method What How Why Code Magic Method Class Internal Methods __new__ __init__ __call__ __get__ __set__ __del__ __slots__ __getattribute__ __getattr__ __getattr__ vs __getattribute__ __enter__ __exit__ __repr__ __str__ __format__ __iter__ __next__ __reversed__ __dir__ __setattr__ __delattr__ __len__ __contains__ __getitem__ Properties Descriptors Protocol Simple Implemetation Property implementation Misc Ways to call a class member function __slots__ : reduce RAM usage Unbound vs Bound Method Unbound Method (Simple Function in Python 3.x) Bound Method Closure TODO: stack queue class design class custom metaclass creation","title":"Python OOPs"},{"location":"Languages/python/python_oops/#oops-basics","text":"","title":"OOPs Basics"},{"location":"Languages/python/python_oops/#uml-2x-unified-modeling-language","text":"UML can be used for many diagrams other then ERD sequence diagram state diagram more for the funcionality of the application (what user can do, who does it, when he does it, before what step, what table he use to do it) other then the tables description. many more (http://agilemodeling.com/essays/umlDiagrams.htm) read more: https://www.omg.org/spec/UML https://en.wikipedia.org/wiki/Unified_Modeling_Language#UML_2","title":"UML 2.x (Unified Modeling Language)"},{"location":"Languages/python/python_oops/#erd-entity-relation-diagram","text":"Gives image of how the tables should connect what fields are going to be on each table the tables connection, if many-to-many, one-to-many.","title":"ERD (Entity Relation Diagram)"},{"location":"Languages/python/python_oops/#oops-features","text":"Inheritance Polymorphism Encapsulation Abstraction","title":"OOPs Features"},{"location":"Languages/python/python_oops/#abstraction","text":"(Implementation hiding) * Core concept in all of computer science. * Without abstraction, we would still be programming in machine code or worse not have computers * Give names to things, so that the name captures the core of what a function or a whole program does. * Used to hide internal details and show only functionalities. * e.g : Any Verb","title":"Abstraction"},{"location":"Languages/python/python_oops/#ex","text":"Imagine a graphics library \"nicepic\" that contains pre-defined functions for: rectangles, squares, triangles, house, village. 1 2 import nicepic draw_house () Suppose an ATM. You simply insert your card and click some buttons and get the money. You dont know what is happening internally on press of these buttons.","title":"Ex"},{"location":"Languages/python/python_oops/#encapsulation","text":"(Information hiding) * Is a characteristic to bind data members and functions in single unit. * Is packing of data and functions operating on that data into a single component and restricting the access to some of the objects components. * Is a mechanism which represent the essential features without including implementation details. * e.g : Any Noun","title":"Encapsulation"},{"location":"Languages/python/python_oops/#ex_1","text":"A class is an example of encapsulation as it encapsulates all the data that is member functions,variables etc. Suppose there is a tree. Tree can have root, stem, branches, leaves, flowers and fruits. But in a single unit we call it a tree.","title":"Ex."},{"location":"Languages/python/python_oops/#difference-between-abstraction-and-encapsulation","text":"Abstraction:Implementation hiding. Encapsulation:Information hiding.","title":"Difference between Abstraction and Encapsulation:"},{"location":"Languages/python/python_oops/#implementation-in-class-using-access-modifiers","text":"Python has no privacy model, there are no access modifiers like in C++, C# or Java. There are no truly 'protected' or 'private' attributes. Public: Protected members: Accessible outside the class and from its subclasses. By prefixing the name of your member with a single underscore Accessing using: obj._protected_mem Just a convention to show, the variable is protected 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 class Person : def __init__ ( self ): self . name = 'Toran' self . _lastname = 'Sahu' # protected self . __gender = 'Male' def PrintName ( self ): return self . name + ' ' + self . _lastname #Outside of class p = Person () print ( p . name ) # Out - Toran print ( p . PrintName ()) # Out - Toran Sahu # print(p._lastname) # Out - Sahu Private members: Add ____ (double underscore ) in front of the variable and function name Accessing private member : Using name mangling : obj._classname__private_mem 1 2 # print(p.__gender) # AttributeError - no attribute '__gender' print ( p . _Person__gender ) # name-mangling : Out - Male","title":"Implementation in Class using Access Modifiers"},{"location":"Languages/python/python_oops/#polymorphism","text":"In general: The ability to appear in many forms. Specifically: The ability to redefine methods for derived classes.","title":"Polymorphism"},{"location":"Languages/python/python_oops/#static-polymorphism-method-overloading","text":"Multiple times method with same name does not support in Python (its always takes the last definition) We can achieve this in python in Single method itself By passing default parameters or by using *arg , **kwargs","title":"Static Polymorphism (Method-Overloading)"},{"location":"Languages/python/python_oops/#runtime-polymorphism-method-overriding","text":"Supports using Inheritance","title":"Runtime Polymorphism (Method-Overriding)"},{"location":"Languages/python/python_oops/#inheritance","text":"keyword: is-a sign:","title":"Inheritance"},{"location":"Languages/python/python_oops/#types","text":"Single Multilevel Multiple Hierchical","title":"Types"},{"location":"Languages/python/python_oops/#diamond-problem","text":"resolved in python using order-preference (Method Resolution Order\" MRO) in multiplle inheritance preference order Search Path using Depth first search with linear search (old style python 2 classes) Search Path with optimization using Depth first search & linear search (new style python3 classes + python2 classes whicj inherits object ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 class A : def m ( self ): print ( \"m of A called\" ) class B ( A ): pass class C ( A ): def m ( self ): print ( \"m of C called\" ) class D ( B , C ): pass d = D () d . m () Out: 1 m of C called","title":"Diamond Problem"},{"location":"Languages/python/python_oops/#need-of-super","text":"lets avoid referring to the base class explicitly (i.e. just one class above) e.g. python 3.x: super().__init__() python 2.x: super(<ChildClassName>, self.__init__()) 2 Basic Uses In class hierarchy with single inheritance used to refer to parent classes without naming them explicitly thus making code more maintainable similar in other languages Support cooperative multiple inheritance in dynamic execution environment unique to python only implements diamond diagram Magic happens like: m of A called printed once instead twice e.g. as shown below https://stackoverflow.com/questions/5033903/python-super-method-and-calling-alternatives applications useful for accessing inherited methods that have been overridden in a child class Note: uses MRO, in case of multiple inheritance, e.g. of class D Facts: if super().m() is used in a member functions of a sub-class D to call its parent's member function, where class D inherits class B and C then super().m() will instantiate both the parent class B and C and will call method m() in both also, if B and C 's method m() calls its super().m(), where B & C inherits class A then only one time m() of A will be called (the smartest decision) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 class A : def m ( self ): print ( \"m of A called\" ) class B ( A ): def m ( self ): print ( \"m of B called\" ) # A.m(self) super () . m () class C ( A ): def m ( self ): print ( \"m of C called\" ) # A.m(self) super () . m () class D ( B , C ): def m ( self ): print ( \"m of D called\" ) # B.m(self) # C.m(self) super () . m () d = D () d . m () Out: 1 2 3 4 m of D called m of B called m of C called m of A called","title":"Need of super()"},{"location":"Languages/python/python_oops/#other-relationships","text":"src: https://javapapers.com/oops/association-aggregation-composition-abstraction-generalization-realization-dependency/ These relationships are totally implementation based how you want to function relationship between two or more classes & their instances Association Aggregation Composition Generalization Specialization Dependency Note: uses-a has-a / uses-a part-of (contains-a, consists-a) / has-a / uses-a i.e. a part-of relation can always use words like has-a, uses-a so, to find out a perfect relationship, try to approach in a top-to-down manner, to make the relationship more specific","title":"Other relationships"},{"location":"Languages/python/python_oops/#association","text":"keyword: uses-a sign: single line with arrow unidirectional if class A uses instance of class B arrow will point towards class B bidirectional explanation defines the multiplicity (cardianality) between objects one-to-one, one-to-many, many-to-one, many-to-many all classes/instances have their own life cycle (in general cases) no body have ownership over another (in general cases) e.g. house uses-an internet provider","title":"Association"},{"location":"Languages/python/python_oops/#weak-association-aggregation","text":"explanation: Lets say we have class A & B class A has-an (or uses-an) instance class B means class A's any method uses instance of class B as a parameter or returns the instance or if an instance of class A calls a member function (an operation) of an instance of the class B or class A holds instance of class B but instance of class B doesn't get destroyed when instance of class B is created & passed to constructor of class A","title":"Weak Association (Aggregation)"},{"location":"Languages/python/python_oops/#strong-association-composition","text":"explanation: Lets say we have class A & B class A has-an instance of class B / class B is part-of class A / class A consists of class B means class A holds instance of class B & the instance gets destroyed with destruction of instance of class A i.e. instance of class B is created & gets destroyed inside class A","title":"Strong Association (Composition)"},{"location":"Languages/python/python_oops/#aggregation","text":"aka Shared Association Weak Association keyword: has-a (uses-a) sign: a line with hollow diamond \"whole\" end have a hollow diamond shaped arrow-head cardianality: one-to-one, one-to-many, many-to-many explanation Classes Within Classes When an object \u2018has-a\u2019 another object when one object is an attribute of another whole/part relationship (i.e. part of relationship) special form of Association e.g. a library has-a student a student can exist without a library a Text-editor has-a file opened if text editor is closed, file still exists conditional e.g. if parts of a car are reusable car uses-a/has-a engine if car is scrapped, engine still exists note: It\u2019s always safe to call a relationship an association,but if class A contains objects of class B , and is organizationally superior to class B , it\u2019s a good candidate for aggregation.","title":"Aggregation"},{"location":"Languages/python/python_oops/#association-vs-aggregation","text":"The association link can replace aggregation link in every situation","title":"Association vs Aggregation"},{"location":"Languages/python/python_oops/#composition-not-shared-association","text":"keyword: has-a (part-of, consists-of, composed-of) sign: a line with solid diamond unidirectional only \"whole\" end have a solid diamond shaped arrow-head cardianality: one-to-many, many-to-many explanation: When an object contains the other object (The part may belong to only one whole) if the contained object cannot exist without the existence of container object i.e. The lifetime of the part is the same as the lifetime of the whole. special form of Aggregation Stronger/restricted aggregation ownership relation e.g a house has-a room if house is destroyed, room also gets a class has (contains) students students cannot exist without a class a text editor has a buffer if text editor is closed, buffer also gets destroyed conditional e.g. if parts of a car are NOT reusable a car has-a wheel if a car is destroyed, wheel also gets destroyed","title":"Composition (Not-Shared Association)"},{"location":"Languages/python/python_oops/#realization","text":"","title":"Realization"},{"location":"Languages/python/python_oops/#dependency","text":"","title":"Dependency"},{"location":"Languages/python/python_oops/#implementations-of-oops","text":"","title":"Implementations of OOPs"},{"location":"Languages/python/python_oops/#class","text":"in python everything is object of some class classes are first-class objects they can be created at runtime, passed as parameters and returned from functions, and assigned to variables even class is object of something this something by default is type class we can create a class which is an object of type class read metaclass for deep knowledge 1 cls_a = type ( 'A' , ( object ,), dict ({ 'foo' : 2 , 'bar' : 3 })) # type(class_name_str, base_classes_tuple, body_dict) this way we can create a class at runtime","title":"class"},{"location":"Languages/python/python_oops/#type","text":"type is the class of python classes 1 2 3 4 5 6 class Example : pass e = Example () e . __class__ # prints: <class __main__.Example> Example . __class__ # prints: <type 'type'> so in above code, e is object of class Example and Example is object of class type in python3 we can use type and class interchangabely which was not used in python2 so, whenever we write keyword class while defining a class, an object of class type gets created","title":"type"},{"location":"Languages/python/python_oops/#metaclass","text":"source: https://eli.thegreenplace.net/2011/08/14/python-metaclasses-by-example/ https://realpython.com/python-metaclasses/#defining-a-class-dynamically a metaclass is defined as the class of a class Any class whose instances are themselves classes, is a metaclass so, type is also a kind of metaclass (which is by default for all python classes and mostly used), but not always we can define a metaclass for a class 1 2 3 4 class Example ( metaclass = SomeMetaclass ): # in python2 define metaclass here like # __metaclass__ = SomeMetaclass pass Since a metaclass is the class of a class it is used to construct classes (just as a class is used to construct objects) as we have already seen, class keyword invokes type function to create a class it was because, type is default metaclass in reality class keyword do followings when python encounters class definiton, it collects all the attributes in a dict when collection is over, python determines metaclass of the class, lets say SomeMetaclass using SomeClass.__metaclass__ then python calls Metaclass(class_name, (base_classes, ), body_dict) , where class_name is the name of the class (string ) (base_clasees, ) is the tuple of base classes, if it was empty in class definition, then by dafault it is object body_dict is a python dict contaning all the class attribute names metaclass defines structure/metadata of the class Methods of metaclass to create & initialize a class are:","title":"metaclass"},{"location":"Languages/python/python_oops/#__new__","text":"used to perform some basic stuff like memory allocation called on creation of an object of the class","title":"__new__()"},{"location":"Languages/python/python_oops/#__init__","text":"works as contructor in python used to perform initialization of data called on creation of an object of the class it is called after above methods __new__() so in case of creation of class (suppose A ), it is called when we call type()","title":"__init__()"},{"location":"Languages/python/python_oops/#__call__","text":"it is called after above two methods in case of creation of class, it is called when an object (suppose a = A() ) of the class (created from above two steps) is created.","title":"__call__()"},{"location":"Languages/python/python_oops/#instance","text":"TODO: - confusion in magic method call orders - how a class instance are created - as a class is object of a metaclass 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 ```python #here class A A = type('A', (object,), {\"a\":1}) ``` - whenever we call any object/instance of any class, it calls the `__call__` of that class ```python a = A() #here A() will call the __call__ of metaclass type ``` - so, a is created by A(), where A() means calling `__call__` of `type` metaclass - if we provide A() parameters, like A(1,2), then `__call__(self, *args, **kwargs)__` handles those parameters - then inside metaclass's `__call__` , `__init__` of the class is called by passing all the parameters passed to `__call__` so, whenever we create an object/instance of a class, following things happens: metaclass's __new__ is called; creates memory (object) for class metaclass's __init__ is called; intializes class with its name and body when class, lets say Example is called Example(), then metaclass's __call__ is called to initialize the object of Example class, Example.__init__ is called","title":"instance"},{"location":"Languages/python/python_oops/#old-vs-new-style","text":"in python realm, there are two varities of classes Old-Style object of a class is always a in python 2.x , classes are old-style by deafult due to compatibility issues in python 2.x, New-Style classes are created by inheriting other New-style classes or top-level type object here object is a keyword which is instance of class type , and we know that type is a metaclass and metaclass is a class of class hence instance of a metaclass is always a class hence object keyword is a kind of class, which is created already in python just to implement a New-Style class syntax are like 1 2 class A ( object ): pass New-Style object of a class is always a in python 3.x has new-style classes only hence no need to inherit object in class 1 2 class Example : pass introduced in python2.2 to unify the concepts of class and type in new-style type(x) and x.__class__ are always same a new-style class is simply a user-defined type more differences at https://stackoverflow.com/questions/4015417/python-class-inherits-object TODO:","title":"Old vs New Style"},{"location":"Languages/python/python_oops/#object-keyword","text":"object is a keyword which is instance of class type , and we know that type is a metaclass and metaclass is a class of class hence instance of a metaclass is always a class hence object keyword is a kind of class, which is created already in python just to implement a New-Style class","title":"object keyword"},{"location":"Languages/python/python_oops/#mro","text":"(Method Resolution Order) - source: - https://makina-corpus.com/blog/metier/2014/python-tutorial-understanding-python-mro-class-search-path - class.__mro__ is only available in New-Style class Script: 1 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 class A : def m ( self ): print ( \"m of A called\" ) class B ( A ): def m ( self ): print ( \"m of B called\" ) class C ( A ): #def m(self): # print(\"m of C called\") pass class D ( B , C ): #def m(self): # print(\"m of D called\") pass d = D () d . m () Script: 2 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 class A1 (): # def who_am_i(self): # print(\"I am a A1\") pass class A2 (): def who_am_i ( self ): print ( \"I am a A2\" ) class B ( A1 , A2 ): # def who_am_i(self): # print(\"I am a B\") pass class C ( A2 ): def who_am_i ( self ): print ( \"I am a C\" ) class D ( B , C ): # def who_am_i(self): # print(\"I am a D\") pass d1 = D () d1 . who_am_i ()","title":"MRO"},{"location":"Languages/python/python_oops/#dlr-old-mro-algorithm-based-on-old-style-class","text":"depth first left to right uses depth-first search followed by linear search (left to right) we call this order search path in script 1 Looking in D If not found, looking in B If not found, looking un B first parent A If not found, going back in B others parents (none) If not found, looking in D others parents : C in script 2 search path will be D, B, A1, A2, C, A2","title":"DLR (Old) MRO Algorithm (Based on Old-Style Class)"},{"location":"Languages/python/python_oops/#c3-new-mro-algorithm-based-on-new-style-class","text":"default in python3 works in python2 with classes who inherits object Algo defines search path same as old algorithm then simplifies the search path like this iterate through the original search path put pointer on current class check if any classes after the pointer are child of current class (in other words, check if any classes after the pointer inherits the current class) if yes, then remove the current class from search path if no, then move the pointer to next class in the right do the same till end of search path list in script 1 original search path will be D, B, A, C, A then simplified search path will be D, B, C, A in script 2 search path will be D, B, A1, A2, C, A2 then simplified search path should be D, B, A1, C, A2","title":"C3 New MRO Algorithm (Based on New-Style Class)"},{"location":"Languages/python/python_oops/#impossible-method-resolution","text":"Script: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 class X (): def who_am_i ( self ): print ( \"I am a X\" ) class Y (): def who_am_i ( self ): print ( \"I am a Y\" ) class A ( X , Y ): def who_am_i ( self ): print ( \"I am a A\" ) class B ( Y , X ): def who_am_i ( self ): print ( \"I am a B\" ) class F ( A , B ): def who_am_i ( self ): print ( \"I am a F\" ) In Old MRO algo, search path is F, A, X, Y, B, Y, X In New MRO algo, simplified search path is F, A, B, Y, X but New MRO algo fails to give __mro__ and throws following exception 1 TypeError: Cannot create a consistent method resolution","title":"Impossible Method Resolution"},{"location":"Languages/python/python_oops/#abstract-base-class-abstract-class","text":"Classes that contains one or more abstract methods as well as concrete methods. A normal class cannot have abstract methods. Cannot instantiate an abstract class that has abstract methods Subclass which implements all the abstract method can be instantiated Python Implementation: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 # Python 3.4+ from abc import ABC , abstractmethod class Abstract ( ABC ): @abstractmethod def foo ( self ): pass # Python 3.0+ from abc import ABCMeta , abstractmethod class Abstract ( metaclass = ABCMeta ): @abstractmethod def foo ( self ): pass # Python 2 from abc import ABCMeta , abstractmethod class Abstract : __metaclass__ = ABCMeta @abstractmethod def foo ( self ): pass","title":"Abstract Base Class (Abstract Class)"},{"location":"Languages/python/python_oops/#bottom-line","text":"in python3: base class of classes are: object metaclass of classes are: type type of classes are: type classes are instance of class: type","title":"Bottom Line"},{"location":"Languages/python/python_oops/#class-decorators","text":"e.g. https://github.com/toransahu/py-misc/blob/master/class_based_class_decorator_with_pre_post.py TODO:","title":"Class Decorators"},{"location":"Languages/python/python_oops/#class-decorators-versus-metaclass","text":"source: https://jfine-python-classes.readthedocs.io/en/latest/decorators-versus-metaclass.html nothing different other than implementation style","title":"Class Decorators versus metaclass"},{"location":"Languages/python/python_oops/#interface","text":"Doesn't need in Python (bcoz, it was need in other lang. to full-fill the Multiple inheritance) Concept: * A class with all the methods abstract * Contains Methods signature * Do not contain definition/method body * Constant variables (which must be Static + Final in other langs.) - in python there is no final or const keyword * Cannot instantiated * Behaviour/methods which must be implemented by classes * Class which implement interface should implement all the methods OR be an abstract class Python Implementation: 1 2 3 4 5 6 7 class Engine (): \"\"\" Engine Interface\"\"\" def ignition ( self ): raise NotImplementedError ( \"Ingnition should have implemented.\" ) def fuel ( self ): raise NotImplementedError ( \"Fuel should have implemented.\" )","title":"Interface"},{"location":"Languages/python/python_oops/#variables","text":"","title":"Variables"},{"location":"Languages/python/python_oops/#instance-variable","text":"variable defined inside class methods different for different objects every object have its own copy","title":"Instance Variable"},{"location":"Languages/python/python_oops/#static-or-class-variable","text":"defined in class level shared by all the objects can be accessed by Class name as well as objects with the same name, there can be one class level variable and one instance level variable, see in e.g. in this case, Class.variable will definitely access the static var but instance.variable will access the instance variable accessing inside methods it can be accessed using Class.var & self.var if there is instance variable with the same name then self.var will access the instance variable 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 class A : variable = 'Static/class Variable' # static/class variable var = 'Static/class Var' # static/class variable def __init__ ( self ): self . variable = 'Instance Variable' def foo ( self ): print ( A . variable , self . variable , self . var ) print ( A . variable ) a = A () a . foo () Out: 1 2 3 4 Static/class Variable Instance Variable Static/class Var Static/class Variable Instance Variable","title":"Static or Class Variable"},{"location":"Languages/python/python_oops/#instance-vs-classstatic-attribute-lookup-order","text":"instance/object > static/class > base class which is: a. dict ['x'] > type(a). dict ['x'] > type(a)","title":"Instance vs Class/Static attribute lookup order"},{"location":"Languages/python/python_oops/#methods","text":"","title":"Methods"},{"location":"Languages/python/python_oops/#abstract-method","text":"a method without definition (only declared) only signature In python: a method decorated with @abstractmethod","title":"Abstract Method"},{"location":"Languages/python/python_oops/#method-vs-function","text":"Method works exactly same as a simple function. But a method's first argument always receives the instance object: Code: 1 2 3 4 5 def outside_foo (): pass def outside_foo ( self ,): pass","title":"Method vs Function"},{"location":"Languages/python/python_oops/#static-method","text":"","title":"Static Method"},{"location":"Languages/python/python_oops/#what","text":"an organization/stylistic feature functionality-wise same as normal module level functions except, module level func can access an instance and hence instance variables, but static method cannot designed to work on class attributes can be called using class as well as instance A static method has no self argument Never receive an automatic self argument, whether called through a class or an instance. can access/modify class or static variables using class_name.var","title":"What"},{"location":"Languages/python/python_oops/#how","text":"two way of declaration using decorator @staticmethod [fn_name] = staticmethod([fn_name])","title":"How"},{"location":"Languages/python/python_oops/#why","text":"to restrict access of instance attributes (unlike a normal method have access) to fix the access of static variable of a that particular class only because it is hardcoded like A.static_variable src: http://radek.io/2011/07/21/static-variables-and-methods-in-python/","title":"Why"},{"location":"Languages/python/python_oops/#code","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 class A : static_variable = 'Static/class Variable of class A' # static/class variable def __init__ ( self ): self . instance_variable = 'Instance Variable' @staticmethod def static_method (): print ( 'Inside static_method()' , A . static_variable ) # works # print('Inside static_method()', A.instance_variable) # error, static method can't access instance attributes # static method def way2 (): pass # making way2() a static method way2 = staticmethod ( way2 ) class B ( A ): static_variable = 'Static/class Variable of class B' # static/class variable # but a module level func can access an instance attribute def module_level_func (): a1 = A () print ( a1 . instance_variable ) a2 = A () a2 . static_method () A . static_method () B . static_method () # still accessing static_variable from class A Out: 1 2 3 Inside static_method() Static/class Variable of class A Inside static_method() Static/class Variable of class A Inside static_method() Static/class Variable of class A","title":"Code"},{"location":"Languages/python/python_oops/#class-method","text":"","title":"Class Method"},{"location":"Languages/python/python_oops/#what_1","text":"an organization/stylistic feature not different from static method, only signature diff it receives one mandatory arguement: a class name it was called from apart from this first parameter, there is no any functionality diff between class & static method designed to work on class attributes can be called using class as well as instance can access/modify class or static variables using cls.var where cls is first parameter provided to a classmethod","title":"What"},{"location":"Languages/python/python_oops/#how_1","text":"implemented using decorator @classmethod assigning to function i.e., method_name = classmethod(method_name)","title":"How"},{"location":"Languages/python/python_oops/#why_1","text":"to restrict access of instance attributes (unlike a normal method have access) to make code more maintanable than static methods","title":"Why"},{"location":"Languages/python/python_oops/#code_1","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 class A : static_variable = 'Static/class Variable of class A' # static/class variable def __init__ ( self ): self . instance_variable = 'Instance Variable' @classmethod def class_method ( cls ): print ( 'Inside class_method()' , cls . static_variable ) # works # print('Inside class_method()', cls.instance_variable) # error, class method can't access instance attributes # static method def way2 ( cls ): pass # making way2() a static method way2 = classmethod ( way2 ) class B ( A ): static_variable = 'Static/class Variable of class B' # static/class variable # but a module level func can access an instance attribute def module_level_func (): a1 = A () print ( a1 . instance_variable ) a2 = A () a2 . class_method () A . class_method () B . class_method () # now it will access static variable of class B Out: 1 2 3 Inside class_method() Static/class Variable of class A Inside class_method() Static/class Variable of class A Inside class_method() Static/class Variable of class B","title":"Code"},{"location":"Languages/python/python_oops/#magic-method","text":"TODO: Source: https://www.python-course.eu/python3_magic_methods.php https://rszalski.github.io/magicmethods/ Methods with dunders (+) : __add()__ (-) : __sub()__ Can be used for operator overloading, as 1 2 3 4 5 6 7 8 9 class Calc ( int ): def __init__ ( self , x ): self . x = x def __add__ ( self , other ): return self . x + other . x + 1 a = Calc ( 1 ) b = Calc ( 1 ) a + b Out: 1 3","title":"Magic Method"},{"location":"Languages/python/python_oops/#class-internal-methods","text":"TODO: Source: http://www.diveintopython3.net/special-method-names.html https://rszalski.github.io/magicmethods/ Some famous magic methods / internal methods of a class","title":"Class Internal Methods"},{"location":"Languages/python/python_oops/#__new___1","text":"allocates memory to class instance","title":"__new__"},{"location":"Languages/python/python_oops/#__init___1","text":"initializes instance with some values","title":"__init__"},{"location":"Languages/python/python_oops/#__call___1","text":"called upon calling an instance (e.g. a = A(); a())","title":"__call__"},{"location":"Languages/python/python_oops/#__get__","text":"get descriptor method","title":"__get__"},{"location":"Languages/python/python_oops/#__set__","text":"set descriptor method","title":"__set__"},{"location":"Languages/python/python_oops/#__del__","text":"del descriptor method","title":"__del__"},{"location":"Languages/python/python_oops/#__slots__","text":"allows us to explicitly declare data members (like properties) and deny the creation of __dict__ & __weakref__ (unless explicitly declared in __slots__ )","title":"__slots__"},{"location":"Languages/python/python_oops/#__getattribute__","text":"called when accessing any attribute via class instance 1 2 c = C () c . name","title":"__getattribute__"},{"location":"Languages/python/python_oops/#__getattr__","text":"same as __getattribute__ but gets called only when attribiute is not found via __getattribute__","title":"__getattr__"},{"location":"Languages/python/python_oops/#__getattr__-vs-__getattribute__","text":"Source: http://www.diveintopython3.net/special-method-names.html","title":"__getattr__ vs  __getattribute__"},{"location":"Languages/python/python_oops/#__enter__","text":"","title":"__enter__"},{"location":"Languages/python/python_oops/#__exit__","text":"","title":"__exit__"},{"location":"Languages/python/python_oops/#__repr__","text":"","title":"__repr__"},{"location":"Languages/python/python_oops/#__str__","text":"","title":"__str__"},{"location":"Languages/python/python_oops/#__format__","text":"","title":"__format__"},{"location":"Languages/python/python_oops/#__iter__","text":"","title":"__iter__"},{"location":"Languages/python/python_oops/#__next__","text":"","title":"__next__"},{"location":"Languages/python/python_oops/#__reversed__","text":"","title":"__reversed__"},{"location":"Languages/python/python_oops/#__dir__","text":"","title":"__dir__"},{"location":"Languages/python/python_oops/#__setattr__","text":"","title":"__setattr__"},{"location":"Languages/python/python_oops/#__delattr__","text":"","title":"__delattr__"},{"location":"Languages/python/python_oops/#__len__","text":"","title":"__len__"},{"location":"Languages/python/python_oops/#__contains__","text":"","title":"__contains__"},{"location":"Languages/python/python_oops/#__getitem__","text":"","title":"__getitem__"},{"location":"Languages/python/python_oops/#properties","text":"Source: https://www.journaldev.com/14893/python-property-decorator https://docs.python.org/3/howto/descriptor.html#properties https://medium.com/shecodeafrica/managing-class-attributes-in-python-c42d501c5ee0 Why to manage access to an attribute to outer world can only allow get can only allow set can only allow del can perform something more while doing above operations to solve problem like this (dependent attribute value issue) e.g. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 class Example : def __init__ ( self , ap , bp ): self . _a = ap self . _b = bp self . c = self . _a + self . _b def get_c ( self ): return self . c e = Example ( 1 , 2 ) print ( e . _a ) print ( e . _b ) print ( e . c ) print ( e . get_c ()) e . _a = 5 print ( e . _a ) print ( e . c ) print ( e . get_c ()) solution e.g. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 class Example : def __init__ ( self , ap , bp ): self . _a = ap self . _b = bp @property def c ( self ): return self . _a + self . _b e = Example ( 1 , 2 ) print ( e . _a ) print ( e . _b ) print ( e . c ) e . _a = 5 print ( e . _a ) print ( e . c ) What property are built -in python decorators provides 3 methods get set del How property in python are implemented using Descriptors uses : pattern 1 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 class Example : def __init__ ( self , ap ): self . _a = ap @property def a ( self ): return self . _a @a . setter def a ( self , value ): self . _a = value @a . deleter def a ( self ): del self . _a e = Example ( 1 ) print ( e . a ) e . a = 5 print ( e . a ) del ( e . a ) print ( e . a ) #AttributeError: 'Example' object has no attribute '_a' uses : pattern 2 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 class Example : def __init__ ( self , ap ): self . _a = ap def get_a ( self ): return self . _a def set_a ( self , value ): self . _a = value def del_a ( self ): del self . _a # will give only get access a = property ( fget = get_a ) # will give only set access a = property ( fset = set_a ) # will give only del access a = property ( fdel = del_a ) #will give all access of var `a` a = property ( get_a , set_a , del_a ) #property(fget=None, fset=None, fdel=None, doc=None) e = Example ( 1 ) print ( e . a ) e . a = 5 print ( e . a ) del ( e . a ) print ( e . a ) # AttributeError: 'Example' object has no attribute '_a'","title":"Properties"},{"location":"Languages/python/python_oops/#descriptors","text":"TODO: Source: https://docs.python.org/3/howto/descriptor.html descriptor is an object attribute with \u201cbinding behavior\u201d, one whose attribute access has been overridden by methods in the descriptor protocol Those methods are get (), set (), and delete () Basic default behavior for attribute access is to get, set, or delete For instance, a.x has a lookup chain starting with a. dict ['x'], then type(a). dict ['x'], and continuing through the base classes of type(a) excluding metaclasses If there is descriptor x defined, then descriptor will override the lookup order and will become number one Use Cases They are the mechanism behind properties, methods, static methods, class methods, and super() They are used throughout Python itself to implement the new style classes introduced in version 2.2","title":"Descriptors"},{"location":"Languages/python/python_oops/#protocol","text":"1 2 3 4 5 descr . __get__ ( self , obj , type = None ) --> value descr . __set__ ( self , obj , value ) --> None descr . __delete__ ( self , obj ) --> None define any of the above in any class, and object will considered as descriptor and descriptor will override the default attribute lookup behavior (in class dictionary lookup) if object defines both __get__ and __set__ ; then its data descriptor if instance have attribute (means entry in instance dict) with same name as descriptor, then here lookup order will become data descriptor > instance dict if object defines only __get__ ; then its non-data descriptor in this case if instance have attribute with same name as descriptor, then here lookup order will become instance dict > data descriptor","title":"Protocol"},{"location":"Languages/python/python_oops/#simple-implemetation","text":"TODO - Issue with __del__ ; not explained in python doc 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 class ExampleDescriptor : def __init__ ( self , val ): self . val = val def __get__ ( self , obj , objtype = None ): print ( \"getting\" ) return self . val def __set__ ( self , obj , val ): print ( \"setting\" ) self . val = val class Example : name = ExampleDescriptor ( \"toran\" ) if __name__ == \"__main__\" : e = Example () print ( e . name ) e . name = \"sahu\" print ( e . name )","title":"Simple Implemetation"},{"location":"Languages/python/python_oops/#property-implementation","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 class Property ( object ): \"Emulate PyProperty_Type() in Objects/descrobject.c\" def __init__ ( self , fget = None , fset = None , fdel = None , doc = None ): self . fget = fget self . fset = fset self . fdel = fdel if doc is None and fget is not None : doc = fget . __doc__ self . __doc__ = doc def __get__ ( self , obj , objtype = None ): if obj is None : return self if self . fget is None : raise AttributeError ( \"unreadable attribute\" ) return self . fget ( obj ) def __set__ ( self , obj , value ): if self . fset is None : raise AttributeError ( \"can't set attribute\" ) self . fset ( obj , value ) def __delete__ ( self , obj ): if self . fdel is None : raise AttributeError ( \"can't delete attribute\" ) self . fdel ( obj ) def getter ( self , fget ): return type ( self )( fget , self . fset , self . fdel , self . __doc__ ) def setter ( self , fset ): return type ( self )( self . fget , fset , self . fdel , self . __doc__ ) def deleter ( self , fdel ): return type ( self )( self . fget , self . fset , fdel , self . __doc__ )","title":"Property implementation"},{"location":"Languages/python/python_oops/#misc","text":"","title":"Misc"},{"location":"Languages/python/python_oops/#ways-to-call-a-class-member-function","text":"1 2 a = A () a . foo ( args ) 1 2 a = A () A . foo ( a , args )","title":"Ways to call a class member function"},{"location":"Languages/python/python_oops/#__slots__-reduce-ram-usage","text":"source http://book.pythontips.com/en/latest/__slots__magic.html","title":"__slots__ : reduce RAM usage"},{"location":"Languages/python/python_oops/#unbound-vs-bound-method","text":"","title":"Unbound vs Bound Method"},{"location":"Languages/python/python_oops/#unbound-method-simple-function-in-python-3x","text":"Unbound means not bound to any instance i.e. callable using class name","title":"Unbound Method (Simple Function in Python 3.x)"},{"location":"Languages/python/python_oops/#bound-method","text":"Bound methods are bound to some instance Need an instance to call it. Note: In Python 3.x, the notion of Unbound Method has been dropped and we treat it as Simple Function.","title":"Bound Method"},{"location":"Languages/python/python_oops/#closure","text":"A combination of code and scope. It's about nested function and the scope of the function Code: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 def startAt ( start ): def incrementBy ( inc ): return start + inc return incrementBy f = startAt ( 10 ) g = startAt ( 100 ) print ( f ( 1 ), g ( 2 )) # Using lamba def startAt ( start ): return lambda inc : start + inc f = startAt ( 10 ) g = startAt ( 100 ) print ( f ( 1 ), g ( 2 )) Out: 1 2 11 102 11 102","title":"Closure"},{"location":"Languages/python/python_production_servers/","text":"Python Production Servers # Python Production Servers Basics Web Server ngnix Application Server django developement server gunicorn Database MySQL PostgreSQL Github Pages Hosting PythonAnywhere Settings WSGI Static Files: Heroku Pre-requisites Install heroku CLI Create app Add Procfile Dependencies Push code Add .env file Add app.json file Troubleshoot AWS Create AMI (Amazon Machine Instance) SSH to your instance Update Set Password Install Prerequisites Python Stuff Recommanded Way Original Way Test your project by starting up the Django development server with this command: Server Stuff Gunicorn celery supervisor nginx Setup Database AWS RDS MySQL (Free Tier) RDS Config Installation Django Configuration Test Django with RDS MySQL Run Confirm before run Enable HTTPS in aws ec2 with nginx Make sure HTTP & HTTPS are opened in Security Groups Setup domain's CNAME record to the public DNS of ec2 Install Certbot in aws instance Stop any existing servers running on the port 80 and 443 Generate certificates Change nginx configurations in /etc/nginx/nginx.conf to enable SSL Reload nginx config Finish Check Certificate Expiry Date Renew Certificate delete the certificate Troubleshoots AWS S3 Storage Cloudflare Manage DNS Manage Cache Purge Cache TODO: Basics # Web Server # ngnix # Application Server # django developement server # gunicorn # Database # MySQL # PostgreSQL # Github Pages Hosting # Source1: https://medium.freecodecamp.org/hosting-custom-domain-on-github-pages-8c598248d2bc Source2: https://medium.com/employbl/launch-a-website-with-a-custom-url-using-github-pages-and-google-domains-3dd8d90cc33b PythonAnywhere # Settings # WSGI # Static Files: # make a folder sitewide say 'allstatic' by default in settings.py STATIC_URL = '/static/' (don't change it) in pythonanywhere web, set URL = /static/ same as above & Directory = path to the dir 'allstatic' Heroku # Pre-requisites # a working django app app in a git repo Install heroku CLI # Create app # create heroku app from existing code (it will only create empty git repo in heroku's private git) 1 heroku create appname add heroku remote to your local git repo 1 git remote add heroku https://git.heroku.<app_name>.git Add Procfile # its starter script which heroku follows add web: gunicorn --pythonpath toransahu-site toransahu.wsgi (here --pythonpath is used if toransahu is not root dir, or if wsgi.py is not directly under toransahu-site) Dependencies # add all requirements inside requirements.txt including gunicorn Push code # push code to heroku's git repo named 1 git push heroku dev:master here heroku is remote name (same as origin by deafult) dev is local repo's brach name from which we want to push code master is heroku's master branch (in master branch only build happens) Add .env file # Unknown Add app.json file # Unknown Troubleshoot # using heroku local heroku local web heroku logs disable static files 1 heroku config:set DISABLE_COLLECTSTATIC=1 AWS # https://pipelines.puppet.com/docs/tutorials/how-to-set-up-aws-ec2/ https://www.agiliq.com/blog/2014/08/deploying-a-django-app-on-amazon-ec2-instance/ https://medium.com/@bsadkhin/deploying-a-django-app-to-amazon-ec2-3f17a735a561 Create AMI (Amazon Machine Instance) # choose os, server edit configs (optional) launch create/choose private key name it download it put it in your ~/.ssh/ dir change permission to read only chmod 400 notedown DNS of your instance ec2-18-136-106-117.ap-southeast-1.compute.amazonaws.com if os was ubuntu, default user will be ubuntu create security group & make changes in inbound tab like: |Type|Port|Source| |----------------| |HTTP|80|Anywhere| |HTTPS|443|Anywhere| |SSH|22|Anywhere| |Custom TCP|8000|Anywhere| SSH to your instance # using instance dns ssh -i ~/.ssh/ethereal-site-backend.pem ubuntu@ec2-18-136-106-117.ap-southeast-1.compute.amazonaws.com or use public ip ssh -i ~/.ssh/ethereal-site-backend.pem ubuntu@18.136.106.117 if .pem file is downloaded from some storage, then give proper permission 1 chmod 600 ~/.ssh/ethereal-site-backend.pem Update # 1 2 sudo apt-get update sudo apt-get upgrade Set Password # Source: https://comtechies.com/password-authentication-aws-ec2.html 1 2 3 4 5 6 7 8 9 10 11 12 13 #1.open sudo vi /etc/ssh/sshd_config #2.edit no to yes PasswordAuthentication yes #3.save and close #4.change/set password sudo passwd ubuntu #5.restart service sudo service sshd restart Install Prerequisites # Python Stuff # Recommanded Way # Using miniconda 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 #download miniconda - for deployment, testing, build wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh #install sudo sh Miniconda3-latest-Linux-x86_64.sh #alias & activate miniconda echo \"alias anaconda='. ~/miniconda3/bin/activate'\" >> ~/.bashrc anaconda ( base ) sudo /home/ubuntu/miniconda3/bin/python3.6 -m pip install -U pipenv #clone repo ( base ) git clone https://toransahu@bitbucket.org/toransahu/ethereal-machines-backend.git #cd to repo & create venv using pipenv & install dependencies ( base ) pipenv --python 3 install Original Way # 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 sudo wget https://www.python.org/ftp/python/3.6.4/Python-3.6.4.tar.xz sudo tar xJf Python-3.6.4.tar.xz cd Python-3.6.4/ ./configure sudo make sudo make install cd .. sudo rm Python-3.6.4.tar.xz sudo rm -rf Python-3.6.4 sudo apt install python3-pip #link pip, python cd /usr/bin unlink python unlink pip unlink virtualenv ln -s python35 python ln -s pip-3.5 pip ln -s virtualenv-3.5 virtualenv #clone repo git clone https://toransahu@bitbucket.org/toransahu/ethereal-machines-backend.git if sudo ./configure gives error Error: no acceptable C compiler found in PATH when installing python install build-essential sudo apt install build-essential Test your project by starting up the Django development server with this command: # before this, disable other https/secure settings in prod_settings.py or test with dev_settings.py allow server to listen on port 8000 sudo ufw allow 8000 python manage.py runserver 0.0.0.0:8000 if you get error, add host to allowed host. ec2-18-136-106-117.ap-southeast-1.compute.amazonaws.com hit the website ec2-18-136-106-117.ap-southeast-1.compute.amazonaws.com:8000 (without http://) Server Stuff # Gunicorn # will server dynamic contents of the site (works as app server) install in venv using pipenv --python 3 (app-venv) pipenv --python 3 install gunicorn test django app with gunicorn allow server to listen on port 8000 sudo ufw allow 8000 cd ethereal-machines-backend/src (where manage.py is) disable other https/secure settings in prod_settings.py to degug, enable DEBUG gunicorn --bind 0.0.0.0:8000 backend.wsgi (try website with http://), create script to start gunicorn cd ethereal-machines-backend/src (where manage.py is) vim start_gunicorn.sh make sure to write #! /bin/bash in first line chmod a+x start_gunicorn.sh start_gunicorn.sh should look like this: https://gist.github.com/toransahu/31874def1bd661db676d0dfe02e1c651 celery # start rabbitmq starts automatically on boot 1 2 3 4 5 sudo systemctl enable rabbitmq-server sudo systemctl start rabbitmq-server #- in WSL do this sudo service rabbitmq-server start start celery worker need to run inside src folder need to be in virtual env 1 celery -A project_name worker -l info or create script to start celery cd ethereal-machines-backend/src (where manage.py is) vim start_celery.sh make sure to write #! /bin/bash in first line chmod a+x start_celery.sh start_celery.sh should look like this: https://gist.github.com/toransahu/31874def1bd661db676d0dfe02e1c651 supervisor # chmod a+x all the start.sh scripts copy gunicorn.conf & celery.conf to /etc/supervisor/conf.d/ create logs folder inside repo dir sudo supervisorctl reread sudo supervisorctl update sudo supervisorctl reload sudo supervisorctl status sudo supervisorctl restart gunicorn sudo supervisorctl restart celery src: https://gist.github.com/toransahu/31874def1bd661db676d0dfe02e1c651 nginx # will create proxy and redirect to gunicorn for dynamic contents will server static & media contents src: https://www.digitalocean.com/community/tutorials/how-to-set-up-django-with-postgres-nginx-and-gunicorn-on-ubuntu-16-04 install sudo apt install nginx start nginx sudo service nginx start hit the site ec2-18-136-106-117.ap-southeast-1.compute.amazonaws.com check whether you are getting a msg Welcome to nginx! or not stop nginx sudo service nginx stop Configure nginx to proxy pass to gunicorn configuration file : https://gist.github.com/toransahu/f5a38976967715e31543dbcf9b01c7d9 sudo vim /etc/nginx/sites-available/ethereal_backend open up new server block should listen on normal port 80 should respond to our server domain name or ip i.e. 18.136.106.117 (try both) for longer name you may have to increase buffer by editing sudo vim /etc/nginx/nginx.conf change count at server_names_hash_bucket_size 512; didn't find it usefull without custom domain create a location / {} block to tell Nginx to ignore any problems with finding a favicon tell Nginx to find static files here prefix will be same as STATIC_URL and alias will be same as STATIC_ROOT tell Nginx to find media files here prefix will be same as MEDIA_URL and alias will be same as MEDIA_ROOT didn't find it usefull without custom domain create a location / {} block to match all other requests include the standard proxy_params file included with the Nginx installation then pass the traffic to the socket that our Gunicorn process created i.e. http://0.0.0.0:8000 or may be now you need to provide link with server_name and different port on which application is running e.g http://server_name:8000 enable the file by linking it to the sites-enabled directory sudo ln -s /etc/nginx/sites-available/ethereal_backend /etc/nginx/sites-enabled test nginx config for syntax errors sudo nginx -t if no errors reported then go ahead & restart nginx sudo systemctl restart nginx Finally, we need to open up our firewall to normal traffic on port 80. Since we no longer need access to the development server, we can remove the rule to open port 8000 as well sudo ufw delete allow 8000 sudo ufw allow 'Nginx Full' set host (for custom root/domain url for hyperlinks inside django app) e.g. https://api.etherealmachines.com/file/media/1/djklfjkdflkfsdaf.png need to set in /etc/nginx/sites-available/ethereal_backend proxy_set_header Host \"api.etherealmachines.com\" refer: https://gist.github.com/toransahu/f5a38976967715e31543dbcf9b01c7d9 make sure \"api.etherealmachines.com\" is included in ALLOWED_HOSTS in prod_settings.py for size limit issues if total cient body exceeds defined limit in nginx.conf file, it (web server) throws HTTP error 413, request before reaching to the application server to handle the size limit, mention client_max_body_size 10M; in nginx.conf inside http {} block Note: comment out SSL on and redirect HTTPS line in ethereal_backend file if don't using ssl & https Setup Database # AWS RDS MySQL (Free Tier) # RDS Config # source: https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_CreateInstance.html in selected security group add Type:MYSQL/Aurora;Protocol:TCP;Range:3306;Source:Anywhere(0.0.0.0/0) enable public use/access to access db over internet http://ec2-18-136-106-117.ap-southeast-1.compute.amazonaws.com:8000/api/v1/ (optional) Installation # install mysqlclient in venv pipenv install mysqlclient if it gives error: mysql_config not found install following sudo apt-get install libmysqlclient-dev Django Configuration # instance etherealbackend db ethereal host etherealbackend.cg17v7dkjshh.ap-southeast-1.rds.amazonaws.com port 3306 master user ethereal master pwd machines Test Django with RDS MySQL # python manage.py makemigrations python manage.py migrate python manage.py runserver 0.0.0.0:8000 Run # Confirm before run # 1 2 3 4 5 6 7 python manage.py makemigrations <apps> python manage.py migrate python manage.py createsuperuser python manage.py collectstatic cd /home/ubuntu/ethereal-machines/ethereal-machines-backend/ ./start_gunicorn.sh sudo systemctl restart nginx Enable HTTPS in aws ec2 with nginx # Blog source Official Source- Ubuntu 16.04 + nginx using lets-encrypt CA (certificate authority) provides free SSL certificates for few weeks/ 3 months using Certbot - the official lets-encrypt client automatically fetches & deploys SSL/TLS certificates in the server Make sure HTTP & HTTPS are opened in Security Groups # inside inbound rules note down public IP & DNS of the aws ec2 instance Setup domain's CNAME record to the public DNS of ec2 # here domain is: etherealmachines.com public dns is: ec2-18-136-106-117.ap-southeast-1.compute.amazonaws.com Go to your DNS config in CloudFlare & create an entry in DNS Records type: CNAME Name: api.etherealmachines.com value: ec2-18-136-106-117.ap-southeast-1.compute.amazonaws.com TTL: Automatic Install Certbot in aws instance # cd to your home or project root wget https://dl.eff.org/certbot-auto chmod a+x certbot-auto Stop any existing servers running on the port 80 and 443 # because those will be used by certbot to verify the domain & generate certificates after generating certificates, restart servers Generate certificates # ./certbot-auto certonly --standalone -d api.etherealmachines.com we can generate certificates for multiple domains also, like ./certbot-auto certonly --standalone -d api.etherealmachines.com -d www.etherealmachines.com Change nginx configurations in /etc/nginx/nginx.conf to enable SSL # https://gist.github.com/toransahu/7b546d1d1a6afdf51dfa6602ecda22fc The Strict-Transport-Security (HSTS) header ensures that any internal links that are not HTTPS will automatically be routed to the HTTPS version during a HTTPS session Reload nginx config # test config for syntaxt errors: sudo nginx -t sudo service nginx reload Finish # now https://api.etherealmachines.com will redirect requests to http://ec2-18-136-106-117.ap-southeast-1.compute.amazonaws.com/ Check Certificate Expiry Date # Install ssl-cert-check 1 sudo apt install ssl-cert-check Run 1 sudo ssl-cert-check -c /etc/letsencrypt/live/api.etherealmachines.com/fullchain.pem or awk the final result 1 sudo ssl-cert-check -c /etc/letsencrypt/live/api.etherealmachines.com/fullchain.pem | awk 'END { print <dollar_sign>NF }' Renew Certificate # if cloudflare DNS + HTTP proxy is configured, then it will interfere while renewal and creation of ssl certificate so, disable http proxy (DNS only should be there in status column) from status column source: https://www.tautvidas.com/blog/2018/06/certbot-renewal-of-letsencrypt-certificate-fails-with-failed-authorization-procedure/ Let's encrypt certificates are only valid for 3 months after issue. So renewal is required. before using renew command turn off nginx to open the port 80 to check/dry run renewal do: 1 sudo certbot renew --dry-run automate autonewal using cron job: 1 0 6 * * * /path/to/certbot/certbot-auto renew --text >> /path/to/certbot/certbot-cron.log && sudo service nginx reload delete the certificate # 1 sudo certbot delete --cert-name api.etherealmachines.com Troubleshoots # while certificate renew/new if it fails with TLS handshake/auth issue: remove certificate entry from /etc/nginx/nginx.conf while renew/new if you get too many request / auth failure / blocked then wait for 1 hr and retry there is 5 limit per account AWS S3 Storage # Source: - https://simpleisbetterthancomplex.com/tutorial/2017/08/01/how-to-setup-amazon-s3-in-a-django-project.html - http://django-storages.readthedocs.io/en/latest/backends/amazon-S3.html#model - http://boto3.readthedocs.io/en/latest/guide/quickstart.html#configuration https://ethereal-erp.s3.amazonaws.com Credentials - Users with AWS Management Console access can sign-in at: - https://908866068066.signin.aws.amazon.com/console - username - ethereal - group - erp-programatic - access key id - AKIAIBBFQZVDCT7BR6QA - secret access key - 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 Cloudflare # Manage DNS # Manage Cache # Purge Cache # By API 1 2 3 4 5 curl -X DELETE \"https://api.cloudflare.com/client/v4/zones/<site-id>/purge_cache\" \\ -H \"X-Auth-Email: <email>\" \\ -H \"X-Auth-Key: <apikey>\" \\ -H \"Content-Type: application/json\" \\ --data '{\"purge_everything\":true}' ) \" source: https://www.supertechcrew.com/clear-cloudflare-cache-when-pushing-from-git-github-pages/ By Git while Pushing Sources: https://www.supertechcrew.com/clear-cloudflare-cache-when-pushing-from-git-github-pages/ https://www.binarymoon.co.uk/2017/06/using-git-hooks-clear-cloudflares-cache/ By Website TODO: # before nginx ~~correct start_gunicorn.sh with log & workers~~ ~~if possible see pipenv shell~~ ~~MySQL setup~~ nginx ~~proxy~~ ~~static issue~~ ~~try by removing STATIC_FINDER in PROD_.py~~ ~~media issue~~ ~~bind with https://api.etherealmachines.com~~ in code: utils/ init .py ~~mysql case: db name dynamic~~ ~~dev & prod settings~~ ~~media url append in urls.py ~~ ~~remove DEBUG is True condition~~ implement task queue using Celery to make post_save faster http://www.dougalmatthews.com/2011/Oct/10/making-djangos-signals-async-with-celery/ configure https in django+aws level","title":"Python Production Servers"},{"location":"Languages/python/python_production_servers/#python-production-servers","text":"Python Production Servers Basics Web Server ngnix Application Server django developement server gunicorn Database MySQL PostgreSQL Github Pages Hosting PythonAnywhere Settings WSGI Static Files: Heroku Pre-requisites Install heroku CLI Create app Add Procfile Dependencies Push code Add .env file Add app.json file Troubleshoot AWS Create AMI (Amazon Machine Instance) SSH to your instance Update Set Password Install Prerequisites Python Stuff Recommanded Way Original Way Test your project by starting up the Django development server with this command: Server Stuff Gunicorn celery supervisor nginx Setup Database AWS RDS MySQL (Free Tier) RDS Config Installation Django Configuration Test Django with RDS MySQL Run Confirm before run Enable HTTPS in aws ec2 with nginx Make sure HTTP & HTTPS are opened in Security Groups Setup domain's CNAME record to the public DNS of ec2 Install Certbot in aws instance Stop any existing servers running on the port 80 and 443 Generate certificates Change nginx configurations in /etc/nginx/nginx.conf to enable SSL Reload nginx config Finish Check Certificate Expiry Date Renew Certificate delete the certificate Troubleshoots AWS S3 Storage Cloudflare Manage DNS Manage Cache Purge Cache TODO:","title":"Python Production Servers"},{"location":"Languages/python/python_production_servers/#basics","text":"","title":"Basics"},{"location":"Languages/python/python_production_servers/#web-server","text":"","title":"Web Server"},{"location":"Languages/python/python_production_servers/#ngnix","text":"","title":"ngnix"},{"location":"Languages/python/python_production_servers/#application-server","text":"","title":"Application Server"},{"location":"Languages/python/python_production_servers/#django-developement-server","text":"","title":"django developement server"},{"location":"Languages/python/python_production_servers/#gunicorn","text":"","title":"gunicorn"},{"location":"Languages/python/python_production_servers/#database","text":"","title":"Database"},{"location":"Languages/python/python_production_servers/#mysql","text":"","title":"MySQL"},{"location":"Languages/python/python_production_servers/#postgresql","text":"","title":"PostgreSQL"},{"location":"Languages/python/python_production_servers/#github-pages-hosting","text":"Source1: https://medium.freecodecamp.org/hosting-custom-domain-on-github-pages-8c598248d2bc Source2: https://medium.com/employbl/launch-a-website-with-a-custom-url-using-github-pages-and-google-domains-3dd8d90cc33b","title":"Github Pages Hosting"},{"location":"Languages/python/python_production_servers/#pythonanywhere","text":"","title":"PythonAnywhere"},{"location":"Languages/python/python_production_servers/#settings","text":"","title":"Settings"},{"location":"Languages/python/python_production_servers/#wsgi","text":"","title":"WSGI"},{"location":"Languages/python/python_production_servers/#static-files","text":"make a folder sitewide say 'allstatic' by default in settings.py STATIC_URL = '/static/' (don't change it) in pythonanywhere web, set URL = /static/ same as above & Directory = path to the dir 'allstatic'","title":"Static Files:"},{"location":"Languages/python/python_production_servers/#heroku","text":"","title":"Heroku"},{"location":"Languages/python/python_production_servers/#pre-requisites","text":"a working django app app in a git repo","title":"Pre-requisites"},{"location":"Languages/python/python_production_servers/#install-heroku-cli","text":"","title":"Install heroku CLI"},{"location":"Languages/python/python_production_servers/#create-app","text":"create heroku app from existing code (it will only create empty git repo in heroku's private git) 1 heroku create appname add heroku remote to your local git repo 1 git remote add heroku https://git.heroku.<app_name>.git","title":"Create app"},{"location":"Languages/python/python_production_servers/#add-procfile","text":"its starter script which heroku follows add web: gunicorn --pythonpath toransahu-site toransahu.wsgi (here --pythonpath is used if toransahu is not root dir, or if wsgi.py is not directly under toransahu-site)","title":"Add Procfile"},{"location":"Languages/python/python_production_servers/#dependencies","text":"add all requirements inside requirements.txt including gunicorn","title":"Dependencies"},{"location":"Languages/python/python_production_servers/#push-code","text":"push code to heroku's git repo named 1 git push heroku dev:master here heroku is remote name (same as origin by deafult) dev is local repo's brach name from which we want to push code master is heroku's master branch (in master branch only build happens)","title":"Push code"},{"location":"Languages/python/python_production_servers/#add-env-file","text":"Unknown","title":"Add .env file"},{"location":"Languages/python/python_production_servers/#add-appjson-file","text":"Unknown","title":"Add app.json file"},{"location":"Languages/python/python_production_servers/#troubleshoot","text":"using heroku local heroku local web heroku logs disable static files 1 heroku config:set DISABLE_COLLECTSTATIC=1","title":"Troubleshoot"},{"location":"Languages/python/python_production_servers/#aws","text":"https://pipelines.puppet.com/docs/tutorials/how-to-set-up-aws-ec2/ https://www.agiliq.com/blog/2014/08/deploying-a-django-app-on-amazon-ec2-instance/ https://medium.com/@bsadkhin/deploying-a-django-app-to-amazon-ec2-3f17a735a561","title":"AWS"},{"location":"Languages/python/python_production_servers/#create-ami-amazon-machine-instance","text":"choose os, server edit configs (optional) launch create/choose private key name it download it put it in your ~/.ssh/ dir change permission to read only chmod 400 notedown DNS of your instance ec2-18-136-106-117.ap-southeast-1.compute.amazonaws.com if os was ubuntu, default user will be ubuntu create security group & make changes in inbound tab like: |Type|Port|Source| |----------------| |HTTP|80|Anywhere| |HTTPS|443|Anywhere| |SSH|22|Anywhere| |Custom TCP|8000|Anywhere|","title":"Create AMI (Amazon Machine Instance)"},{"location":"Languages/python/python_production_servers/#ssh-to-your-instance","text":"using instance dns ssh -i ~/.ssh/ethereal-site-backend.pem ubuntu@ec2-18-136-106-117.ap-southeast-1.compute.amazonaws.com or use public ip ssh -i ~/.ssh/ethereal-site-backend.pem ubuntu@18.136.106.117 if .pem file is downloaded from some storage, then give proper permission 1 chmod 600 ~/.ssh/ethereal-site-backend.pem","title":"SSH to your instance"},{"location":"Languages/python/python_production_servers/#update","text":"1 2 sudo apt-get update sudo apt-get upgrade","title":"Update"},{"location":"Languages/python/python_production_servers/#set-password","text":"Source: https://comtechies.com/password-authentication-aws-ec2.html 1 2 3 4 5 6 7 8 9 10 11 12 13 #1.open sudo vi /etc/ssh/sshd_config #2.edit no to yes PasswordAuthentication yes #3.save and close #4.change/set password sudo passwd ubuntu #5.restart service sudo service sshd restart","title":"Set Password"},{"location":"Languages/python/python_production_servers/#install-prerequisites","text":"","title":"Install Prerequisites"},{"location":"Languages/python/python_production_servers/#python-stuff","text":"","title":"Python Stuff"},{"location":"Languages/python/python_production_servers/#recommanded-way","text":"Using miniconda 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 #download miniconda - for deployment, testing, build wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh #install sudo sh Miniconda3-latest-Linux-x86_64.sh #alias & activate miniconda echo \"alias anaconda='. ~/miniconda3/bin/activate'\" >> ~/.bashrc anaconda ( base ) sudo /home/ubuntu/miniconda3/bin/python3.6 -m pip install -U pipenv #clone repo ( base ) git clone https://toransahu@bitbucket.org/toransahu/ethereal-machines-backend.git #cd to repo & create venv using pipenv & install dependencies ( base ) pipenv --python 3 install","title":"Recommanded Way"},{"location":"Languages/python/python_production_servers/#original-way","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 sudo wget https://www.python.org/ftp/python/3.6.4/Python-3.6.4.tar.xz sudo tar xJf Python-3.6.4.tar.xz cd Python-3.6.4/ ./configure sudo make sudo make install cd .. sudo rm Python-3.6.4.tar.xz sudo rm -rf Python-3.6.4 sudo apt install python3-pip #link pip, python cd /usr/bin unlink python unlink pip unlink virtualenv ln -s python35 python ln -s pip-3.5 pip ln -s virtualenv-3.5 virtualenv #clone repo git clone https://toransahu@bitbucket.org/toransahu/ethereal-machines-backend.git if sudo ./configure gives error Error: no acceptable C compiler found in PATH when installing python install build-essential sudo apt install build-essential","title":"Original Way"},{"location":"Languages/python/python_production_servers/#test-your-project-by-starting-up-the-django-development-server-with-this-command","text":"before this, disable other https/secure settings in prod_settings.py or test with dev_settings.py allow server to listen on port 8000 sudo ufw allow 8000 python manage.py runserver 0.0.0.0:8000 if you get error, add host to allowed host. ec2-18-136-106-117.ap-southeast-1.compute.amazonaws.com hit the website ec2-18-136-106-117.ap-southeast-1.compute.amazonaws.com:8000 (without http://)","title":"Test your project by starting up the Django development server with this command:"},{"location":"Languages/python/python_production_servers/#server-stuff","text":"","title":"Server Stuff"},{"location":"Languages/python/python_production_servers/#gunicorn_1","text":"will server dynamic contents of the site (works as app server) install in venv using pipenv --python 3 (app-venv) pipenv --python 3 install gunicorn test django app with gunicorn allow server to listen on port 8000 sudo ufw allow 8000 cd ethereal-machines-backend/src (where manage.py is) disable other https/secure settings in prod_settings.py to degug, enable DEBUG gunicorn --bind 0.0.0.0:8000 backend.wsgi (try website with http://), create script to start gunicorn cd ethereal-machines-backend/src (where manage.py is) vim start_gunicorn.sh make sure to write #! /bin/bash in first line chmod a+x start_gunicorn.sh start_gunicorn.sh should look like this: https://gist.github.com/toransahu/31874def1bd661db676d0dfe02e1c651","title":"Gunicorn"},{"location":"Languages/python/python_production_servers/#celery","text":"start rabbitmq starts automatically on boot 1 2 3 4 5 sudo systemctl enable rabbitmq-server sudo systemctl start rabbitmq-server #- in WSL do this sudo service rabbitmq-server start start celery worker need to run inside src folder need to be in virtual env 1 celery -A project_name worker -l info or create script to start celery cd ethereal-machines-backend/src (where manage.py is) vim start_celery.sh make sure to write #! /bin/bash in first line chmod a+x start_celery.sh start_celery.sh should look like this: https://gist.github.com/toransahu/31874def1bd661db676d0dfe02e1c651","title":"celery"},{"location":"Languages/python/python_production_servers/#supervisor","text":"chmod a+x all the start.sh scripts copy gunicorn.conf & celery.conf to /etc/supervisor/conf.d/ create logs folder inside repo dir sudo supervisorctl reread sudo supervisorctl update sudo supervisorctl reload sudo supervisorctl status sudo supervisorctl restart gunicorn sudo supervisorctl restart celery src: https://gist.github.com/toransahu/31874def1bd661db676d0dfe02e1c651","title":"supervisor"},{"location":"Languages/python/python_production_servers/#nginx","text":"will create proxy and redirect to gunicorn for dynamic contents will server static & media contents src: https://www.digitalocean.com/community/tutorials/how-to-set-up-django-with-postgres-nginx-and-gunicorn-on-ubuntu-16-04 install sudo apt install nginx start nginx sudo service nginx start hit the site ec2-18-136-106-117.ap-southeast-1.compute.amazonaws.com check whether you are getting a msg Welcome to nginx! or not stop nginx sudo service nginx stop Configure nginx to proxy pass to gunicorn configuration file : https://gist.github.com/toransahu/f5a38976967715e31543dbcf9b01c7d9 sudo vim /etc/nginx/sites-available/ethereal_backend open up new server block should listen on normal port 80 should respond to our server domain name or ip i.e. 18.136.106.117 (try both) for longer name you may have to increase buffer by editing sudo vim /etc/nginx/nginx.conf change count at server_names_hash_bucket_size 512; didn't find it usefull without custom domain create a location / {} block to tell Nginx to ignore any problems with finding a favicon tell Nginx to find static files here prefix will be same as STATIC_URL and alias will be same as STATIC_ROOT tell Nginx to find media files here prefix will be same as MEDIA_URL and alias will be same as MEDIA_ROOT didn't find it usefull without custom domain create a location / {} block to match all other requests include the standard proxy_params file included with the Nginx installation then pass the traffic to the socket that our Gunicorn process created i.e. http://0.0.0.0:8000 or may be now you need to provide link with server_name and different port on which application is running e.g http://server_name:8000 enable the file by linking it to the sites-enabled directory sudo ln -s /etc/nginx/sites-available/ethereal_backend /etc/nginx/sites-enabled test nginx config for syntax errors sudo nginx -t if no errors reported then go ahead & restart nginx sudo systemctl restart nginx Finally, we need to open up our firewall to normal traffic on port 80. Since we no longer need access to the development server, we can remove the rule to open port 8000 as well sudo ufw delete allow 8000 sudo ufw allow 'Nginx Full' set host (for custom root/domain url for hyperlinks inside django app) e.g. https://api.etherealmachines.com/file/media/1/djklfjkdflkfsdaf.png need to set in /etc/nginx/sites-available/ethereal_backend proxy_set_header Host \"api.etherealmachines.com\" refer: https://gist.github.com/toransahu/f5a38976967715e31543dbcf9b01c7d9 make sure \"api.etherealmachines.com\" is included in ALLOWED_HOSTS in prod_settings.py for size limit issues if total cient body exceeds defined limit in nginx.conf file, it (web server) throws HTTP error 413, request before reaching to the application server to handle the size limit, mention client_max_body_size 10M; in nginx.conf inside http {} block Note: comment out SSL on and redirect HTTPS line in ethereal_backend file if don't using ssl & https","title":"nginx"},{"location":"Languages/python/python_production_servers/#setup-database","text":"","title":"Setup Database"},{"location":"Languages/python/python_production_servers/#aws-rds-mysql-free-tier","text":"","title":"AWS RDS MySQL (Free Tier)"},{"location":"Languages/python/python_production_servers/#rds-config","text":"source: https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_CreateInstance.html in selected security group add Type:MYSQL/Aurora;Protocol:TCP;Range:3306;Source:Anywhere(0.0.0.0/0) enable public use/access to access db over internet http://ec2-18-136-106-117.ap-southeast-1.compute.amazonaws.com:8000/api/v1/ (optional)","title":"RDS Config"},{"location":"Languages/python/python_production_servers/#installation","text":"install mysqlclient in venv pipenv install mysqlclient if it gives error: mysql_config not found install following sudo apt-get install libmysqlclient-dev","title":"Installation"},{"location":"Languages/python/python_production_servers/#django-configuration","text":"instance etherealbackend db ethereal host etherealbackend.cg17v7dkjshh.ap-southeast-1.rds.amazonaws.com port 3306 master user ethereal master pwd machines","title":"Django Configuration"},{"location":"Languages/python/python_production_servers/#test-django-with-rds-mysql","text":"python manage.py makemigrations python manage.py migrate python manage.py runserver 0.0.0.0:8000","title":"Test Django with RDS MySQL"},{"location":"Languages/python/python_production_servers/#run","text":"","title":"Run"},{"location":"Languages/python/python_production_servers/#confirm-before-run","text":"1 2 3 4 5 6 7 python manage.py makemigrations <apps> python manage.py migrate python manage.py createsuperuser python manage.py collectstatic cd /home/ubuntu/ethereal-machines/ethereal-machines-backend/ ./start_gunicorn.sh sudo systemctl restart nginx","title":"Confirm before run"},{"location":"Languages/python/python_production_servers/#enable-https-in-aws-ec2-with-nginx","text":"Blog source Official Source- Ubuntu 16.04 + nginx using lets-encrypt CA (certificate authority) provides free SSL certificates for few weeks/ 3 months using Certbot - the official lets-encrypt client automatically fetches & deploys SSL/TLS certificates in the server","title":"Enable HTTPS in aws ec2 with nginx"},{"location":"Languages/python/python_production_servers/#make-sure-http-https-are-opened-in-security-groups","text":"inside inbound rules note down public IP & DNS of the aws ec2 instance","title":"Make sure HTTP &amp; HTTPS are opened in Security Groups"},{"location":"Languages/python/python_production_servers/#setup-domains-cname-record-to-the-public-dns-of-ec2","text":"here domain is: etherealmachines.com public dns is: ec2-18-136-106-117.ap-southeast-1.compute.amazonaws.com Go to your DNS config in CloudFlare & create an entry in DNS Records type: CNAME Name: api.etherealmachines.com value: ec2-18-136-106-117.ap-southeast-1.compute.amazonaws.com TTL: Automatic","title":"Setup domain's CNAME record to the public DNS of ec2"},{"location":"Languages/python/python_production_servers/#install-certbot-in-aws-instance","text":"cd to your home or project root wget https://dl.eff.org/certbot-auto chmod a+x certbot-auto","title":"Install Certbot in aws instance"},{"location":"Languages/python/python_production_servers/#stop-any-existing-servers-running-on-the-port-80-and-443","text":"because those will be used by certbot to verify the domain & generate certificates after generating certificates, restart servers","title":"Stop any existing servers running on the port 80 and 443"},{"location":"Languages/python/python_production_servers/#generate-certificates","text":"./certbot-auto certonly --standalone -d api.etherealmachines.com we can generate certificates for multiple domains also, like ./certbot-auto certonly --standalone -d api.etherealmachines.com -d www.etherealmachines.com","title":"Generate certificates"},{"location":"Languages/python/python_production_servers/#change-nginx-configurations-in-etcnginxnginxconf-to-enable-ssl","text":"https://gist.github.com/toransahu/7b546d1d1a6afdf51dfa6602ecda22fc The Strict-Transport-Security (HSTS) header ensures that any internal links that are not HTTPS will automatically be routed to the HTTPS version during a HTTPS session","title":"Change nginx configurations in /etc/nginx/nginx.conf to enable SSL"},{"location":"Languages/python/python_production_servers/#reload-nginx-config","text":"test config for syntaxt errors: sudo nginx -t sudo service nginx reload","title":"Reload nginx config"},{"location":"Languages/python/python_production_servers/#finish","text":"now https://api.etherealmachines.com will redirect requests to http://ec2-18-136-106-117.ap-southeast-1.compute.amazonaws.com/","title":"Finish"},{"location":"Languages/python/python_production_servers/#check-certificate-expiry-date","text":"Install ssl-cert-check 1 sudo apt install ssl-cert-check Run 1 sudo ssl-cert-check -c /etc/letsencrypt/live/api.etherealmachines.com/fullchain.pem or awk the final result 1 sudo ssl-cert-check -c /etc/letsencrypt/live/api.etherealmachines.com/fullchain.pem | awk 'END { print <dollar_sign>NF }'","title":"Check Certificate Expiry Date"},{"location":"Languages/python/python_production_servers/#renew-certificate","text":"if cloudflare DNS + HTTP proxy is configured, then it will interfere while renewal and creation of ssl certificate so, disable http proxy (DNS only should be there in status column) from status column source: https://www.tautvidas.com/blog/2018/06/certbot-renewal-of-letsencrypt-certificate-fails-with-failed-authorization-procedure/ Let's encrypt certificates are only valid for 3 months after issue. So renewal is required. before using renew command turn off nginx to open the port 80 to check/dry run renewal do: 1 sudo certbot renew --dry-run automate autonewal using cron job: 1 0 6 * * * /path/to/certbot/certbot-auto renew --text >> /path/to/certbot/certbot-cron.log && sudo service nginx reload","title":"Renew Certificate"},{"location":"Languages/python/python_production_servers/#delete-the-certificate","text":"1 sudo certbot delete --cert-name api.etherealmachines.com","title":"delete the certificate"},{"location":"Languages/python/python_production_servers/#troubleshoots","text":"while certificate renew/new if it fails with TLS handshake/auth issue: remove certificate entry from /etc/nginx/nginx.conf while renew/new if you get too many request / auth failure / blocked then wait for 1 hr and retry there is 5 limit per account","title":"Troubleshoots"},{"location":"Languages/python/python_production_servers/#aws-s3-storage","text":"Source: - https://simpleisbetterthancomplex.com/tutorial/2017/08/01/how-to-setup-amazon-s3-in-a-django-project.html - http://django-storages.readthedocs.io/en/latest/backends/amazon-S3.html#model - http://boto3.readthedocs.io/en/latest/guide/quickstart.html#configuration https://ethereal-erp.s3.amazonaws.com Credentials - Users with AWS Management Console access can sign-in at: - https://908866068066.signin.aws.amazon.com/console - username - ethereal - group - erp-programatic - access key id - AKIAIBBFQZVDCT7BR6QA - secret access key - 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","title":"AWS S3 Storage"},{"location":"Languages/python/python_production_servers/#cloudflare","text":"","title":"Cloudflare"},{"location":"Languages/python/python_production_servers/#manage-dns","text":"","title":"Manage DNS"},{"location":"Languages/python/python_production_servers/#manage-cache","text":"","title":"Manage Cache"},{"location":"Languages/python/python_production_servers/#purge-cache","text":"By API 1 2 3 4 5 curl -X DELETE \"https://api.cloudflare.com/client/v4/zones/<site-id>/purge_cache\" \\ -H \"X-Auth-Email: <email>\" \\ -H \"X-Auth-Key: <apikey>\" \\ -H \"Content-Type: application/json\" \\ --data '{\"purge_everything\":true}' ) \" source: https://www.supertechcrew.com/clear-cloudflare-cache-when-pushing-from-git-github-pages/ By Git while Pushing Sources: https://www.supertechcrew.com/clear-cloudflare-cache-when-pushing-from-git-github-pages/ https://www.binarymoon.co.uk/2017/06/using-git-hooks-clear-cloudflares-cache/ By Website","title":"Purge Cache"},{"location":"Languages/python/python_production_servers/#todo","text":"before nginx ~~correct start_gunicorn.sh with log & workers~~ ~~if possible see pipenv shell~~ ~~MySQL setup~~ nginx ~~proxy~~ ~~static issue~~ ~~try by removing STATIC_FINDER in PROD_.py~~ ~~media issue~~ ~~bind with https://api.etherealmachines.com~~ in code: utils/ init .py ~~mysql case: db name dynamic~~ ~~dev & prod settings~~ ~~media url append in urls.py ~~ ~~remove DEBUG is True condition~~ implement task queue using Celery to make post_save faster http://www.dougalmatthews.com/2011/Oct/10/making-djangos-signals-async-with-celery/ configure https in django+aws level","title":"TODO:"},{"location":"Languages/python/python_testing/","text":"Python Testing # Python Testing Testing Framework Possible Ways Of Doing 1. unittest Testing Framework Intro mock Library (Python 2.x) & unittest.mock Library (Python 3.x) Intro Features Why 2. pytest Framework Types assert Running multiple tests Asserting that a certain exception is raised Grouping multiple tests in a class Resource Setup Features 3. coverage.py Tool Intro Quick Start Testing Framework # Possible Ways Of Doing # Python Using assert; no module require Using assert with setup_module(module) & teardown_module(module) methods for resource setup (define global variables) unittest framework class level: inherit unittest.TestCase override setUP(self) method for resource setup as class members use self.assertEqual(var1, var2) or self.assertNotEqual(var1, var2) pytest framework by functions: using assert & @pytest.fixture(scope=\"module\") for resource setup by classes: (define all test cases inside) using assert & @pytest.fixture(scope=\"class\") for resource setup Django Using django.test from django.test import TestCase same as unittest define class and inherit TestCase define test_cases(self) method use self.assertIs(var, bool) 1. unittest Testing Framework # Intro # Inbuilt Class based import class unittest.TestCase define setUp method for setup resources define testcase method for assertion/testing execution: run the module or python -m unittest test_module_name.py Note: Module & method should have 'test_' prefix & class have 'Test' prefix e.g. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 import unittest try : from .binary_tree import in_order , Node except : from binary_tree import in_order , Node class TestBinaryTree ( unittest . TestCase ): def setUp ( self ): self . root = Node ( 1 ) self . root . left = Node ( 2 ) self . root . right = Node ( 3 ) self . root . left . left = Node ( 4 ) self . root . left . right = Node ( 5 ) self . result = [] self . func_in_order = in_order ( self . root , self . result ) def test_in_order_positive ( self ): self . assertEqual ( self . func_in_order , [ 4 , 2 , 5 , 1 , 3 ], msg = None ) def test_in_order_negative ( self ): self . assertNotEqual ( self . func_in_order , [], msg = None ) if __name__ == '__main__' : root = None unittest . main () mock Library (Python 2.x) & unittest.mock Library (Python 3.x) # Intro # unittest.mock is a library for testing in Python. It allows us to replace parts of our system under test with mock objects and make assertions about how they have been used. It is based on the 'action --> assertion' pattern instead of 'record --> replay' used by many mocking framework. Features # Mock class patch() decorator MagicMock class Why # Increased speed \u00e2\u0080\u0094 Tests that run quickly are extremely beneficial. E.g. if you have a very resource intensive function, a mock of that function would cut down on unnecessary resource usage during testing, therefore reducing test run time. Avoiding undesired side effects during testing \u00e2\u0080\u0094 If you are testing a function which makes calls to an external API, you may not want to make an actual API call every time you run your tests. You'd have to change your code every time that API changes, or there may be some rate limits, but mocking helps you avoid that. 2. pytest Framework # Introduction: The pytest framework makes it easy to write small tests, yet scales to support complex functional testing for applications and libraries. Source: https://pypi.python.org/pypi/pytest/3.2.3 https://docs.pytest.org/en/latest/contents.html#toc Installation: pip install pytest execution: execute: pytest pytest_ex1.py Types # assert # It's an standard python statement for verifying expectations and values. Input type: logical conditions Output: * Nothing/blank, if true * Raise AssertionError exception, if false e.g. 1 2 3 4 5 6 7 8 9 10 # py-misc/py-testing-examples/pytest_ex1.py def add ( x , y ): return x + y def test_add_positive (): assert add ( 3 , 4 ) == 7 def test_add_negative (): assert add ( 3 , 3 ) == 7 Running multiple tests # pytest will run all files in the current directory and its subdirectories of the form test_ .py or _test.py. More generally, it follows standard test discovery rules. Asserting that a certain exception is raised # If you want to assert that some code raises an exception you can use the raises helper: 1 2 3 4 5 6 7 8 # content of pytest_ex2.py import pytest def f (): raise SystemExit ( 1 ) def test_mytest (): with pytest . raises ( SystemExit ): f () Grouping multiple tests in a class # Once you start to have more than a few tests it often makes sense to group tests logically, in classes and modules. Let\u00e2\u0080\u0099s write a class containing two tests: 1 2 3 4 5 6 7 8 9 # content of test_class.py class TestClass ( object ): def test_one ( self ): x = \"this\" assert 'h' in x def test_two ( self ): x = \"hello\" assert hasattr ( x , 'check' ) The two tests are found because of the standard Conventions for Python test discovery. There is no need to subclass anything. We can simply run the module by passing its filename: 1 pytest -q test_class.py Resource Setup # setup & teardown (classic xunit style) fixture (recommended) complies with dependency injection Features # Detailed informations on assert statements Auto-discovery of test modules and functions to study Modular fixtures for managing small or parameterized long-lived test resources to study 3. coverage.py Tool # Intro # a tool for measuring code coverage of Python programs It monitors your program, noting which parts of the code have been executed, then analyzes the source to identify code that could have been executed but was not Use typically used to gauge the effectiveness of tests. It can show which parts of your code are being exercised by tests, and which are not. Quick Start # Install using 1 pip install coverage 2. run the module 1 coverage run my_program.py arg1 arg2 3. get coverage report 1 coverage report -m get coverage report in html 1 coverage html","title":"Python Testing"},{"location":"Languages/python/python_testing/#python-testing","text":"Python Testing Testing Framework Possible Ways Of Doing 1. unittest Testing Framework Intro mock Library (Python 2.x) & unittest.mock Library (Python 3.x) Intro Features Why 2. pytest Framework Types assert Running multiple tests Asserting that a certain exception is raised Grouping multiple tests in a class Resource Setup Features 3. coverage.py Tool Intro Quick Start","title":"Python Testing"},{"location":"Languages/python/python_testing/#testing-framework","text":"","title":"Testing Framework"},{"location":"Languages/python/python_testing/#possible-ways-of-doing","text":"Python Using assert; no module require Using assert with setup_module(module) & teardown_module(module) methods for resource setup (define global variables) unittest framework class level: inherit unittest.TestCase override setUP(self) method for resource setup as class members use self.assertEqual(var1, var2) or self.assertNotEqual(var1, var2) pytest framework by functions: using assert & @pytest.fixture(scope=\"module\") for resource setup by classes: (define all test cases inside) using assert & @pytest.fixture(scope=\"class\") for resource setup Django Using django.test from django.test import TestCase same as unittest define class and inherit TestCase define test_cases(self) method use self.assertIs(var, bool)","title":"Possible Ways Of Doing"},{"location":"Languages/python/python_testing/#1-unittest-testing-framework","text":"","title":"1. unittest Testing Framework"},{"location":"Languages/python/python_testing/#intro","text":"Inbuilt Class based import class unittest.TestCase define setUp method for setup resources define testcase method for assertion/testing execution: run the module or python -m unittest test_module_name.py Note: Module & method should have 'test_' prefix & class have 'Test' prefix e.g. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 import unittest try : from .binary_tree import in_order , Node except : from binary_tree import in_order , Node class TestBinaryTree ( unittest . TestCase ): def setUp ( self ): self . root = Node ( 1 ) self . root . left = Node ( 2 ) self . root . right = Node ( 3 ) self . root . left . left = Node ( 4 ) self . root . left . right = Node ( 5 ) self . result = [] self . func_in_order = in_order ( self . root , self . result ) def test_in_order_positive ( self ): self . assertEqual ( self . func_in_order , [ 4 , 2 , 5 , 1 , 3 ], msg = None ) def test_in_order_negative ( self ): self . assertNotEqual ( self . func_in_order , [], msg = None ) if __name__ == '__main__' : root = None unittest . main ()","title":"Intro"},{"location":"Languages/python/python_testing/#mock-library-python-2x-unittestmock-library-python-3x","text":"","title":"mock Library (Python 2.x) &amp; unittest.mock Library (Python 3.x)"},{"location":"Languages/python/python_testing/#intro_1","text":"unittest.mock is a library for testing in Python. It allows us to replace parts of our system under test with mock objects and make assertions about how they have been used. It is based on the 'action --> assertion' pattern instead of 'record --> replay' used by many mocking framework.","title":"Intro"},{"location":"Languages/python/python_testing/#features","text":"Mock class patch() decorator MagicMock class","title":"Features"},{"location":"Languages/python/python_testing/#why","text":"Increased speed \u00e2\u0080\u0094 Tests that run quickly are extremely beneficial. E.g. if you have a very resource intensive function, a mock of that function would cut down on unnecessary resource usage during testing, therefore reducing test run time. Avoiding undesired side effects during testing \u00e2\u0080\u0094 If you are testing a function which makes calls to an external API, you may not want to make an actual API call every time you run your tests. You'd have to change your code every time that API changes, or there may be some rate limits, but mocking helps you avoid that.","title":"Why"},{"location":"Languages/python/python_testing/#2-pytest-framework","text":"Introduction: The pytest framework makes it easy to write small tests, yet scales to support complex functional testing for applications and libraries. Source: https://pypi.python.org/pypi/pytest/3.2.3 https://docs.pytest.org/en/latest/contents.html#toc Installation: pip install pytest execution: execute: pytest pytest_ex1.py","title":"2. pytest Framework"},{"location":"Languages/python/python_testing/#types","text":"","title":"Types"},{"location":"Languages/python/python_testing/#assert","text":"It's an standard python statement for verifying expectations and values. Input type: logical conditions Output: * Nothing/blank, if true * Raise AssertionError exception, if false e.g. 1 2 3 4 5 6 7 8 9 10 # py-misc/py-testing-examples/pytest_ex1.py def add ( x , y ): return x + y def test_add_positive (): assert add ( 3 , 4 ) == 7 def test_add_negative (): assert add ( 3 , 3 ) == 7","title":"assert"},{"location":"Languages/python/python_testing/#running-multiple-tests","text":"pytest will run all files in the current directory and its subdirectories of the form test_ .py or _test.py. More generally, it follows standard test discovery rules.","title":"Running multiple tests"},{"location":"Languages/python/python_testing/#asserting-that-a-certain-exception-is-raised","text":"If you want to assert that some code raises an exception you can use the raises helper: 1 2 3 4 5 6 7 8 # content of pytest_ex2.py import pytest def f (): raise SystemExit ( 1 ) def test_mytest (): with pytest . raises ( SystemExit ): f ()","title":"Asserting that a certain exception is raised"},{"location":"Languages/python/python_testing/#grouping-multiple-tests-in-a-class","text":"Once you start to have more than a few tests it often makes sense to group tests logically, in classes and modules. Let\u00e2\u0080\u0099s write a class containing two tests: 1 2 3 4 5 6 7 8 9 # content of test_class.py class TestClass ( object ): def test_one ( self ): x = \"this\" assert 'h' in x def test_two ( self ): x = \"hello\" assert hasattr ( x , 'check' ) The two tests are found because of the standard Conventions for Python test discovery. There is no need to subclass anything. We can simply run the module by passing its filename: 1 pytest -q test_class.py","title":"Grouping multiple tests in a class"},{"location":"Languages/python/python_testing/#resource-setup","text":"setup & teardown (classic xunit style) fixture (recommended) complies with dependency injection","title":"Resource Setup"},{"location":"Languages/python/python_testing/#features_1","text":"Detailed informations on assert statements Auto-discovery of test modules and functions to study Modular fixtures for managing small or parameterized long-lived test resources to study","title":"Features"},{"location":"Languages/python/python_testing/#3-coveragepy-tool","text":"","title":"3. coverage.py Tool"},{"location":"Languages/python/python_testing/#intro_2","text":"a tool for measuring code coverage of Python programs It monitors your program, noting which parts of the code have been executed, then analyzes the source to identify code that could have been executed but was not Use typically used to gauge the effectiveness of tests. It can show which parts of your code are being exercised by tests, and which are not.","title":"Intro"},{"location":"Languages/python/python_testing/#quick-start","text":"Install using 1 pip install coverage 2. run the module 1 coverage run my_program.py arg1 arg2 3. get coverage report 1 coverage report -m get coverage report in html 1 coverage html","title":"Quick Start"},{"location":"Music/piano/","text":"Piano # Piano Piano Basics Keys Black Keys White Keys Notes Type Observations Octave Types Notes Practice Chords Types Observation How to find chords Piano Basics # https://www.key-notes.com/blog/piano-key-chart https://spinditty.com/learning/chord-building-for-musicians Keys # Keys are not notes Sharps and flats are not the black keys. All black keys are either a sharp or flat, but not all sharps and flats are black keys e.g. E# is a white key Black Keys # half notes White Keys # pure notes Notes # each next key is half note higher than the prev one Type # pure/natural \u266e flat \u266d half notes low sharp \u266f half notes up Observations # there is no sharp of B there is no sharp of E Octave # C (white key before 2 black) to next C (8 keys in total) are called octave Types # middle octave (middle one) higher octave (right one) lower octave (left one) Notes Practice # Chords # Types # Major (3rd, 5th) Minor (2nd, 5th) 7th (If 4th finger used) Observation # all 3 fingers on alternate white keys gives: CM,Dm,Em,FM,GM,Am,BM... minor chords gives sadness most of the time major chords gives happiness most of the time How to find chords # (based of notes) - if a song starts from C, then most probably the sca -","title":"Piano"},{"location":"Music/piano/#piano","text":"Piano Piano Basics Keys Black Keys White Keys Notes Type Observations Octave Types Notes Practice Chords Types Observation How to find chords","title":"Piano"},{"location":"Music/piano/#piano-basics","text":"https://www.key-notes.com/blog/piano-key-chart https://spinditty.com/learning/chord-building-for-musicians","title":"Piano Basics"},{"location":"Music/piano/#keys","text":"Keys are not notes Sharps and flats are not the black keys. All black keys are either a sharp or flat, but not all sharps and flats are black keys e.g. E# is a white key","title":"Keys"},{"location":"Music/piano/#black-keys","text":"half notes","title":"Black Keys"},{"location":"Music/piano/#white-keys","text":"pure notes","title":"White Keys"},{"location":"Music/piano/#notes","text":"each next key is half note higher than the prev one","title":"Notes"},{"location":"Music/piano/#type","text":"pure/natural \u266e flat \u266d half notes low sharp \u266f half notes up","title":"Type"},{"location":"Music/piano/#observations","text":"there is no sharp of B there is no sharp of E","title":"Observations"},{"location":"Music/piano/#octave","text":"C (white key before 2 black) to next C (8 keys in total) are called octave","title":"Octave"},{"location":"Music/piano/#types","text":"middle octave (middle one) higher octave (right one) lower octave (left one)","title":"Types"},{"location":"Music/piano/#notes-practice","text":"","title":"Notes Practice"},{"location":"Music/piano/#chords","text":"","title":"Chords"},{"location":"Music/piano/#types_1","text":"Major (3rd, 5th) Minor (2nd, 5th) 7th (If 4th finger used)","title":"Types"},{"location":"Music/piano/#observation","text":"all 3 fingers on alternate white keys gives: CM,Dm,Em,FM,GM,Am,BM... minor chords gives sadness most of the time major chords gives happiness most of the time","title":"Observation"},{"location":"Music/piano/#how-to-find-chords","text":"(based of notes) - if a song starts from C, then most probably the sca -","title":"How to find chords"}]}